{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, RobertaConfig\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/Notebooks/crystal/NLP/transformers/examples'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we're in the transformers directory with fine-tuned model output.\n",
    "os.chdir('/home/jupyter/Notebooks/crystal/NLP/transformers/examples/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# and Transformers documentation: https://huggingface.co/transformers/model_doc/roberta.html#robertaformaskedlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################### BEGIN TESTING #####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-beeef4b101dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvocab_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/data/vocab_files/vocab_checked.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/data/vocab_files/vocab_checked.txt'\n",
    "vocab = make_vocab(vocab_file)\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=1024, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('./output_synonyms_lg2/')\n",
    "config = RobertaConfig.from_pretrained('./output_synonyms_lg2/')\n",
    "model = RobertaForMaskedLM.from_pretrained('./output_synonyms_lg2/', config=config)\n",
    "# Outputting hidden states allows direct access to hidden layers of the model.\n",
    "# Outputting hidden states must be set to \"true\" in the config file during fine-tuning.\n",
    "# config.output_hidden_states = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She             264\n",
      "made            156\n",
      "an               41\n",
      "abhor        35,350\n",
      "red           2,050\n",
      "expression    8,151\n",
      "as               25\n",
      "he               37\n",
      "began           880\n",
      "making          442\n",
      "excuses      19,791\n",
      ".                 4\n",
      "He               91\n",
      "wanted          770\n",
      "to                7\n",
      "keep            489\n",
      "playing         816\n",
      ",                 6\n",
      "but              53\n",
      "abhor        35,350\n",
      "red           2,050\n",
      "the               5\n",
      "poor          2,129\n",
      "sports        1,612\n",
      "manship      17,187\n",
      "of                9\n",
      "his              39\n",
      "fellows      36,304\n",
      ".                 4\n",
      "<s> She made an abhorred expression as he began making excuses. He wanted to keep playing, but abhorred the poor sportsmanship of his fellows.</s>\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple contexts for the word \"abhorred\"\n",
    "test_text = \"She made an abhorred expression as he began making excuses. He wanted to keep playing, but abhorred the poor sportsmanship of his fellows.\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_test_text = tokenizer.encode(test_text, add_special_tokens=True)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for t in tokenized_test_text[1:-1]:\n",
    "        print('{:<12} {:>6,}'.format(tokenizer.decode(t).strip(), t))\n",
    "        \n",
    "print(tokenizer.decode(tokenized_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokens_tensor = torch.tensor([tokenized_test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor(1.7705)\n",
      "tensor([[[ 7.5622, -6.5695,  7.1052,  ..., -0.4609, -0.3673,  2.0783],\n",
      "         [ 0.1524, -6.5327,  0.7621,  ..., -4.3035, -2.9045,  0.0805],\n",
      "         [-3.1635, -4.4289,  5.9348,  ...,  0.1579, -0.5408, -0.8563],\n",
      "         ...,\n",
      "         [ 0.4510, -4.8339,  0.3958,  ..., -0.3920, -3.4539, -2.0414],\n",
      "         [ 3.5524, -6.6096,  7.0987,  ..., -1.0119, -0.9505,  0.7903],\n",
      "         [ 9.0676, -5.9170,  7.8096,  ..., -0.8526, -2.6835,  3.2866]]])\n",
      "(tensor([[[ 1.8019e-01, -1.1417e-04, -6.6993e-03,  ..., -1.6295e-01,\n",
      "           1.0452e-01,  5.8282e-02],\n",
      "         [-1.0831e-01, -6.4354e-02,  1.7192e-01,  ...,  3.5061e-03,\n",
      "           1.8987e-01,  2.1542e-01],\n",
      "         [-1.2822e-01, -2.1817e-01,  6.4316e-01,  ..., -4.8211e-02,\n",
      "          -6.1839e-01,  3.5666e-02],\n",
      "         ...,\n",
      "         [-9.0066e-03, -5.7980e-01,  2.9104e-01,  ..., -2.0536e-01,\n",
      "          -1.5077e-01,  4.3876e-01],\n",
      "         [ 2.1391e-01,  2.3605e-01,  2.8134e-01,  ..., -9.9283e-01,\n",
      "           7.0846e-02, -2.6021e-02],\n",
      "         [-2.8819e-02,  1.3861e-02,  9.9487e-02,  ...,  3.0337e-01,\n",
      "           5.7967e-02, -1.0622e-01]]]), tensor([[[-0.0194,  0.0764,  0.0146,  ..., -0.0348, -0.0062, -0.1448],\n",
      "         [-0.0227,  0.2523,  0.3291,  ...,  0.0078,  0.3105,  0.2376],\n",
      "         [-0.3139, -0.2821,  1.1143,  ..., -0.3124, -0.8467,  0.3096],\n",
      "         ...,\n",
      "         [-0.2822, -0.2980, -0.1088,  ...,  0.1394, -0.1195,  0.2946],\n",
      "         [-0.1342,  0.7053,  0.5908,  ..., -0.9701, -0.0841, -0.3778],\n",
      "         [-0.2697,  0.6944,  0.3517,  ...,  0.4750, -0.0811, -0.6889]]]), tensor([[[ 1.8568e-02,  3.8564e-02, -1.4663e-02,  ...,  2.9165e-02,\n",
      "          -4.5352e-02, -5.1730e-02],\n",
      "         [ 1.9095e-01,  9.9823e-02, -1.7351e-02,  ...,  7.3615e-01,\n",
      "           4.3653e-01,  3.3434e-01],\n",
      "         [-3.3423e-01, -4.8860e-01,  2.9507e-01,  ...,  1.2972e-01,\n",
      "          -6.7957e-01,  2.0166e-01],\n",
      "         ...,\n",
      "         [-1.9469e-01, -2.9252e-01, -2.6590e-01,  ...,  1.8910e-01,\n",
      "          -5.7457e-04,  2.9377e-01],\n",
      "         [-3.3827e-01,  6.1956e-01,  2.6846e-01,  ..., -7.1669e-01,\n",
      "          -2.0058e-01, -4.4525e-01],\n",
      "         [-6.6667e-02,  4.8731e-01, -7.0325e-02,  ...,  6.4226e-01,\n",
      "          -1.2545e-01, -3.9972e-01]]]), tensor([[[ 3.6324e-02, -5.8494e-04, -5.9752e-02,  ...,  5.2585e-02,\n",
      "           1.6391e-02,  2.4749e-02],\n",
      "         [ 4.0926e-01,  4.4425e-01, -4.1642e-01,  ...,  5.2047e-01,\n",
      "           3.5061e-01,  1.1310e-02],\n",
      "         [-3.8806e-01, -9.9768e-02, -3.4308e-02,  ...,  1.5285e-01,\n",
      "          -5.3732e-01,  4.7839e-01],\n",
      "         ...,\n",
      "         [-1.3257e-01, -1.4027e-01, -6.7999e-01,  ...,  2.0128e-01,\n",
      "          -2.5158e-01, -1.3119e-02],\n",
      "         [ 1.4565e-02,  4.6510e-01,  4.7993e-02,  ..., -3.6194e-01,\n",
      "          -5.1847e-02, -1.6189e-01],\n",
      "         [ 1.0532e-01,  4.5982e-01, -3.9443e-01,  ...,  4.7468e-01,\n",
      "          -4.3131e-01, -1.6237e-02]]]), tensor([[[-0.0090,  0.0369, -0.0152,  ...,  0.0675, -0.0460, -0.0403],\n",
      "         [-0.0843,  0.5411,  0.0025,  ...,  0.3486, -0.0702, -0.0493],\n",
      "         [-0.5779, -0.2170, -0.4219,  ...,  0.3470, -0.2153,  0.5121],\n",
      "         ...,\n",
      "         [-0.3107, -0.1969, -0.8142,  ...,  0.3001, -0.2618, -0.3543],\n",
      "         [ 0.0669,  0.1117,  0.0540,  ..., -0.0256,  0.0475, -0.0820],\n",
      "         [-0.3435, -0.2084, -0.3879,  ...,  0.4914, -0.2322, -0.0851]]]), tensor([[[ 2.6087e-02,  5.4714e-02,  9.7303e-02,  ...,  1.7241e-02,\n",
      "           6.3153e-02, -6.6731e-04],\n",
      "         [ 4.5865e-01,  9.6175e-01,  1.4356e-01,  ...,  1.6432e-01,\n",
      "           3.2828e-01,  2.8310e-02],\n",
      "         [-4.1768e-01,  2.7133e-01, -3.4684e-01,  ...,  1.1838e-03,\n",
      "          -7.5984e-02,  4.5574e-01],\n",
      "         ...,\n",
      "         [ 9.8100e-03, -2.0757e-01, -5.2181e-01,  ...,  2.7993e-01,\n",
      "          -2.9966e-01,  1.7465e-01],\n",
      "         [ 1.6045e-02,  2.8483e-02,  5.5184e-02,  ...,  2.5428e-02,\n",
      "          -1.8848e-02,  8.5035e-03],\n",
      "         [-1.4578e-01, -2.2437e-01, -1.3547e-01,  ...,  5.3106e-01,\n",
      "          -1.1917e-01,  2.0143e-01]]]), tensor([[[ 7.2100e-02,  2.7962e-02,  1.6751e-01,  ...,  6.8243e-02,\n",
      "          -2.6371e-02, -2.0811e-02],\n",
      "         [ 5.4534e-01,  8.9012e-01,  1.1363e-01,  ...,  2.6464e-01,\n",
      "           4.8917e-02,  2.5457e-03],\n",
      "         [-1.1798e-01, -2.3232e-01, -9.5495e-01,  ...,  5.3966e-02,\n",
      "          -2.5861e-01, -8.8637e-02],\n",
      "         ...,\n",
      "         [ 2.3772e-01, -4.0016e-01, -3.1013e-01,  ...,  5.5014e-01,\n",
      "          -3.5293e-01,  1.7337e-01],\n",
      "         [ 4.2053e-03,  1.5951e-02,  1.4549e-02,  ..., -6.3136e-03,\n",
      "          -3.9868e-04, -2.4888e-04],\n",
      "         [-1.9541e-01, -3.8660e-01, -2.7640e-02,  ...,  5.1317e-01,\n",
      "          -4.8830e-01,  1.6001e-02]]]), tensor([[[-0.0121,  0.0783,  0.0171,  ...,  0.1197, -0.0457, -0.0181],\n",
      "         [ 0.3533,  0.8103,  0.0108,  ...,  0.3777,  0.2570, -0.1511],\n",
      "         [ 0.2871, -0.2177, -0.3207,  ...,  0.2117,  0.2348,  0.0324],\n",
      "         ...,\n",
      "         [ 0.1632, -0.5620, -0.4064,  ...,  0.8832, -0.3654,  0.4859],\n",
      "         [ 0.0440,  0.0406,  0.0396,  ..., -0.0041,  0.0273,  0.0137],\n",
      "         [ 0.0258, -0.5824, -0.0634,  ...,  0.8975, -0.3263,  0.3931]]]), tensor([[[-0.0623,  0.0494,  0.0047,  ...,  0.0696, -0.0437, -0.0477],\n",
      "         [ 0.2006,  0.4681, -0.0920,  ...,  0.3580, -0.1862, -0.2406],\n",
      "         [ 0.0825, -0.3152, -0.4958,  ...,  0.1175,  0.5171,  0.1164],\n",
      "         ...,\n",
      "         [-0.0315, -0.2858, -0.4449,  ...,  0.8931, -0.0771,  0.2394],\n",
      "         [ 0.0171,  0.0190, -0.0046,  ...,  0.0238, -0.0077,  0.0115],\n",
      "         [-0.0701, -0.8252, -0.0708,  ...,  1.0366, -0.1576,  0.4237]]]), tensor([[[-0.0510,  0.0155, -0.0483,  ...,  0.0729,  0.0109, -0.0380],\n",
      "         [ 0.0018,  0.4823,  0.3796,  ...,  0.4061, -0.1688,  0.1099],\n",
      "         [-0.0959, -0.3420, -0.3084,  ..., -0.2042,  0.1365,  0.1223],\n",
      "         ...,\n",
      "         [ 0.2233, -0.3382, -0.3637,  ...,  0.6812,  0.2028,  0.0103],\n",
      "         [-0.0055,  0.0098,  0.0118,  ...,  0.0085,  0.0354, -0.0234],\n",
      "         [-0.4019, -0.8217, -0.0759,  ...,  1.0582, -0.0419,  0.1801]]]), tensor([[[ 0.0491, -0.0161, -0.1281,  ...,  0.0888,  0.0164,  0.0106],\n",
      "         [ 0.4013,  0.5247,  0.1606,  ...,  0.3726, -0.4969, -0.1478],\n",
      "         [ 0.1644, -0.3630, -0.5572,  ...,  0.1183, -0.1717,  0.0582],\n",
      "         ...,\n",
      "         [ 0.3621, -0.4771, -0.1148,  ...,  0.7687,  0.4067, -0.4836],\n",
      "         [-0.0065,  0.0246, -0.0119,  ...,  0.0154,  0.0341, -0.0104],\n",
      "         [-0.0297, -0.8385,  0.0419,  ...,  0.8240, -0.1669,  0.2809]]]), tensor([[[ 2.1450e-02,  2.1180e-03,  3.3275e-04,  ...,  1.3312e-02,\n",
      "          -1.8146e-02,  1.3896e-02],\n",
      "         [ 6.5798e-01,  2.5997e-01, -1.5571e-01,  ...,  5.3914e-01,\n",
      "          -4.1038e-01, -1.6410e-01],\n",
      "         [ 3.3531e-01, -5.4308e-01, -3.1268e-01,  ...,  1.6133e-01,\n",
      "           3.5004e-02,  6.1205e-02],\n",
      "         ...,\n",
      "         [ 2.0768e-01, -3.3955e-01, -1.0485e-01,  ...,  6.9186e-01,\n",
      "           3.5707e-01, -2.8340e-01],\n",
      "         [ 9.4144e-03,  1.3318e-02,  8.5048e-03,  ..., -2.0511e-03,\n",
      "           2.8379e-04,  8.8129e-03],\n",
      "         [-1.1433e-01, -5.5490e-01, -1.4033e-01,  ...,  5.0772e-01,\n",
      "           4.5314e-01,  1.2575e-01]]]), tensor([[[-0.0856,  0.0437,  0.0218,  ..., -0.1900, -0.0590, -0.0357],\n",
      "         [ 0.1620,  0.0440, -0.0609,  ...,  0.1450, -0.0883, -0.0423],\n",
      "         [ 0.0113, -0.2482, -0.0325,  ..., -0.0160, -0.0606, -0.1395],\n",
      "         ...,\n",
      "         [ 0.0003, -0.0885,  0.0088,  ...,  0.0280,  0.1790,  0.0499],\n",
      "         [-0.0881,  0.0415,  0.0114,  ..., -0.2035, -0.0613, -0.0465],\n",
      "         [-0.0983, -0.0917, -0.0595,  ...,  0.0215,  0.1370,  0.1072]]]))\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(tokenized_test_text).unsqueeze(0)  # Batch size 1\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "    # Documentation for the \"forward\" method of RobertaForMaskedLM\n",
    "    # details what is returned for each index of \"outputs.\"\n",
    "    print(len(outputs))\n",
    "    print(outputs[0])  # masked_lm_loss\n",
    "    print(outputs[1])  # prediction_scores\n",
    "    print(outputs[2])  # hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13\n",
      "Number of batches: 1\n",
      "Number of tokens: 31\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(outputs[2]))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(outputs[2][layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(outputs[2][layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(outputs[2][layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_layers = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAI/CAYAAAC4QOfKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAVY0lEQVR4nO3dYajd933f8c93vjUdWVns+k6IuN4Nq0nJkzjdJWtpKW2cFHcatQfBJIwihof2oBktG2x3e7IN9kAdbFkfjILWZNWDNomX1dhEJa2npYTBSCs3XuvEDXaNTG1kS20T2nWw4u67Bzp2ZSHlnq/uvTpHV68XXM7//z//w/laRzJvfufc/6nuDgAAy/tLqx4AAOBWI6AAAIYEFADAkIACABgSUAAAQwIKAGBo42Y+2T333NNbW1s38ykBAG7IM8888wfdvXmt+25qQG1tbeXcuXM38ykBAG5IVb18vfu8hQcAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQWHzNbOmWztnFn1GACHmoACABgSUAAAQ7sGVFW9p6qeveLnj6vqp6vq7qp6uqpeWNzedTMGBgBYtV0Dqru/3t0PdPcDSf5mkv+T5IkkO0nOdvf9Sc4u9gEADr3pW3gPJvm97n45ycNJTi+On07yyH4OBgCwrqYB9dEkn15sH+nuC4vt15Ic2bepAADW2NIBVVV3JvnxJP/l6vu6u5P0dR53oqrOVdW5S5cu3fCgAADrYrIC9WNJfqu7X1/sv15VR5NkcXvxWg/q7lPdvd3d25ubm3ubFgBgDUwC6mP5i7fvkuSpJMcX28eTPLlfQwEArLOlAqqq3pHkw0l++YrDJ5N8uKpeSPKhxT4AwKG3scxJ3f2nSb7zqmN/mMu/lQcAcFtxJXIAgCEBBQAwJKAAAIaW+gwUcGvb2jnz1vb5k8dWOAnA4WAFCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgILbzNbOmbdd1gCAOQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGNpY9QDAamztnHlr+/zJYyucBODWYwUKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMDQxqoHAA7G1s6ZVY8AcGhZgQIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABhaKqCq6p1V9bmq+t2qer6qvr+q7q6qp6vqhcXtXQc9LADAOlh2Bepnk3yhu78nyfuSPJ9kJ8nZ7r4/ydnFPgDAobdrQFXVX03yQ0k+mSTd/Wfd/c0kDyc5vTjtdJJHDmpIAIB1sswK1LuTXEryn6vqK1X181X1jiRHuvvC4pzXkhw5qCEBANbJMgG1keR7k/xcd78/yZ/mqrfruruT9LUeXFUnqupcVZ27dOnSXucFAFi5ZQLqlSSvdPeXF/ufy+Wger2qjibJ4vbitR7c3ae6e7u7tzc3N/djZgCAldo1oLr7tSS/X1XvWRx6MMnXkjyV5Pji2PEkTx7IhAAAa2ZjyfP+UZJfrKo7k7yU5O/ncnw9XlWPJXk5yaMHMyIAwHpZKqC6+9kk29e468H9HQcAYP25EjkAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADA0MaqBwD2bmvnzKpHALitWIECABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwtLHMSVV1PsmfJPnzJG9093ZV3Z3ks0m2kpxP8mh3f+NgxgQAWB+TFagf6e4Hunt7sb+T5Gx335/k7GIfAODQ28tbeA8nOb3YPp3kkb2PAwCw/pYNqE7ya1X1TFWdWBw70t0XFtuvJTmy79MBAKyhpT4DleQHu/vVqvprSZ6uqt+98s7u7qrqaz1wEVwnkuS+++7b07DAwdjaOZMkOX/y2IonAbg1LLUC1d2vLm4vJnkiyQeSvF5VR5NkcXvxOo891d3b3b29ubm5P1MDAKzQrgFVVe+oqu94czvJjyZ5LslTSY4vTjue5MmDGhIAYJ0s8xbekSRPVNWb5/9Sd3+hqn4zyeNV9ViSl5M8enBjAgCsj10DqrtfSvK+axz/wyQPHsRQAADrzJXIAQCGBBQAwJCAAgAYWvY6UMAaevP6TQDcXFagAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDSwdUVd1RVV+pqs8v9t9dVV+uqher6rNVdefBjQkAsD4mK1A/leT5K/Z/Jsknuvu7k3wjyWP7ORgAwLpaKqCq6t4kx5L8/GK/knwwyecWp5xO8shBDAgAsG6WXYH6D0n+aZL/t9j/ziTf7O43FvuvJHnXPs8GALCWNnY7oar+TpKL3f1MVf3w9Amq6kSSE0ly3333jQcEbp6tnTNvbZ8/eWyFkwCst2VWoH4gyY9X1fkkn8nlt+5+Nsk7q+rNALs3yavXenB3n+ru7e7e3tzc3IeRAQBWa9eA6u5/3t33dvdWko8m+e/d/feSfDHJRxanHU/y5IFNCQCwRvZyHah/luQfV9WLufyZqE/uz0gAAOtt189AXam7fz3Jry+2X0rygf0fCQBgvbkSOQDAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIY2Vj0AMLO1c2bVIwDc9qxAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBRwTVs7Z7K1c2bVYwCsJQEFADAkoAAAhnYNqKr69qr6jar6X1X11ar614vj766qL1fVi1X12aq68+DHBQBYvWVWoP5vkg929/uSPJDkoar6viQ/k+QT3f3dSb6R5LGDGxMAYH3sGlB92f9e7H7b4qeTfDDJ5xbHTyd55EAmBABYM0t9Bqqq7qiqZ5NcTPJ0kt9L8s3ufmNxyitJ3nUwIwIArJelAqq7/7y7H0hyb5IPJPmeZZ+gqk5U1bmqOnfp0qUbHBMAYH2Mfguvu7+Z5ItJvj/JO6tqY3HXvUlevc5jTnX3dndvb25u7mlYAIB1sMxv4W1W1TsX2385yYeTPJ/LIfWRxWnHkzx5UEMCAKyTjd1PydEkp6vqjlwOrse7+/NV9bUkn6mqf5PkK0k+eYBzAgCsjV0Dqrt/O8n7r3H8pVz+PBQAwG3FlcgBAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwtLHqAYD1trVz5q3t8yePrXASgPVhBQoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwtLHqAYDlbO2cWfUIACxYgQIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQ7sGVFV9V1V9saq+VlVfraqfWhy/u6qerqoXFrd3Hfy4AACrt8wK1BtJ/kl3vzfJ9yX5yap6b5KdJGe7+/4kZxf7AACH3q4B1d0Xuvu3Ftt/kuT5JO9K8nCS04vTTid55KCGBABYJ6PPQFXVVpL3J/lykiPdfWFx12tJjuzrZAAAa2rpgKqqv5Lkvyb56e7+4yvv6+5O0td53ImqOldV5y5durSnYQEA1sFSAVVV35bL8fSL3f3Li8OvV9XRxf1Hk1y81mO7+1R3b3f39ubm5n7MDACwUsv8Fl4l+WSS57v7319x11NJji+2jyd5cv/HAwBYPxtLnPMDSX4iye9U1bOLY/8iyckkj1fVY0leTvLowYwIALBedg2o7v4fSeo6dz+4v+MAAKw/VyIHABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGNlY9AHDZ1s6Zt7bPnzy2wkkA2I0VKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYMh1oGCNXXltqHXw5jyuUwXc7qxAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABhyGQNYQ+t2+QIA3s4KFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMDQrgFVVZ+qqotV9dwVx+6uqqer6oXF7V0HOyYAwPpYZgXqF5I8dNWxnSRnu/v+JGcX+wAAt4VdA6q7v5Tkj646/HCS04vt00ke2ee5AADW1o1+BupId19YbL+W5Mg+zQMAsPb2/CHy7u4kfb37q+pEVZ2rqnOXLl3a69MBAKzcjQbU61V1NEkWtxevd2J3n+ru7e7e3tzcvMGnAwBYHzcaUE8lOb7YPp7kyf0ZBwBg/S1zGYNPJ/mfSd5TVa9U1WNJTib5cFW9kORDi30AgNvCxm4ndPfHrnPXg/s8CwDALcGVyAEAhgQUAMDQrm/hAVxta+fMW9vnTx5b4SQAq2EFCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGNlY9ABx2Wztn3to+f/LYCic5WLfLfydAYgUKAGBMQAEADAkoAIAhAQUAMCSgAACGBBQAwJDLGAB7cuXlC64+duXlDFzmADhMrEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDrgMF+2jZax25JhLArc0KFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhlzGAA3LlpQpuV/4MgMPKChQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAoerum/Zk29vbfe7cuZv2fHCz+bX9mfMnj33L+9/889ztPICDUFXPdPf2te6zAgUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAxtrHqA/XbldXhcO4ZlLfv3xt+v/bWX62Yd5GvhdYb1tE7XhrMCBQAwJKAAAIb2FFBV9VBVfb2qXqyqnf0aCgBgnd1wQFXVHUn+Y5IfS/LeJB+rqvfu12AAAOtqLytQH0jyYne/1N1/luQzSR7en7EAANbXXgLqXUl+/4r9VxbHAAAOteruG3tg1UeSPNTd/2Cx/xNJ/lZ3f/yq804kObHYfU+Sr9/4uNwE9yT5g1UPwb7zuh5eXtvDy2u7en+9uzevdcdergP1apLvumL/3sWxt+nuU0lO7eF5uImq6lx3b696DvaX1/Xw8toeXl7b9baXt/B+M8n9VfXuqrozyUeTPLU/YwEArK8bXoHq7jeq6uNJfjXJHUk+1d1f3bfJAADW1J6+yqW7fyXJr+zTLKwHb7ceTl7Xw8tre3h5bdfYDX+IHADgduWrXAAAhgQUb1NV/6qqXq2qZxc/f3vVM7E3vnLp8Kqq81X1O4t/q+dWPQ83rqo+VVUXq+q5K47dXVVPV9ULi9u7VjkjbyeguJZPdPcDix+fcbuF+cql28KPLP6t+nX3W9svJHnoqmM7Sc529/1Jzi72WRMCCg43X7kEt4Du/lKSP7rq8MNJTi+2Tyd55KYOxbckoLiWj1fVby+WlC0Z39p85dLh1kl+raqeWXzrA4fLke6+sNh+LcmRVQ7D2wmo21BV/beqeu4aPw8n+bkkfyPJA0kuJPl3Kx0W+FZ+sLu/N5ffov3JqvqhVQ/EwejLvzLv1+bXyJ6uA8Wtqbs/tMx5VfWfknz+gMfhYC31lUvcmrr71cXtxap6Ipffsv3SaqdiH71eVUe7+0JVHU1ycdUD8ResQPE2i3+kb/q7SZ673rncEnzl0iFVVe+oqu94czvJj8a/18PmqSTHF9vHkzy5wlm4ihUorvZvq+qBXF4qPp/kH652HPbCVy4dakeSPFFVyeX/l/9Sd39htSNxo6rq00l+OMk9VfVKkn+Z5GSSx6vqsSQvJ3l0dRNyNVciBwAY8hYeAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIb+P92jd8G+8mv+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For our token, select its feature values from layer 5.\n",
    "token_i = 4\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "# print(vec)\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Type of encoded_layers:  <class 'list'>\n",
      "Tensor shape for each layer:  torch.Size([1, 32, 768])\n"
     ]
    }
   ],
   "source": [
    "# `encoded_layers` is a tuple.\n",
    "print('     Type of encoded_layers: ', type(list(encoded_layers)))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', encoded_layers[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 32, 768])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 32, 768])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 13, 768])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  disgusted\n",
      "21  disgusted\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_test_text):\n",
    "      if tokenizer.decode(token_str).strip() == \"disgusted\":\n",
    "        print (i, tokenizer.decode(token_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 32 x 768\n"
     ]
    }
   ],
   "source": [
    "# Create token vectors by summing the last 4 layers of the model.\n",
    "# Stores the token vectors, with shape [32 x 768]\n",
    "token_vecs_sum = []\n",
    "# `token_embeddings` is a [32 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # `token` is a [13 x 768] tensor\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "# Shape is: 32 x 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 32 x 768\n"
     ]
    }
   ],
   "source": [
    "# Extract the last layer's features\n",
    "token_vecs_last = []\n",
    "# `token_embeddings` is a [32 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # `token` is a [13 x 768] tensor\n",
    "    # Extract the vector from the last layer.\n",
    "    last_vec = token[-1]\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_last.append(last_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_last), len(token_vecs_last[0])))\n",
    "# Shape is: 32 x 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 32 x 768\n"
     ]
    }
   ],
   "source": [
    "# Extract the embedding layer's features (layer 0 is the embedding layer)\n",
    "token_vecs_embed = []\n",
    "# `token_embeddings` is a [32 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # `token` is a [13 x 768] tensor\n",
    "    # Extract the vector from the last layer.\n",
    "    embed_vec = token[0]\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_embed.append(last_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_embed), len(token_vecs_embed[0])))\n",
    "# Shape is: 32 x 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 21]\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "for i, token_str in enumerate(tokenized_test_text):\n",
    "      if tokenizer.decode(token_str).strip() == \"disgusted\":\n",
    "        indices.append(i)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 32 x 768\n",
      "disgusted at index 4: tensor([-0.7637, -1.0375, -0.4840, -0.2251, -0.1792])\n",
      "disgusted at index 21: tensor([-0.8459, -0.2973, -0.3336, -0.6372, -0.7399])\n"
     ]
    }
   ],
   "source": [
    "token_vecs_sum_last_four = sum_last_four_token_vecs(token_embeddings)\n",
    "for i in range(len(indices)):\n",
    "    print(f'disgusted at index {indices[i]}: {str(token_vecs_sum_last_four[indices[i]][:5])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 open\n",
      "1 openness\n",
      "2 opposed\n",
      "3 oppositional\n",
      "4 oppressed\n",
      "5 optimism\n",
      "6 optimistic\n",
      "7 ordering\n",
      "8 orgasmic\n",
      "9 ornery\n",
      "10 ouch\n",
      "11 out\n",
      "12 outburst\n",
      "13 outcry\n",
      "14 outed\n",
      "15 outlandish\n",
      "16 outrage\n",
      "17 outraged\n",
      "18 outspoken\n",
      "19 overbearing\n",
      "20 overexcited\n",
      "21 overjoyed\n",
      "22 overshadowed\n",
      "23 overstrung\n",
      "24 overwhelmed\n",
      "25 overworked\n",
      "26 overwrought\n",
      "27 pain\n",
      "28 pained\n",
      "29 painful\n",
      "30 painfully\n",
      "31 panic\n",
      "32 panicked\n",
      "33 panicky\n",
      "34 paralyzed\n",
      "35 paranoid\n",
      "36 passionate\n",
      "37 passive\n",
      "38 patience\n",
      "39 patient\n",
      "40 patronizing\n",
      "41 pause\n",
      "42 pausing\n",
      "43 peaceful\n",
      "44 peculiar\n",
      "45 peering\n",
      "46 peeved\n",
      "47 peevish\n",
      "48 pensive\n",
      "49 peppy\n",
      "50 perceptive\n",
      "51 perfidious\n",
      "52 perky\n",
      "53 perplexed\n",
      "54 perplexing\n",
      "55 persistent\n",
      "56 personable\n",
      "57 perturbed\n",
      "58 perverse\n",
      "59 pesky\n",
      "60 pessimism\n",
      "61 pessimistic\n",
      "62 pestered\n",
      "63 petitioning\n",
      "64 petrified\n",
      "65 petty\n",
      "66 petulant\n",
      "67 picked\n",
      "68 piercing\n",
      "69 pinched\n",
      "70 pious\n",
      "71 piqued\n",
      "72 pissed\n",
      "73 pitiable\n",
      "74 pitiful\n",
      "75 pity\n",
      "76 pitying\n",
      "77 placated\n",
      "78 placation\n",
      "79 placid\n",
      "80 plain\n",
      "81 plaintive\n",
      "82 planning\n",
      "83 playful\n",
      "84 playfully\n",
      "85 pleading\n",
      "86 pleasant\n",
      "87 pleased\n",
      "88 pleasing\n",
      "89 pleasurable\n",
      "90 pleasure\n",
      "91 pleasured\n",
      "92 pliant\n",
      "93 plotting\n",
      "94 poignant\n",
      "95 pointed\n",
      "96 poised\n",
      "97 polite\n",
      "98 pompous\n",
      "99 ponder\n",
      "100 pondering\n",
      "101 pooping\n",
      "102 pop\n",
      "103 posing\n",
      "104 positive\n",
      "105 positivity\n",
      "106 possibly\n",
      "107 pout\n",
      "108 pouting\n",
      "109 pouty\n",
      "110 powerful\n",
      "111 powerless\n",
      "112 pranking\n",
      "113 precarious\n",
      "114 predatory\n",
      "115 prejudiced\n",
      "116 preoccupied\n",
      "117 prepared\n",
      "118 preparing\n",
      "119 pretending\n",
      "120 pretentious\n",
      "121 prideful\n",
      "122 priggish\n",
      "123 primed\n",
      "124 private\n",
      "125 processing\n",
      "126 propositioning\n",
      "127 proud\n",
      "128 provocative\n",
      "129 provoke\n",
      "130 provoked\n",
      "131 provoking\n",
      "132 prying\n",
      "133 psycho\n",
      "134 psychotic\n",
      "135 puckish\n",
      "136 puerile\n",
      "137 pugnacious\n",
      "138 punished\n",
      "139 punishing\n",
      "140 punitive\n",
      "141 punk\n",
      "142 puppyish\n",
      "143 purposeful\n",
      "144 pursed\n",
      "145 put\n",
      "146 putting\n",
      "147 puzzled\n",
      "148 puzzlement\n",
      "149 qualms\n",
      "150 quarrelsome\n",
      "151 queasy\n",
      "152 quenched\n",
      "153 questionable\n",
      "154 questioning\n",
      "155 questioningly\n",
      "156 quiet\n",
      "157 quietness\n",
      "158 quilt\n",
      "159 quirky\n",
      "160 quizzical\n",
      "161 rabid\n",
      "162 racked\n",
      "163 radiant\n",
      "164 rage\n",
      "165 raged\n",
      "166 ragged\n",
      "167 raging\n",
      "168 rancorous\n",
      "169 randy\n",
      "170 rapt\n",
      "171 rattled\n",
      "172 raving\n",
      "173 reactive\n",
      "174 ready\n",
      "175 realization\n",
      "176 reassured\n",
      "177 rebellious\n",
      "178 rebuke\n",
      "179 recalling\n",
      "180 receptive\n",
      "181 reckless\n",
      "182 recoil\n",
      "183 recoiling\n",
      "184 reflecting\n",
      "185 reflection\n",
      "186 reflective\n",
      "187 refulgent\n",
      "188 refusing\n",
      "189 regret\n",
      "190 regretful\n",
      "191 rejected\n",
      "192 rejecting\n",
      "193 rejection\n",
      "194 rejoicing\n",
      "195 relaxation\n",
      "196 relaxed\n",
      "197 relentless\n",
      "198 relief\n",
      "199 relieved\n",
      "200 relived\n",
      "201 reluctant\n",
      "202 reluctantly\n",
      "203 remorse\n",
      "204 remorseful\n",
      "205 repelled\n",
      "206 repressed\n",
      "207 reproach\n",
      "208 reproachful\n",
      "209 repugnance\n",
      "210 repugnant\n",
      "211 repulsed\n",
      "212 repulsion\n",
      "213 resent\n",
      "214 resentful\n",
      "215 resenting\n",
      "216 resentment\n",
      "217 reserved\n",
      "218 resignation\n",
      "219 resigned\n",
      "220 resilience\n",
      "221 resistance\n",
      "222 resistant\n",
      "223 resistent\n",
      "224 resisting\n",
      "225 resolute\n",
      "226 resolved\n",
      "227 responsive\n",
      "228 restful\n",
      "229 resting\n",
      "230 restless\n",
      "231 restlessness\n",
      "232 restrained\n",
      "233 restraint\n",
      "234 retaliating\n",
      "235 retaliatory\n",
      "236 rethinking\n",
      "237 reticence\n",
      "238 reticent\n",
      "239 revengeful\n",
      "240 reverent\n",
      "241 revolted\n",
      "242 revulsion\n",
      "243 righteous\n",
      "244 rigid\n",
      "245 riled\n",
      "246 riotous\n",
      "247 riveted\n",
      "248 roar\n",
      "249 roguish\n",
      "250 roiled\n",
      "251 rough\n",
      "252 roused\n",
      "253 rude\n",
      "254 rueful\n",
      "255 ruffled\n",
      "256 ruminating\n",
      "257 rustled\n",
      "258 ruthless\n",
      "259 sad\n",
      "260 sadden\n",
      "261 saddened\n",
      "262 sadistic\n",
      "263 sadness\n",
      "264 salacious\n",
      "265 salivating\n",
      "266 sanctimonious\n",
      "267 sane\n",
      "268 sanguine\n",
      "269 sappy\n",
      "270 sarcasm\n",
      "271 sarcastic\n",
      "272 sardonic\n",
      "273 sassy\n",
      "274 sated\n",
      "275 satiated\n",
      "276 satirical\n",
      "277 satisfaction\n",
      "278 satisfied\n",
      "279 satisfy\n",
      "280 saturnine\n",
      "281 saucy\n",
      "282 savage\n",
      "283 scandalized\n",
      "284 scare\n",
      "285 scared\n",
      "286 scary\n",
      "287 scattered\n",
      "288 schadenfreude\n",
      "289 scheming\n",
      "290 scoffer\n",
      "291 scoffing\n",
      "292 scorn\n",
      "293 scorned\n",
      "294 scornful\n",
      "295 scowl\n",
      "296 scowling\n",
      "297 scream\n",
      "298 screaming\n",
      "299 scrutinizing\n",
      "300 sealed\n",
      "301 searching\n",
      "302 secretive\n",
      "303 secretively\n",
      "304 secure\n",
      "305 sedate\n",
      "306 seduction\n",
      "307 seductive\n",
      "308 seething\n",
      "309 self\n",
      "310 sensual\n",
      "311 sentimental\n",
      "312 serene\n",
      "313 serious\n",
      "314 seriousness\n",
      "315 servile\n",
      "316 set\n",
      "317 severe\n",
      "318 shabby\n",
      "319 shady\n",
      "320 shaken\n",
      "321 shaky\n",
      "322 shame\n",
      "323 shamed\n",
      "324 shamefaced\n",
      "325 shameful\n",
      "326 shameless\n",
      "327 sharp\n",
      "328 sheepish\n",
      "329 sheepishness\n",
      "330 shelled\n",
      "331 shifty\n",
      "332 shock\n",
      "333 shocked\n",
      "334 shocking\n",
      "335 shockingly\n",
      "336 shook\n",
      "337 shout\n",
      "338 shouting\n",
      "339 shrewd\n",
      "340 shy\n",
      "341 shyness\n",
      "342 sick\n",
      "343 sicken\n",
      "344 sickened\n",
      "345 sigh\n",
      "346 silenced\n",
      "347 silent\n",
      "348 silliness\n",
      "349 silly\n",
      "350 simmering\n",
      "351 simper\n",
      "352 simpering\n",
      "353 simple\n",
      "354 simplicity\n",
      "355 sincere\n",
      "356 sinful\n",
      "357 singing\n",
      "358 sinister\n",
      "359 sinisterly\n",
      "360 sizing\n",
      "361 skeptic\n",
      "362 skeptical\n",
      "363 skeptically\n",
      "364 skepticism\n",
      "365 sketchy\n",
      "366 skittish\n",
      "367 slack\n",
      "368 sleazy\n",
      "369 sleepy\n",
      "370 slick\n",
      "371 slothful\n",
      "372 slow\n",
      "373 sluggish\n",
      "374 sly\n",
      "375 smarmy\n",
      "376 smart\n",
      "377 smashed\n",
      "378 smile\n",
      "379 smiley\n",
      "380 smiling\n",
      "381 smirk\n",
      "382 smirking\n",
      "383 smoldering\n",
      "384 smooching\n",
      "385 smooth\n",
      "386 smug\n",
      "387 smugness\n",
      "388 snake\n",
      "389 snappy\n",
      "390 snarky\n",
      "391 snarl\n",
      "392 snarled\n",
      "393 snarling\n",
      "394 snarly\n",
      "395 sneaky\n",
      "396 sneer\n",
      "397 sneering\n",
      "398 sneeze\n",
      "399 sneezing\n",
      "400 snicker\n",
      "401 snickering\n",
      "402 snide\n",
      "403 sniggering\n",
      "404 sniveling\n",
      "405 snobbish\n",
      "406 snobby\n",
      "407 snooty\n",
      "408 snotty\n",
      "409 sociable\n",
      "410 soft\n",
      "411 solemn\n",
      "412 solicitous\n",
      "413 solitary\n",
      "414 solitude\n",
      "415 somber\n",
      "416 somberly\n",
      "417 somnolent\n",
      "418 soothed\n",
      "419 sore\n",
      "420 sorrow\n",
      "421 sorrowful\n",
      "422 sorry\n",
      "423 sour\n",
      "424 spaced\n",
      "425 spacing\n",
      "426 spastic\n",
      "427 speaking\n",
      "428 specious\n",
      "429 speculative\n",
      "430 speechless\n",
      "431 spent\n",
      "432 spirited\n",
      "433 spiritless\n",
      "434 spite\n",
      "435 spiteful\n",
      "436 spoiled\n",
      "437 spooked\n",
      "438 squeamish\n",
      "439 staggered\n",
      "440 stalker\n",
      "441 stare\n",
      "442 staring\n",
      "443 starstruck\n",
      "444 started\n",
      "445 startled\n",
      "446 stately\n",
      "447 steadfast\n",
      "448 steady\n",
      "449 stealthy\n",
      "450 steamed\n",
      "451 steaming\n",
      "452 steeling\n",
      "453 steely\n",
      "454 stern\n",
      "455 stiff\n",
      "456 stifled\n",
      "457 stifling\n",
      "458 still\n",
      "459 stillness\n",
      "460 stimulated\n",
      "461 stinky\n",
      "462 stirred\n",
      "463 stoic\n",
      "464 stoical\n",
      "465 stolid\n",
      "466 stoned\n",
      "467 storming\n",
      "468 stormy\n",
      "469 stout\n",
      "470 straight\n",
      "471 strained\n",
      "472 strange\n",
      "473 stressed\n",
      "474 stricken\n",
      "475 strict\n",
      "476 strong\n",
      "477 struck\n",
      "478 stubborn\n",
      "479 stubbornness\n",
      "480 studious\n",
      "481 studying\n",
      "482 stumped\n",
      "483 stung\n",
      "484 stunned\n",
      "485 stupefaction\n",
      "486 stupefied\n",
      "487 stupefy\n",
      "488 stupid\n",
      "489 stuporous\n",
      "490 suave\n",
      "491 subdued\n",
      "492 sublime\n",
      "493 submissive\n",
      "494 suffering\n",
      "495 suggestive\n",
      "496 sulking\n",
      "497 sulky\n",
      "498 sullen\n",
      "499 sullenness\n",
      "500 sunny\n",
      "501 superior\n",
      "502 superiority\n",
      "503 suppressed\n",
      "504 suppressing\n",
      "505 suppression\n",
      "506 sure\n",
      "507 surly\n",
      "508 surprise\n",
      "509 surprised\n",
      "510 surprising\n",
      "511 surprisingly\n",
      "512 surreptitious\n",
      "513 suspect\n",
      "514 suspecting\n",
      "515 suspense\n",
      "516 suspicion\n",
      "517 suspicious\n",
      "518 suspiciously\n",
      "519 suspiciousness\n",
      "520 swaggering\n",
      "521 swearing\n",
      "522 sympathetic\n",
      "523 sympathizing\n",
      "524 sympathy\n",
      "525 taciturn\n",
      "526 talkative\n",
      "527 talking\n",
      "528 tantalized\n",
      "529 tart\n",
      "530 tasteful\n",
      "531 tattling\n",
      "532 taunt\n",
      "533 taunting\n",
      "534 taut\n",
      "535 tearful\n",
      "536 teary\n",
      "537 tease\n",
      "538 teasing\n",
      "539 tempered\n",
      "540 tempest\n",
      "541 tempestuous\n",
      "542 tempted\n",
      "543 tenacious\n",
      "544 tender\n",
      "545 tenderness\n",
      "546 tense\n",
      "547 tensed\n",
      "548 tension\n",
      "549 tentative\n",
      "550 terrified\n",
      "551 terror\n",
      "552 terrorized\n",
      "553 terrorizing\n",
      "554 terse\n",
      "555 testy\n",
      "556 tetchy\n",
      "557 thankful\n",
      "558 thinking\n",
      "559 thought\n",
      "560 thoughtful\n",
      "561 thoughtfulness\n",
      "562 threat\n",
      "563 threatened\n",
      "564 threatening\n",
      "565 thrilled\n",
      "566 thrown\n",
      "567 thunderstruck\n",
      "568 thwarted\n",
      "569 ticked\n",
      "570 tickled\n",
      "571 tied\n",
      "572 tiered\n",
      "573 tight\n",
      "574 tightlipped\n",
      "575 timid\n",
      "576 timidly\n",
      "577 timidness\n",
      "578 tired\n",
      "579 tiredly\n",
      "580 tiredness\n",
      "581 titillated\n",
      "582 tolerant\n",
      "583 tongue\n",
      "584 tormented\n",
      "585 touched\n",
      "586 tough\n",
      "587 toying\n",
      "588 tragic\n",
      "589 tragical\n",
      "590 tranquil\n",
      "591 tranquility\n",
      "592 transfixed\n",
      "593 traumatized\n",
      "594 trembling\n",
      "595 trepid\n",
      "596 trepidation\n",
      "597 trickster\n",
      "598 tricky\n",
      "599 triumphant\n",
      "600 troubled\n",
      "601 troublesome\n",
      "602 troubling\n",
      "603 trusting\n",
      "604 trustworthy\n",
      "605 tumultuous\n",
      "606 turbulent\n",
      "607 twinkly\n",
      "608 umbrage\n",
      "609 umbrageous\n",
      "610 unaffected\n",
      "611 unagitated\n",
      "612 unamused\n",
      "613 unappreciative\n",
      "614 unapproachable\n",
      "615 unassertive\n",
      "616 unassuming\n",
      "617 unaware\n",
      "618 unbelief\n",
      "619 unbelievable\n",
      "620 unbelieving\n",
      "621 unbothered\n",
      "622 uncaring\n",
      "623 uncertain\n",
      "624 uncertainly\n",
      "625 uncertainty\n",
      "626 uncivil\n",
      "627 uncomfortable\n",
      "628 uncommitted\n",
      "629 uncommunicative\n",
      "630 uncomprehending\n",
      "631 uncompromising\n",
      "632 unconcerned\n",
      "633 unconfident\n",
      "634 unconvinced\n",
      "635 uncooperative\n",
      "636 uncurious\n",
      "637 undecided\n",
      "638 underhanded\n",
      "639 understanding\n",
      "640 undesirable\n",
      "641 unease\n",
      "642 uneasily\n",
      "643 uneasiness\n",
      "644 uneasy\n",
      "645 unemotional\n",
      "646 unenthusiastic\n",
      "647 unexcited\n",
      "648 unexpected\n",
      "649 unfamiliar\n",
      "650 unfathomable\n",
      "651 unfazed\n",
      "652 unfeeling\n",
      "653 unfocused\n",
      "654 unforeseen\n",
      "655 unforgiving\n",
      "656 unforthcoming\n",
      "657 unfortunate\n",
      "658 unfriendly\n",
      "659 unhappy\n",
      "660 unhinged\n",
      "661 unimpressed\n",
      "662 uninformed\n",
      "663 uninspired\n",
      "664 uninterested\n",
      "665 uninvolved\n",
      "666 unique\n",
      "667 unlikeable\n",
      "668 unmoved\n",
      "669 unnerved\n",
      "670 unpleasant\n",
      "671 unprepared\n",
      "672 unquiet\n",
      "673 unreactive\n",
      "674 unresolved\n",
      "675 unrestrained\n",
      "676 unruffled\n",
      "677 unsatisfied\n",
      "678 unsettled\n",
      "679 unsociable\n",
      "680 unspeaking\n",
      "681 unspoken\n",
      "682 unstrung\n",
      "683 unsuccessful\n",
      "684 unsure\n",
      "685 unsurprised\n",
      "686 unsuspecting\n",
      "687 unswayed\n",
      "688 unsympathetic\n",
      "689 untouched\n",
      "690 untroubled\n",
      "691 untrusting\n",
      "692 unwanted\n",
      "693 unwavering\n",
      "694 unwelcoming\n",
      "695 unwell\n",
      "696 unwilling\n",
      "697 unyielding\n",
      "698 up\n",
      "699 upbeat\n",
      "700 uplifting\n",
      "701 uppity\n",
      "702 upset\n",
      "703 uptight\n",
      "704 useless\n",
      "705 vacant\n",
      "706 vacuous\n",
      "707 vanquished\n",
      "708 vehement\n",
      "709 vengeful\n",
      "710 venomous\n",
      "711 vex\n",
      "712 vexation\n",
      "713 vexed\n",
      "714 vicious\n",
      "715 victorious\n",
      "716 vigilant\n",
      "717 vile\n",
      "718 villainous\n",
      "719 vindictive\n",
      "720 violence\n",
      "721 violent\n",
      "722 viperous\n",
      "723 vituperative\n",
      "724 vocal\n",
      "725 vocalized\n",
      "726 vulgar\n",
      "727 vulnerability\n",
      "728 vulnerable\n",
      "729 wacky\n",
      "730 waiting\n",
      "731 wanted\n",
      "732 wanting\n",
      "733 wanton\n",
      "734 wariness\n",
      "735 warm\n",
      "736 wary\n",
      "737 wasted\n",
      "738 watch\n",
      "739 watchful\n",
      "740 watching\n",
      "741 wavering\n",
      "742 weariness\n",
      "743 weary\n",
      "744 weeping\n",
      "745 weird\n",
      "746 welcome\n",
      "747 welcoming\n",
      "748 whatever\n",
      "749 whimpering\n",
      "750 whimsical\n",
      "751 whisper\n",
      "752 whistle\n",
      "753 white\n",
      "754 wicked\n",
      "755 wild\n",
      "756 willful\n",
      "757 willing\n",
      "758 wily\n",
      "759 wink\n",
      "760 wired\n",
      "761 wishful\n",
      "762 wistful\n",
      "763 wistfully\n",
      "764 withdraw\n",
      "765 withdrawn\n",
      "766 withheld\n",
      "767 withholding\n",
      "768 woe\n",
      "769 woeful\n",
      "770 wonder\n",
      "771 wondering\n",
      "772 wonderment\n",
      "773 wooly\n",
      "774 woozy\n",
      "775 worn\n",
      "776 worried\n",
      "777 worrisome\n",
      "778 worry\n",
      "779 worrying\n",
      "780 worryingly\n",
      "781 wounded\n",
      "782 wow\n",
      "783 wrathful\n",
      "784 wrathfully\n",
      "785 wrecked\n",
      "786 wretched\n",
      "787 wronged\n",
      "788 wroth\n",
      "789 wry\n",
      "790 yawn\n",
      "791 yawning\n",
      "792 yearning\n",
      "793 yell\n",
      "794 yelling\n",
      "795 yielding\n",
      "796 yuck\n",
      "797 zany\n",
      "798 zealous\n",
      "799 zen\n",
      "800 zoned\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(vocab):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 outburst\n",
      "1 outcry\n",
      "2 outed\n",
      "3 outlandish\n",
      "4 outrage\n",
      "5 outraged\n",
      "6 outspoken\n",
      "7 overbearing\n",
      "8 overexcited\n",
      "9 overjoyed\n",
      "10 overshadowed\n",
      "11 overstrung\n",
      "12 overwhelmed\n",
      "13 overworked\n",
      "14 overwrought\n",
      "15 pain\n",
      "16 pained\n",
      "17 painful\n",
      "18 painfully\n",
      "19 panic\n",
      "20 panicked\n",
      "21 panicky\n",
      "22 paralyzed\n",
      "23 paranoid\n",
      "24 passionate\n",
      "25 passive\n",
      "26 patience\n",
      "27 patient\n",
      "28 patronizing\n",
      "29 pause\n",
      "30 pausing\n",
      "31 peaceful\n",
      "32 peculiar\n",
      "33 peering\n",
      "34 peeved\n",
      "35 peevish\n",
      "36 pensive\n",
      "37 peppy\n",
      "38 perceptive\n",
      "39 perfidious\n",
      "40 perky\n",
      "41 perplexed\n",
      "42 perplexing\n",
      "43 persistent\n",
      "44 personable\n",
      "45 perturbed\n",
      "46 perverse\n",
      "47 pesky\n",
      "48 pessimism\n",
      "49 pessimistic\n",
      "50 pestered\n",
      "51 petitioning\n",
      "52 petrified\n",
      "53 petty\n",
      "54 petulant\n",
      "55 picked\n",
      "56 piercing\n",
      "57 pinched\n",
      "58 pious\n",
      "59 piqued\n",
      "60 pissed\n",
      "61 pitiable\n",
      "62 pitiful\n",
      "63 pity\n",
      "64 pitying\n",
      "65 placated\n",
      "66 placation\n",
      "67 placid\n",
      "68 plain\n",
      "69 plaintive\n",
      "70 planning\n",
      "71 playful\n",
      "72 playfully\n",
      "73 pleading\n",
      "74 pleasant\n",
      "75 pleased\n",
      "76 pleasing\n",
      "77 pleasurable\n",
      "78 pleasure\n",
      "79 pleasured\n",
      "80 pliant\n",
      "81 plotting\n",
      "82 poignant\n",
      "83 pointed\n",
      "84 poised\n",
      "85 polite\n",
      "86 pompous\n",
      "87 ponder\n",
      "88 pondering\n",
      "89 pooping\n",
      "90 posing\n",
      "91 positive\n",
      "92 positivity\n",
      "93 possibly\n",
      "94 pout\n",
      "95 pouting\n",
      "96 pouty\n",
      "97 powerful\n",
      "98 powerless\n",
      "99 pranking\n",
      "100 precarious\n",
      "101 predatory\n",
      "102 prejudiced\n",
      "103 preoccupied\n",
      "104 prepared\n",
      "105 preparing\n",
      "106 pretending\n",
      "107 pretentious\n",
      "108 prideful\n",
      "109 priggish\n",
      "110 primed\n",
      "111 private\n",
      "112 processing\n",
      "113 propositioning\n",
      "114 proud\n",
      "115 provocative\n",
      "116 provoke\n",
      "117 provoked\n",
      "118 provoking\n",
      "119 prying\n",
      "120 psycho\n",
      "121 psychotic\n",
      "122 puckish\n",
      "123 puerile\n",
      "124 pugnacious\n",
      "125 punished\n",
      "126 punishing\n",
      "127 punitive\n",
      "128 punk\n",
      "129 puppyish\n",
      "130 purposeful\n",
      "131 pursed\n",
      "132 putting\n",
      "133 puzzled\n",
      "134 puzzlement\n",
      "135 qualms\n",
      "136 quarrelsome\n",
      "137 queasy\n",
      "138 quenched\n",
      "139 questionable\n",
      "140 questioning\n",
      "141 questioningly\n",
      "142 quiet\n",
      "143 quietness\n",
      "144 quilt\n",
      "145 quirky\n",
      "146 quizzical\n",
      "147 rabid\n",
      "148 racked\n",
      "149 radiant\n",
      "150 rage\n",
      "151 raged\n",
      "152 ragged\n",
      "153 raging\n",
      "154 rancorous\n",
      "155 randy\n",
      "156 rapt\n",
      "157 rattled\n",
      "158 raving\n",
      "159 reactive\n",
      "160 ready\n",
      "161 realization\n",
      "162 reassured\n",
      "163 rebellious\n",
      "164 rebuke\n",
      "165 recalling\n",
      "166 receptive\n",
      "167 reckless\n",
      "168 recoil\n",
      "169 recoiling\n",
      "170 reflecting\n",
      "171 reflection\n",
      "172 reflective\n",
      "173 refulgent\n",
      "174 refusing\n",
      "175 regret\n",
      "176 regretful\n",
      "177 rejected\n",
      "178 rejecting\n",
      "179 rejection\n",
      "180 rejoicing\n",
      "181 relaxation\n",
      "182 relaxed\n",
      "183 relentless\n",
      "184 relief\n",
      "185 relieved\n",
      "186 relived\n",
      "187 reluctant\n",
      "188 reluctantly\n",
      "189 remorse\n",
      "190 remorseful\n",
      "191 repelled\n",
      "192 repressed\n",
      "193 reproach\n",
      "194 reproachful\n",
      "195 repugnance\n",
      "196 repugnant\n",
      "197 repulsed\n",
      "198 repulsion\n",
      "199 resent\n",
      "200 resentful\n",
      "201 resenting\n",
      "202 resentment\n",
      "203 reserved\n",
      "204 resignation\n",
      "205 resigned\n",
      "206 resilience\n",
      "207 resistance\n",
      "208 resistant\n",
      "209 resistent\n",
      "210 resisting\n",
      "211 resolute\n",
      "212 resolved\n",
      "213 responsive\n",
      "214 restful\n",
      "215 resting\n",
      "216 restless\n",
      "217 restlessness\n",
      "218 restrained\n",
      "219 restraint\n",
      "220 retaliating\n",
      "221 retaliatory\n",
      "222 rethinking\n",
      "223 reticence\n",
      "224 reticent\n",
      "225 revengeful\n",
      "226 reverent\n",
      "227 revolted\n",
      "228 revulsion\n",
      "229 righteous\n",
      "230 rigid\n",
      "231 riled\n",
      "232 riotous\n",
      "233 riveted\n",
      "234 roar\n",
      "235 roguish\n",
      "236 roiled\n",
      "237 rough\n",
      "238 roused\n",
      "239 rude\n",
      "240 rueful\n",
      "241 ruffled\n",
      "242 ruminating\n",
      "243 rustled\n",
      "244 ruthless\n",
      "245 sad\n",
      "246 sadden\n",
      "247 saddened\n",
      "248 sadistic\n",
      "249 sadness\n",
      "250 salacious\n",
      "251 salivating\n",
      "252 sanctimonious\n",
      "253 sane\n",
      "254 sanguine\n",
      "255 sappy\n",
      "256 sarcasm\n",
      "257 sarcastic\n",
      "258 sardonic\n",
      "259 sassy\n",
      "260 sated\n",
      "261 satiated\n",
      "262 satirical\n",
      "263 satisfaction\n",
      "264 satisfied\n",
      "265 satisfy\n",
      "266 saturnine\n",
      "267 saucy\n",
      "268 savage\n",
      "269 scandalized\n",
      "270 scare\n",
      "271 scared\n",
      "272 scary\n",
      "273 scattered\n",
      "274 schadenfreude\n",
      "275 scheming\n",
      "276 scoffer\n",
      "277 scoffing\n",
      "278 scorn\n",
      "279 scorned\n",
      "280 scornful\n",
      "281 scowl\n",
      "282 scowling\n",
      "283 scream\n",
      "284 screaming\n",
      "285 scrutinizing\n",
      "286 sealed\n",
      "287 searching\n",
      "288 secretive\n",
      "289 secretively\n",
      "290 secure\n",
      "291 sedate\n",
      "292 seduction\n",
      "293 seductive\n",
      "294 seething\n",
      "295 sensual\n",
      "296 sentimental\n",
      "297 serene\n",
      "298 serious\n",
      "299 seriousness\n",
      "300 servile\n",
      "301 severe\n",
      "302 shabby\n",
      "303 shady\n",
      "304 shaken\n",
      "305 shaky\n",
      "306 shame\n",
      "307 shamed\n",
      "308 shamefaced\n",
      "309 shameful\n",
      "310 shameless\n",
      "311 sharp\n",
      "312 sheepish\n",
      "313 sheepishness\n",
      "314 shelled\n",
      "315 shifty\n",
      "316 shock\n",
      "317 shocked\n",
      "318 shocking\n",
      "319 shockingly\n",
      "320 shook\n",
      "321 shout\n",
      "322 shouting\n",
      "323 shrewd\n",
      "324 shy\n",
      "325 shyness\n",
      "326 sick\n",
      "327 sicken\n",
      "328 sickened\n",
      "329 sigh\n",
      "330 silenced\n",
      "331 silent\n",
      "332 silliness\n",
      "333 silly\n",
      "334 simmering\n",
      "335 simper\n",
      "336 simpering\n",
      "337 simple\n",
      "338 simplicity\n",
      "339 sincere\n",
      "340 sinful\n",
      "341 singing\n",
      "342 sinister\n",
      "343 sinisterly\n",
      "344 sizing\n",
      "345 skeptic\n",
      "346 skeptical\n",
      "347 skeptically\n",
      "348 skepticism\n",
      "349 sketchy\n",
      "350 skittish\n",
      "351 slack\n",
      "352 sleazy\n",
      "353 sleepy\n",
      "354 slick\n",
      "355 slothful\n",
      "356 slow\n",
      "357 sluggish\n",
      "358 sly\n",
      "359 smarmy\n",
      "360 smart\n",
      "361 smashed\n",
      "362 smile\n",
      "363 smiley\n",
      "364 smiling\n",
      "365 smirk\n",
      "366 smirking\n",
      "367 smoldering\n",
      "368 smooching\n",
      "369 smooth\n",
      "370 smug\n",
      "371 smugness\n",
      "372 snake\n",
      "373 snappy\n",
      "374 snarky\n",
      "375 snarl\n",
      "376 snarled\n",
      "377 snarling\n",
      "378 snarly\n",
      "379 sneaky\n",
      "380 sneer\n",
      "381 sneering\n",
      "382 sneeze\n",
      "383 sneezing\n",
      "384 snicker\n",
      "385 snickering\n",
      "386 snide\n",
      "387 sniggering\n",
      "388 sniveling\n",
      "389 snobbish\n",
      "390 snobby\n",
      "391 snooty\n",
      "392 snotty\n",
      "393 sociable\n",
      "394 solemn\n",
      "395 solicitous\n",
      "396 solitary\n",
      "397 solitude\n",
      "398 somber\n",
      "399 somberly\n",
      "400 somnolent\n",
      "401 soothed\n",
      "402 sore\n",
      "403 sorrow\n",
      "404 sorrowful\n",
      "405 sorry\n",
      "406 sour\n",
      "407 spaced\n",
      "408 spacing\n",
      "409 spastic\n",
      "410 speaking\n",
      "411 specious\n",
      "412 speculative\n",
      "413 speechless\n",
      "414 spent\n",
      "415 spirited\n",
      "416 spiritless\n",
      "417 spite\n",
      "418 spiteful\n",
      "419 spoiled\n",
      "420 spooked\n",
      "421 squeamish\n",
      "422 staggered\n",
      "423 stalker\n",
      "424 stare\n",
      "425 staring\n",
      "426 starstruck\n",
      "427 started\n",
      "428 startled\n",
      "429 stately\n",
      "430 steadfast\n",
      "431 steady\n",
      "432 stealthy\n",
      "433 steamed\n",
      "434 steaming\n",
      "435 steeling\n",
      "436 steely\n",
      "437 stern\n",
      "438 stiff\n",
      "439 stifled\n",
      "440 stifling\n",
      "441 still\n",
      "442 stillness\n",
      "443 stimulated\n",
      "444 stinky\n",
      "445 stirred\n",
      "446 stoic\n",
      "447 stoical\n",
      "448 stolid\n",
      "449 stoned\n",
      "450 storming\n",
      "451 stormy\n",
      "452 stout\n",
      "453 straight\n",
      "454 strained\n",
      "455 strange\n",
      "456 stressed\n",
      "457 stricken\n",
      "458 strict\n",
      "459 strong\n",
      "460 struck\n",
      "461 stubborn\n",
      "462 stubbornness\n",
      "463 studious\n",
      "464 studying\n",
      "465 stumped\n",
      "466 stung\n",
      "467 stunned\n",
      "468 stupefaction\n",
      "469 stupefied\n",
      "470 stupefy\n",
      "471 stupid\n",
      "472 stuporous\n",
      "473 suave\n",
      "474 subdued\n",
      "475 sublime\n",
      "476 submissive\n",
      "477 suffering\n",
      "478 suggestive\n",
      "479 sulking\n",
      "480 sulky\n",
      "481 sullen\n",
      "482 sullenness\n",
      "483 sunny\n",
      "484 superior\n",
      "485 superiority\n",
      "486 suppressed\n",
      "487 suppressing\n",
      "488 suppression\n",
      "489 sure\n",
      "490 surly\n",
      "491 surprise\n",
      "492 surprised\n",
      "493 surprising\n",
      "494 surprisingly\n",
      "495 surreptitious\n",
      "496 suspect\n",
      "497 suspecting\n",
      "498 suspense\n",
      "499 suspicion\n",
      "500 suspicious\n",
      "501 suspiciously\n",
      "502 suspiciousness\n",
      "503 swaggering\n",
      "504 swearing\n",
      "505 sympathetic\n",
      "506 sympathizing\n",
      "507 sympathy\n",
      "508 taciturn\n",
      "509 talkative\n",
      "510 talking\n",
      "511 tantalized\n",
      "512 tart\n",
      "513 tasteful\n",
      "514 tattling\n",
      "515 taunt\n",
      "516 taunting\n",
      "517 taut\n",
      "518 tearful\n",
      "519 teary\n",
      "520 tease\n",
      "521 teasing\n",
      "522 tempered\n",
      "523 tempest\n",
      "524 tempestuous\n",
      "525 tempted\n",
      "526 tenacious\n",
      "527 tender\n",
      "528 tenderness\n",
      "529 tense\n",
      "530 tensed\n",
      "531 tension\n",
      "532 tentative\n",
      "533 terrified\n",
      "534 terror\n",
      "535 terrorized\n",
      "536 terrorizing\n",
      "537 terse\n",
      "538 testy\n",
      "539 tetchy\n",
      "540 thankful\n",
      "541 thinking\n",
      "542 thought\n",
      "543 thoughtful\n",
      "544 thoughtfulness\n",
      "545 threat\n",
      "546 threatened\n",
      "547 threatening\n",
      "548 thrilled\n",
      "549 thrown\n",
      "550 thunderstruck\n",
      "551 thwarted\n",
      "552 ticked\n",
      "553 tickled\n",
      "554 tied\n",
      "555 tiered\n",
      "556 tight\n",
      "557 tightlipped\n",
      "558 timid\n",
      "559 timidly\n",
      "560 timidness\n",
      "561 tired\n",
      "562 tiredly\n",
      "563 tiredness\n",
      "564 titillated\n",
      "565 tolerant\n",
      "566 tongue\n",
      "567 tormented\n",
      "568 touched\n",
      "569 tough\n",
      "570 toying\n",
      "571 tragic\n",
      "572 tragical\n",
      "573 tranquil\n",
      "574 tranquility\n",
      "575 transfixed\n",
      "576 traumatized\n",
      "577 trembling\n",
      "578 trepid\n",
      "579 trepidation\n",
      "580 trickster\n",
      "581 tricky\n",
      "582 triumphant\n",
      "583 troubled\n",
      "584 troublesome\n",
      "585 troubling\n",
      "586 trusting\n",
      "587 trustworthy\n",
      "588 tumultuous\n",
      "589 turbulent\n",
      "590 twinkly\n",
      "591 umbrage\n",
      "592 umbrageous\n",
      "593 unaffected\n",
      "594 unagitated\n",
      "595 unamused\n",
      "596 unappreciative\n",
      "597 unapproachable\n",
      "598 unassertive\n",
      "599 unassuming\n",
      "600 unaware\n",
      "601 unbelief\n",
      "602 unbelievable\n",
      "603 unbelieving\n",
      "604 unbothered\n",
      "605 uncaring\n",
      "606 uncertain\n",
      "607 uncertainly\n",
      "608 uncertainty\n",
      "609 uncivil\n",
      "610 uncomfortable\n",
      "611 uncommitted\n",
      "612 uncommunicative\n",
      "613 uncomprehending\n",
      "614 uncompromising\n",
      "615 unconcerned\n",
      "616 unconfident\n",
      "617 unconvinced\n",
      "618 uncooperative\n",
      "619 uncurious\n",
      "620 undecided\n",
      "621 underhanded\n",
      "622 understanding\n",
      "623 undesirable\n",
      "624 unease\n",
      "625 uneasily\n",
      "626 uneasiness\n",
      "627 uneasy\n",
      "628 unemotional\n",
      "629 unenthusiastic\n",
      "630 unexcited\n",
      "631 unexpected\n",
      "632 unfamiliar\n",
      "633 unfathomable\n",
      "634 unfazed\n",
      "635 unfeeling\n",
      "636 unfocused\n",
      "637 unforeseen\n",
      "638 unforgiving\n",
      "639 unforthcoming\n",
      "640 unfortunate\n",
      "641 unfriendly\n",
      "642 unhappy\n",
      "643 unhinged\n",
      "644 unimpressed\n",
      "645 uninformed\n",
      "646 uninspired\n",
      "647 uninterested\n",
      "648 uninvolved\n",
      "649 unique\n",
      "650 unlikeable\n",
      "651 unmoved\n",
      "652 unnerved\n",
      "653 unpleasant\n",
      "654 unprepared\n",
      "655 unquiet\n",
      "656 unreactive\n",
      "657 unresolved\n",
      "658 unrestrained\n",
      "659 unruffled\n",
      "660 unsatisfied\n",
      "661 unsettled\n",
      "662 unsociable\n",
      "663 unspeaking\n",
      "664 unspoken\n",
      "665 unstrung\n",
      "666 unsuccessful\n",
      "667 unsure\n",
      "668 unsurprised\n",
      "669 unsuspecting\n",
      "670 unswayed\n",
      "671 unsympathetic\n",
      "672 untouched\n",
      "673 untroubled\n",
      "674 untrusting\n",
      "675 unwanted\n",
      "676 unwavering\n",
      "677 unwelcoming\n",
      "678 unwell\n",
      "679 unwilling\n",
      "680 unyielding\n",
      "681 upbeat\n",
      "682 uplifting\n",
      "683 uppity\n",
      "684 upset\n",
      "685 uptight\n",
      "686 useless\n",
      "687 vacant\n",
      "688 vacuous\n",
      "689 vanquished\n",
      "690 vehement\n",
      "691 vengeful\n",
      "692 venomous\n",
      "693 vex\n",
      "694 vexation\n",
      "695 vexed\n",
      "696 vicious\n",
      "697 victorious\n",
      "698 vigilant\n",
      "699 vile\n",
      "700 villainous\n",
      "701 vindictive\n",
      "702 violence\n",
      "703 violent\n",
      "704 viperous\n",
      "705 vituperative\n",
      "706 vocal\n",
      "707 vocalized\n",
      "708 vulgar\n",
      "709 vulnerability\n",
      "710 vulnerable\n",
      "711 wacky\n",
      "712 waiting\n",
      "713 wanted\n",
      "714 wanting\n",
      "715 wanton\n",
      "716 wariness\n",
      "717 warm\n",
      "718 wary\n",
      "719 wasted\n",
      "720 watch\n",
      "721 watchful\n",
      "722 watching\n",
      "723 wavering\n",
      "724 weariness\n",
      "725 weary\n",
      "726 weeping\n",
      "727 weird\n",
      "728 welcome\n",
      "729 welcoming\n",
      "730 whatever\n",
      "731 whimpering\n",
      "732 whimsical\n",
      "733 whisper\n",
      "734 whistle\n",
      "735 white\n",
      "736 wicked\n",
      "737 wild\n",
      "738 willful\n",
      "739 willing\n",
      "740 wily\n",
      "741 wink\n",
      "742 wired\n",
      "743 wishful\n",
      "744 wistful\n",
      "745 wistfully\n",
      "746 withdraw\n",
      "747 withdrawn\n",
      "748 withheld\n",
      "749 withholding\n",
      "750 woe\n",
      "751 woeful\n",
      "752 wonder\n",
      "753 wondering\n",
      "754 wonderment\n",
      "755 wooly\n",
      "756 woozy\n",
      "757 worn\n",
      "758 worried\n",
      "759 worrisome\n",
      "760 worry\n",
      "761 worrying\n",
      "762 worryingly\n",
      "763 wounded\n",
      "764 wow\n",
      "765 wrathful\n",
      "766 wrathfully\n",
      "767 wrecked\n",
      "768 wretched\n",
      "769 wronged\n",
      "770 wroth\n",
      "771 wry\n",
      "772 yawn\n",
      "773 yawning\n",
      "774 yearning\n",
      "775 yell\n",
      "776 yelling\n",
      "777 yielding\n",
      "778 yuck\n",
      "779 zany\n",
      "780 zealous\n",
      "781 zen\n",
      "782 zoned\n"
     ]
    }
   ],
   "source": [
    "v_indices = (102, 145, 309, 316, 410, 698)\n",
    "\n",
    "start = 12\n",
    "sections = []\n",
    "for end in v_indices:\n",
    "    for i in range(start, end):\n",
    "#         print(i)\n",
    "        sections.append(vocab[i])\n",
    "        start = end + 1\n",
    "for i in range(start, len(vocab)):\n",
    "    sections.append(vocab[i])\n",
    "#     print(i)\n",
    "\n",
    "for i, v in enumerate(sections):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity at the last layer:  0.91\n",
      "Vector similarity at the embed layer:  1.00\n",
      "Vector similarity at the embed layer:  0.89\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cosine similarity between the word disgusted \n",
    "# in \"disgusted expression\" vs \"too disgusted\" (different contexts)\n",
    "# using the last layer of the model.\n",
    "diff_disgusted_last = 1 - cosine(token_vecs_last[4], token_vecs_last[21])\n",
    "\n",
    "# Calculate the cosine similarity between the word disgusted \n",
    "# in \"disgusted expression\" vs \"too disgusted\" (different contexts)\n",
    "# using the embed layer of the model.\n",
    "diff_disgusted_embed = 1 - cosine(token_vecs_embed[4], token_vecs_embed[21])\n",
    "\n",
    "# Calculate the cosine similarity between the word disgusted \n",
    "# in \"disgusted expression\" vs \"too disgusted\" (different contexts)\n",
    "# using the sum of the last 4 layers of the model.\n",
    "diff_disgusted_sum = 1 - cosine(token_vecs_sum_last_four[4], token_vecs_sum_last_four[21])\n",
    "\n",
    "print('Vector similarity at the last layer:  %.2f' % diff_disgusted_last)\n",
    "print('Vector similarity at the embed layer:  %.2f' % diff_disgusted_embed)\n",
    "print('Vector similarity at the embed layer:  %.2f' % diff_disgusted_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################ END TESTING ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################ START PRODUCTION: GET CONTEXTUAL EMBEDDINGS ##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('./output_CC-ab/')\n",
    "config = RobertaConfig.from_pretrained('./output_CC-ab/')\n",
    "model = RobertaForMaskedLM.from_pretrained('./output_CC-ab/', config=config)\n",
    "# Outputting hidden states allows direct access to hidden layers of the model.\n",
    "# Outputting hidden states must be set to \"true\" in the config file during fine-tuning.\n",
    "# config.output_hidden_states = True\n",
    "model.eval()\n",
    "\n",
    "context_file = \"/home/jupyter/Notebooks/crystal/NLP/transformers/examples/CC_WET_test_ab\"\n",
    "output_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_lastfour_CC_ab_testvocab.txt'\n",
    "count_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_lastfour_CC_ab_testvocab_counts.txt'\n",
    "# vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/data/vocab_files/vocab_checked.txt'\n",
    "# vocab = make_vocab(vocab_file)\n",
    "vocab = [\"car\", \"fluent\", \"donut\", \"triangular\", \"jumping\", \"forever\"]\n",
    "\n",
    "FEATURE_COUNT = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "car\n",
      "\n",
      "Instance 1 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 11: [-0.11239327490329742, 2.276723623275757, 0.23585066199302673, -0.7831698656082153, -0.4414422810077667]\n",
      "Grand sum of 1 tensor sets is: [-0.11239327490329742, 2.276723623275757, 0.23585066199302673, -0.7831698656082153, -0.4414422810077667]\n",
      "\n",
      "Instance 2 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 20: [0.2089410126209259, 1.297297716140747, -1.2435903549194336, -0.076203852891922, -0.047389835119247437]\n",
      "Grand sum of 2 tensor sets is: [0.09654773771762848, 3.574021339416504, -1.0077396631240845, -0.8593736886978149, -0.48883211612701416]\n",
      "\n",
      "Instance 3 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 17: [0.11075587570667267, 1.4980964660644531, 0.18493768572807312, -2.02364182472229, -2.800924301147461]\n",
      "Grand sum of 3 tensor sets is: [0.20730361342430115, 5.072117805480957, -0.822801947593689, -2.8830156326293945, -3.2897562980651855]\n",
      "\n",
      "Instance 4 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [83]\n",
      "Size of token embeddings is torch.Size([146, 13, 768])\n",
      "Shape of summed layers is: 146 x 768\n",
      "car at index 83: [0.7842099666595459, -0.8667717576026917, 1.303114891052246, 0.48844218254089355, 1.3442045450210571]\n",
      "Grand sum of 4 tensor sets is: [0.9915136098861694, 4.20534610748291, 0.48031294345855713, -2.394573450088501, -1.9455517530441284]\n",
      "\n",
      "Instance 5 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "car at index 43: [-0.11025874316692352, 2.4198193550109863, -1.1462124586105347, -2.8966169357299805, 2.185056209564209]\n",
      "Grand sum of 5 tensor sets is: [0.8812548518180847, 6.6251654624938965, -0.6658995151519775, -5.291190147399902, 0.23950445652008057]\n",
      "\n",
      "Instance 6 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 7 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 2: [0.5054959654808044, 1.3924981355667114, 0.44301319122314453, -1.3699108362197876, 0.6656923890113831]\n",
      "Grand sum of 6 tensor sets is: [1.3867508172988892, 8.017663955688477, -0.222886323928833, -6.6611008644104, 0.9051968455314636]\n",
      "\n",
      "Instance 8 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 5: [-0.45572641491889954, 0.2086430788040161, -0.6527961492538452, 0.24876725673675537, -1.6597230434417725]\n",
      "Grand sum of 7 tensor sets is: [0.931024432182312, 8.226306915283203, -0.8756824731826782, -6.4123334884643555, -0.7545261979103088]\n",
      "\n",
      "Instance 9 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 10 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 14: [1.2491605281829834, 2.0161969661712646, -0.2929166257381439, -0.5496404767036438, 0.23802223801612854]\n",
      "Grand sum of 8 tensor sets is: [2.180184841156006, 10.242504119873047, -1.1685991287231445, -6.961974143981934, -0.5165039300918579]\n",
      "\n",
      "Instance 11 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 6: [0.1500711441040039, 0.16214600205421448, -1.2416856288909912, -0.6172637939453125, -2.415762186050415]\n",
      "Grand sum of 9 tensor sets is: [2.3302559852600098, 10.40464973449707, -2.4102847576141357, -7.579237937927246, -2.9322662353515625]\n",
      "\n",
      "Instance 12 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [61]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "car at index 61: [0.26709651947021484, 2.372762680053711, -0.29672813415527344, -0.06655357778072357, 0.993546724319458]\n",
      "Grand sum of 10 tensor sets is: [2.5973525047302246, 12.777412414550781, -2.707012891769409, -7.645791530609131, -1.9387195110321045]\n",
      "\n",
      "Instance 13 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 10: [0.25111931562423706, 0.820430338382721, -0.019425280392169952, 1.301325798034668, 0.9744128584861755]\n",
      "Grand sum of 11 tensor sets is: [2.8484718799591064, 13.597843170166016, -2.726438283920288, -6.344465732574463, -0.964306652545929]\n",
      "\n",
      "Instance 14 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 15 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 9: [1.0898689031600952, 1.706153154373169, 0.6870834231376648, -0.3266046643257141, 0.030456721782684326]\n",
      "Grand sum of 12 tensor sets is: [3.938340663909912, 15.303996086120605, -2.0393548011779785, -6.671070575714111, -0.9338499307632446]\n",
      "\n",
      "Instance 16 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 21: [0.4324463903903961, 2.4772047996520996, -0.24498251080513, -0.636415421962738, -0.021625563502311707]\n",
      "Grand sum of 13 tensor sets is: [4.370787143707275, 17.781200408935547, -2.284337282180786, -7.307486057281494, -0.9554755091667175]\n",
      "\n",
      "Instance 17 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 23: [-0.6425123810768127, 0.9266269207000732, -0.15669681131839752, -1.3772735595703125, -0.8364338278770447]\n",
      "Grand sum of 14 tensor sets is: [3.7282748222351074, 18.707826614379883, -2.4410340785980225, -8.684759140014648, -1.7919093370437622]\n",
      "\n",
      "Instance 18 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 19 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 19: [-0.40588322281837463, 2.7134175300598145, 0.06640737503767014, 0.6818013787269592, -3.339782476425171]\n",
      "Grand sum of 15 tensor sets is: [3.3223915100097656, 21.42124366760254, -2.374626636505127, -8.002957344055176, -5.131691932678223]\n",
      "\n",
      "Instance 20 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 7: [0.3790583312511444, 1.4943478107452393, -1.158699870109558, -0.36944854259490967, -0.6350803375244141]\n",
      "Grand sum of 16 tensor sets is: [3.7014498710632324, 22.915592193603516, -3.5333266258239746, -8.372406005859375, -5.766772270202637]\n",
      "\n",
      "Instance 21 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 10: [0.05521337687969208, 1.866254210472107, -0.59792160987854, -0.010453209280967712, 0.7434837222099304]\n",
      "Grand sum of 17 tensor sets is: [3.7566633224487305, 24.78184700012207, -4.131248474121094, -8.382859230041504, -5.023288726806641]\n",
      "\n",
      "Instance 22 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 21: [0.2573581337928772, 2.7988452911376953, 1.4782519340515137, -1.6699291467666626, -3.2416141033172607]\n",
      "Grand sum of 18 tensor sets is: [4.014021396636963, 27.580692291259766, -2.65299654006958, -10.052788734436035, -8.26490306854248]\n",
      "\n",
      "Instance 23 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 4: [-1.4736682176589966, 0.7163794040679932, 0.42129194736480713, -0.9189162254333496, 0.8461757302284241]\n",
      "Grand sum of 19 tensor sets is: [2.540353298187256, 28.29707145690918, -2.2317047119140625, -10.971704483032227, -7.418727397918701]\n",
      "\n",
      "Instance 24 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([106, 13, 768])\n",
      "Shape of summed layers is: 106 x 768\n",
      "car at index 28: [1.2835642099380493, 1.0744446516036987, -0.31154027581214905, 1.350542426109314, 4.17838716506958]\n",
      "Grand sum of 20 tensor sets is: [3.8239173889160156, 29.37151527404785, -2.5432450771331787, -9.621162414550781, -3.240340232849121]\n",
      "\n",
      "Instance 25 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 7: [0.5372360348701477, 1.7701480388641357, -0.1351802498102188, 1.2869988679885864, -1.8224024772644043]\n",
      "Grand sum of 21 tensor sets is: [4.361153602600098, 31.14166259765625, -2.6784253120422363, -8.334163665771484, -5.062742710113525]\n",
      "\n",
      "Instance 26 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 16: [0.20254574716091156, 0.3577185273170471, 0.15442126989364624, -2.2251803874969482, -1.3704707622528076]\n",
      "Grand sum of 22 tensor sets is: [4.563699245452881, 31.49938201904297, -2.5240039825439453, -10.559344291687012, -6.433213233947754]\n",
      "\n",
      "Instance 27 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 9: [-0.3140302896499634, 3.9240407943725586, 0.9464907646179199, -0.40138566493988037, -4.314627170562744]\n",
      "Grand sum of 23 tensor sets is: [4.249669075012207, 35.423423767089844, -1.5775132179260254, -10.960729598999023, -10.747840881347656]\n",
      "\n",
      "Instance 28 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 29 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 30 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18, 36, 39]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 18: [1.008553147315979, 0.7180819511413574, -0.4834301769733429, 0.8011695146560669, -1.229126214981079]\n",
      "car at index 36: [0.4757181406021118, 0.12569986283779144, -0.8739262819290161, 1.0634573698043823, -2.5049126148223877]\n",
      "car at index 39: [0.8404521346092224, -0.006466269493103027, 0.984977126121521, 0.03675761818885803, 4.1455888748168945]\n",
      "Grand sum of 24 tensor sets is: [5.0245771408081055, 35.70252990722656, -1.7016396522521973, -10.326934814453125, -10.610657691955566]\n",
      "\n",
      "Instance 31 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8, 35]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 8: [0.6396079659461975, 1.5702219009399414, -0.8651847243309021, -3.3535349369049072, -0.9886137247085571]\n",
      "car at index 35: [-0.4109412431716919, 3.0204410552978516, -0.43902871012687683, -2.186741828918457, -3.1406302452087402]\n",
      "Grand sum of 25 tensor sets is: [5.138910293579102, 37.997859954833984, -2.3537464141845703, -13.09707260131836, -12.67527961730957]\n",
      "\n",
      "Instance 32 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 18: [0.5172147750854492, 2.464460611343384, -0.7779809236526489, -1.0772387981414795, -0.591015100479126]\n",
      "Grand sum of 26 tensor sets is: [5.656125068664551, 40.46232223510742, -3.1317272186279297, -14.174311637878418, -13.266294479370117]\n",
      "\n",
      "Instance 33 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 34 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 35 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 32: [0.26205629110336304, 2.021803140640259, 0.8377887606620789, 1.4949498176574707, -1.6222968101501465]\n",
      "Grand sum of 27 tensor sets is: [5.918181419372559, 42.484127044677734, -2.293938398361206, -12.679361343383789, -14.888591766357422]\n",
      "\n",
      "Instance 36 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 25: [0.3377879559993744, 1.916154146194458, -0.3702370524406433, -1.0954673290252686, -1.0930280685424805]\n",
      "Grand sum of 28 tensor sets is: [6.255969524383545, 44.4002799987793, -2.664175510406494, -13.774828910827637, -15.981619834899902]\n",
      "\n",
      "Instance 37 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([94, 13, 768])\n",
      "Shape of summed layers is: 94 x 768\n",
      "car at index 22: [2.7189714908599854, -1.3484264612197876, 0.040259674191474915, -0.3770224153995514, 0.37548041343688965]\n",
      "Grand sum of 29 tensor sets is: [8.97494125366211, 43.05185317993164, -2.623915910720825, -14.151851654052734, -15.606139183044434]\n",
      "\n",
      "Instance 38 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 30 tensor sets is: [11.092596054077148, 44.72823715209961, -3.8581936359405518, -15.817374229431152, -15.750641822814941]\n",
      "\n",
      "Instance 39 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 5: [0.5025638937950134, 1.1802399158477783, -0.20999222993850708, -1.2621824741363525, 0.6906836032867432]\n",
      "Grand sum of 31 tensor sets is: [11.595159530639648, 45.908477783203125, -4.068185806274414, -17.079557418823242, -15.059958457946777]\n",
      "\n",
      "Instance 40 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 4: [0.8758795261383057, 0.34384721517562866, -0.1533660888671875, 0.16642780601978302, 0.11296439170837402]\n",
      "Grand sum of 32 tensor sets is: [12.471038818359375, 46.252323150634766, -4.221551895141602, -16.913129806518555, -14.946993827819824]\n",
      "\n",
      "Instance 41 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 23: [0.4382396340370178, 1.9211881160736084, -0.1499175727367401, 0.478835791349411, -1.6148945093154907]\n",
      "Grand sum of 33 tensor sets is: [12.909278869628906, 48.17351150512695, -4.371469497680664, -16.434293746948242, -16.561887741088867]\n",
      "\n",
      "Instance 42 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 13: [-0.8019858598709106, 2.874577045440674, 1.0172337293624878, 0.36889779567718506, -0.7132115960121155]\n",
      "Grand sum of 34 tensor sets is: [12.107293128967285, 51.04808807373047, -3.3542356491088867, -16.06539535522461, -17.27509880065918]\n",
      "\n",
      "Instance 43 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "car at index 17: [1.458144187927246, 0.9081264138221741, -1.2463363409042358, -0.5547851324081421, 1.5847339630126953]\n",
      "Grand sum of 35 tensor sets is: [13.565437316894531, 51.956214904785156, -4.600572109222412, -16.620180130004883, -15.690364837646484]\n",
      "\n",
      "Instance 44 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 7: [0.2967076301574707, 2.24153470993042, -0.02024250477552414, -1.562117099761963, -0.9566192626953125]\n",
      "Grand sum of 36 tensor sets is: [13.862144470214844, 54.197750091552734, -4.620814800262451, -18.182296752929688, -16.646984100341797]\n",
      "\n",
      "Instance 45 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 18: [0.4174315929412842, 2.316634178161621, -1.0639362335205078, 0.7535429000854492, -0.5622205138206482]\n",
      "Grand sum of 37 tensor sets is: [14.279576301574707, 56.51438522338867, -5.684751033782959, -17.428752899169922, -17.209203720092773]\n",
      "\n",
      "Instance 46 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 9: [-0.4383270740509033, 2.905641555786133, -0.8354190587997437, -2.3573451042175293, 0.07224510610103607]\n",
      "Grand sum of 38 tensor sets is: [13.841249465942383, 59.42002868652344, -6.520170211791992, -19.78609848022461, -17.136959075927734]\n",
      "\n",
      "Instance 47 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20, 28]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 20: [0.14655718207359314, 1.8203628063201904, -0.141265869140625, 0.4922492206096649, -0.34129881858825684]\n",
      "car at index 28: [0.16529814898967743, 2.3381991386413574, 1.2471718788146973, -1.610947608947754, -1.395195484161377]\n",
      "Grand sum of 39 tensor sets is: [13.997177124023438, 61.49930953979492, -5.967217445373535, -20.345447540283203, -18.005207061767578]\n",
      "\n",
      "Instance 48 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 40: [1.234213948249817, 2.063514232635498, 0.37098973989486694, 0.4424876272678375, -1.024994134902954]\n",
      "Grand sum of 40 tensor sets is: [15.231390953063965, 63.56282424926758, -5.596227645874023, -19.9029598236084, -19.030200958251953]\n",
      "\n",
      "Instance 49 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 6: [0.9459009766578674, 0.845543384552002, -0.6645747423171997, -0.20815183222293854, -1.8452147245407104]\n",
      "Grand sum of 41 tensor sets is: [16.177291870117188, 64.40837097167969, -6.260802268981934, -20.111112594604492, -20.875415802001953]\n",
      "\n",
      "Instance 50 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 13: [0.7335004806518555, 2.850229263305664, -0.6034294366836548, 0.4422808885574341, 0.5380503535270691]\n",
      "Grand sum of 42 tensor sets is: [16.91079330444336, 67.25859832763672, -6.864231586456299, -19.66883087158203, -20.337366104125977]\n",
      "\n",
      "Instance 51 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 18: [-0.6860682964324951, 0.691684365272522, -0.47195255756378174, 3.4746522903442383, -1.68611478805542]\n",
      "Grand sum of 43 tensor sets is: [16.2247257232666, 67.95027923583984, -7.336184024810791, -16.19417953491211, -22.023481369018555]\n",
      "\n",
      "Instance 52 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 8: [0.35196545720100403, 2.9589993953704834, -0.40783417224884033, 0.894097626209259, -2.5660059452056885]\n",
      "Grand sum of 44 tensor sets is: [16.576690673828125, 70.9092788696289, -7.744018077850342, -15.300082206726074, -24.589487075805664]\n",
      "\n",
      "Instance 53 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17, 23]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 17: [0.7382417917251587, 2.378593921661377, -1.007306694984436, -0.027425456792116165, -0.38694238662719727]\n",
      "car at index 23: [0.041699282824993134, 1.2528116703033447, -0.4462926685810089, -1.798431396484375, 1.201312780380249]\n",
      "Grand sum of 45 tensor sets is: [16.96666145324707, 72.72498321533203, -8.470817565917969, -16.213010787963867, -24.182302474975586]\n",
      "\n",
      "Instance 54 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 55 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 9: [0.19558745622634888, 2.6938438415527344, -0.5749129056930542, 0.7013193964958191, -0.14200572669506073]\n",
      "Grand sum of 46 tensor sets is: [17.162248611450195, 75.4188232421875, -9.045730590820312, -15.511691093444824, -24.324308395385742]\n",
      "\n",
      "Instance 56 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 26: [0.6547094583511353, 0.22935807704925537, 0.38975009322166443, 1.746890902519226, -0.84079909324646]\n",
      "Grand sum of 47 tensor sets is: [17.816957473754883, 75.64817810058594, -8.655980110168457, -13.764800071716309, -25.16510772705078]\n",
      "\n",
      "Instance 57 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 6: [0.5138108730316162, 0.1688801497220993, -0.8725735545158386, 0.47663289308547974, 2.0569000244140625]\n",
      "Grand sum of 48 tensor sets is: [18.330768585205078, 75.81705474853516, -9.52855396270752, -13.288166999816895, -23.10820770263672]\n",
      "\n",
      "Instance 58 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 27: [0.6397938132286072, 0.445493221282959, -0.43330395221710205, -0.6994163393974304, 0.0982658639550209]\n",
      "Grand sum of 49 tensor sets is: [18.970561981201172, 76.2625503540039, -9.961857795715332, -13.98758316040039, -23.00994110107422]\n",
      "\n",
      "Instance 59 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 60 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 4: [0.4031905233860016, 0.7604975700378418, 0.38608723878860474, -0.27984464168548584, -1.0814388990402222]\n",
      "Grand sum of 50 tensor sets is: [19.37375259399414, 77.0230484008789, -9.575770378112793, -14.267427444458008, -24.091379165649414]\n",
      "\n",
      "Instance 61 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 29: [-0.030980184674263, 1.941820502281189, -0.2385808527469635, 0.3305789530277252, -3.5557847023010254]\n",
      "Grand sum of 51 tensor sets is: [19.342771530151367, 78.9648666381836, -9.814351081848145, -13.936848640441895, -27.64716339111328]\n",
      "\n",
      "Instance 62 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 22: [-0.06868985295295715, 1.5429240465164185, -0.5760259628295898, -0.5686459541320801, 0.6342799067497253]\n",
      "Grand sum of 52 tensor sets is: [19.27408218383789, 80.5077896118164, -10.390377044677734, -14.505495071411133, -27.01288414001465]\n",
      "\n",
      "Instance 63 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 33: [-0.3242695927619934, 1.8972482681274414, -0.1609637290239334, -1.3059234619140625, -1.430038571357727]\n",
      "Grand sum of 53 tensor sets is: [18.949811935424805, 82.40503692626953, -10.55134105682373, -15.811418533325195, -28.442922592163086]\n",
      "\n",
      "Instance 64 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 20: [0.9009941220283508, 2.0467259883880615, -1.096698522567749, -0.032943274825811386, 0.12340392172336578]\n",
      "Grand sum of 54 tensor sets is: [19.850805282592773, 84.4517593383789, -11.648039817810059, -15.844362258911133, -28.31951904296875]\n",
      "\n",
      "Instance 65 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 8: [-0.26790884137153625, 0.4652869701385498, 0.7330657243728638, 0.2546887993812561, -2.0070528984069824]\n",
      "Grand sum of 55 tensor sets is: [19.582897186279297, 84.91704559326172, -10.914974212646484, -15.589673042297363, -30.32657241821289]\n",
      "\n",
      "Instance 66 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 67 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [84]\n",
      "Size of token embeddings is torch.Size([264, 13, 768])\n",
      "Shape of summed layers is: 264 x 768\n",
      "car at index 84: [1.1442819833755493, 2.831942558288574, -0.42596670985221863, 0.3354288935661316, -2.7719569206237793]\n",
      "Grand sum of 56 tensor sets is: [20.7271785736084, 87.74898529052734, -11.340940475463867, -15.254243850708008, -33.09852981567383]\n",
      "\n",
      "Instance 68 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 13: [0.8997526168823242, 1.5337984561920166, -0.7024408578872681, -0.4511600732803345, -0.23417145013809204]\n",
      "Grand sum of 57 tensor sets is: [21.626930236816406, 89.28278350830078, -12.043381690979004, -15.705404281616211, -33.33270263671875]\n",
      "\n",
      "Instance 69 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 17: [-0.5908451080322266, 4.832721710205078, 0.15654818713665009, 2.381490707397461, -1.1117761135101318]\n",
      "Grand sum of 58 tensor sets is: [21.03608512878418, 94.11550903320312, -11.886833190917969, -13.32391357421875, -34.44447708129883]\n",
      "\n",
      "Instance 70 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 44: [0.15537072718143463, 3.111154079437256, -1.0361799001693726, -0.11480734497308731, 0.11322516202926636]\n",
      "Grand sum of 59 tensor sets is: [21.191455841064453, 97.2266616821289, -12.923012733459473, -13.438720703125, -34.33125305175781]\n",
      "\n",
      "Instance 71 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 72 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 7: [1.121520757675171, 1.4542300701141357, -0.1405198574066162, -0.6225812435150146, 0.8777378797531128]\n",
      "Grand sum of 60 tensor sets is: [22.312976837158203, 98.68089294433594, -13.063532829284668, -14.061302185058594, -33.453514099121094]\n",
      "\n",
      "Instance 73 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8, 20]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 8: [0.11397308111190796, 2.0101513862609863, -0.9423019886016846, -0.4406611919403076, 1.5107927322387695]\n",
      "car at index 20: [-0.18740695714950562, 1.484227180480957, -1.0092941522598267, 0.364944726228714, -1.3309000730514526]\n",
      "Grand sum of 61 tensor sets is: [22.276260375976562, 100.42808532714844, -14.03933048248291, -14.099160194396973, -33.36356735229492]\n",
      "\n",
      "Instance 74 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 9: [0.036373063921928406, 2.2592785358428955, -0.7512549757957458, -2.8557443618774414, -0.8759498596191406]\n",
      "Grand sum of 62 tensor sets is: [22.312633514404297, 102.68736267089844, -14.7905855178833, -16.954904556274414, -34.23951721191406]\n",
      "\n",
      "Instance 75 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 6: [1.1078126430511475, 2.3231468200683594, -0.16220815479755402, -0.1313980221748352, -2.806844711303711]\n",
      "Grand sum of 63 tensor sets is: [23.420446395874023, 105.01051330566406, -14.952794075012207, -17.086301803588867, -37.046363830566406]\n",
      "\n",
      "Instance 76 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 77 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "car at index 43: [-0.23366211354732513, 2.2071590423583984, -0.04314441606402397, -2.769869327545166, -1.7054595947265625]\n",
      "Grand sum of 64 tensor sets is: [23.186784744262695, 107.2176742553711, -14.995938301086426, -19.856170654296875, -38.75182342529297]\n",
      "\n",
      "Instance 78 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 10: [0.43675678968429565, 2.347930669784546, 0.7976111173629761, -1.9688007831573486, -0.537100613117218]\n",
      "Grand sum of 65 tensor sets is: [23.6235408782959, 109.56560516357422, -14.19832706451416, -21.82497215270996, -39.28892517089844]\n",
      "\n",
      "Instance 79 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "car at index 40: [-0.5342097282409668, 3.1651976108551025, -0.1682455986738205, -0.22710055112838745, 0.07186047732830048]\n",
      "Grand sum of 66 tensor sets is: [23.089330673217773, 112.73080444335938, -14.366572380065918, -22.052072525024414, -39.217063903808594]\n",
      "\n",
      "Instance 80 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 12: [0.5828202962875366, 2.6147844791412354, 0.060376062989234924, 1.5881550312042236, -1.8878728151321411]\n",
      "Grand sum of 67 tensor sets is: [23.672151565551758, 115.34558868408203, -14.306196212768555, -20.463916778564453, -41.10493850708008]\n",
      "\n",
      "Instance 81 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 5: [0.6426032781600952, 1.541120171546936, -0.2828267812728882, 0.9673961400985718, 1.7389025688171387]\n",
      "Grand sum of 68 tensor sets is: [24.314754486083984, 116.88671112060547, -14.589022636413574, -19.49652099609375, -39.36603546142578]\n",
      "\n",
      "Instance 82 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 19: [0.15587365627288818, 3.1357059478759766, -0.00866977870464325, -0.8203450441360474, -1.6884373426437378]\n",
      "Grand sum of 69 tensor sets is: [24.47062873840332, 120.02241516113281, -14.597692489624023, -20.316865921020508, -41.054473876953125]\n",
      "\n",
      "Instance 83 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 15: [0.664270281791687, -0.6938228011131287, -0.7661392688751221, -1.3910194635391235, 3.0353846549987793]\n",
      "Grand sum of 70 tensor sets is: [25.134899139404297, 119.3285903930664, -15.363831520080566, -21.7078857421875, -38.01908874511719]\n",
      "\n",
      "Instance 84 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 85 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8, 60]\n",
      "Size of token embeddings is torch.Size([70, 13, 768])\n",
      "Shape of summed layers is: 70 x 768\n",
      "car at index 8: [0.17665496468544006, 1.9775595664978027, 0.06169038265943527, 1.0628021955490112, 1.1390225887298584]\n",
      "car at index 60: [-0.45067456364631653, 2.848512887954712, -1.5771485567092896, 0.9049484133720398, 0.9934552907943726]\n",
      "Grand sum of 71 tensor sets is: [24.997888565063477, 121.74162292480469, -16.12156105041504, -20.724010467529297, -36.952850341796875]\n",
      "\n",
      "Instance 86 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 72 tensor sets is: [27.115543365478516, 123.41800689697266, -17.355838775634766, -22.38953399658203, -37.097354888916016]\n",
      "\n",
      "Instance 87 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 2: [1.299675464630127, 0.8584426641464233, -0.2777252793312073, -0.5020190477371216, 0.7617824077606201]\n",
      "Grand sum of 73 tensor sets is: [28.415218353271484, 124.27645111083984, -17.633563995361328, -22.89155387878418, -36.3355712890625]\n",
      "\n",
      "Instance 88 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([167, 13, 768])\n",
      "Shape of summed layers is: 167 x 768\n",
      "car at index 36: [1.3584723472595215, 0.7657484412193298, -0.06704919040203094, 0.21383538842201233, -1.4879825115203857]\n",
      "Grand sum of 74 tensor sets is: [29.773691177368164, 125.04219818115234, -17.700613021850586, -22.677719116210938, -37.82355499267578]\n",
      "\n",
      "Instance 89 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 90 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 91 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 92 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 14: [1.0867626667022705, 2.23476505279541, 0.5281102657318115, 0.2604995667934418, -1.7726504802703857]\n",
      "Grand sum of 75 tensor sets is: [30.860454559326172, 127.27696228027344, -17.172502517700195, -22.417219161987305, -39.59620666503906]\n",
      "\n",
      "Instance 93 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 4: [0.4298901855945587, 2.964761972427368, 1.0209167003631592, -0.6102659106254578, -1.23263680934906]\n",
      "Grand sum of 76 tensor sets is: [31.29034423828125, 130.24172973632812, -16.151586532592773, -23.027484893798828, -40.82884216308594]\n",
      "\n",
      "Instance 94 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 25: [0.5480866432189941, 2.3816077709198, -0.954827070236206, -0.2196853905916214, 1.3735555410385132]\n",
      "Grand sum of 77 tensor sets is: [31.838430404663086, 132.6233367919922, -17.106412887573242, -23.247169494628906, -39.45528793334961]\n",
      "\n",
      "Instance 95 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([86, 13, 768])\n",
      "Shape of summed layers is: 86 x 768\n",
      "car at index 6: [0.1742737889289856, -1.8250946998596191, 0.6313999891281128, -0.14637938141822815, -1.7558317184448242]\n",
      "Grand sum of 78 tensor sets is: [32.01270294189453, 130.79824829101562, -16.475013732910156, -23.3935489654541, -41.21112060546875]\n",
      "\n",
      "Instance 96 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 97 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([66, 13, 768])\n",
      "Shape of summed layers is: 66 x 768\n",
      "car at index 22: [0.6943637132644653, -0.8089115619659424, -0.5972657203674316, 2.2903943061828613, 4.974179267883301]\n",
      "Grand sum of 79 tensor sets is: [32.70706558227539, 129.9893341064453, -17.07227897644043, -21.1031551361084, -36.236942291259766]\n",
      "\n",
      "Instance 98 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 5: [0.010226532816886902, 2.8816583156585693, -0.1372315138578415, -1.7631304264068604, -0.4536413550376892]\n",
      "Grand sum of 80 tensor sets is: [32.71729278564453, 132.87098693847656, -17.209510803222656, -22.86628532409668, -36.690582275390625]\n",
      "\n",
      "Instance 99 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 24: [0.19029879570007324, 0.5861178040504456, -0.9271973967552185, 0.6874502897262573, 2.9167819023132324]\n",
      "Grand sum of 81 tensor sets is: [32.9075927734375, 133.4571075439453, -18.136707305908203, -22.178834915161133, -33.773799896240234]\n",
      "\n",
      "Instance 100 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17, 20, 25, 30]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "car at index 17: [0.7242787480354309, 1.222398042678833, -0.19793209433555603, -1.2858877182006836, -2.2380378246307373]\n",
      "car at index 20: [0.4404619336128235, 1.8278440237045288, 0.2716705799102783, -0.8880179524421692, -1.6394338607788086]\n",
      "car at index 25: [0.02067974954843521, 1.470453143119812, 0.07614052295684814, -1.6293517351150513, -2.8649094104766846]\n",
      "car at index 30: [0.08886465430259705, 1.290534257888794, 0.08438226580619812, -1.7489922046661377, -2.898124933242798]\n",
      "Grand sum of 82 tensor sets is: [33.226165771484375, 134.909912109375, -18.078142166137695, -23.566898345947266, -36.18392562866211]\n",
      "\n",
      "Instance 101 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 24: [-0.4194004535675049, 1.2174780368804932, -0.4400056004524231, -1.783496618270874, -2.2415361404418945]\n",
      "Grand sum of 83 tensor sets is: [32.806766510009766, 136.1273956298828, -18.51814842224121, -25.35039520263672, -38.42546081542969]\n",
      "\n",
      "Instance 102 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6, 15, 24]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 6: [0.4081728458404541, 1.0373289585113525, 0.5662714242935181, -1.3829247951507568, -0.9489670395851135]\n",
      "car at index 15: [0.2661207318305969, 0.8174206018447876, 1.4055464267730713, -1.9509587287902832, -2.1385183334350586]\n",
      "car at index 24: [-0.11577357351779938, 1.2314231395721436, 1.1801384687423706, -2.0750014781951904, -0.961125373840332]\n",
      "Grand sum of 84 tensor sets is: [32.99293899536133, 137.15611267089844, -17.467496871948242, -27.153356552124023, -39.77499771118164]\n",
      "\n",
      "Instance 103 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 11, 15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 7: [0.5483179688453674, 2.11129093170166, -1.1556273698806763, -2.7887632846832275, -1.826869249343872]\n",
      "car at index 11: [0.5158922672271729, 2.1187961101531982, -0.7911282181739807, -3.164207696914673, -2.5074472427368164]\n",
      "car at index 15: [1.1938906908035278, 1.4278596639633179, -1.3348671197891235, -1.0347094535827637, -1.031449317932129]\n",
      "Grand sum of 85 tensor sets is: [33.74563980102539, 139.04209899902344, -18.561370849609375, -29.48258399963379, -41.5635871887207]\n",
      "\n",
      "Instance 104 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 11: [0.5848574638366699, 2.697166919708252, -0.5176512598991394, -1.2254154682159424, -1.2176148891448975]\n",
      "Grand sum of 86 tensor sets is: [34.33049774169922, 141.73927307128906, -19.079021453857422, -30.70800018310547, -42.78120040893555]\n",
      "\n",
      "Instance 105 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4, 13, 22]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 4: [-0.08881567418575287, 0.9793773889541626, 0.06227175146341324, -1.6487079858779907, 0.14078208804130554]\n",
      "car at index 13: [-0.24339933693408966, 1.40309476852417, 0.6184512376785278, 0.7095543146133423, -1.9213097095489502]\n",
      "car at index 22: [-0.37512698769569397, 2.6017327308654785, 0.5150443911552429, 0.500370442867279, -1.6154428720474243]\n",
      "Grand sum of 87 tensor sets is: [34.09471893310547, 143.4006805419922, -18.680431365966797, -30.85426139831543, -43.91318893432617]\n",
      "\n",
      "Instance 106 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 9: [-0.15597757697105408, 2.090524196624756, -1.037608027458191, -1.4940094947814941, -0.5747926235198975]\n",
      "Grand sum of 88 tensor sets is: [33.93873977661133, 145.4912109375, -19.71803855895996, -32.348270416259766, -44.487979888916016]\n",
      "\n",
      "Instance 107 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [39, 61]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "car at index 39: [0.22386345267295837, 0.9970278739929199, -0.46792834997177124, 2.300550699234009, -1.0454241037368774]\n",
      "car at index 61: [1.1719145774841309, 0.4850696921348572, -0.5615242123603821, 0.9000957012176514, 0.33993232250213623]\n",
      "Grand sum of 89 tensor sets is: [34.636627197265625, 146.2322540283203, -20.232765197753906, -30.747947692871094, -44.84072494506836]\n",
      "\n",
      "Instance 108 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 13: [-0.5527543425559998, 2.0917906761169434, 0.039525020867586136, 0.3890889286994934, -1.001237392425537]\n",
      "Grand sum of 90 tensor sets is: [34.0838737487793, 148.3240509033203, -20.193241119384766, -30.358858108520508, -45.84196090698242]\n",
      "\n",
      "Instance 109 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "car at index 33: [0.7339714169502258, -1.962481141090393, -1.115707516670227, 1.8935515880584717, 2.828836441040039]\n",
      "Grand sum of 91 tensor sets is: [34.81784439086914, 146.361572265625, -21.308948516845703, -28.465307235717773, -43.01312255859375]\n",
      "\n",
      "Instance 110 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 111 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([90, 13, 768])\n",
      "Shape of summed layers is: 90 x 768\n",
      "car at index 14: [0.298759400844574, 2.782789707183838, -0.2617481052875519, -1.096219778060913, -3.274583339691162]\n",
      "Grand sum of 92 tensor sets is: [35.11660385131836, 149.1443634033203, -21.570695877075195, -29.561527252197266, -46.28770446777344]\n",
      "\n",
      "Instance 112 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6, 18]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 6: [0.2176039218902588, 1.7871766090393066, -1.14387047290802, -0.2611369788646698, 1.5381637811660767]\n",
      "car at index 18: [-0.266015887260437, 1.541349172592163, -0.9149268865585327, -0.1889817714691162, -2.387542486190796]\n",
      "Grand sum of 93 tensor sets is: [35.09239959716797, 150.80862426757812, -22.600093841552734, -29.78658676147461, -46.71239471435547]\n",
      "\n",
      "Instance 113 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 114 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 21: [-0.45162320137023926, 2.059293746948242, -0.054011933505535126, -2.3077852725982666, 0.5217005014419556]\n",
      "Grand sum of 94 tensor sets is: [34.640777587890625, 152.867919921875, -22.65410614013672, -32.0943717956543, -46.19069290161133]\n",
      "\n",
      "Instance 115 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([167, 13, 768])\n",
      "Shape of summed layers is: 167 x 768\n",
      "car at index 36: [1.3584723472595215, 0.7657484412193298, -0.06704919040203094, 0.21383538842201233, -1.4879825115203857]\n",
      "Grand sum of 95 tensor sets is: [35.99924850463867, 153.6336669921875, -22.721155166625977, -31.880537033081055, -47.67867660522461]\n",
      "\n",
      "Instance 116 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 13: [0.27397167682647705, 0.26154762506484985, -0.7232778072357178, 1.0136864185333252, 3.8410184383392334]\n",
      "Grand sum of 96 tensor sets is: [36.27322006225586, 153.8952178955078, -23.444433212280273, -30.866849899291992, -43.8376579284668]\n",
      "\n",
      "Instance 117 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 118 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 40: [-0.30577564239501953, 2.765390396118164, -0.1999814361333847, -1.8468358516693115, -1.8745882511138916]\n",
      "Grand sum of 97 tensor sets is: [35.967445373535156, 156.66061401367188, -23.6444149017334, -32.71368408203125, -45.71224594116211]\n",
      "\n",
      "Instance 119 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 31: [0.8941063284873962, 1.5937288999557495, -1.3680455684661865, -1.287471055984497, 0.4352916479110718]\n",
      "Grand sum of 98 tensor sets is: [36.86155319213867, 158.2543487548828, -25.012460708618164, -34.001155853271484, -45.276954650878906]\n",
      "\n",
      "Instance 120 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 47: [0.5102503299713135, -0.18840710818767548, 0.9525222182273865, 1.0520069599151611, 0.1822492629289627]\n",
      "Grand sum of 99 tensor sets is: [37.371803283691406, 158.06594848632812, -24.059938430786133, -32.94915008544922, -45.094703674316406]\n",
      "\n",
      "Instance 121 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 5: [0.4409579038619995, 1.4016839265823364, -0.875424325466156, -2.3400635719299316, 0.38653138279914856]\n",
      "Grand sum of 100 tensor sets is: [37.81275939941406, 159.46763610839844, -24.935361862182617, -35.289215087890625, -44.70817184448242]\n",
      "\n",
      "Instance 122 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 12: [0.8344154357910156, 1.264846682548523, -0.6905739307403564, 0.5917173027992249, -1.2709665298461914]\n",
      "Grand sum of 101 tensor sets is: [38.64717483520508, 160.73248291015625, -25.62593650817871, -34.6974983215332, -45.9791374206543]\n",
      "\n",
      "Instance 123 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 23: [0.3234608471393585, 1.636246681213379, 0.6032643914222717, 1.7719160318374634, -1.591728687286377]\n",
      "Grand sum of 102 tensor sets is: [38.97063446044922, 162.3687286376953, -25.022672653198242, -32.92558288574219, -47.570865631103516]\n",
      "\n",
      "Instance 124 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 4: [0.044902049005031586, 2.8579838275909424, -0.11559522151947021, -1.0296052694320679, -2.1370160579681396]\n",
      "Grand sum of 103 tensor sets is: [39.01553726196289, 165.22671508789062, -25.138267517089844, -33.9551887512207, -49.707881927490234]\n",
      "\n",
      "Instance 125 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 12: [0.28438785672187805, 2.425251007080078, -0.20857501029968262, -0.5475935339927673, -0.45298340916633606]\n",
      "Grand sum of 104 tensor sets is: [39.2999267578125, 167.65196228027344, -25.34684181213379, -34.50278091430664, -50.160865783691406]\n",
      "\n",
      "Instance 126 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 30: [1.1128356456756592, 1.1293060779571533, -0.13333895802497864, -1.31976318359375, 0.5084978938102722]\n",
      "Grand sum of 105 tensor sets is: [40.41276168823242, 168.78126525878906, -25.480180740356445, -35.82254409790039, -49.652366638183594]\n",
      "\n",
      "Instance 127 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [324]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 324: [1.3664517402648926, 0.2892419099807739, -0.3747793734073639, -0.8461757898330688, 2.0539252758026123]\n",
      "Grand sum of 106 tensor sets is: [41.779212951660156, 169.0705108642578, -25.85495948791504, -36.66872024536133, -47.59844207763672]\n",
      "\n",
      "Instance 128 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 39: [0.5787360668182373, 1.7814347743988037, -1.2300578355789185, -1.4919240474700928, 0.592222273349762]\n",
      "Grand sum of 107 tensor sets is: [42.357948303222656, 170.85194396972656, -27.085018157958984, -38.16064453125, -47.00621795654297]\n",
      "\n",
      "Instance 129 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 130 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 7: [0.05790460854768753, 1.7345370054244995, -1.4080653190612793, -1.5671114921569824, -1.2495607137680054]\n",
      "Grand sum of 108 tensor sets is: [42.41585159301758, 172.58648681640625, -28.493083953857422, -39.72775650024414, -48.25577926635742]\n",
      "\n",
      "Instance 131 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 9: [0.5643395781517029, 2.3382177352905273, -0.254970908164978, 0.26470446586608887, -3.1800596714019775]\n",
      "Grand sum of 109 tensor sets is: [42.98019027709961, 174.92469787597656, -28.74805450439453, -39.463050842285156, -51.43584060668945]\n",
      "\n",
      "Instance 132 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 133 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 17: [-0.3164026439189911, -0.8339542746543884, -0.006143614649772644, -1.0141446590423584, 1.3068360090255737]\n",
      "Grand sum of 110 tensor sets is: [42.663787841796875, 174.0907440185547, -28.75419807434082, -40.477195739746094, -50.129005432128906]\n",
      "\n",
      "Instance 134 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 135 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 136 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [290]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 290: [0.761275053024292, 2.151305675506592, 0.04005197808146477, -0.8923160433769226, -0.6928818225860596]\n",
      "Grand sum of 111 tensor sets is: [43.42506408691406, 176.24205017089844, -28.71414566040039, -41.369510650634766, -50.8218879699707]\n",
      "\n",
      "Instance 137 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 6: [0.31782108545303345, 1.4494569301605225, 0.003092817962169647, 1.779106616973877, 0.9462782144546509]\n",
      "Grand sum of 112 tensor sets is: [43.74288558959961, 177.69151306152344, -28.71105194091797, -39.59040451049805, -49.8756103515625]\n",
      "\n",
      "Instance 138 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9, 57]\n",
      "Size of token embeddings is torch.Size([115, 13, 768])\n",
      "Shape of summed layers is: 115 x 768\n",
      "car at index 9: [0.6643335819244385, 2.235461950302124, 0.6267019510269165, -2.2531485557556152, -1.0566271543502808]\n",
      "car at index 57: [0.5079420208930969, 1.4023195505142212, -0.8743511438369751, -1.4744027853012085, -0.14342722296714783]\n",
      "Grand sum of 113 tensor sets is: [44.32902526855469, 179.51040649414062, -28.834877014160156, -41.45418167114258, -50.47563934326172]\n",
      "\n",
      "Instance 139 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [152, 161, 167, 175, 180]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 152: [0.4051520824432373, -0.16856463253498077, -0.8911154270172119, -0.6227053999900818, 1.1528552770614624]\n",
      "car at index 161: [0.05099841207265854, 0.38951361179351807, 0.02213326096534729, 0.6677376627922058, -0.11113528162240982]\n",
      "car at index 167: [0.9581778645515442, 1.5813491344451904, 0.18593768775463104, 1.3303533792495728, -0.9209010004997253]\n",
      "car at index 175: [0.22772233188152313, -0.13922181725502014, -0.05631580948829651, 1.498756766319275, -0.8691897392272949]\n",
      "car at index 180: [0.5789232850074768, -0.0030663758516311646, 1.918604850769043, 0.3855048716068268, -0.8767062425613403]\n",
      "Grand sum of 114 tensor sets is: [44.77322006225586, 179.8424072265625, -28.599027633666992, -40.80225372314453, -50.800655364990234]\n",
      "\n",
      "Instance 140 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 15: [0.9186987280845642, 0.676726758480072, 1.2750310897827148, 3.9893195629119873, -2.642906904220581]\n",
      "Grand sum of 115 tensor sets is: [45.691917419433594, 180.51913452148438, -27.323997497558594, -36.81293487548828, -53.44356155395508]\n",
      "\n",
      "Instance 141 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 6: [-0.04267805442214012, 1.6997531652450562, -0.30884188413619995, -0.21108871698379517, -1.017525553703308]\n",
      "Grand sum of 116 tensor sets is: [45.64923858642578, 182.21888732910156, -27.63283920288086, -37.024024963378906, -54.46108627319336]\n",
      "\n",
      "Instance 142 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 143 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 144 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 145 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 14: [0.5217151045799255, 2.2255189418792725, -0.33492645621299744, 0.7554633021354675, -0.17177006602287292]\n",
      "Grand sum of 117 tensor sets is: [46.17095184326172, 184.4444122314453, -27.96776580810547, -36.26856231689453, -54.63285446166992]\n",
      "\n",
      "Instance 146 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 147 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31, 41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 31: [1.1906914710998535, 0.9529949426651001, -0.3479090929031372, -1.3917882442474365, -1.4549565315246582]\n",
      "car at index 41: [-0.2535417079925537, 0.7503781914710999, 1.8482145071029663, -0.03764127939939499, 0.20022112131118774]\n",
      "Grand sum of 118 tensor sets is: [46.6395263671875, 185.2960968017578, -27.217613220214844, -36.9832763671875, -55.260223388671875]\n",
      "\n",
      "Instance 148 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 18: [-0.031218647956848145, 4.545552730560303, 0.10483325272798538, -1.922903060913086, -2.2534618377685547]\n",
      "Grand sum of 119 tensor sets is: [46.608306884765625, 189.84164428710938, -27.11277961730957, -38.90618133544922, -57.51368713378906]\n",
      "\n",
      "Instance 149 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 15: [0.6483964323997498, 1.1380560398101807, -0.8906511068344116, -0.9712923169136047, -0.01790507882833481]\n",
      "Grand sum of 120 tensor sets is: [47.2567024230957, 190.97970581054688, -28.00343132019043, -39.877471923828125, -57.531593322753906]\n",
      "\n",
      "Instance 150 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "car at index 19: [-0.1965515911579132, 2.382239580154419, -0.058359790593385696, 1.5800340175628662, -1.9845008850097656]\n",
      "Grand sum of 121 tensor sets is: [47.060150146484375, 193.3619384765625, -28.061790466308594, -38.29743957519531, -59.51609420776367]\n",
      "\n",
      "Instance 151 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [257]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 257: [-0.07299801707267761, 2.6116719245910645, -0.9237363338470459, -1.4957081079483032, 1.4672380685806274]\n",
      "Grand sum of 122 tensor sets is: [46.987152099609375, 195.97361755371094, -28.98552703857422, -39.793148040771484, -58.04885482788086]\n",
      "\n",
      "Instance 152 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([167, 13, 768])\n",
      "Shape of summed layers is: 167 x 768\n",
      "car at index 36: [1.3584723472595215, 0.7657484412193298, -0.06704919040203094, 0.21383538842201233, -1.4879825115203857]\n",
      "Grand sum of 123 tensor sets is: [48.34562301635742, 196.73936462402344, -29.052576065063477, -39.57931137084961, -59.53683853149414]\n",
      "\n",
      "Instance 153 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 18: [-0.2646983563899994, 3.121554374694824, 0.43004462122917175, -2.1017777919769287, -0.6837749481201172]\n",
      "Grand sum of 124 tensor sets is: [48.08092498779297, 199.8609161376953, -28.62253189086914, -41.681087493896484, -60.220611572265625]\n",
      "\n",
      "Instance 154 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 21: [0.13368633389472961, 2.7644400596618652, -0.5619599223136902, -2.1632840633392334, -1.4844107627868652]\n",
      "Grand sum of 125 tensor sets is: [48.2146110534668, 202.62535095214844, -29.184492111206055, -43.8443717956543, -61.705020904541016]\n",
      "\n",
      "Instance 155 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 6: [0.8596345782279968, 1.5577360391616821, -1.2136564254760742, -2.5203707218170166, 0.26575005054473877]\n",
      "Grand sum of 126 tensor sets is: [49.07424545288086, 204.18309020996094, -30.398147583007812, -46.364742279052734, -61.43927001953125]\n",
      "\n",
      "Instance 156 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 19: [-0.27829551696777344, 2.7730205059051514, -0.718204915523529, -0.03546111285686493, -0.47817692160606384]\n",
      "Grand sum of 127 tensor sets is: [48.79595184326172, 206.95611572265625, -31.116352081298828, -46.400203704833984, -61.91744613647461]\n",
      "\n",
      "Instance 157 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 18: [0.24230262637138367, 3.13735032081604, 0.4692535400390625, -0.29361966252326965, -3.1165289878845215]\n",
      "Grand sum of 128 tensor sets is: [49.03825378417969, 210.0934600830078, -30.647098541259766, -46.693824768066406, -65.03397369384766]\n",
      "\n",
      "Instance 158 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "car at index 16: [0.42696598172187805, 1.9312231540679932, -0.04809798672795296, -2.1445271968841553, -0.11301425099372864]\n",
      "Grand sum of 129 tensor sets is: [49.4652214050293, 212.02468872070312, -30.6951961517334, -48.83835220336914, -65.14698791503906]\n",
      "\n",
      "Instance 159 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 160 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 161 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([92, 13, 768])\n",
      "Shape of summed layers is: 92 x 768\n",
      "car at index 17: [0.8607405424118042, 2.269263744354248, 0.6473211646080017, 1.5853753089904785, -1.0971256494522095]\n",
      "Grand sum of 130 tensor sets is: [50.32596206665039, 214.2939453125, -30.047874450683594, -47.25297546386719, -66.24411010742188]\n",
      "\n",
      "Instance 162 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 40: [0.8114537596702576, 2.7536182403564453, -0.918533444404602, -0.671536386013031, 0.4981531500816345]\n",
      "Grand sum of 131 tensor sets is: [51.13741683959961, 217.0475616455078, -30.966407775878906, -47.92451095581055, -65.74595642089844]\n",
      "\n",
      "Instance 163 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 132 tensor sets is: [53.25507354736328, 218.72393798828125, -32.20068359375, -49.59003448486328, -65.89045715332031]\n",
      "\n",
      "Instance 164 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 21: [0.26131075620651245, -0.0593305267393589, 0.09007680416107178, -0.20387573540210724, -1.6691176891326904]\n",
      "Grand sum of 133 tensor sets is: [53.51638412475586, 218.66461181640625, -32.1106071472168, -49.79391098022461, -67.55957794189453]\n",
      "\n",
      "Instance 165 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 14: [0.8497463464736938, 3.324172019958496, 0.5927605032920837, -1.7743825912475586, 0.08836042881011963]\n",
      "Grand sum of 134 tensor sets is: [54.36613082885742, 221.98878479003906, -31.517847061157227, -51.568294525146484, -67.4712142944336]\n",
      "\n",
      "Instance 166 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 21: [0.4076079726219177, 1.5039688348770142, 0.7083802819252014, 0.9634261131286621, -2.6179800033569336]\n",
      "Grand sum of 135 tensor sets is: [54.773738861083984, 223.4927520751953, -30.809467315673828, -50.6048698425293, -70.08919525146484]\n",
      "\n",
      "Instance 167 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 10: [0.544560968875885, 2.3736186027526855, -0.7095190286636353, 0.21651139855384827, 1.004407286643982]\n",
      "Grand sum of 136 tensor sets is: [55.31829833984375, 225.86636352539062, -31.518985748291016, -50.38835906982422, -69.08478546142578]\n",
      "\n",
      "Instance 168 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 21: [1.1344349384307861, 2.2694523334503174, -0.666872501373291, -1.4653778076171875, 0.5354971885681152]\n",
      "Grand sum of 137 tensor sets is: [56.45273208618164, 228.1358184814453, -32.18585968017578, -51.853736877441406, -68.54928588867188]\n",
      "\n",
      "Instance 169 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([477, 13, 768])\n",
      "Shape of summed layers is: 477 x 768\n",
      "car at index 181: [0.6160955429077148, 0.7458405494689941, -0.36355793476104736, -1.132871150970459, 1.9463517665863037]\n",
      "Grand sum of 138 tensor sets is: [57.06882858276367, 228.88165283203125, -32.54941940307617, -52.98660659790039, -66.60293579101562]\n",
      "\n",
      "Instance 170 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 16: [0.95624840259552, 2.3375775814056396, -0.1807548999786377, -1.3380558490753174, -2.215815544128418]\n",
      "Grand sum of 139 tensor sets is: [58.02507781982422, 231.21922302246094, -32.73017501831055, -54.32466125488281, -68.8187484741211]\n",
      "\n",
      "Instance 171 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [56]\n",
      "Size of token embeddings is torch.Size([70, 13, 768])\n",
      "Shape of summed layers is: 70 x 768\n",
      "car at index 56: [-0.3336373269557953, 2.3345236778259277, -1.0388747453689575, -1.5537807941436768, 0.7066106796264648]\n",
      "Grand sum of 140 tensor sets is: [57.69144058227539, 233.55374145507812, -33.76905059814453, -55.878440856933594, -68.11213684082031]\n",
      "\n",
      "Instance 172 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 14: [0.7765300273895264, 1.2720884084701538, 0.006203431636095047, -3.478240966796875, -0.336495041847229]\n",
      "Grand sum of 141 tensor sets is: [58.46797180175781, 234.82583618164062, -33.762847900390625, -59.35668182373047, -68.4486312866211]\n",
      "\n",
      "Instance 173 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 20: [0.1894250065088272, 4.21900749206543, 0.3457777798175812, -1.7180655002593994, 0.18350014090538025]\n",
      "Grand sum of 142 tensor sets is: [58.65739822387695, 239.0448455810547, -33.41706848144531, -61.07474899291992, -68.26512908935547]\n",
      "\n",
      "Instance 174 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 175 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 27: [1.0585591793060303, 2.0579118728637695, -0.5080869197845459, 1.738524079322815, -1.8689407110214233]\n",
      "Grand sum of 143 tensor sets is: [59.71595764160156, 241.10275268554688, -33.92515563964844, -59.33622360229492, -70.13407135009766]\n",
      "\n",
      "Instance 176 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 177 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [62, 74]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "car at index 62: [0.985599935054779, 1.4232683181762695, 0.16284729540348053, -0.5677041411399841, -1.907717227935791]\n",
      "car at index 74: [0.13075217604637146, 2.1514532566070557, 0.24819087982177734, -1.562641978263855, -1.791578769683838]\n",
      "Grand sum of 144 tensor sets is: [60.274131774902344, 242.89010620117188, -33.719635009765625, -60.401397705078125, -71.98371887207031]\n",
      "\n",
      "Instance 178 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 179 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 19]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 7: [0.382207453250885, 2.0308172702789307, -0.8103448748588562, -0.1497579663991928, 1.5011307001113892]\n",
      "car at index 19: [0.04748823493719101, 1.8314841985702515, -0.8591640591621399, 0.36988404393196106, -1.6889045238494873]\n",
      "Grand sum of 145 tensor sets is: [60.48897933959961, 244.82125854492188, -34.55438995361328, -60.29133605957031, -72.07760620117188]\n",
      "\n",
      "Instance 180 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 20]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 7: [0.15313251316547394, 2.8069262504577637, -0.45689547061920166, -2.074295997619629, 0.5552228689193726]\n",
      "car at index 20: [-0.47272491455078125, 2.8631725311279297, -0.009543381631374359, -1.1047966480255127, -1.59076726436615]\n",
      "Grand sum of 146 tensor sets is: [60.32918167114258, 247.65631103515625, -34.7876091003418, -61.880882263183594, -72.59537506103516]\n",
      "\n",
      "Instance 181 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 25: [-0.0031004101037979126, 1.9167805910110474, 1.1838141679763794, -0.5806001424789429, -2.189582109451294]\n",
      "Grand sum of 147 tensor sets is: [60.326080322265625, 249.57308959960938, -33.60379409790039, -62.461483001708984, -74.78495788574219]\n",
      "\n",
      "Instance 182 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 19: [-0.6467179656028748, 2.6616616249084473, 0.7738696932792664, -2.3366119861602783, -2.8262078762054443]\n",
      "Grand sum of 148 tensor sets is: [59.67936325073242, 252.23475646972656, -32.829925537109375, -64.798095703125, -77.61116790771484]\n",
      "\n",
      "Instance 183 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 184 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 13: [1.0680699348449707, 1.487248420715332, -0.15902531147003174, 0.1640884280204773, -1.2846590280532837]\n",
      "Grand sum of 149 tensor sets is: [60.747432708740234, 253.7220001220703, -32.98895263671875, -64.6340103149414, -78.89582824707031]\n",
      "\n",
      "Instance 185 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 186 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 3: [0.33151888847351074, 1.7523343563079834, -0.751324474811554, -1.5192961692810059, 0.2141129970550537]\n",
      "Grand sum of 150 tensor sets is: [61.07895278930664, 255.47433471679688, -33.74027633666992, -66.15330505371094, -78.68171691894531]\n",
      "\n",
      "Instance 187 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 2: [-0.3649144470691681, 3.25197696685791, 1.0639675855636597, -1.7102762460708618, -2.936018943786621]\n",
      "Grand sum of 151 tensor sets is: [60.71403884887695, 258.726318359375, -32.676307678222656, -67.86357879638672, -81.61773681640625]\n",
      "\n",
      "Instance 188 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 189 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 190 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 8: [0.46699392795562744, 1.2367697954177856, -0.8291842937469482, -1.5297609567642212, -1.0884764194488525]\n",
      "Grand sum of 152 tensor sets is: [61.181034088134766, 259.96307373046875, -33.5054931640625, -69.39334106445312, -82.70621490478516]\n",
      "\n",
      "Instance 191 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 10: [1.0679476261138916, 2.3874828815460205, 0.18063801527023315, -0.7059134244918823, -1.0621017217636108]\n",
      "Grand sum of 153 tensor sets is: [62.24898147583008, 262.3505554199219, -33.32485580444336, -70.09925079345703, -83.76831817626953]\n",
      "\n",
      "Instance 192 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 193 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 194 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 195 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 2: [0.41664642095565796, 2.6237380504608154, -0.6451783180236816, -1.379205584526062, -1.762001395225525]\n",
      "Grand sum of 154 tensor sets is: [62.665626525878906, 264.97430419921875, -33.970035552978516, -71.47845458984375, -85.53031921386719]\n",
      "\n",
      "Instance 196 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 11: [-0.2723165452480316, 2.5328571796417236, -0.05734051764011383, -0.8991259336471558, -1.0596853494644165]\n",
      "Grand sum of 155 tensor sets is: [62.393310546875, 267.5071716308594, -34.027374267578125, -72.37757873535156, -86.59000396728516]\n",
      "\n",
      "Instance 197 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 198 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 4: [-0.3133159875869751, 2.445826292037964, 0.5172484517097473, -3.0164074897766113, -0.81694495677948]\n",
      "Grand sum of 156 tensor sets is: [62.079994201660156, 269.9530029296875, -33.51012420654297, -75.39398956298828, -87.40695190429688]\n",
      "\n",
      "Instance 199 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 7: [0.05507851392030716, 1.5280632972717285, -0.7630370855331421, -1.3874225616455078, -2.754587411880493]\n",
      "Grand sum of 157 tensor sets is: [62.135074615478516, 271.4810791015625, -34.273162841796875, -76.78141021728516, -90.16153717041016]\n",
      "\n",
      "Instance 200 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 29: [1.375557541847229, 1.9839088916778564, 0.2842656373977661, -0.4154740273952484, -1.4022001028060913]\n",
      "Grand sum of 158 tensor sets is: [63.5106315612793, 273.4649963378906, -33.988895416259766, -77.19688415527344, -91.56373596191406]\n",
      "\n",
      "Instance 201 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 16: [-0.9107468128204346, 2.7630138397216797, 0.29974138736724854, -0.1552421599626541, -2.155397415161133]\n",
      "Grand sum of 159 tensor sets is: [62.599884033203125, 276.2279968261719, -33.68915557861328, -77.35212707519531, -93.71913146972656]\n",
      "\n",
      "Instance 202 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [182]\n",
      "Size of token embeddings is torch.Size([241, 13, 768])\n",
      "Shape of summed layers is: 241 x 768\n",
      "car at index 182: [-0.010292544960975647, 3.8563098907470703, -0.18100973963737488, -1.1367206573486328, -2.8146464824676514]\n",
      "Grand sum of 160 tensor sets is: [62.58959197998047, 280.0843200683594, -33.87016677856445, -78.48884582519531, -96.53377532958984]\n",
      "\n",
      "Instance 203 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "car at index 21: [1.74110746383667, -1.035272479057312, -0.8364502191543579, 1.3901641368865967, 2.0412817001342773]\n",
      "Grand sum of 161 tensor sets is: [64.33069610595703, 279.0490417480469, -34.70661544799805, -77.09867858886719, -94.49249267578125]\n",
      "\n",
      "Instance 204 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 8: [0.3275696635246277, 1.249335765838623, -0.3345232605934143, -2.995504856109619, 0.3393188714981079]\n",
      "Grand sum of 162 tensor sets is: [64.65826416015625, 280.2983703613281, -35.0411376953125, -80.09418487548828, -94.1531753540039]\n",
      "\n",
      "Instance 205 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26, 35, 40]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 26: [-0.0006705969572067261, 1.0866447687149048, 0.012205347418785095, -0.11664402484893799, -1.5075206756591797]\n",
      "car at index 35: [0.42656654119491577, 1.9712477922439575, 0.2534731328487396, -1.5578376054763794, -0.9707282781600952]\n",
      "car at index 40: [-0.21382373571395874, 1.7348542213439941, 0.7662650346755981, -1.9927833080291748, -0.06787389516830444]\n",
      "Grand sum of 163 tensor sets is: [64.72895812988281, 281.8959655761719, -34.6971549987793, -81.31660461425781, -95.00188446044922]\n",
      "\n",
      "Instance 206 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 207 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 10: [0.5862237215042114, 2.4833686351776123, 0.054163578897714615, -1.9146318435668945, -2.6352884769439697]\n",
      "Grand sum of 164 tensor sets is: [65.315185546875, 284.37933349609375, -34.64299011230469, -83.23123931884766, -97.63717651367188]\n",
      "\n",
      "Instance 208 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 8: [0.4518852233886719, 2.7256431579589844, 0.9498395323753357, -0.7618569135665894, 0.41230401396751404]\n",
      "Grand sum of 165 tensor sets is: [65.76707458496094, 287.10498046875, -33.69314956665039, -83.99309539794922, -97.22486877441406]\n",
      "\n",
      "Instance 209 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 210 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "car at index 41: [0.36795467138290405, -0.22719375789165497, -0.4945552349090576, 1.6399414539337158, 3.7144851684570312]\n",
      "Grand sum of 166 tensor sets is: [66.1350326538086, 286.8777770996094, -34.187705993652344, -82.35315704345703, -93.51038360595703]\n",
      "\n",
      "Instance 211 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 4: [-0.3133159875869751, 2.445826292037964, 0.5172484517097473, -3.0164074897766113, -0.81694495677948]\n",
      "Grand sum of 167 tensor sets is: [65.82171630859375, 289.3236083984375, -33.67045593261719, -85.36956787109375, -94.32733154296875]\n",
      "\n",
      "Instance 212 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 6: [0.13539278507232666, 1.5344674587249756, -0.357633501291275, 0.7305566668510437, -0.33577612042427063]\n",
      "Grand sum of 168 tensor sets is: [65.95710754394531, 290.8580627441406, -34.0280876159668, -84.63900756835938, -94.6631088256836]\n",
      "\n",
      "Instance 213 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 214 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 16: [-0.558860182762146, 2.034468412399292, 0.938220739364624, -2.5986249446868896, -2.5190811157226562]\n",
      "Grand sum of 169 tensor sets is: [65.39824676513672, 292.89251708984375, -33.089866638183594, -87.23763275146484, -97.18218994140625]\n",
      "\n",
      "Instance 215 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 216 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 217 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([84, 13, 768])\n",
      "Shape of summed layers is: 84 x 768\n",
      "car at index 44: [1.0672168731689453, 1.5122480392456055, -0.551846981048584, -1.9356296062469482, 1.132877230644226]\n",
      "Grand sum of 170 tensor sets is: [66.46546173095703, 294.4047546386719, -33.6417121887207, -89.17326354980469, -96.04931640625]\n",
      "\n",
      "Instance 218 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 6: [1.074045181274414, 3.062133550643921, -0.803735077381134, -1.5059304237365723, -0.11539527773857117]\n",
      "Grand sum of 171 tensor sets is: [67.53950500488281, 297.4668884277344, -34.4454460144043, -90.67919158935547, -96.16471099853516]\n",
      "\n",
      "Instance 219 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "car at index 46: [0.38619858026504517, -0.7381449937820435, -1.6077353954315186, 1.1783438920974731, 0.9788527488708496]\n",
      "Grand sum of 172 tensor sets is: [67.92570495605469, 296.7287292480469, -36.05318069458008, -89.50084686279297, -95.18585968017578]\n",
      "\n",
      "Instance 220 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 12: [0.7705811858177185, 1.3929734230041504, 0.053172335028648376, 1.0571447610855103, -0.3229498267173767]\n",
      "Grand sum of 173 tensor sets is: [68.6962890625, 298.1217041015625, -36.00000762939453, -88.4437026977539, -95.5088119506836]\n",
      "\n",
      "Instance 221 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [86]\n",
      "Size of token embeddings is torch.Size([102, 13, 768])\n",
      "Shape of summed layers is: 102 x 768\n",
      "car at index 86: [0.13140982389450073, -0.89628005027771, -1.6271318197250366, 1.2563323974609375, 1.0579174757003784]\n",
      "Grand sum of 174 tensor sets is: [68.82769775390625, 297.2254333496094, -37.627140045166016, -87.18737030029297, -94.45089721679688]\n",
      "\n",
      "Instance 222 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([96, 13, 768])\n",
      "Shape of summed layers is: 96 x 768\n",
      "car at index 17: [0.5774305462837219, 0.38065510988235474, 0.0006335973739624023, 2.23725962638855, 2.7368459701538086]\n",
      "Grand sum of 175 tensor sets is: [69.4051284790039, 297.6060791015625, -37.62650680541992, -84.95011138916016, -91.71405029296875]\n",
      "\n",
      "Instance 223 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17, 27]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 17: [1.177507758140564, -0.14736416935920715, -0.32870951294898987, 0.38885563611984253, -2.0149476528167725]\n",
      "car at index 27: [0.6891254186630249, 1.8721381425857544, -0.22515271604061127, -0.022961746901273727, -3.417999267578125]\n",
      "Grand sum of 176 tensor sets is: [70.33844757080078, 298.4684753417969, -37.903438568115234, -84.76716613769531, -94.43052673339844]\n",
      "\n",
      "Instance 224 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 225 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "car at index 40: [0.7790860533714294, -0.6390666365623474, -0.49310263991355896, -0.8263850808143616, 4.53363561630249]\n",
      "Grand sum of 177 tensor sets is: [71.1175308227539, 297.82940673828125, -38.396541595458984, -85.59355163574219, -89.89688873291016]\n",
      "\n",
      "Instance 226 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 227 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 10: [0.8655958771705627, 1.569597601890564, -1.0724458694458008, 0.1356285959482193, 1.9962475299835205]\n",
      "Grand sum of 178 tensor sets is: [71.98312377929688, 299.3990173339844, -39.46898651123047, -85.45792388916016, -87.90064239501953]\n",
      "\n",
      "Instance 228 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([122, 13, 768])\n",
      "Shape of summed layers is: 122 x 768\n",
      "car at index 16: [-0.2016315758228302, 1.2222708463668823, -0.6591224074363708, -1.2548673152923584, 0.2008773535490036]\n",
      "Grand sum of 179 tensor sets is: [71.781494140625, 300.62127685546875, -40.128108978271484, -86.7127914428711, -87.69976806640625]\n",
      "\n",
      "Instance 229 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 27: [1.6140352487564087, 2.2673165798187256, -0.23487745225429535, -1.3970918655395508, 1.3163237571716309]\n",
      "Grand sum of 180 tensor sets is: [73.3955307006836, 302.8885803222656, -40.36298751831055, -88.1098861694336, -86.3834457397461]\n",
      "\n",
      "Instance 230 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 5: [1.154725193977356, 3.01639986038208, -0.8649160861968994, 1.1476469039916992, -0.36671286821365356]\n",
      "Grand sum of 181 tensor sets is: [74.55025482177734, 305.90496826171875, -41.2279052734375, -86.96224212646484, -86.75016021728516]\n",
      "\n",
      "Instance 231 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 11: [-0.011962324380874634, 2.0693154335021973, 0.10767916589975357, -0.15100416541099548, 0.595252275466919]\n",
      "Grand sum of 182 tensor sets is: [74.53829193115234, 307.9742736816406, -41.1202278137207, -87.11324310302734, -86.1549072265625]\n",
      "\n",
      "Instance 232 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 233 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [75, 106]\n",
      "Size of token embeddings is torch.Size([116, 13, 768])\n",
      "Shape of summed layers is: 116 x 768\n",
      "car at index 75: [0.6216971278190613, 1.1972389221191406, -0.29820990562438965, -1.5835672616958618, 0.3559592068195343]\n",
      "car at index 106: [0.4582969546318054, 1.2115226984024048, -0.19500651955604553, -0.42124778032302856, 0.5471714735031128]\n",
      "Grand sum of 183 tensor sets is: [75.07828521728516, 309.17864990234375, -41.36683654785156, -88.11565399169922, -85.70333862304688]\n",
      "\n",
      "Instance 234 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 9: [0.4225877821445465, 2.768467664718628, -0.8241783380508423, -0.07530437409877777, -0.9391668438911438]\n",
      "Grand sum of 184 tensor sets is: [75.50086975097656, 311.9471130371094, -42.19101333618164, -88.19095611572266, -86.64250183105469]\n",
      "\n",
      "Instance 235 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 11: [-0.5712490081787109, 1.7486186027526855, -1.3120245933532715, -2.099252700805664, -0.9436187744140625]\n",
      "car at index 15: [-1.1396976709365845, 2.8101768493652344, 0.44345545768737793, -2.2842137813568115, -3.984623908996582]\n",
      "Grand sum of 185 tensor sets is: [74.64539337158203, 314.22650146484375, -42.62529754638672, -90.3826904296875, -89.10662078857422]\n",
      "\n",
      "Instance 236 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 14: [0.34805190563201904, -0.14279597997665405, 0.11689432710409164, 0.042281389236450195, 0.1471802294254303]\n",
      "Grand sum of 186 tensor sets is: [74.99344635009766, 314.0837097167969, -42.50840377807617, -90.34040832519531, -88.95944213867188]\n",
      "\n",
      "Instance 237 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 21: [0.7190678119659424, 1.7286748886108398, 1.0591729879379272, 3.29835844039917, -0.41922199726104736]\n",
      "Grand sum of 187 tensor sets is: [75.71251678466797, 315.8123779296875, -41.4492301940918, -87.04205322265625, -89.378662109375]\n",
      "\n",
      "Instance 238 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 11: [-0.5712490081787109, 1.7486186027526855, -1.3120245933532715, -2.099252700805664, -0.9436187744140625]\n",
      "car at index 15: [-1.1396976709365845, 2.8101768493652344, 0.44345545768737793, -2.2842137813568115, -3.984623908996582]\n",
      "Grand sum of 188 tensor sets is: [74.85704040527344, 318.0917663574219, -41.883514404296875, -89.2337875366211, -91.84278106689453]\n",
      "\n",
      "Instance 239 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 7: [1.4064910411834717, 1.2698805332183838, 0.9746394157409668, 0.12399732321500778, -1.2688605785369873]\n",
      "Grand sum of 189 tensor sets is: [76.26353454589844, 319.36163330078125, -40.90887451171875, -89.10978698730469, -93.11164093017578]\n",
      "\n",
      "Instance 240 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 22: [0.521226167678833, 1.2899720668792725, -1.1590584516525269, -0.36815863847732544, -0.6829904913902283]\n",
      "Grand sum of 190 tensor sets is: [76.78475952148438, 320.651611328125, -42.06793212890625, -89.47794342041016, -93.79463195800781]\n",
      "\n",
      "Instance 241 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 25: [0.5850703716278076, 1.7124615907669067, -0.32297155261039734, -1.2895838022232056, -2.154829263687134]\n",
      "Grand sum of 191 tensor sets is: [77.36982727050781, 322.36407470703125, -42.39090347290039, -90.76752471923828, -95.949462890625]\n",
      "\n",
      "Instance 242 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "car at index 28: [-0.18839383125305176, 1.5602364540100098, 0.5779149532318115, 1.4218270778656006, -2.3871054649353027]\n",
      "Grand sum of 192 tensor sets is: [77.18143463134766, 323.92431640625, -41.81298828125, -89.34569549560547, -98.3365707397461]\n",
      "\n",
      "Instance 243 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [108]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 108: [0.17435309290885925, -1.5973320007324219, -0.49079588055610657, -1.88312828540802, 3.755692958831787]\n",
      "Grand sum of 193 tensor sets is: [77.35578918457031, 322.3269958496094, -42.30378341674805, -91.22882080078125, -94.58087921142578]\n",
      "\n",
      "Instance 244 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 4: [0.7885767221450806, 0.10073910653591156, -0.283708393573761, -1.346303105354309, 3.1294312477111816]\n",
      "Grand sum of 194 tensor sets is: [78.14436340332031, 322.427734375, -42.58749008178711, -92.57512664794922, -91.45144653320312]\n",
      "\n",
      "Instance 245 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 246 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 2: [0.6359524726867676, 0.7619052529335022, 0.12591132521629333, -2.6652276515960693, 1.0038267374038696]\n",
      "Grand sum of 195 tensor sets is: [78.78031921386719, 323.18963623046875, -42.461578369140625, -95.2403564453125, -90.44761657714844]\n",
      "\n",
      "Instance 247 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 248 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 249 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 3: [0.7437233328819275, 0.8526618480682373, -0.9970613121986389, -1.7862378358840942, -0.7186897397041321]\n",
      "Grand sum of 196 tensor sets is: [79.52404022216797, 324.04229736328125, -43.458641052246094, -97.02659606933594, -91.16630554199219]\n",
      "\n",
      "Instance 250 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 18: [0.013426221907138824, 2.6519246101379395, 0.9037453532218933, -0.2606932520866394, -1.8712575435638428]\n",
      "Grand sum of 197 tensor sets is: [79.53746795654297, 326.6942138671875, -42.55489730834961, -97.28729248046875, -93.03755950927734]\n",
      "\n",
      "Instance 251 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "car at index 35: [0.7926640510559082, 1.5923269987106323, -0.07397492974996567, -1.133849859237671, -3.5022013187408447]\n",
      "Grand sum of 198 tensor sets is: [80.33013153076172, 328.2865295410156, -42.62887191772461, -98.421142578125, -96.53976440429688]\n",
      "\n",
      "Instance 252 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 8: [-0.5315588116645813, 1.275542974472046, -0.40824317932128906, -0.5729978084564209, -1.4242758750915527]\n",
      "Grand sum of 199 tensor sets is: [79.79857635498047, 329.56207275390625, -43.03711700439453, -98.994140625, -97.96404266357422]\n",
      "\n",
      "Instance 253 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 8: [0.8916155099868774, 2.0862231254577637, -0.2866131067276001, -0.03440617024898529, -3.8145127296447754]\n",
      "Grand sum of 200 tensor sets is: [80.69019317626953, 331.6482849121094, -43.32373046875, -99.02854919433594, -101.77855682373047]\n",
      "\n",
      "Instance 254 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 255 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 256 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 29: [0.9753046631813049, -0.7754667401313782, -0.5397326350212097, -0.02427731454372406, -0.8018248677253723]\n",
      "Grand sum of 201 tensor sets is: [81.66549682617188, 330.8728332519531, -43.86346435546875, -99.05282592773438, -102.58038330078125]\n",
      "\n",
      "Instance 257 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([96, 13, 768])\n",
      "Shape of summed layers is: 96 x 768\n",
      "car at index 39: [0.1055450364947319, 0.11225355416536331, -0.22603771090507507, 2.260390520095825, 4.252927303314209]\n",
      "Grand sum of 202 tensor sets is: [81.77104187011719, 330.9850769042969, -44.089500427246094, -96.79243469238281, -98.32745361328125]\n",
      "\n",
      "Instance 258 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18, 25]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 18: [-0.003603711724281311, 1.970051646232605, 0.2930489480495453, -2.4771621227264404, -0.44368934631347656]\n",
      "car at index 25: [-0.5551748275756836, 2.8703346252441406, 0.3517225980758667, -1.6210546493530273, -1.4635977745056152]\n",
      "Grand sum of 203 tensor sets is: [81.49165344238281, 333.4052734375, -43.767112731933594, -98.84154510498047, -99.28109741210938]\n",
      "\n",
      "Instance 259 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "car at index 13: [0.08404417335987091, 0.48060688376426697, -0.5633092522621155, 0.9586687684059143, 0.4062429964542389]\n",
      "Grand sum of 204 tensor sets is: [81.57569885253906, 333.8858947753906, -44.330421447753906, -97.88287353515625, -98.8748550415039]\n",
      "\n",
      "Instance 260 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 5: [0.09880561381578445, 1.8221898078918457, -0.916042685508728, -2.0433616638183594, -1.1229664087295532]\n",
      "Grand sum of 205 tensor sets is: [81.67450714111328, 335.7080993652344, -45.246463775634766, -99.92623901367188, -99.99781799316406]\n",
      "\n",
      "Instance 261 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 34: [0.6199284791946411, 3.601330041885376, 0.36959195137023926, -1.530540943145752, -0.6044279336929321]\n",
      "Grand sum of 206 tensor sets is: [82.29443359375, 339.3094177246094, -44.87687301635742, -101.45677947998047, -100.60224914550781]\n",
      "\n",
      "Instance 262 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 9: [-0.0240105539560318, 0.5974718332290649, -0.442843496799469, -1.7405954599380493, -0.15559907257556915]\n",
      "Grand sum of 207 tensor sets is: [82.27042388916016, 339.9068908691406, -45.31971740722656, -103.19737243652344, -100.75785064697266]\n",
      "\n",
      "Instance 263 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 11: [0.08653607964515686, 2.171304225921631, -1.2001785039901733, -0.42634087800979614, 2.5708742141723633]\n",
      "Grand sum of 208 tensor sets is: [82.3569564819336, 342.07818603515625, -46.5198974609375, -103.62371063232422, -98.18697357177734]\n",
      "\n",
      "Instance 264 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 209 tensor sets is: [84.474609375, 343.75457763671875, -47.754173278808594, -105.28923034667969, -98.33147430419922]\n",
      "\n",
      "Instance 265 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 266 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 42: [-0.46594899892807007, 0.6520903706550598, 0.026579922065138817, -2.237245798110962, -0.2413671761751175]\n",
      "Grand sum of 210 tensor sets is: [84.00865936279297, 344.40667724609375, -47.72759246826172, -107.52647399902344, -98.57283782958984]\n",
      "\n",
      "Instance 267 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 10: [0.28220927715301514, 2.0588252544403076, -0.28490516543388367, -0.4472689628601074, 0.9074052572250366]\n",
      "Grand sum of 211 tensor sets is: [84.2908706665039, 346.46551513671875, -48.01249694824219, -107.97373962402344, -97.66543579101562]\n",
      "\n",
      "Instance 268 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 269 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "car at index 42: [1.4123419523239136, -1.4974327087402344, -0.5254272222518921, 1.5234460830688477, 2.6648051738739014]\n",
      "Grand sum of 212 tensor sets is: [85.70320892333984, 344.96807861328125, -48.537925720214844, -106.4502944946289, -95.0006332397461]\n",
      "\n",
      "Instance 270 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "car at index 49: [0.21638624370098114, 0.672644317150116, 0.5408775806427002, -0.886469841003418, 0.3335264325141907]\n",
      "Grand sum of 213 tensor sets is: [85.91959381103516, 345.6407165527344, -47.997047424316406, -107.33676147460938, -94.66710662841797]\n",
      "\n",
      "Instance 271 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 15: [1.7506084442138672, -2.0197091102600098, -0.6316063404083252, 1.248868465423584, 1.6552388668060303]\n",
      "Grand sum of 214 tensor sets is: [87.67020416259766, 343.6210021972656, -48.62865447998047, -106.087890625, -93.01187133789062]\n",
      "\n",
      "Instance 272 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [90]\n",
      "Size of token embeddings is torch.Size([200, 13, 768])\n",
      "Shape of summed layers is: 200 x 768\n",
      "car at index 90: [-0.09332287311553955, 2.7812187671661377, 0.5552948117256165, 1.0599504709243774, 0.9671144485473633]\n",
      "Grand sum of 215 tensor sets is: [87.5768814086914, 346.4022216796875, -48.073360443115234, -105.02793884277344, -92.04475402832031]\n",
      "\n",
      "Instance 273 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 274 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 13: [0.40015268325805664, 2.4599876403808594, -1.1004433631896973, -2.3109347820281982, -0.9017301797866821]\n",
      "Grand sum of 216 tensor sets is: [87.97703552246094, 348.8622131347656, -49.173805236816406, -107.33887481689453, -92.94648742675781]\n",
      "\n",
      "Instance 275 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 276 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 4: [0.31239256262779236, 2.0490715503692627, -0.1573771983385086, -0.34427452087402344, -1.4335768222808838]\n",
      "Grand sum of 217 tensor sets is: [88.2894287109375, 350.9112854003906, -49.331180572509766, -107.68315124511719, -94.38006591796875]\n",
      "\n",
      "Instance 277 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 8: [1.3421818017959595, 1.9344258308410645, -0.8507840037345886, -2.473397731781006, 0.07962249219417572]\n",
      "Grand sum of 218 tensor sets is: [89.63160705566406, 352.845703125, -50.18196487426758, -110.15654754638672, -94.30044555664062]\n",
      "\n",
      "Instance 278 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 219 tensor sets is: [91.74925994873047, 354.5220947265625, -51.41624450683594, -111.82206726074219, -94.4449462890625]\n",
      "\n",
      "Instance 279 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 8: [-0.057283565402030945, 0.16881096363067627, -0.7453168034553528, 0.181726336479187, -3.6315524578094482]\n",
      "Grand sum of 220 tensor sets is: [91.69197845458984, 354.69091796875, -52.16156005859375, -111.64034271240234, -98.07649993896484]\n",
      "\n",
      "Instance 280 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 281 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 10: [0.3212505877017975, 0.018703287467360497, 0.03580686077475548, 2.638598918914795, 0.20435553789138794]\n",
      "Grand sum of 221 tensor sets is: [92.01322937011719, 354.7096252441406, -52.12575149536133, -109.00174713134766, -97.87214660644531]\n",
      "\n",
      "Instance 282 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([88, 13, 768])\n",
      "Shape of summed layers is: 88 x 768\n",
      "car at index 40: [0.6503336429595947, 3.7353265285491943, 0.15110614895820618, 0.44679632782936096, -1.1657228469848633]\n",
      "Grand sum of 222 tensor sets is: [92.66356658935547, 358.4449462890625, -51.97464370727539, -108.5549545288086, -99.03787231445312]\n",
      "\n",
      "Instance 283 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 5: [0.23114228248596191, 2.2015292644500732, -0.28233134746551514, -2.5101118087768555, -1.2305858135223389]\n",
      "Grand sum of 223 tensor sets is: [92.89470672607422, 360.646484375, -52.25697326660156, -111.0650634765625, -100.2684555053711]\n",
      "\n",
      "Instance 284 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 11: [-0.5712490081787109, 1.7486186027526855, -1.3120245933532715, -2.099252700805664, -0.9436187744140625]\n",
      "car at index 15: [-1.1396976709365845, 2.8101768493652344, 0.44345545768737793, -2.2842137813568115, -3.984623908996582]\n",
      "Grand sum of 224 tensor sets is: [92.03923034667969, 362.9258728027344, -52.69125747680664, -113.25679779052734, -102.73257446289062]\n",
      "\n",
      "Instance 285 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 11: [0.6871891617774963, 2.8466267585754395, -0.9243778586387634, -1.1493769884109497, -1.568576693534851]\n",
      "Grand sum of 225 tensor sets is: [92.7264175415039, 365.7724914550781, -53.61563491821289, -114.40617370605469, -104.3011474609375]\n",
      "\n",
      "Instance 286 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 287 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 288 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 28: [1.0353952646255493, 3.361809730529785, -0.2689094543457031, 1.408613920211792, -1.0963785648345947]\n",
      "Grand sum of 226 tensor sets is: [93.76181030273438, 369.1343078613281, -53.884544372558594, -112.99755859375, -105.39752960205078]\n",
      "\n",
      "Instance 289 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([167, 13, 768])\n",
      "Shape of summed layers is: 167 x 768\n",
      "car at index 36: [1.3584723472595215, 0.7657484412193298, -0.06704919040203094, 0.21383538842201233, -1.4879825115203857]\n",
      "Grand sum of 227 tensor sets is: [95.12028503417969, 369.9000549316406, -53.951595306396484, -112.78372192382812, -106.88551330566406]\n",
      "\n",
      "Instance 290 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 291 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([88, 13, 768])\n",
      "Shape of summed layers is: 88 x 768\n",
      "car at index 48: [-0.5097814202308655, 2.683976650238037, -0.38352930545806885, -1.8731426000595093, -1.6761982440948486]\n",
      "Grand sum of 228 tensor sets is: [94.61050415039062, 372.58404541015625, -54.33512496948242, -114.65686798095703, -108.56171417236328]\n",
      "\n",
      "Instance 292 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 293 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 7: [0.24774087965488434, 2.1266214847564697, -1.1870800256729126, -1.009212851524353, -0.6404874324798584]\n",
      "Grand sum of 229 tensor sets is: [94.85824584960938, 374.7106628417969, -55.5222053527832, -115.66608428955078, -109.20220184326172]\n",
      "\n",
      "Instance 294 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 9: [0.6809444427490234, 0.031184561550617218, -0.5989188551902771, -0.7693403363227844, 0.7732234001159668]\n",
      "Grand sum of 230 tensor sets is: [95.53919219970703, 374.7418518066406, -56.121124267578125, -116.4354248046875, -108.4289779663086]\n",
      "\n",
      "Instance 295 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([75, 13, 768])\n",
      "Shape of summed layers is: 75 x 768\n",
      "car at index 20: [0.18066830933094025, 0.7673148512840271, -0.08580951392650604, -1.5447094440460205, 0.7302999496459961]\n",
      "Grand sum of 231 tensor sets is: [95.71986389160156, 375.5091552734375, -56.206932067871094, -117.98013305664062, -107.69867706298828]\n",
      "\n",
      "Instance 296 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "car at index 20: [0.8327075839042664, -0.7739441394805908, -0.7100584506988525, 1.5049982070922852, 3.7865304946899414]\n",
      "Grand sum of 232 tensor sets is: [96.55257415771484, 374.7351989746094, -56.9169921875, -116.47513580322266, -103.91214752197266]\n",
      "\n",
      "Instance 297 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 20]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 11: [-0.9477659463882446, 0.46655604243278503, 0.10306024551391602, -0.711403489112854, 2.640813112258911]\n",
      "car at index 20: [-0.5568633079528809, 2.0030806064605713, 1.190125584602356, -2.0223593711853027, -1.6812491416931152]\n",
      "Grand sum of 233 tensor sets is: [95.80026245117188, 375.97003173828125, -56.27040100097656, -117.8420181274414, -103.43236541748047]\n",
      "\n",
      "Instance 298 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "car at index 20: [0.6567178964614868, 0.5787249803543091, 0.004069508984684944, -2.9491446018218994, -1.770578384399414]\n",
      "Grand sum of 234 tensor sets is: [96.45697784423828, 376.54876708984375, -56.26633071899414, -120.7911605834961, -105.20294189453125]\n",
      "\n",
      "Instance 299 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 300 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 9: [-0.5147325992584229, 2.826895236968994, -1.0031770467758179, 0.3412069082260132, -2.310422897338867]\n",
      "Grand sum of 235 tensor sets is: [95.94224548339844, 379.37567138671875, -57.269508361816406, -120.449951171875, -107.51336669921875]\n",
      "\n",
      "Instance 301 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 302 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 7: [1.1498355865478516, 3.717456340789795, -1.1818732023239136, -1.5370187759399414, 0.2633815407752991]\n",
      "Grand sum of 236 tensor sets is: [97.09207916259766, 383.0931396484375, -58.45138168334961, -121.98696899414062, -107.24998474121094]\n",
      "\n",
      "Instance 303 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 6: [0.6512176990509033, 1.5687395334243774, -0.7039967179298401, -1.5962258577346802, -1.0541187524795532]\n",
      "Grand sum of 237 tensor sets is: [97.74329376220703, 384.661865234375, -59.15538024902344, -123.58319854736328, -108.3041000366211]\n",
      "\n",
      "Instance 304 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([101, 13, 768])\n",
      "Shape of summed layers is: 101 x 768\n",
      "car at index 36: [-0.35194462537765503, 0.008183911442756653, 0.916136622428894, -1.6283725500106812, -1.881239414215088]\n",
      "Grand sum of 238 tensor sets is: [97.39134979248047, 384.6700439453125, -58.23924255371094, -125.2115707397461, -110.18534088134766]\n",
      "\n",
      "Instance 305 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 306 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 4: [-0.14317846298217773, 0.8486553430557251, 0.1493569016456604, 2.6868479251861572, -3.1497299671173096]\n",
      "Grand sum of 239 tensor sets is: [97.2481689453125, 385.5187072753906, -58.08988571166992, -122.52471923828125, -113.33506774902344]\n",
      "\n",
      "Instance 307 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27, 87]\n",
      "Size of token embeddings is torch.Size([89, 13, 768])\n",
      "Shape of summed layers is: 89 x 768\n",
      "car at index 27: [0.9333356022834778, 1.9404330253601074, -1.524499535560608, -0.22878356277942657, 0.9875697493553162]\n",
      "car at index 87: [0.41362154483795166, 1.8649189472198486, -1.047406792640686, -2.0739290714263916, -0.9613142013549805]\n",
      "Grand sum of 240 tensor sets is: [97.92164611816406, 387.42138671875, -59.37583923339844, -123.67607879638672, -113.32193756103516]\n",
      "\n",
      "Instance 308 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 19: [-0.03367997705936432, 0.3829629123210907, 0.3000972867012024, -1.303255558013916, -0.7373561859130859]\n",
      "Grand sum of 241 tensor sets is: [97.8879623413086, 387.8043518066406, -59.075740814208984, -124.97933197021484, -114.05929565429688]\n",
      "\n",
      "Instance 309 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 9: [-0.4196585714817047, 3.0473809242248535, -1.36417555809021, -1.4000048637390137, -1.0999476909637451]\n",
      "Grand sum of 242 tensor sets is: [97.46830749511719, 390.85174560546875, -60.43991470336914, -126.37933349609375, -115.15924072265625]\n",
      "\n",
      "Instance 310 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "car at index 30: [0.1099596694111824, 1.6339845657348633, -0.6329456567764282, -0.740348756313324, -0.752906322479248]\n",
      "Grand sum of 243 tensor sets is: [97.5782699584961, 392.4857177734375, -61.07286071777344, -127.11968231201172, -115.91214752197266]\n",
      "\n",
      "Instance 311 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 11: [0.7225823402404785, 2.942596673965454, -0.14774969220161438, -2.1784870624542236, -0.2875673174858093]\n",
      "Grand sum of 244 tensor sets is: [98.30084991455078, 395.4283142089844, -61.220611572265625, -129.2981719970703, -116.19971466064453]\n",
      "\n",
      "Instance 312 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 22: [1.2756723165512085, 1.9942306280136108, -1.553816795349121, 1.372470498085022, -1.8245465755462646]\n",
      "Grand sum of 245 tensor sets is: [99.57652282714844, 397.42254638671875, -62.77442932128906, -127.92570495605469, -118.02426147460938]\n",
      "\n",
      "Instance 313 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 4: [-0.75674968957901, 3.725853204727173, -1.3743728399276733, -0.06503403931856155, -1.649048089981079]\n",
      "Grand sum of 246 tensor sets is: [98.81977081298828, 401.1484069824219, -64.1488037109375, -127.99073791503906, -119.67330932617188]\n",
      "\n",
      "Instance 314 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 22: [0.4052867293357849, 0.9926459193229675, 0.1336062252521515, -0.692996621131897, 1.3051536083221436]\n",
      "Grand sum of 247 tensor sets is: [99.22505950927734, 402.14105224609375, -64.01519775390625, -128.68373107910156, -118.36815643310547]\n",
      "\n",
      "Instance 315 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 7: [-0.40480348467826843, 1.6708492040634155, -0.45439785718917847, 2.439749002456665, -2.0448851585388184]\n",
      "Grand sum of 248 tensor sets is: [98.82025909423828, 403.8118896484375, -64.46959686279297, -126.24398040771484, -120.41304016113281]\n",
      "\n",
      "Instance 316 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 14: [0.5029087066650391, 2.9287805557250977, -1.0334947109222412, -1.0079838037490845, 1.7281945943832397]\n",
      "Grand sum of 249 tensor sets is: [99.32316589355469, 406.74066162109375, -65.50308990478516, -127.25196075439453, -118.68484497070312]\n",
      "\n",
      "Instance 317 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 14: [0.35566985607147217, 1.4636858701705933, 0.8048521876335144, 1.7569226026535034, -2.7615654468536377]\n",
      "Grand sum of 250 tensor sets is: [99.6788330078125, 408.204345703125, -64.69823455810547, -125.49504089355469, -121.4464111328125]\n",
      "\n",
      "Instance 318 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 319 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 7: [0.08662188053131104, 1.4893945455551147, 0.389471173286438, -1.4886119365692139, -1.4163093566894531]\n",
      "Grand sum of 251 tensor sets is: [99.76545715332031, 409.6937255859375, -64.30876159667969, -126.98365020751953, -122.86271667480469]\n",
      "\n",
      "Instance 320 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 321 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [80, 103]\n",
      "Size of token embeddings is torch.Size([124, 13, 768])\n",
      "Shape of summed layers is: 124 x 768\n",
      "car at index 80: [0.7963470816612244, 1.5009839534759521, -0.19650550186634064, -2.358612298965454, -0.8404633402824402]\n",
      "car at index 103: [0.6648902297019958, 3.0249993801116943, 0.24852201342582703, -1.9817942380905151, -2.6019725799560547]\n",
      "Grand sum of 252 tensor sets is: [100.49607849121094, 411.95672607421875, -64.28275299072266, -129.1538543701172, -124.58393096923828]\n",
      "\n",
      "Instance 322 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 323 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 4: [-0.31102582812309265, 4.4378461837768555, -0.3245270252227783, 1.987246036529541, 3.6619675159454346]\n",
      "Grand sum of 253 tensor sets is: [100.18505096435547, 416.3945617675781, -64.6072769165039, -127.16661071777344, -120.92196655273438]\n",
      "\n",
      "Instance 324 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 26: [1.0415836572647095, 2.015671968460083, -0.06174086779356003, -1.1496520042419434, -2.042900323867798]\n",
      "Grand sum of 254 tensor sets is: [101.22663116455078, 418.4102478027344, -64.66901397705078, -128.31626892089844, -122.9648666381836]\n",
      "\n",
      "Instance 325 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 19: [-0.1511068195104599, 1.3725941181182861, 0.2970918118953705, 0.6365293860435486, -3.481501579284668]\n",
      "Grand sum of 255 tensor sets is: [101.07552337646484, 419.7828369140625, -64.3719253540039, -127.67974090576172, -126.44636535644531]\n",
      "\n",
      "Instance 326 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4, 13, 27]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 4: [0.39297249913215637, 3.3290066719055176, 0.1620582789182663, 1.14976966381073, 1.10996675491333]\n",
      "car at index 13: [0.41614580154418945, 3.690920829772949, 0.36461931467056274, 0.5009605884552002, 0.47430238127708435]\n",
      "car at index 27: [0.0407707542181015, 3.9132742881774902, 0.3402125835418701, -0.5215816497802734, -0.7034075260162354]\n",
      "Grand sum of 256 tensor sets is: [101.35881805419922, 423.42724609375, -64.08296203613281, -127.30335998535156, -126.15274810791016]\n",
      "\n",
      "Instance 327 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "car at index 28: [1.0577956438064575, -0.8027135133743286, -0.49346429109573364, 0.6223074793815613, 0.03674500435590744]\n",
      "Grand sum of 257 tensor sets is: [102.41661071777344, 422.6245422363281, -64.57642364501953, -126.6810531616211, -126.11600494384766]\n",
      "\n",
      "Instance 328 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 29: [0.45255374908447266, 1.7113795280456543, 0.6195990443229675, -2.3364622592926025, -1.2957873344421387]\n",
      "Grand sum of 258 tensor sets is: [102.8691635131836, 424.3359069824219, -63.956825256347656, -129.01751708984375, -127.41178894042969]\n",
      "\n",
      "Instance 329 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 3: [0.19727209210395813, 1.6136974096298218, 0.3720413148403168, -0.9573647379875183, -1.8443139791488647]\n",
      "Grand sum of 259 tensor sets is: [103.06643676757812, 425.9496154785156, -63.58478546142578, -129.97488403320312, -129.256103515625]\n",
      "\n",
      "Instance 330 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 13: [-0.052301838994026184, 1.3796550035476685, 0.3114803731441498, -1.9507275819778442, -0.6062602400779724]\n",
      "Grand sum of 260 tensor sets is: [103.0141372680664, 427.32928466796875, -63.2733039855957, -131.9256134033203, -129.86236572265625]\n",
      "\n",
      "Instance 331 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 36: [0.043062008917331696, 1.4333328008651733, -0.4938437044620514, -2.383451223373413, -2.3035385608673096]\n",
      "Grand sum of 261 tensor sets is: [103.05719757080078, 428.7626037597656, -63.767147064208984, -134.30906677246094, -132.16590881347656]\n",
      "\n",
      "Instance 332 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 333 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 334 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 335 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 28: [-0.20270653069019318, 2.741823434829712, -0.6038618683815002, -1.0609477758407593, -2.616445541381836]\n",
      "Grand sum of 262 tensor sets is: [102.8544921875, 431.5044250488281, -64.37100982666016, -135.37001037597656, -134.7823486328125]\n",
      "\n",
      "Instance 336 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 6: [-0.20445887744426727, 1.481347918510437, -1.496980905532837, -1.979897379875183, -1.7465459108352661]\n",
      "Grand sum of 263 tensor sets is: [102.65003204345703, 432.98577880859375, -65.86798858642578, -137.34991455078125, -136.52890014648438]\n",
      "\n",
      "Instance 337 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 45: [1.5734740495681763, -0.6063459515571594, 0.6305170655250549, 0.7076625227928162, 1.610748052597046]\n",
      "Grand sum of 264 tensor sets is: [104.22350311279297, 432.3794250488281, -65.23747253417969, -136.6422576904297, -134.91815185546875]\n",
      "\n",
      "Instance 338 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 13: [0.48901909589767456, -2.0173184871673584, -0.45117729902267456, 0.44012629985809326, 3.212414503097534]\n",
      "Grand sum of 265 tensor sets is: [104.7125244140625, 430.36212158203125, -65.68865203857422, -136.20213317871094, -131.7057342529297]\n",
      "\n",
      "Instance 339 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 10: [0.05521337687969208, 1.866254210472107, -0.59792160987854, -0.010453209280967712, 0.7434837222099304]\n",
      "Grand sum of 266 tensor sets is: [104.76773834228516, 432.2283630371094, -66.28657531738281, -136.21258544921875, -130.96224975585938]\n",
      "\n",
      "Instance 340 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 341 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 342 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 343 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 17: [0.878839373588562, 3.222839832305908, 0.15119093656539917, 0.3534024655818939, -1.0753192901611328]\n",
      "Grand sum of 267 tensor sets is: [105.64657592773438, 435.4512023925781, -66.13538360595703, -135.8591766357422, -132.03756713867188]\n",
      "\n",
      "Instance 344 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 345 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 346 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [71, 78, 85, 121, 127, 153]\n",
      "Size of token embeddings is torch.Size([211, 13, 768])\n",
      "Shape of summed layers is: 211 x 768\n",
      "car at index 71: [0.2549892067909241, 1.9952900409698486, -0.8858636617660522, -1.1756064891815186, 1.383126139640808]\n",
      "car at index 78: [0.5164280533790588, 2.5674655437469482, -0.6502816677093506, -2.525986909866333, -1.4536082744598389]\n",
      "car at index 85: [0.5106374025344849, 3.0072803497314453, -0.0851643905043602, -3.321744680404663, -2.2403838634490967]\n",
      "car at index 121: [0.4161572754383087, 2.2829856872558594, -0.6527640223503113, -2.7723820209503174, -0.13348734378814697]\n",
      "car at index 127: [0.6586505770683289, 1.4012739658355713, -0.7546115517616272, -1.9314879179000854, -2.337750196456909]\n",
      "car at index 153: [0.3124640882015228, 2.2049896717071533, -0.13829849660396576, -0.7153351306915283, -3.714038610458374]\n",
      "Grand sum of 268 tensor sets is: [106.09146118164062, 437.6944274902344, -66.66321563720703, -137.9329376220703, -133.45359802246094]\n",
      "\n",
      "Instance 347 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 7: [0.19545859098434448, 4.362252712249756, -0.7579545378684998, 1.6989240646362305, -1.679015874862671]\n",
      "Grand sum of 269 tensor sets is: [106.28691864013672, 442.0566711425781, -67.42117309570312, -136.2340087890625, -135.1326141357422]\n",
      "\n",
      "Instance 348 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([82, 13, 768])\n",
      "Shape of summed layers is: 82 x 768\n",
      "car at index 43: [1.453947901725769, -0.5247268676757812, -0.4447471499443054, 0.2656375467777252, 0.8764070868492126]\n",
      "Grand sum of 270 tensor sets is: [107.7408676147461, 441.5319519042969, -67.86592102050781, -135.96836853027344, -134.25621032714844]\n",
      "\n",
      "Instance 349 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 350 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 16: [0.1228426992893219, 1.3577381372451782, -0.6801146268844604, 0.4068198502063751, -3.069549798965454]\n",
      "Grand sum of 271 tensor sets is: [107.86370849609375, 442.8896789550781, -68.54603576660156, -135.56155395507812, -137.3257598876953]\n",
      "\n",
      "Instance 351 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 2: [-0.3991358280181885, 1.9837183952331543, 0.3573395609855652, -1.2237355709075928, -1.5121296644210815]\n",
      "Grand sum of 272 tensor sets is: [107.46456909179688, 444.8733825683594, -68.1886978149414, -136.78529357910156, -138.837890625]\n",
      "\n",
      "Instance 352 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 353 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [73]\n",
      "Size of token embeddings is torch.Size([99, 13, 768])\n",
      "Shape of summed layers is: 99 x 768\n",
      "car at index 73: [0.7794312834739685, 2.392294406890869, 0.45455020666122437, -2.225006341934204, -2.511747360229492]\n",
      "Grand sum of 273 tensor sets is: [108.24400329589844, 447.26568603515625, -67.73414611816406, -139.0102996826172, -141.34963989257812]\n",
      "\n",
      "Instance 354 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 355 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 21: [-0.19339880347251892, 2.718813896179199, 1.024788737297058, 1.5579853057861328, -1.9429370164871216]\n",
      "Grand sum of 274 tensor sets is: [108.05060577392578, 449.9844970703125, -66.70935821533203, -137.4523162841797, -143.29257202148438]\n",
      "\n",
      "Instance 356 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 357 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 5: [-0.0354202538728714, 2.39206862449646, -0.5702202916145325, -2.658612012863159, -0.6331132054328918]\n",
      "Grand sum of 275 tensor sets is: [108.01518249511719, 452.3765563964844, -67.27957916259766, -140.11093139648438, -143.92568969726562]\n",
      "\n",
      "Instance 358 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 14: [0.2504769563674927, 2.348339557647705, -0.4740675091743469, -2.041555881500244, -0.12447374314069748]\n",
      "Grand sum of 276 tensor sets is: [108.26566314697266, 454.7248840332031, -67.75364685058594, -142.15248107910156, -144.0501708984375]\n",
      "\n",
      "Instance 359 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 5: [-0.3461233377456665, 4.587497234344482, -0.2787996232509613, 0.596646785736084, -0.3264232277870178]\n",
      "Grand sum of 277 tensor sets is: [107.91954040527344, 459.3123779296875, -68.0324478149414, -141.5558319091797, -144.3765869140625]\n",
      "\n",
      "Instance 360 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "car at index 28: [1.2071598768234253, 0.36139315366744995, -1.2531672716140747, -1.8531054258346558, 2.2738707065582275]\n",
      "Grand sum of 278 tensor sets is: [109.12670135498047, 459.67376708984375, -69.28561401367188, -143.408935546875, -142.10272216796875]\n",
      "\n",
      "Instance 361 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 13: [0.8534107804298401, 1.476534128189087, -0.6268295049667358, -0.8925166726112366, 0.24245837330818176]\n",
      "Grand sum of 279 tensor sets is: [109.98011016845703, 461.1502990722656, -69.91244506835938, -144.30145263671875, -141.86026000976562]\n",
      "\n",
      "Instance 362 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 2: [0.5448067784309387, 1.7142807245254517, 0.1882137656211853, -2.392642021179199, -0.24798406660556793]\n",
      "Grand sum of 280 tensor sets is: [110.52491760253906, 462.8645935058594, -69.72422790527344, -146.694091796875, -142.10824584960938]\n",
      "\n",
      "Instance 363 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 11: [-0.30943530797958374, 2.0042264461517334, -1.316504716873169, -0.9176093339920044, -1.382266640663147]\n",
      "Grand sum of 281 tensor sets is: [110.21548461914062, 464.8688049316406, -71.04073333740234, -147.6116943359375, -143.49050903320312]\n",
      "\n",
      "Instance 364 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 9: [0.19304749369621277, 3.774818181991577, -1.7454349994659424, 0.18460063636302948, -2.2250185012817383]\n",
      "Grand sum of 282 tensor sets is: [110.40853118896484, 468.64361572265625, -72.78617095947266, -147.42709350585938, -145.7155303955078]\n",
      "\n",
      "Instance 365 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 10: [-0.326947420835495, 2.2386116981506348, -0.4597579538822174, 0.06567780673503876, -1.9182348251342773]\n",
      "Grand sum of 283 tensor sets is: [110.08158111572266, 470.8822326660156, -73.24592590332031, -147.36141967773438, -147.63375854492188]\n",
      "\n",
      "Instance 366 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 6: [-1.2647346258163452, 2.6363067626953125, -0.6064010858535767, -0.08409248292446136, -0.5722644329071045]\n",
      "Grand sum of 284 tensor sets is: [108.81684875488281, 473.5185546875, -73.85232543945312, -147.4455108642578, -148.20602416992188]\n",
      "\n",
      "Instance 367 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "car at index 5: [0.5524657368659973, 0.3107348084449768, -0.5547842383384705, -0.5016297698020935, 1.2784912586212158]\n",
      "Grand sum of 285 tensor sets is: [109.36931610107422, 473.82928466796875, -74.40711212158203, -147.9471435546875, -146.9275360107422]\n",
      "\n",
      "Instance 368 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12, 20]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 12: [0.4180876612663269, 1.0720003843307495, -1.3315341472625732, -1.9577668905258179, -1.2400166988372803]\n",
      "car at index 20: [0.06677987426519394, 2.0749504566192627, 0.12737241387367249, -1.591738224029541, -2.5687172412872314]\n",
      "Grand sum of 286 tensor sets is: [109.61174774169922, 475.40277099609375, -75.00919342041016, -149.72189331054688, -148.8319091796875]\n",
      "\n",
      "Instance 369 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 3: [0.09068596363067627, 2.0838465690612793, -0.13566240668296814, 0.3923790454864502, -1.8710981607437134]\n",
      "Grand sum of 287 tensor sets is: [109.70243072509766, 477.4866027832031, -75.14485931396484, -149.3295135498047, -150.7030029296875]\n",
      "\n",
      "Instance 370 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [202]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 202: [0.5920625329017639, 0.4113866090774536, -0.6825777292251587, -0.04026274383068085, -2.696099042892456]\n",
      "Grand sum of 288 tensor sets is: [110.29449462890625, 477.8979797363281, -75.82743835449219, -149.36978149414062, -153.39910888671875]\n",
      "\n",
      "Instance 371 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 15: [-0.20504435896873474, 1.1223243474960327, -0.9586445689201355, 0.8240480422973633, 0.28772562742233276]\n",
      "Grand sum of 289 tensor sets is: [110.08944702148438, 479.0202941894531, -76.78607940673828, -148.5457305908203, -153.11138916015625]\n",
      "\n",
      "Instance 372 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 22: [-0.12656831741333008, 1.6672757863998413, -0.41960716247558594, -1.0340969562530518, 0.01388019323348999]\n",
      "Grand sum of 290 tensor sets is: [109.96287536621094, 480.68756103515625, -77.2056884765625, -149.579833984375, -153.09750366210938]\n",
      "\n",
      "Instance 373 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 374 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 375 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 19: [0.1635509729385376, 2.1219935417175293, 0.14167143404483795, 0.19138580560684204, 0.32828742265701294]\n",
      "Grand sum of 291 tensor sets is: [110.12642669677734, 482.8095397949219, -77.06401824951172, -149.38844299316406, -152.7692108154297]\n",
      "\n",
      "Instance 376 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 377 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 14: [-0.6411811113357544, 2.9732584953308105, 0.1371973603963852, 1.7931902408599854, -2.5565311908721924]\n",
      "Grand sum of 292 tensor sets is: [109.48524475097656, 485.7828063964844, -76.92681884765625, -147.59524536132812, -155.32574462890625]\n",
      "\n",
      "Instance 378 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 379 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 24: [0.9593051075935364, -0.03409267216920853, 0.6474431157112122, 1.4149672985076904, -0.14530816674232483]\n",
      "Grand sum of 293 tensor sets is: [110.44454956054688, 485.74871826171875, -76.27937316894531, -146.18028259277344, -155.47105407714844]\n",
      "\n",
      "Instance 380 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 17: [0.626288890838623, -0.3587939143180847, -0.8821303248405457, 1.520390510559082, 1.219503402709961]\n",
      "Grand sum of 294 tensor sets is: [111.07083892822266, 485.3899230957031, -77.16150665283203, -144.65989685058594, -154.25155639648438]\n",
      "\n",
      "Instance 381 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 7: [-0.09482262283563614, 1.3057340383529663, 0.2678621709346771, -1.3278416395187378, 1.1494057178497314]\n",
      "Grand sum of 295 tensor sets is: [110.97601318359375, 486.6956481933594, -76.89364624023438, -145.98773193359375, -153.10215759277344]\n",
      "\n",
      "Instance 382 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 7: [1.0216975212097168, 0.9749870300292969, 0.2712981700897217, -2.4382972717285156, -1.8094371557235718]\n",
      "Grand sum of 296 tensor sets is: [111.99771118164062, 487.6706237792969, -76.62234497070312, -148.426025390625, -154.91159057617188]\n",
      "\n",
      "Instance 383 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "car at index 34: [1.1340982913970947, -0.9922190308570862, -0.8673510551452637, 2.0625600814819336, 1.311204195022583]\n",
      "Grand sum of 297 tensor sets is: [113.1318130493164, 486.67840576171875, -77.48969268798828, -146.36346435546875, -153.6003875732422]\n",
      "\n",
      "Instance 384 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 4: [-0.6838325262069702, 1.655768871307373, 0.044221460819244385, -1.0215929746627808, -0.937497079372406]\n",
      "Grand sum of 298 tensor sets is: [112.44798278808594, 488.33416748046875, -77.44547271728516, -147.3850555419922, -154.5378875732422]\n",
      "\n",
      "Instance 385 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 19: [0.49311715364456177, 1.144973635673523, -1.1675302982330322, -0.1336013674736023, 3.0823512077331543]\n",
      "Grand sum of 299 tensor sets is: [112.94110107421875, 489.4791259765625, -78.61300659179688, -147.51866149902344, -151.45553588867188]\n",
      "\n",
      "Instance 386 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 15: [0.6002349853515625, 3.17563533782959, -0.6915233731269836, -1.492796778678894, -2.4183883666992188]\n",
      "Grand sum of 300 tensor sets is: [113.54133605957031, 492.6547546386719, -79.30452728271484, -149.01145935058594, -153.87393188476562]\n",
      "\n",
      "Instance 387 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 13: [1.0294138193130493, 0.8648844957351685, -0.39983513951301575, -1.8357548713684082, 1.3329765796661377]\n",
      "Grand sum of 301 tensor sets is: [114.57074737548828, 493.5196533203125, -79.70436096191406, -150.8472137451172, -152.54095458984375]\n",
      "\n",
      "Instance 388 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([65, 13, 768])\n",
      "Shape of summed layers is: 65 x 768\n",
      "car at index 43: [1.7032259702682495, -2.569124221801758, 0.16374537348747253, 0.39684441685676575, 0.41876527667045593]\n",
      "Grand sum of 302 tensor sets is: [116.27397155761719, 490.9505310058594, -79.54061889648438, -150.4503631591797, -152.1221923828125]\n",
      "\n",
      "Instance 389 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 390 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 4: [0.7984819412231445, 2.345582962036133, -1.1117743253707886, -1.7935439348220825, -1.0437946319580078]\n",
      "Grand sum of 303 tensor sets is: [117.07245635986328, 493.2961120605469, -80.65238952636719, -152.24391174316406, -153.16598510742188]\n",
      "\n",
      "Instance 391 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [244]\n",
      "Size of token embeddings is torch.Size([318, 13, 768])\n",
      "Shape of summed layers is: 318 x 768\n",
      "car at index 244: [0.27983102202415466, 1.07780122756958, -0.34132397174835205, -0.5707794427871704, -0.6406430602073669]\n",
      "Grand sum of 304 tensor sets is: [117.35228729248047, 494.3739013671875, -80.99371337890625, -152.814697265625, -153.80662536621094]\n",
      "\n",
      "Instance 392 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 305 tensor sets is: [119.46994018554688, 496.05029296875, -82.22798919677734, -154.480224609375, -153.9511260986328]\n",
      "\n",
      "Instance 393 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 8: [0.19597335159778595, 2.403013229370117, -0.3024715483188629, -2.315322160720825, -0.8949152827262878]\n",
      "Grand sum of 306 tensor sets is: [119.6659164428711, 498.45330810546875, -82.53046417236328, -156.79554748535156, -154.84603881835938]\n",
      "\n",
      "Instance 394 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 4: [0.03902234137058258, 2.818358898162842, -0.393310010433197, -0.9091503620147705, -0.7600345015525818]\n",
      "Grand sum of 307 tensor sets is: [119.70494079589844, 501.27166748046875, -82.92377471923828, -157.70469665527344, -155.6060791015625]\n",
      "\n",
      "Instance 395 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 396 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 5: [-0.7993060946464539, 0.2266157567501068, 0.08072378486394882, -1.2479567527770996, 1.2212413549423218]\n",
      "Grand sum of 308 tensor sets is: [118.90563201904297, 501.498291015625, -82.84304809570312, -158.95265197753906, -154.3848419189453]\n",
      "\n",
      "Instance 397 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 4: [-0.05119122564792633, 2.3505852222442627, -1.2093887329101562, 1.4997581243515015, -0.7782152891159058]\n",
      "Grand sum of 309 tensor sets is: [118.85443878173828, 503.848876953125, -84.05243682861328, -157.45289611816406, -155.16305541992188]\n",
      "\n",
      "Instance 398 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 11: [1.2216846942901611, 2.474586248397827, -0.4027293026447296, -0.37884074449539185, 1.6180765628814697]\n",
      "Grand sum of 310 tensor sets is: [120.07612609863281, 506.3234558105469, -84.45516967773438, -157.8317413330078, -153.54498291015625]\n",
      "\n",
      "Instance 399 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 4: [0.9862337708473206, 2.708782196044922, -0.022860556840896606, 0.412441223859787, -1.0756977796554565]\n",
      "Grand sum of 311 tensor sets is: [121.06236267089844, 509.0322265625, -84.47802734375, -157.41929626464844, -154.6206817626953]\n",
      "\n",
      "Instance 400 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2, 8]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 2: [0.8706681728363037, 3.271397829055786, -0.5447748899459839, -1.063900351524353, -0.34072622656822205]\n",
      "car at index 8: [0.21532118320465088, 3.343280076980591, 1.0236990451812744, -0.9845895767211914, -3.775111198425293]\n",
      "Grand sum of 312 tensor sets is: [121.60535430908203, 512.3395385742188, -84.23856353759766, -158.44354248046875, -156.67860412597656]\n",
      "\n",
      "Instance 401 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 12: [0.3701901435852051, 0.9090680480003357, -0.8697989583015442, -0.2292972207069397, 0.39654064178466797]\n",
      "Grand sum of 313 tensor sets is: [121.97554779052734, 513.2485961914062, -85.10836029052734, -158.67283630371094, -156.2820587158203]\n",
      "\n",
      "Instance 402 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 403 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 4: [0.3355943560600281, 3.357269763946533, -0.32943713665008545, -1.5764744281768799, -0.5669611692428589]\n",
      "Grand sum of 314 tensor sets is: [122.31114196777344, 516.6058959960938, -85.43779754638672, -160.2493133544922, -156.84901428222656]\n",
      "\n",
      "Instance 404 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 405 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 18: [1.2261555194854736, -0.2921515703201294, 0.07779042422771454, -0.8931485414505005, 1.215226173400879]\n",
      "Grand sum of 315 tensor sets is: [123.53730010986328, 516.313720703125, -85.3600082397461, -161.1424560546875, -155.6337890625]\n",
      "\n",
      "Instance 406 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 7: [0.8946872353553772, 0.9666154980659485, -0.07864901423454285, -0.5478191375732422, -1.5932390689849854]\n",
      "Grand sum of 316 tensor sets is: [124.4319839477539, 517.2803344726562, -85.43865966796875, -161.69027709960938, -157.22703552246094]\n",
      "\n",
      "Instance 407 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([110, 13, 768])\n",
      "Shape of summed layers is: 110 x 768\n",
      "car at index 93: [0.1246965080499649, -1.391373634338379, -0.6454691886901855, 0.6377633213996887, 3.6895134449005127]\n",
      "Grand sum of 317 tensor sets is: [124.55667877197266, 515.8889770507812, -86.0841293334961, -161.05252075195312, -153.5375213623047]\n",
      "\n",
      "Instance 408 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 409 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 410 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 6: [0.4739779233932495, 1.7066333293914795, -1.0934429168701172, -0.0930216833949089, -0.17024904489517212]\n",
      "Grand sum of 318 tensor sets is: [125.03065490722656, 517.5955810546875, -87.17757415771484, -161.14553833007812, -153.707763671875]\n",
      "\n",
      "Instance 411 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 9: [-0.38294434547424316, 2.2891156673431396, 0.18060195446014404, -2.4034504890441895, -1.078202247619629]\n",
      "Grand sum of 319 tensor sets is: [124.64771270751953, 519.8847045898438, -86.9969711303711, -163.5489959716797, -154.7859649658203]\n",
      "\n",
      "Instance 412 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 14: [0.9903478622436523, 2.5264580249786377, -0.7923456430435181, -0.5031968355178833, -1.5395785570144653]\n",
      "Grand sum of 320 tensor sets is: [125.6380615234375, 522.4111328125, -87.78931427001953, -164.0522003173828, -156.32554626464844]\n",
      "\n",
      "Instance 413 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 10: [0.7808197140693665, 2.01969051361084, -0.7989949584007263, 0.13284899294376373, 0.424242228269577]\n",
      "Grand sum of 321 tensor sets is: [126.41888427734375, 524.4308471679688, -88.58831024169922, -163.9193572998047, -155.90130615234375]\n",
      "\n",
      "Instance 414 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 5: [0.41216492652893066, 2.506484270095825, -0.15834000706672668, -3.947136878967285, -0.043623000383377075]\n",
      "Grand sum of 322 tensor sets is: [126.83104705810547, 526.9373168945312, -88.74665069580078, -167.8665008544922, -155.94493103027344]\n",
      "\n",
      "Instance 415 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 2: [0.6885337233543396, 1.047433614730835, -1.2072112560272217, -0.20268839597702026, 1.0103659629821777]\n",
      "Grand sum of 323 tensor sets is: [127.51957702636719, 527.9847412109375, -89.95386505126953, -168.06918334960938, -154.9345703125]\n",
      "\n",
      "Instance 416 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 16: [0.26352787017822266, 1.986680030822754, -0.3478291928768158, -2.0285093784332275, -0.06176704168319702]\n",
      "Grand sum of 324 tensor sets is: [127.7831039428711, 529.971435546875, -90.30169677734375, -170.09768676757812, -154.996337890625]\n",
      "\n",
      "Instance 417 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 48: [1.1038955450057983, 1.4813116788864136, 0.5337240695953369, -0.8162374496459961, -0.17947983741760254]\n",
      "Grand sum of 325 tensor sets is: [128.88699340820312, 531.4527587890625, -89.76797485351562, -170.91392517089844, -155.17581176757812]\n",
      "\n",
      "Instance 418 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 26: [1.0415836572647095, 2.015671968460083, -0.06174086779356003, -1.1496520042419434, -2.042900323867798]\n",
      "Grand sum of 326 tensor sets is: [129.92857360839844, 533.4684448242188, -89.8297119140625, -172.06358337402344, -157.21871948242188]\n",
      "\n",
      "Instance 419 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 21: [0.7189528942108154, 0.1914486140012741, 0.0503079928457737, 3.4230213165283203, -2.31663179397583]\n",
      "Grand sum of 327 tensor sets is: [130.64752197265625, 533.659912109375, -89.77940368652344, -168.64056396484375, -159.5353546142578]\n",
      "\n",
      "Instance 420 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "car at index 18: [1.0759221315383911, 2.9108777046203613, -0.6165807843208313, -0.43150320649147034, -0.887512743473053]\n",
      "Grand sum of 328 tensor sets is: [131.72344970703125, 536.57080078125, -90.39598083496094, -169.0720672607422, -160.42286682128906]\n",
      "\n",
      "Instance 421 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 11: [-0.23231573402881622, 1.3899849653244019, -1.4357669353485107, -2.0701093673706055, -0.1280215084552765]\n",
      "Grand sum of 329 tensor sets is: [131.4911346435547, 537.9608154296875, -91.83174896240234, -171.14218139648438, -160.55088806152344]\n",
      "\n",
      "Instance 422 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 423 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 13: [0.47765398025512695, 2.43542742729187, -0.32682502269744873, -1.395068883895874, -1.021371603012085]\n",
      "Grand sum of 330 tensor sets is: [131.9687957763672, 540.396240234375, -92.15857696533203, -172.53724670410156, -161.572265625]\n",
      "\n",
      "Instance 424 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 4: [1.0743470191955566, 2.460909843444824, 0.7353526949882507, 2.5253076553344727, -1.333719253540039]\n",
      "Grand sum of 331 tensor sets is: [133.0431365966797, 542.857177734375, -91.42322540283203, -170.01193237304688, -162.90599060058594]\n",
      "\n",
      "Instance 425 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28, 42]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 28: [0.3767508268356323, 2.9014384746551514, -0.28451406955718994, -0.1394987404346466, 0.7627851963043213]\n",
      "car at index 42: [0.8871376514434814, 2.272238254547119, -0.8910757899284363, 1.633959412574768, 3.165679931640625]\n",
      "Grand sum of 332 tensor sets is: [133.67507934570312, 545.4440307617188, -92.01101684570312, -169.26470947265625, -160.94175720214844]\n",
      "\n",
      "Instance 426 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 15: [0.41964206099510193, 1.667968988418579, -0.7313964366912842, -1.4021127223968506, -1.0729851722717285]\n",
      "Grand sum of 333 tensor sets is: [134.0947265625, 547.1119995117188, -92.74241638183594, -170.6668243408203, -162.01473999023438]\n",
      "\n",
      "Instance 427 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 2: [0.8322902917861938, -0.18066328763961792, -0.4571811258792877, -2.654418468475342, 0.134853333234787]\n",
      "Grand sum of 334 tensor sets is: [134.92701721191406, 546.9313354492188, -93.19960021972656, -173.3212432861328, -161.8798828125]\n",
      "\n",
      "Instance 428 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 26: [-0.4581974744796753, 2.551347494125366, 0.7303729057312012, -1.2247097492218018, -2.4853901863098145]\n",
      "Grand sum of 335 tensor sets is: [134.4688262939453, 549.482666015625, -92.46923065185547, -174.54595947265625, -164.3652801513672]\n",
      "\n",
      "Instance 429 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 8: [0.4069819152355194, 0.2695434093475342, -0.2922598123550415, 0.907774031162262, 1.7789682149887085]\n",
      "Grand sum of 336 tensor sets is: [134.8758087158203, 549.752197265625, -92.76148986816406, -173.63818359375, -162.58631896972656]\n",
      "\n",
      "Instance 430 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 6: [0.149325430393219, 2.244922399520874, -0.5309382677078247, -0.7042561769485474, -1.539474606513977]\n",
      "Grand sum of 337 tensor sets is: [135.02513122558594, 551.9971313476562, -93.29242706298828, -174.34243774414062, -164.12579345703125]\n",
      "\n",
      "Instance 431 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 13: [0.5469253063201904, 2.283252000808716, -1.1983590126037598, -2.55601167678833, -0.614571750164032]\n",
      "Grand sum of 338 tensor sets is: [135.57205200195312, 554.2803955078125, -94.49078369140625, -176.89845275878906, -164.74037170410156]\n",
      "\n",
      "Instance 432 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 433 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 434 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 8: [0.26028314232826233, 0.6491790413856506, -0.6574479341506958, -0.810054361820221, -0.9034333825111389]\n",
      "Grand sum of 339 tensor sets is: [135.83233642578125, 554.9295654296875, -95.14823150634766, -177.70851135253906, -165.643798828125]\n",
      "\n",
      "Instance 435 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 5: [0.5070884823799133, 0.6364556550979614, -0.600834310054779, 0.9662835001945496, 0.27908235788345337]\n",
      "Grand sum of 340 tensor sets is: [136.3394317626953, 555.5660400390625, -95.74906921386719, -176.7422332763672, -165.36471557617188]\n",
      "\n",
      "Instance 436 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10, 29, 34]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 10: [0.06379257142543793, 2.715104579925537, 0.1826101839542389, -2.0023605823516846, -0.5821075439453125]\n",
      "car at index 29: [-1.1381171941757202, 0.5176036357879639, -1.3494983911514282, -0.19230879843235016, 3.7179830074310303]\n",
      "car at index 34: [0.23672492802143097, 1.9892981052398682, -1.0361040830612183, -0.6271408796310425, 1.3992183208465576]\n",
      "Grand sum of 341 tensor sets is: [136.0602264404297, 557.3067016601562, -96.4833984375, -177.68283081054688, -163.85301208496094]\n",
      "\n",
      "Instance 437 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 13: [0.13683241605758667, 0.05054934695363045, -0.74679034948349, -0.515249490737915, -1.8147729635238647]\n",
      "Grand sum of 342 tensor sets is: [136.19705200195312, 557.3572387695312, -97.23018646240234, -178.1980743408203, -165.66778564453125]\n",
      "\n",
      "Instance 438 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 9: [0.644549548625946, 1.072659969329834, -0.38831189274787903, 0.3961562514305115, -1.6686755418777466]\n",
      "Grand sum of 343 tensor sets is: [136.8415985107422, 558.4298706054688, -97.61849975585938, -177.8019256591797, -167.33645629882812]\n",
      "\n",
      "Instance 439 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 12: [0.2337702363729477, 1.7839183807373047, -1.4331212043762207, -2.209064245223999, 2.251455068588257]\n",
      "Grand sum of 344 tensor sets is: [137.0753631591797, 560.2138061523438, -99.05162048339844, -180.010986328125, -165.0850067138672]\n",
      "\n",
      "Instance 440 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24, 31]\n",
      "Size of token embeddings is torch.Size([90, 13, 768])\n",
      "Shape of summed layers is: 90 x 768\n",
      "car at index 24: [0.12845328450202942, 0.4844805598258972, -1.1627213954925537, 0.15513193607330322, 2.078367233276367]\n",
      "car at index 31: [0.8251590728759766, 1.6745623350143433, 0.2386614978313446, 0.12708795070648193, 0.1465202122926712]\n",
      "Grand sum of 345 tensor sets is: [137.5521697998047, 561.2933349609375, -99.5136489868164, -179.869873046875, -163.97256469726562]\n",
      "\n",
      "Instance 441 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 12: [-0.7240705490112305, -0.425949364900589, 0.009973570704460144, -0.02660425379872322, 3.2528185844421387]\n",
      "Grand sum of 346 tensor sets is: [136.82809448242188, 560.8673706054688, -99.50367736816406, -179.896484375, -160.71974182128906]\n",
      "\n",
      "Instance 442 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 4: [-1.087526798248291, 2.0608410835266113, -0.5781281590461731, -1.00679349899292, -1.3534055948257446]\n",
      "Grand sum of 347 tensor sets is: [135.74057006835938, 562.92822265625, -100.08180236816406, -180.9032745361328, -162.07315063476562]\n",
      "\n",
      "Instance 443 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 444 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 445 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 446 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 7: [0.9843472242355347, 1.7888858318328857, -0.26994821429252625, -0.2707080841064453, -1.2288143634796143]\n",
      "Grand sum of 348 tensor sets is: [136.72491455078125, 564.7171020507812, -100.35175323486328, -181.17398071289062, -163.30197143554688]\n",
      "\n",
      "Instance 447 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 8: [0.4470725953578949, 2.8002724647521973, 0.5797140598297119, -1.9738084077835083, -2.7573020458221436]\n",
      "Grand sum of 349 tensor sets is: [137.17198181152344, 567.5173950195312, -99.77204132080078, -183.14779663085938, -166.0592803955078]\n",
      "\n",
      "Instance 448 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 25: [1.243632197380066, 0.08655061572790146, -1.0052305459976196, 1.3921175003051758, 1.5044739246368408]\n",
      "Grand sum of 350 tensor sets is: [138.41561889648438, 567.6039428710938, -100.77727508544922, -181.75567626953125, -164.5548095703125]\n",
      "\n",
      "Instance 449 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 19: [-0.8770463466644287, 2.3584444522857666, -0.9713541865348816, 2.2464067935943604, -3.7268214225769043]\n",
      "Grand sum of 351 tensor sets is: [137.53857421875, 569.96240234375, -101.74862670898438, -179.50926208496094, -168.28163146972656]\n",
      "\n",
      "Instance 450 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 8: [0.9001522064208984, -1.6615262031555176, -0.34176504611968994, 2.6409108638763428, 2.464395523071289]\n",
      "Grand sum of 352 tensor sets is: [138.438720703125, 568.3009033203125, -102.09039306640625, -176.86834716796875, -165.81723022460938]\n",
      "\n",
      "Instance 451 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 26]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 7: [-0.1612161099910736, 1.3224862813949585, -1.4740134477615356, 1.1528141498565674, -2.570404529571533]\n",
      "car at index 26: [0.4908154010772705, 2.349756956100464, -0.22117879986763, -1.3732266426086426, -1.9686386585235596]\n",
      "Grand sum of 353 tensor sets is: [138.603515625, 570.1370239257812, -102.93798828125, -176.97854614257812, -168.0867462158203]\n",
      "\n",
      "Instance 452 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 4: [0.7138966917991638, 1.6262640953063965, 0.12204430252313614, 0.8318769931793213, 1.2118091583251953]\n",
      "Grand sum of 354 tensor sets is: [139.31741333007812, 571.7633056640625, -102.8159408569336, -176.14666748046875, -166.87493896484375]\n",
      "\n",
      "Instance 453 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 4: [1.2161328792572021, 1.5045976638793945, 0.07304747402667999, 0.22255916893482208, -0.12487603724002838]\n",
      "Grand sum of 355 tensor sets is: [140.53353881835938, 573.2678833007812, -102.7428970336914, -175.92410278320312, -166.99981689453125]\n",
      "\n",
      "Instance 454 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 16: [0.343478262424469, 2.2798521518707275, -0.0430365689098835, 1.5243773460388184, -0.9732587933540344]\n",
      "Grand sum of 356 tensor sets is: [140.87701416015625, 575.5477294921875, -102.78593444824219, -174.39971923828125, -167.9730682373047]\n",
      "\n",
      "Instance 455 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 13: [-0.10208351910114288, 2.6199867725372314, -0.0009655638132244349, -1.915022373199463, -1.2462221384048462]\n",
      "Grand sum of 357 tensor sets is: [140.77493286132812, 578.167724609375, -102.78690338134766, -176.3147430419922, -169.2192840576172]\n",
      "\n",
      "Instance 456 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "car at index 30: [0.2855684459209442, 0.744084894657135, -0.9314594268798828, 1.1044799089431763, 0.3495582342147827]\n",
      "Grand sum of 358 tensor sets is: [141.0605010986328, 578.9118041992188, -103.7183609008789, -175.21026611328125, -168.86972045898438]\n",
      "\n",
      "Instance 457 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 4: [0.9677586555480957, 2.2568700313568115, -0.5572208166122437, -0.9860966205596924, -0.3638218939304352]\n",
      "Grand sum of 359 tensor sets is: [142.02825927734375, 581.168701171875, -104.27558135986328, -176.1963653564453, -169.23353576660156]\n",
      "\n",
      "Instance 458 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 28: [0.26821786165237427, 1.0432816743850708, -0.21907195448875427, -1.254104733467102, -1.622941255569458]\n",
      "Grand sum of 360 tensor sets is: [142.29647827148438, 582.2119750976562, -104.4946517944336, -177.45046997070312, -170.85647583007812]\n",
      "\n",
      "Instance 459 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "car at index 32: [0.29387354850769043, 2.096377372741699, 0.1450914591550827, 1.2363312244415283, -1.6591726541519165]\n",
      "Grand sum of 361 tensor sets is: [142.59034729003906, 584.308349609375, -104.34956359863281, -176.21414184570312, -172.51565551757812]\n",
      "\n",
      "Instance 460 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 461 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 462 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 7: [0.7777096033096313, 2.3774333000183105, -1.2515910863876343, -0.08653264492750168, -1.6919312477111816]\n",
      "Grand sum of 362 tensor sets is: [143.36805725097656, 586.685791015625, -105.60115814208984, -176.30067443847656, -174.20758056640625]\n",
      "\n",
      "Instance 463 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 464 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 17: [0.43943050503730774, 1.5181951522827148, 0.761936366558075, 0.274836003780365, -1.1576857566833496]\n",
      "Grand sum of 363 tensor sets is: [143.8074951171875, 588.2039794921875, -104.83921813964844, -176.0258331298828, -175.36526489257812]\n",
      "\n",
      "Instance 465 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 38]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 11: [-0.35130247473716736, 1.1970731019973755, -0.22023624181747437, -0.30349865555763245, 1.4957518577575684]\n",
      "car at index 38: [0.19579508900642395, 1.3092209100723267, 0.34306132793426514, -0.6682029962539673, -1.2406563758850098]\n",
      "Grand sum of 364 tensor sets is: [143.729736328125, 589.4571533203125, -104.7778091430664, -176.51168823242188, -175.2377166748047]\n",
      "\n",
      "Instance 466 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 467 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 468 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 7: [0.27276816964149475, 1.6426448822021484, -0.4876312017440796, 1.097376823425293, -3.001771926879883]\n",
      "Grand sum of 365 tensor sets is: [144.00250244140625, 591.0997924804688, -105.26544189453125, -175.414306640625, -178.23948669433594]\n",
      "\n",
      "Instance 469 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([95, 13, 768])\n",
      "Shape of summed layers is: 95 x 768\n",
      "car at index 38: [1.0784648656845093, -0.259628564119339, -0.9062941670417786, 2.2294058799743652, 2.507660150527954]\n",
      "Grand sum of 366 tensor sets is: [145.08096313476562, 590.8401489257812, -106.17173767089844, -173.18490600585938, -175.73182678222656]\n",
      "\n",
      "Instance 470 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 471 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [56]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "car at index 56: [0.35257697105407715, 1.0829854011535645, -0.11393070966005325, -2.626082181930542, 0.9132336974143982]\n",
      "Grand sum of 367 tensor sets is: [145.43353271484375, 591.9231567382812, -106.2856674194336, -175.8109893798828, -174.81858825683594]\n",
      "\n",
      "Instance 472 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 473 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 474 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 12: [0.8519378900527954, 2.8689799308776855, 0.7527816295623779, 0.03981081396341324, 0.31139978766441345]\n",
      "Grand sum of 368 tensor sets is: [146.2854766845703, 594.7921142578125, -105.53288269042969, -175.77117919921875, -174.50718688964844]\n",
      "\n",
      "Instance 475 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 14: [-0.5255922675132751, 2.2718944549560547, 0.9960905909538269, -1.325736403465271, -3.1449475288391113]\n",
      "Grand sum of 369 tensor sets is: [145.7598876953125, 597.0640258789062, -104.53678894042969, -177.09690856933594, -177.65213012695312]\n",
      "\n",
      "Instance 476 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 477 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 22: [0.3926890194416046, 2.3187947273254395, -0.45652681589126587, -1.296282172203064, -0.5425872802734375]\n",
      "Grand sum of 370 tensor sets is: [146.15257263183594, 599.3828125, -104.99331665039062, -178.3931884765625, -178.19471740722656]\n",
      "\n",
      "Instance 478 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 17: [0.025498364120721817, 1.9729957580566406, -0.23400667309761047, 0.544478714466095, -1.8472096920013428]\n",
      "Grand sum of 371 tensor sets is: [146.17807006835938, 601.3558349609375, -105.22732543945312, -177.8487091064453, -180.04193115234375]\n",
      "\n",
      "Instance 479 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 10: [0.05521337687969208, 1.866254210472107, -0.59792160987854, -0.010453209280967712, 0.7434837222099304]\n",
      "Grand sum of 372 tensor sets is: [146.2332763671875, 603.2221069335938, -105.82524871826172, -177.85916137695312, -179.29844665527344]\n",
      "\n",
      "Instance 480 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 18: [0.41294029355049133, 1.8713879585266113, -0.10025812685489655, -1.5754495859146118, -0.9838671088218689]\n",
      "Grand sum of 373 tensor sets is: [146.64620971679688, 605.093505859375, -105.92550659179688, -179.4346160888672, -180.28231811523438]\n",
      "\n",
      "Instance 481 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 8: [-0.04236839711666107, 1.5077775716781616, -0.2729131579399109, -0.7114459872245789, 0.5644267201423645]\n",
      "Grand sum of 374 tensor sets is: [146.6038360595703, 606.6012573242188, -106.19841766357422, -180.14605712890625, -179.7178955078125]\n",
      "\n",
      "Instance 482 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 11: [0.5335162878036499, 3.2483673095703125, -1.1040942668914795, -0.13979846239089966, 1.765655517578125]\n",
      "Grand sum of 375 tensor sets is: [147.13735961914062, 609.849609375, -107.3025131225586, -180.28585815429688, -177.95223999023438]\n",
      "\n",
      "Instance 483 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 12: [-0.0711386650800705, 1.7487798929214478, -0.9113752841949463, -0.8606587052345276, -1.371340274810791]\n",
      "Grand sum of 376 tensor sets is: [147.06622314453125, 611.598388671875, -108.2138900756836, -181.14651489257812, -179.32357788085938]\n",
      "\n",
      "Instance 484 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 39: [0.18504676222801208, 2.2615065574645996, -0.9813490509986877, -0.7828158140182495, -0.7942208647727966]\n",
      "Grand sum of 377 tensor sets is: [147.2512664794922, 613.8599243164062, -109.19523620605469, -181.92933654785156, -180.1177978515625]\n",
      "\n",
      "Instance 485 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 486 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([88, 13, 768])\n",
      "Shape of summed layers is: 88 x 768\n",
      "car at index 23: [0.48948949575424194, 0.9472306966781616, -1.006411075592041, -0.13037174940109253, 2.240997314453125]\n",
      "Grand sum of 378 tensor sets is: [147.74075317382812, 614.80712890625, -110.20164489746094, -182.05970764160156, -177.87680053710938]\n",
      "\n",
      "Instance 487 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 488 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 489 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 33: [0.021689623594284058, 0.5746254324913025, 0.3911862075328827, -1.9885351657867432, -2.6514077186584473]\n",
      "Grand sum of 379 tensor sets is: [147.76243591308594, 615.3817749023438, -109.81045532226562, -184.04824829101562, -180.52821350097656]\n",
      "\n",
      "Instance 490 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 8: [0.010798223316669464, 2.6458938121795654, -0.23778943717479706, -1.3706427812576294, -1.6440765857696533]\n",
      "Grand sum of 380 tensor sets is: [147.7732391357422, 618.0276489257812, -110.04824829101562, -185.41888427734375, -182.1722869873047]\n",
      "\n",
      "Instance 491 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 7: [0.27845293283462524, 2.104553461074829, 0.41275250911712646, 0.4705488085746765, -2.4905214309692383]\n",
      "Grand sum of 381 tensor sets is: [148.05169677734375, 620.1322021484375, -109.635498046875, -184.94833374023438, -184.66281127929688]\n",
      "\n",
      "Instance 492 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "car at index 72: [-0.7067630887031555, 1.8062316179275513, 0.1990259438753128, -2.2175607681274414, -0.81867516040802]\n",
      "Grand sum of 382 tensor sets is: [147.34494018554688, 621.9384155273438, -109.43647003173828, -187.1658935546875, -185.4814910888672]\n",
      "\n",
      "Instance 493 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 21: [0.108668252825737, 1.9764162302017212, 0.19958937168121338, 2.043700695037842, -2.095926284790039]\n",
      "Grand sum of 383 tensor sets is: [147.45361328125, 623.9148559570312, -109.23687744140625, -185.1221923828125, -187.57742309570312]\n",
      "\n",
      "Instance 494 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 16: [0.032308802008628845, 2.7816433906555176, 0.7268868088722229, -2.083400011062622, -2.9761486053466797]\n",
      "Grand sum of 384 tensor sets is: [147.4859161376953, 626.6964721679688, -108.5099868774414, -187.20559692382812, -190.55357360839844]\n",
      "\n",
      "Instance 495 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 496 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 14: [0.8011747598648071, 2.5871999263763428, -0.48776888847351074, -1.3634274005889893, -0.11514811962842941]\n",
      "Grand sum of 385 tensor sets is: [148.28709411621094, 629.28369140625, -108.99775695800781, -188.56903076171875, -190.66871643066406]\n",
      "\n",
      "Instance 497 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [239]\n",
      "Size of token embeddings is torch.Size([479, 13, 768])\n",
      "Shape of summed layers is: 479 x 768\n",
      "car at index 239: [0.5092828869819641, 2.3015294075012207, -1.2857946157455444, -1.3571546077728271, 1.0762240886688232]\n",
      "Grand sum of 386 tensor sets is: [148.79637145996094, 631.585205078125, -110.28355407714844, -189.92617797851562, -189.59249877929688]\n",
      "\n",
      "Instance 498 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 499 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [94]\n",
      "Size of token embeddings is torch.Size([102, 13, 768])\n",
      "Shape of summed layers is: 102 x 768\n",
      "car at index 94: [0.22952422499656677, 1.2106962203979492, -1.5969711542129517, -0.484553724527359, 0.42193126678466797]\n",
      "Grand sum of 387 tensor sets is: [149.02589416503906, 632.7958984375, -111.88052368164062, -190.41073608398438, -189.17056274414062]\n",
      "\n",
      "Instance 500 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 501 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 10: [0.49154549837112427, 1.838173747062683, -0.10559230297803879, 1.8641197681427002, -0.7551779747009277]\n",
      "Grand sum of 388 tensor sets is: [149.51744079589844, 634.6340942382812, -111.98611450195312, -188.54661560058594, -189.9257354736328]\n",
      "\n",
      "Instance 502 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 20: [-0.09143774211406708, 0.84428471326828, -1.0830179452896118, -0.5708245635032654, 2.350158214569092]\n",
      "Grand sum of 389 tensor sets is: [149.42601013183594, 635.4783935546875, -113.06912994384766, -189.11744689941406, -187.57557678222656]\n",
      "\n",
      "Instance 503 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 6: [0.8474651575088501, 1.2962942123413086, -0.3142300844192505, -0.877011239528656, -1.8376013040542603]\n",
      "Grand sum of 390 tensor sets is: [150.27346801757812, 636.774658203125, -113.38336181640625, -189.9944610595703, -189.41317749023438]\n",
      "\n",
      "Instance 504 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 505 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9, 28]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 9: [0.37108510732650757, 1.5912389755249023, -0.3206738233566284, 2.2604541778564453, -2.5518016815185547]\n",
      "car at index 28: [-0.4322262406349182, 1.8756133317947388, 0.8683567047119141, 1.9392597675323486, -3.2711193561553955]\n",
      "Grand sum of 391 tensor sets is: [150.24290466308594, 638.508056640625, -113.1095199584961, -187.8946075439453, -192.3246307373047]\n",
      "\n",
      "Instance 506 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "car at index 4: [0.8557190895080566, 2.9219398498535156, -0.4479459226131439, -0.1622987687587738, -1.232448697090149]\n",
      "Grand sum of 392 tensor sets is: [151.09861755371094, 641.4299926757812, -113.55746459960938, -188.05690002441406, -193.5570831298828]\n",
      "\n",
      "Instance 507 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 17: [0.8383500576019287, 2.348914384841919, -0.746776819229126, -0.8713098168373108, -2.1721103191375732]\n",
      "Grand sum of 393 tensor sets is: [151.9369659423828, 643.7789306640625, -114.30424499511719, -188.92820739746094, -195.72918701171875]\n",
      "\n",
      "Instance 508 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 11: [-0.5712490081787109, 1.7486186027526855, -1.3120245933532715, -2.099252700805664, -0.9436187744140625]\n",
      "car at index 15: [-1.1396976709365845, 2.8101768493652344, 0.44345545768737793, -2.2842137813568115, -3.984623908996582]\n",
      "Grand sum of 394 tensor sets is: [151.0814971923828, 646.058349609375, -114.73853302001953, -191.11993408203125, -198.1933135986328]\n",
      "\n",
      "Instance 509 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 11: [0.3824690878391266, 0.00011278688907623291, 0.36278849840164185, -1.3080021142959595, 0.9468058347702026]\n",
      "Grand sum of 395 tensor sets is: [151.46395874023438, 646.0584716796875, -114.37574768066406, -192.4279327392578, -197.2465057373047]\n",
      "\n",
      "Instance 510 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 22: [0.13972903788089752, 1.6508605480194092, 0.12112656980752945, 2.1546783447265625, -1.567554235458374]\n",
      "Grand sum of 396 tensor sets is: [151.6036834716797, 647.7093505859375, -114.25462341308594, -190.27325439453125, -198.81405639648438]\n",
      "\n",
      "Instance 511 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 512 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9, 15]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of summed layers is: 29 x 768\n",
      "car at index 9: [0.9632831811904907, 1.932384729385376, -0.17724575102329254, 0.4109402894973755, 1.4906935691833496]\n",
      "car at index 15: [0.5347787141799927, 2.005988359451294, -0.11364760994911194, -0.20806507766246796, 1.259199619293213]\n",
      "Grand sum of 397 tensor sets is: [152.3527069091797, 649.6785278320312, -114.40007019042969, -190.17181396484375, -197.43911743164062]\n",
      "\n",
      "Instance 513 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 514 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 11: [0.11484791338443756, 0.11328140646219254, -0.5895557403564453, -0.6117208003997803, 1.0949852466583252]\n",
      "Grand sum of 398 tensor sets is: [152.46755981445312, 649.7918090820312, -114.9896240234375, -190.78353881835938, -196.34413146972656]\n",
      "\n",
      "Instance 515 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3, 17]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 3: [0.22148124873638153, 1.5659033060073853, -0.7977163791656494, -2.4462766647338867, 0.3301709294319153]\n",
      "car at index 17: [-0.9005393981933594, 0.10270652174949646, -0.1068553626537323, -1.1438729763031006, 0.7169685363769531]\n",
      "Grand sum of 399 tensor sets is: [152.12803649902344, 650.6260986328125, -115.44190979003906, -192.57861328125, -195.820556640625]\n",
      "\n",
      "Instance 516 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 13: [0.13004562258720398, 0.6193621754646301, -0.3062000870704651, -2.667123317718506, 0.6703780889511108]\n",
      "Grand sum of 400 tensor sets is: [152.25808715820312, 651.2454833984375, -115.74810791015625, -195.24574279785156, -195.15017700195312]\n",
      "\n",
      "Instance 517 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 518 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 15: [0.6786848306655884, 2.9257547855377197, -1.204655647277832, 0.6486389636993408, -0.48496532440185547]\n",
      "Grand sum of 401 tensor sets is: [152.936767578125, 654.1712646484375, -116.95276641845703, -194.59710693359375, -195.63514709472656]\n",
      "\n",
      "Instance 519 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 520 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18, 25]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 18: [-0.003603711724281311, 1.970051646232605, 0.2930489480495453, -2.4771621227264404, -0.44368934631347656]\n",
      "car at index 25: [-0.5551748275756836, 2.8703346252441406, 0.3517225980758667, -1.6210546493530273, -1.4635977745056152]\n",
      "Grand sum of 402 tensor sets is: [152.65737915039062, 656.5914306640625, -116.63037872314453, -196.64620971679688, -196.5887908935547]\n",
      "\n",
      "Instance 521 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 522 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 7: [0.5711167454719543, 1.6237972974777222, -0.34582459926605225, -2.4094462394714355, -0.5213291645050049]\n",
      "Grand sum of 403 tensor sets is: [153.22850036621094, 658.2152099609375, -116.97620391845703, -199.05564880371094, -197.11012268066406]\n",
      "\n",
      "Instance 523 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 4: [-0.6140874624252319, 0.8816219568252563, -0.5185337066650391, -2.616683006286621, 0.20548829436302185]\n",
      "Grand sum of 404 tensor sets is: [152.61441040039062, 659.0968017578125, -117.49473571777344, -201.67233276367188, -196.90463256835938]\n",
      "\n",
      "Instance 524 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 11: [-0.5712490081787109, 1.7486186027526855, -1.3120245933532715, -2.099252700805664, -0.9436187744140625]\n",
      "car at index 15: [-1.1396976709365845, 2.8101768493652344, 0.44345545768737793, -2.2842137813568115, -3.984623908996582]\n",
      "Grand sum of 405 tensor sets is: [151.75894165039062, 661.376220703125, -117.92902374267578, -203.8640594482422, -199.36875915527344]\n",
      "\n",
      "Instance 525 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 9: [-0.8804936408996582, 0.30569812655448914, -0.2187635451555252, 0.3660091459751129, -1.1881449222564697]\n",
      "Grand sum of 406 tensor sets is: [150.87844848632812, 661.6819458007812, -118.14778900146484, -203.498046875, -200.55690002441406]\n",
      "\n",
      "Instance 526 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 28: [0.9740496277809143, 1.4473093748092651, -0.16698570549488068, -1.2074788808822632, -1.4072633981704712]\n",
      "Grand sum of 407 tensor sets is: [151.8524932861328, 663.1292724609375, -118.31477355957031, -204.7055206298828, -201.9641571044922]\n",
      "\n",
      "Instance 527 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 6: [0.3311442732810974, 1.3595337867736816, 0.13302725553512573, -1.9638500213623047, -2.728050947189331]\n",
      "Grand sum of 408 tensor sets is: [152.1836395263672, 664.4888305664062, -118.18174743652344, -206.66937255859375, -204.6922149658203]\n",
      "\n",
      "Instance 528 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5, 18]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 5: [0.6244977712631226, 1.7069101333618164, -1.1706483364105225, -2.716416358947754, 3.3088126182556152]\n",
      "car at index 18: [0.19902917742729187, 1.1734966039657593, -0.9049861431121826, -0.7307264804840088, 0.27703699469566345]\n",
      "Grand sum of 409 tensor sets is: [152.59539794921875, 665.9290161132812, -119.21956634521484, -208.3929443359375, -202.8992919921875]\n",
      "\n",
      "Instance 529 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "car at index 4: [0.4838919937610626, -0.7686741948127747, -0.3494938015937805, 2.4379758834838867, -0.6602668166160583]\n",
      "Grand sum of 410 tensor sets is: [153.07928466796875, 665.1603393554688, -119.56906127929688, -205.95497131347656, -203.55955505371094]\n",
      "\n",
      "Instance 530 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 3: [0.38231903314590454, 0.41505181789398193, -0.706990659236908, -1.3644216060638428, -1.0797584056854248]\n",
      "Grand sum of 411 tensor sets is: [153.46160888671875, 665.5753784179688, -120.27605438232422, -207.31939697265625, -204.63931274414062]\n",
      "\n",
      "Instance 531 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 17: [0.8031042814254761, 2.594121217727661, 0.5965563654899597, -1.964526653289795, -1.86554753780365]\n",
      "Grand sum of 412 tensor sets is: [154.26470947265625, 668.1694946289062, -119.67949676513672, -209.28392028808594, -206.50486755371094]\n",
      "\n",
      "Instance 532 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 533 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 27: [0.4309835135936737, 1.7531554698944092, -0.4335808753967285, -1.205444574356079, 0.5238633751869202]\n",
      "Grand sum of 413 tensor sets is: [154.69569396972656, 669.9226684570312, -120.11307525634766, -210.48936462402344, -205.9810028076172]\n",
      "\n",
      "Instance 534 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23, 26]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 23: [0.605850338935852, -0.24745018780231476, -1.2923083305358887, -0.4960530996322632, 2.3215582370758057]\n",
      "car at index 26: [1.4227017164230347, -0.9223092794418335, -1.150046944618225, -2.0586299896240234, 0.9472756385803223]\n",
      "Grand sum of 414 tensor sets is: [155.70997619628906, 669.3377685546875, -121.3342514038086, -211.76670837402344, -204.34658813476562]\n",
      "\n",
      "Instance 535 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 536 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "car at index 13: [0.12920592725276947, 1.259018898010254, -0.27367350459098816, -1.4303935766220093, -1.0446326732635498]\n",
      "Grand sum of 415 tensor sets is: [155.8391876220703, 670.5968017578125, -121.60792541503906, -213.1970977783203, -205.39122009277344]\n",
      "\n",
      "Instance 537 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14, 32]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 14: [-0.028741423040628433, 2.6669607162475586, -0.4537283182144165, -0.4470214247703552, -2.0245518684387207]\n",
      "car at index 32: [-1.0330157279968262, 2.6409780979156494, 1.1550147533416748, -0.42903220653533936, -2.314284086227417]\n",
      "Grand sum of 416 tensor sets is: [155.3083038330078, 673.2507934570312, -121.25727844238281, -213.6351318359375, -207.56063842773438]\n",
      "\n",
      "Instance 538 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 10: [0.8846102952957153, 1.0995186567306519, 0.647093653678894, 0.38549065589904785, -1.7942328453063965]\n",
      "Grand sum of 417 tensor sets is: [156.1929168701172, 674.350341796875, -120.61018371582031, -213.2496337890625, -209.35487365722656]\n",
      "\n",
      "Instance 539 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 9: [0.1513398438692093, 0.4508417248725891, -0.9161699414253235, -0.15987929701805115, 0.7395389080047607]\n",
      "Grand sum of 418 tensor sets is: [156.34425354003906, 674.8012084960938, -121.52635192871094, -213.40951538085938, -208.61534118652344]\n",
      "\n",
      "Instance 540 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "car at index 26: [0.2142302691936493, 1.4943246841430664, -0.4761042594909668, -1.9918711185455322, -0.5535510778427124]\n",
      "Grand sum of 419 tensor sets is: [156.55848693847656, 676.2955322265625, -122.00245666503906, -215.40138244628906, -209.1688995361328]\n",
      "\n",
      "Instance 541 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 6: [0.2799994945526123, 1.7301357984542847, 0.6284930109977722, 1.168021321296692, -1.9865148067474365]\n",
      "Grand sum of 420 tensor sets is: [156.83848571777344, 678.0256958007812, -121.37396240234375, -214.23336791992188, -211.15541076660156]\n",
      "\n",
      "Instance 542 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 4: [-0.47766852378845215, 1.9498649835586548, 0.5655648708343506, 1.778384804725647, -0.9608526229858398]\n",
      "Grand sum of 421 tensor sets is: [156.36082458496094, 679.9755859375, -120.80839538574219, -212.45498657226562, -212.1162567138672]\n",
      "\n",
      "Instance 543 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 7: [0.9945201277732849, 2.4604671001434326, -0.7000160813331604, -2.6965742111206055, -0.14506962895393372]\n",
      "Grand sum of 422 tensor sets is: [157.3553466796875, 682.43603515625, -121.50841522216797, -215.1515655517578, -212.26132202148438]\n",
      "\n",
      "Instance 544 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "car at index 5: [0.08103294670581818, 1.0153964757919312, 0.3794095814228058, 2.432288408279419, -1.9490017890930176]\n",
      "Grand sum of 423 tensor sets is: [157.43638610839844, 683.451416015625, -121.1290054321289, -212.7192840576172, -214.2103271484375]\n",
      "\n",
      "Instance 545 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 546 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 11: [0.8021911382675171, 2.108504295349121, -0.5213167071342468, -1.4388600587844849, 1.9471386671066284]\n",
      "Grand sum of 424 tensor sets is: [158.2385711669922, 685.5599365234375, -121.65032196044922, -214.15814208984375, -212.26318359375]\n",
      "\n",
      "Instance 547 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 548 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18, 25]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 18: [-0.003603711724281311, 1.970051646232605, 0.2930489480495453, -2.4771621227264404, -0.44368934631347656]\n",
      "car at index 25: [-0.5551748275756836, 2.8703346252441406, 0.3517225980758667, -1.6210546493530273, -1.4635977745056152]\n",
      "Grand sum of 425 tensor sets is: [157.9591827392578, 687.9801025390625, -121.32793426513672, -216.20724487304688, -213.21682739257812]\n",
      "\n",
      "Instance 549 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 5: [0.49584248661994934, 2.763484001159668, 0.03819391876459122, -1.4199602603912354, -0.9309557676315308]\n",
      "Grand sum of 426 tensor sets is: [158.4550323486328, 690.7435913085938, -121.28974151611328, -217.62721252441406, -214.1477813720703]\n",
      "\n",
      "Instance 550 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [144, 224]\n",
      "Size of token embeddings is torch.Size([332, 13, 768])\n",
      "Shape of summed layers is: 332 x 768\n",
      "car at index 144: [0.2464122325181961, 0.27416467666625977, 0.5557887554168701, -1.2691649198532104, 3.539153575897217]\n",
      "car at index 224: [0.14813807606697083, 0.5486316084861755, 0.4255189895629883, -0.8433776497840881, 3.0580368041992188]\n",
      "Grand sum of 427 tensor sets is: [158.65231323242188, 691.1549682617188, -120.79908752441406, -218.68348693847656, -210.84918212890625]\n",
      "\n",
      "Instance 551 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2, 13]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 2: [0.0024353861808776855, -0.5221388936042786, -0.6291570663452148, 0.20091865956783295, 0.07873540371656418]\n",
      "car at index 13: [-0.08519414067268372, 0.6121347546577454, 0.37979966402053833, -0.3082943558692932, -0.5118504762649536]\n",
      "Grand sum of 428 tensor sets is: [158.61093139648438, 691.199951171875, -120.92376708984375, -218.73716735839844, -211.06573486328125]\n",
      "\n",
      "Instance 552 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([80, 13, 768])\n",
      "Shape of summed layers is: 80 x 768\n",
      "car at index 53: [1.2981661558151245, -1.2669943571090698, -1.1602296829223633, 0.9607440829277039, 1.4633166790008545]\n",
      "Grand sum of 429 tensor sets is: [159.9091033935547, 689.9329833984375, -122.08399963378906, -217.77642822265625, -209.6024169921875]\n",
      "\n",
      "Instance 553 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 554 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 18: [-0.04235263168811798, 2.957305431365967, -0.3664664030075073, 0.14195924997329712, 1.540205478668213]\n",
      "Grand sum of 430 tensor sets is: [159.8667449951172, 692.8902587890625, -122.4504623413086, -217.6344757080078, -208.0622100830078]\n",
      "\n",
      "Instance 555 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 556 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [160]\n",
      "Size of token embeddings is torch.Size([206, 13, 768])\n",
      "Shape of summed layers is: 206 x 768\n",
      "car at index 160: [0.17268992960453033, -1.7038044929504395, -0.020817428827285767, -0.19675084948539734, 3.894482374191284]\n",
      "Grand sum of 431 tensor sets is: [160.0394287109375, 691.1864624023438, -122.47128295898438, -217.8312225341797, -204.167724609375]\n",
      "\n",
      "Instance 557 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([118, 13, 768])\n",
      "Shape of summed layers is: 118 x 768\n",
      "car at index 10: [1.6979825496673584, -1.329162359237671, 0.3829689025878906, 0.1275065541267395, 4.000295639038086]\n",
      "Grand sum of 432 tensor sets is: [161.73741149902344, 689.8572998046875, -122.08831787109375, -217.70372009277344, -200.1674346923828]\n",
      "\n",
      "Instance 558 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 20: [0.19971168041229248, 2.0627970695495605, -0.10978315770626068, 2.129563331604004, -1.388912558555603]\n",
      "Grand sum of 433 tensor sets is: [161.93711853027344, 691.9201049804688, -122.1980972290039, -215.57415771484375, -201.5563507080078]\n",
      "\n",
      "Instance 559 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 560 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 561 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 29: [-0.05906020104885101, 3.3108887672424316, 0.9607949256896973, 1.2531485557556152, -1.4364572763442993]\n",
      "Grand sum of 434 tensor sets is: [161.8780517578125, 695.2310180664062, -121.2373046875, -214.32101440429688, -202.99281311035156]\n",
      "\n",
      "Instance 562 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "car at index 18: [0.5444456934928894, -0.37898167967796326, 0.7440263032913208, 2.4872515201568604, 0.9466641545295715]\n",
      "Grand sum of 435 tensor sets is: [162.42250061035156, 694.85205078125, -120.49327850341797, -211.83375549316406, -202.046142578125]\n",
      "\n",
      "Instance 563 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 3: [1.0338411331176758, 2.0618643760681152, -1.0974833965301514, -0.40717214345932007, -0.5691633224487305]\n",
      "Grand sum of 436 tensor sets is: [163.4563446044922, 696.9139404296875, -121.59075927734375, -212.2409210205078, -202.6153106689453]\n",
      "\n",
      "Instance 564 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 9: [0.8083247542381287, 2.9312000274658203, -0.18349823355674744, -1.4067795276641846, -0.8584414720535278]\n",
      "Grand sum of 437 tensor sets is: [164.26466369628906, 699.8451538085938, -121.77425384521484, -213.647705078125, -203.4737548828125]\n",
      "\n",
      "Instance 565 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 24: [0.7956030964851379, 0.8015857934951782, -0.30931586027145386, -0.8150531053543091, -0.2117961198091507]\n",
      "Grand sum of 438 tensor sets is: [165.06027221679688, 700.646728515625, -122.08357238769531, -214.46275329589844, -203.685546875]\n",
      "\n",
      "Instance 566 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 567 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 3: [-0.36164748668670654, 1.2718875408172607, 0.2184043675661087, -0.2922457158565521, 0.40338170528411865]\n",
      "Grand sum of 439 tensor sets is: [164.69862365722656, 701.9186401367188, -121.86516571044922, -214.7550048828125, -203.28216552734375]\n",
      "\n",
      "Instance 568 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 6: [0.9470612406730652, 3.072425603866577, -0.6384882926940918, 0.14136841893196106, -1.8074969053268433]\n",
      "Grand sum of 440 tensor sets is: [165.64569091796875, 704.9910888671875, -122.50365447998047, -214.61363220214844, -205.08966064453125]\n",
      "\n",
      "Instance 569 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 14: [1.360046625137329, 1.9964683055877686, -0.6792974472045898, 0.2722092866897583, -1.3093910217285156]\n",
      "Grand sum of 441 tensor sets is: [167.0057373046875, 706.987548828125, -123.18295288085938, -214.34141540527344, -206.3990478515625]\n",
      "\n",
      "Instance 570 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 23: [-0.7837616205215454, 2.223278522491455, 0.570953905582428, -0.4504159986972809, -1.596336007118225]\n",
      "Grand sum of 442 tensor sets is: [166.2219696044922, 709.2108154296875, -122.61199951171875, -214.7918243408203, -207.99537658691406]\n",
      "\n",
      "Instance 571 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 3: [0.1664494127035141, 2.1014699935913086, -0.27840590476989746, 0.19985230267047882, -0.5481601357460022]\n",
      "Grand sum of 443 tensor sets is: [166.38841247558594, 711.312255859375, -122.8904037475586, -214.5919647216797, -208.5435333251953]\n",
      "\n",
      "Instance 572 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 36: [0.4690802991390228, 3.324023962020874, -0.4083843231201172, -1.3767503499984741, 0.9569007158279419]\n",
      "Grand sum of 444 tensor sets is: [166.8574981689453, 714.6362915039062, -123.29878997802734, -215.96871948242188, -207.58663940429688]\n",
      "\n",
      "Instance 573 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 445 tensor sets is: [168.97515869140625, 716.3126831054688, -124.53306579589844, -217.63424682617188, -207.73114013671875]\n",
      "\n",
      "Instance 574 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 15: [0.12409508228302002, 3.4820594787597656, 1.121012568473816, 0.129553884267807, -1.0708402395248413]\n",
      "Grand sum of 446 tensor sets is: [169.09925842285156, 719.7947387695312, -123.41205596923828, -217.50469970703125, -208.80198669433594]\n",
      "\n",
      "Instance 575 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 6: [1.1098469495773315, 1.8769505023956299, -0.372756689786911, 0.836814284324646, -0.7352032661437988]\n",
      "Grand sum of 447 tensor sets is: [170.2091064453125, 721.6716918945312, -123.7848129272461, -216.6678924560547, -209.5371856689453]\n",
      "\n",
      "Instance 576 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 577 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 2: [0.9286375045776367, 2.976095199584961, -0.28917741775512695, -0.6824988126754761, 1.9505293369293213]\n",
      "Grand sum of 448 tensor sets is: [171.1377410888672, 724.6477661132812, -124.07398986816406, -217.3503875732422, -207.58665466308594]\n",
      "\n",
      "Instance 578 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 30: [1.5888558626174927, -1.327389121055603, -0.4883344769477844, -0.13780832290649414, 1.8493813276290894]\n",
      "Grand sum of 449 tensor sets is: [172.72659301757812, 723.3203735351562, -124.56232452392578, -217.48818969726562, -205.73727416992188]\n",
      "\n",
      "Instance 579 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 5: [0.04316799342632294, 1.3872640132904053, -1.5585182905197144, 0.9965226650238037, -1.6701709032058716]\n",
      "Grand sum of 450 tensor sets is: [172.76976013183594, 724.7076416015625, -126.12084197998047, -216.49166870117188, -207.40744018554688]\n",
      "\n",
      "Instance 580 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 581 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 582 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "car at index 24: [0.8113118410110474, 3.062002182006836, -1.2332797050476074, 0.7734972238540649, 0.019958078861236572]\n",
      "Grand sum of 451 tensor sets is: [173.58106994628906, 727.7696533203125, -127.35411834716797, -215.71817016601562, -207.38748168945312]\n",
      "\n",
      "Instance 583 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23, 36]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 23: [0.9872066974639893, 3.3340373039245605, 0.1428825557231903, -0.4420894384384155, -0.7177202701568604]\n",
      "car at index 36: [0.6268947124481201, 4.110189914703369, -0.32236799597740173, -0.006169438362121582, -0.5030748248100281]\n",
      "Grand sum of 452 tensor sets is: [174.38812255859375, 731.4917602539062, -127.44386291503906, -215.9423065185547, -207.9978790283203]\n",
      "\n",
      "Instance 584 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 23: [0.3165281414985657, 2.4757118225097656, -0.04171029478311539, -0.30292588472366333, 0.15111295878887177]\n",
      "Grand sum of 453 tensor sets is: [174.70465087890625, 733.9674682617188, -127.4855728149414, -216.2452392578125, -207.84677124023438]\n",
      "\n",
      "Instance 585 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 9: [0.8256231546401978, 1.5452812910079956, -1.3581942319869995, -0.7057311534881592, -1.415822982788086]\n",
      "Grand sum of 454 tensor sets is: [175.5302734375, 735.5127563476562, -128.84376525878906, -216.9509735107422, -209.26258850097656]\n",
      "\n",
      "Instance 586 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 587 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 23: [1.4639137983322144, 1.3161334991455078, 0.10234138369560242, 1.0238252878189087, -1.214837908744812]\n",
      "Grand sum of 455 tensor sets is: [176.9941864013672, 736.8289184570312, -128.74142456054688, -215.92715454101562, -210.47743225097656]\n",
      "\n",
      "Instance 588 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 589 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 7: [-1.1403917074203491, 4.203405380249023, -1.009628415107727, 0.46133244037628174, -3.053309440612793]\n",
      "Grand sum of 456 tensor sets is: [175.85379028320312, 741.0323486328125, -129.7510528564453, -215.4658203125, -213.53074645996094]\n",
      "\n",
      "Instance 590 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [83]\n",
      "Size of token embeddings is torch.Size([146, 13, 768])\n",
      "Shape of summed layers is: 146 x 768\n",
      "car at index 83: [0.7842099666595459, -0.8667717576026917, 1.303114891052246, 0.48844218254089355, 1.3442045450210571]\n",
      "Grand sum of 457 tensor sets is: [176.63800048828125, 740.1655883789062, -128.44793701171875, -214.9773712158203, -212.18653869628906]\n",
      "\n",
      "Instance 591 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 25: [0.36736658215522766, 2.0594091415405273, -1.6782796382904053, -1.575953483581543, -0.5797396898269653]\n",
      "Grand sum of 458 tensor sets is: [177.00537109375, 742.2249755859375, -130.126220703125, -216.55332946777344, -212.7662811279297]\n",
      "\n",
      "Instance 592 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 12: [0.8065090179443359, 0.9600868225097656, -1.3372300863265991, -0.40366330742836, 1.6530613899230957]\n",
      "Grand sum of 459 tensor sets is: [177.81187438964844, 743.18505859375, -131.4634552001953, -216.9569854736328, -211.11322021484375]\n",
      "\n",
      "Instance 593 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 594 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 23: [1.0524808168411255, 2.283161163330078, -0.033179569989442825, -2.327906847000122, -0.8988870978355408]\n",
      "Grand sum of 460 tensor sets is: [178.86434936523438, 745.4682006835938, -131.4966278076172, -219.28489685058594, -212.01210021972656]\n",
      "\n",
      "Instance 595 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 596 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 597 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 9: [-0.18140725791454315, 2.0152664184570312, 0.012727023102343082, -1.5131803750991821, -0.058187663555145264]\n",
      "Grand sum of 461 tensor sets is: [178.6829376220703, 747.4834594726562, -131.48390197753906, -220.79808044433594, -212.07028198242188]\n",
      "\n",
      "Instance 598 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([110, 13, 768])\n",
      "Shape of summed layers is: 110 x 768\n",
      "car at index 46: [1.2576770782470703, 1.9927743673324585, -0.4800252914428711, -0.46635907888412476, -0.5625057816505432]\n",
      "Grand sum of 462 tensor sets is: [179.94061279296875, 749.4762573242188, -131.96392822265625, -221.26443481445312, -212.63278198242188]\n",
      "\n",
      "Instance 599 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 6: [0.5690521597862244, 0.8528171181678772, -1.1802172660827637, -2.4760379791259766, -1.0819191932678223]\n",
      "Grand sum of 463 tensor sets is: [180.50965881347656, 750.3291015625, -133.14414978027344, -223.740478515625, -213.71470642089844]\n",
      "\n",
      "Instance 600 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 10: [0.23716282844543457, 2.4993896484375, 0.5380183458328247, 0.14206495881080627, 0.03744518756866455]\n",
      "Grand sum of 464 tensor sets is: [180.746826171875, 752.8284912109375, -132.6061248779297, -223.59841918945312, -213.67726135253906]\n",
      "\n",
      "Instance 601 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 4: [1.1547125577926636, 1.4460234642028809, -0.4590444564819336, -0.5727095603942871, -1.1441417932510376]\n",
      "Grand sum of 465 tensor sets is: [181.9015350341797, 754.2745361328125, -133.06517028808594, -224.17112731933594, -214.82139587402344]\n",
      "\n",
      "Instance 602 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9, 11, 25]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 9: [1.0221197605133057, 2.627936840057373, -1.1270254850387573, -1.7888299226760864, -0.7128924131393433]\n",
      "car at index 11: [0.8547915816307068, 1.6604028940200806, -0.6173455119132996, 1.7401516437530518, -0.7341998815536499]\n",
      "car at index 25: [0.4586843252182007, 2.728553295135498, -1.406118392944336, -2.6187710762023926, -0.9235165119171143]\n",
      "Grand sum of 466 tensor sets is: [182.68006896972656, 756.613525390625, -134.11532592773438, -225.06027221679688, -215.61160278320312]\n",
      "\n",
      "Instance 603 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 12: [0.3245459794998169, -0.27608251571655273, -0.2766389846801758, 1.36332106590271, 0.9001325964927673]\n",
      "Grand sum of 467 tensor sets is: [183.00460815429688, 756.3374633789062, -134.3919677734375, -223.6969451904297, -214.7114715576172]\n",
      "\n",
      "Instance 604 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "car at index 28: [-0.002449825406074524, 2.7289223670959473, 0.15151116251945496, 0.9561336040496826, -2.121441602706909]\n",
      "Grand sum of 468 tensor sets is: [183.0021514892578, 759.06640625, -134.24046325683594, -222.74081420898438, -216.83291625976562]\n",
      "\n",
      "Instance 605 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 31: [0.6858248710632324, 1.8446846008300781, 1.2180219888687134, -1.2596189975738525, -0.06505727767944336]\n",
      "Grand sum of 469 tensor sets is: [183.68797302246094, 760.9110717773438, -133.02244567871094, -224.00042724609375, -216.89797973632812]\n",
      "\n",
      "Instance 606 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 23: [0.2069847583770752, 3.192807674407959, 0.7086495161056519, -1.8927772045135498, 1.1214475631713867]\n",
      "Grand sum of 470 tensor sets is: [183.89495849609375, 764.1038818359375, -132.3137969970703, -225.89320373535156, -215.7765350341797]\n",
      "\n",
      "Instance 607 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 16: [0.6514792442321777, -1.2050334215164185, -0.33801037073135376, 0.4429014027118683, 0.4755682349205017]\n",
      "Grand sum of 471 tensor sets is: [184.5464324951172, 762.8988647460938, -132.6518096923828, -225.45030212402344, -215.30096435546875]\n",
      "\n",
      "Instance 608 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 23: [0.10444489866495132, 2.493574857711792, -0.9026837944984436, -0.4518832564353943, -0.8408510088920593]\n",
      "Grand sum of 472 tensor sets is: [184.65087890625, 765.3924560546875, -133.5544891357422, -225.90219116210938, -216.14181518554688]\n",
      "\n",
      "Instance 609 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 23: [-0.21814580261707306, 2.522782802581787, 1.0267125368118286, 0.46808958053588867, -1.0155264139175415]\n",
      "Grand sum of 473 tensor sets is: [184.4327392578125, 767.9152221679688, -132.52777099609375, -225.43409729003906, -217.1573486328125]\n",
      "\n",
      "Instance 610 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 14: [0.47925981879234314, 1.0221582651138306, -0.6787846088409424, -0.11922379583120346, -2.5155277252197266]\n",
      "Grand sum of 474 tensor sets is: [184.91200256347656, 768.9373779296875, -133.20655822753906, -225.55331420898438, -219.67288208007812]\n",
      "\n",
      "Instance 611 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 5: [-0.6512193083763123, 1.562031626701355, 0.09609468281269073, 1.343556523323059, -3.361367702484131]\n",
      "Grand sum of 475 tensor sets is: [184.2607879638672, 770.4993896484375, -133.11045837402344, -224.2097625732422, -223.0342559814453]\n",
      "\n",
      "Instance 612 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 613 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 19: [0.20864510536193848, 0.528213381767273, -0.8324506878852844, 0.6481639742851257, 0.08941221237182617]\n",
      "Grand sum of 476 tensor sets is: [184.4694366455078, 771.027587890625, -133.94290161132812, -223.5615997314453, -222.94483947753906]\n",
      "\n",
      "Instance 614 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 11: [0.5145206451416016, 2.3579771518707275, -0.02716769650578499, 0.035578638315200806, -1.2680357694625854]\n",
      "Grand sum of 477 tensor sets is: [184.9839630126953, 773.3855590820312, -133.97006225585938, -223.52601623535156, -224.21287536621094]\n",
      "\n",
      "Instance 615 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 8: [0.8916155099868774, 2.0862231254577637, -0.2866131067276001, -0.03440617024898529, -3.8145127296447754]\n",
      "Grand sum of 478 tensor sets is: [185.87557983398438, 775.4718017578125, -134.2566680908203, -223.5604248046875, -228.0273895263672]\n",
      "\n",
      "Instance 616 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 617 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 618 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "car at index 27: [1.8979566097259521, -1.6668119430541992, -0.23919779062271118, 1.1969927549362183, 3.458889961242676]\n",
      "Grand sum of 479 tensor sets is: [187.77352905273438, 773.8049926757812, -134.49586486816406, -222.36343383789062, -224.56849670410156]\n",
      "\n",
      "Instance 619 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 10: [-0.051830120384693146, 1.2083117961883545, -0.1761532723903656, -1.218134880065918, -0.7902078628540039]\n",
      "Grand sum of 480 tensor sets is: [187.72169494628906, 775.0133056640625, -134.67201232910156, -223.58157348632812, -225.35870361328125]\n",
      "\n",
      "Instance 620 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 8: [-0.13222122192382812, 2.29166316986084, 0.19893907010555267, -0.30232396721839905, 0.05245440453290939]\n",
      "Grand sum of 481 tensor sets is: [187.5894775390625, 777.3049926757812, -134.4730682373047, -223.88389587402344, -225.30624389648438]\n",
      "\n",
      "Instance 621 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 18: [0.18487009406089783, 2.5902061462402344, 1.4309618473052979, 0.24427542090415955, -2.6599128246307373]\n",
      "Grand sum of 482 tensor sets is: [187.77435302734375, 779.8952026367188, -133.04209899902344, -223.63961791992188, -227.96615600585938]\n",
      "\n",
      "Instance 622 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 623 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 624 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 24: [1.0156389474868774, 3.0184764862060547, -0.25579655170440674, -0.7324115633964539, 0.39677590131759644]\n",
      "Grand sum of 483 tensor sets is: [188.7899932861328, 782.9136962890625, -133.2978973388672, -224.3720245361328, -227.5693817138672]\n",
      "\n",
      "Instance 625 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 626 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 14: [-1.4728047847747803, 2.7675299644470215, 0.20477361977100372, 0.004998542368412018, -0.4309442937374115]\n",
      "Grand sum of 484 tensor sets is: [187.3171844482422, 785.6812133789062, -133.09312438964844, -224.3670196533203, -228.0003204345703]\n",
      "\n",
      "Instance 627 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 2: [0.3924827575683594, 0.9357943534851074, 1.1163246631622314, 1.0398179292678833, 0.911665678024292]\n",
      "Grand sum of 485 tensor sets is: [187.7096710205078, 786.6170043945312, -131.976806640625, -223.3271942138672, -227.08865356445312]\n",
      "\n",
      "Instance 628 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 629 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 630 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([87, 13, 768])\n",
      "Shape of summed layers is: 87 x 768\n",
      "car at index 48: [1.1520445346832275, 2.4122283458709717, -0.6225188374519348, -1.1313326358795166, -0.8222741484642029]\n",
      "Grand sum of 486 tensor sets is: [188.86170959472656, 789.0292358398438, -132.5993194580078, -224.45852661132812, -227.9109344482422]\n",
      "\n",
      "Instance 631 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 14: [0.5818383097648621, 1.5810022354125977, 0.902985692024231, -0.6758278608322144, -0.5512489080429077]\n",
      "Grand sum of 487 tensor sets is: [189.44354248046875, 790.6102294921875, -131.6963348388672, -225.1343536376953, -228.46218872070312]\n",
      "\n",
      "Instance 632 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15, 41]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "car at index 15: [-0.42171770334243774, 0.6357195973396301, -0.11652188748121262, -2.139312267303467, 1.6000550985336304]\n",
      "car at index 41: [0.6065757870674133, 2.345060348510742, -0.15884320437908173, -1.9625933170318604, -0.19823843240737915]\n",
      "Grand sum of 488 tensor sets is: [189.5359649658203, 792.1006469726562, -131.83401489257812, -227.185302734375, -227.7612762451172]\n",
      "\n",
      "Instance 633 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 36: [-0.3881128430366516, 3.4157817363739014, 0.5318788886070251, -3.521881103515625, 0.02846851944923401]\n",
      "Grand sum of 489 tensor sets is: [189.14785766601562, 795.5164184570312, -131.30213928222656, -230.70718383789062, -227.73280334472656]\n",
      "\n",
      "Instance 634 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 9: [-0.38279297947883606, 3.55733585357666, -0.12135566771030426, -0.7512006163597107, -2.113339424133301]\n",
      "Grand sum of 490 tensor sets is: [188.7650604248047, 799.07373046875, -131.42349243164062, -231.45838928222656, -229.8461456298828]\n",
      "\n",
      "Instance 635 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 28: [0.34548473358154297, 3.4775819778442383, -0.27947935461997986, 0.9140952229499817, -0.7896252870559692]\n",
      "Grand sum of 491 tensor sets is: [189.1105499267578, 802.5513305664062, -131.70297241210938, -230.54429626464844, -230.63577270507812]\n",
      "\n",
      "Instance 636 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [69]\n",
      "Size of token embeddings is torch.Size([90, 13, 768])\n",
      "Shape of summed layers is: 90 x 768\n",
      "car at index 69: [0.06810688227415085, -0.9255341291427612, -0.7322084903717041, 3.246033191680908, 4.406833171844482]\n",
      "Grand sum of 492 tensor sets is: [189.17864990234375, 801.6257934570312, -132.4351806640625, -227.2982635498047, -226.22894287109375]\n",
      "\n",
      "Instance 637 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 638 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 639 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 7: [1.0216975212097168, 0.9749870300292969, 0.2712981700897217, -2.4382972717285156, -1.8094371557235718]\n",
      "Grand sum of 493 tensor sets is: [190.20034790039062, 802.6007690429688, -132.16387939453125, -229.73655700683594, -228.0383758544922]\n",
      "\n",
      "Instance 640 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 641 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "car at index 27: [-0.09060084819793701, 2.9299252033233643, -0.7520326375961304, -1.1667709350585938, -1.928722858428955]\n",
      "Grand sum of 494 tensor sets is: [190.1097412109375, 805.5307006835938, -132.91590881347656, -230.9033203125, -229.96710205078125]\n",
      "\n",
      "Instance 642 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16, 28]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 16: [0.24288912117481232, 1.6066820621490479, -0.3249158263206482, -1.3882222175598145, -1.2676911354064941]\n",
      "car at index 28: [-0.3484705984592438, 2.3084545135498047, -0.1747998595237732, -0.44403839111328125, -2.2450900077819824]\n",
      "Grand sum of 495 tensor sets is: [190.05694580078125, 807.48828125, -133.165771484375, -231.8194580078125, -231.72349548339844]\n",
      "\n",
      "Instance 643 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3, 18]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 3: [0.7508453130722046, 1.6119987964630127, 0.9865305423736572, -2.364004135131836, 0.5723811388015747]\n",
      "car at index 18: [-0.04529206454753876, 2.1847238540649414, -0.08631803840398788, -3.0892930030822754, -0.1418301910161972]\n",
      "Grand sum of 496 tensor sets is: [190.40972900390625, 809.3866577148438, -132.71566772460938, -234.54611206054688, -231.5082244873047]\n",
      "\n",
      "Instance 644 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 6: [0.5396361947059631, 0.7763411998748779, 0.3038579523563385, 0.5948324799537659, -1.9727692604064941]\n",
      "Grand sum of 497 tensor sets is: [190.94937133789062, 810.1630249023438, -132.41180419921875, -233.95127868652344, -233.48098754882812]\n",
      "\n",
      "Instance 645 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 23: [0.22601139545440674, 1.4125843048095703, -0.1257346123456955, 1.7960231304168701, -1.0172873735427856]\n",
      "Grand sum of 498 tensor sets is: [191.17538452148438, 811.5756225585938, -132.53753662109375, -232.15525817871094, -234.49827575683594]\n",
      "\n",
      "Instance 646 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 647 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 648 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 649 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 650 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([5, 13, 768])\n",
      "Shape of summed layers is: 5 x 768\n",
      "car at index 3: [0.0807458758354187, 1.6833573579788208, -0.8410484194755554, -2.303215265274048, 0.11825253814458847]\n",
      "Grand sum of 499 tensor sets is: [191.25613403320312, 813.2589721679688, -133.3785858154297, -234.45848083496094, -234.38002014160156]\n",
      "\n",
      "Instance 651 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 652 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 10: [-0.26498842239379883, 2.878275156021118, -0.10746953636407852, -2.3896214962005615, -1.7482173442840576]\n",
      "Grand sum of 500 tensor sets is: [190.99114990234375, 816.1372680664062, -133.48605346679688, -236.8480987548828, -236.12823486328125]\n",
      "\n",
      "Instance 653 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 654 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 23: [0.0019192993640899658, 2.39351487159729, -1.1497750282287598, -2.004185914993286, -0.6154099702835083]\n",
      "Grand sum of 501 tensor sets is: [190.99307250976562, 818.53076171875, -134.63583374023438, -238.85227966308594, -236.74365234375]\n",
      "\n",
      "Instance 655 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 656 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 20: [1.1587355136871338, 2.095869541168213, -0.3105320632457733, -0.1939237117767334, 0.4795284569263458]\n",
      "Grand sum of 502 tensor sets is: [192.1518096923828, 820.6266479492188, -134.9463653564453, -239.04620361328125, -236.26412963867188]\n",
      "\n",
      "Instance 657 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 658 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 26: [-0.14612393081188202, 3.192808151245117, 0.7404145002365112, 1.3760485649108887, -3.1811938285827637]\n",
      "Grand sum of 503 tensor sets is: [192.0056915283203, 823.8194580078125, -134.20594787597656, -237.67015075683594, -239.44532775878906]\n",
      "\n",
      "Instance 659 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 3: [-0.4038412570953369, 2.994166135787964, -0.553799569606781, 0.8747879862785339, -2.318236827850342]\n",
      "Grand sum of 504 tensor sets is: [191.6018524169922, 826.8135986328125, -134.75975036621094, -236.7953643798828, -241.76356506347656]\n",
      "\n",
      "Instance 660 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "car at index 14: [0.7003384828567505, 1.7249201536178589, 0.08349494636058807, -0.9231811165809631, -0.3372246325016022]\n",
      "Grand sum of 505 tensor sets is: [192.30218505859375, 828.5385131835938, -134.67625427246094, -237.7185516357422, -242.1007843017578]\n",
      "\n",
      "Instance 661 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 12: [0.5470281839370728, 1.8314635753631592, -0.4898092746734619, -1.7891747951507568, -0.06500645726919174]\n",
      "Grand sum of 506 tensor sets is: [192.84921264648438, 830.3699951171875, -135.1660614013672, -239.50772094726562, -242.16578674316406]\n",
      "\n",
      "Instance 662 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 12: [0.5516207218170166, 1.9465337991714478, -0.5705599188804626, -0.37448650598526, 0.6958062052726746]\n",
      "Grand sum of 507 tensor sets is: [193.4008331298828, 832.3165283203125, -135.7366180419922, -239.8822021484375, -241.46998596191406]\n",
      "\n",
      "Instance 663 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 664 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10, 23]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 10: [0.3876902163028717, 0.7206913828849792, -1.2407773733139038, -3.3853142261505127, 1.2414757013320923]\n",
      "car at index 23: [0.2951699495315552, 1.5867664813995361, -1.3013249635696411, -2.0846786499023438, 1.2588433027267456]\n",
      "Grand sum of 508 tensor sets is: [193.7422637939453, 833.4702758789062, -137.00767517089844, -242.61720275878906, -240.21983337402344]\n",
      "\n",
      "Instance 665 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([167, 13, 768])\n",
      "Shape of summed layers is: 167 x 768\n",
      "car at index 36: [1.3584723472595215, 0.7657484412193298, -0.06704919040203094, 0.21383538842201233, -1.4879825115203857]\n",
      "Grand sum of 509 tensor sets is: [195.10073852539062, 834.2360229492188, -137.07472229003906, -242.4033660888672, -241.7078094482422]\n",
      "\n",
      "Instance 666 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [202, 491]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 202: [1.3089878559112549, -1.2895859479904175, -0.7796294689178467, 0.987564742565155, 3.2326111793518066]\n",
      "car at index 491: [1.2946124076843262, -0.34852778911590576, 0.637068510055542, -0.11835034191608429, 5.714822292327881]\n",
      "Grand sum of 510 tensor sets is: [196.4025421142578, 833.4169921875, -137.14599609375, -241.96876525878906, -237.23410034179688]\n",
      "\n",
      "Instance 667 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 29: [0.5127764940261841, 2.332409381866455, -0.06869649887084961, -0.8752202987670898, 1.7985825538635254]\n",
      "Grand sum of 511 tensor sets is: [196.91531372070312, 835.7493896484375, -137.21469116210938, -242.84397888183594, -235.43551635742188]\n",
      "\n",
      "Instance 668 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15, 31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 15: [0.26406288146972656, 1.4098763465881348, -0.22390681505203247, -1.6382091045379639, -1.9407070875167847]\n",
      "car at index 31: [0.10873191803693771, 1.4211125373840332, -0.07013298571109772, -2.245872735977173, -0.8031834363937378]\n",
      "Grand sum of 512 tensor sets is: [197.10171508789062, 837.1648559570312, -137.36170959472656, -244.78602600097656, -236.80746459960938]\n",
      "\n",
      "Instance 669 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "car at index 29: [0.6044973134994507, 0.42732346057891846, -0.3366888761520386, 2.9751505851745605, 2.8035669326782227]\n",
      "Grand sum of 513 tensor sets is: [197.70620727539062, 837.5921630859375, -137.69839477539062, -241.81088256835938, -234.00389099121094]\n",
      "\n",
      "Instance 670 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 17: [0.8961243033409119, 2.5810985565185547, 0.48764124512672424, -1.0077906847000122, -3.052189350128174]\n",
      "Grand sum of 514 tensor sets is: [198.60232543945312, 840.1732788085938, -137.21075439453125, -242.8186798095703, -237.0560760498047]\n",
      "\n",
      "Instance 671 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 6: [0.49043411016464233, 2.040360450744629, 0.6830322742462158, 1.6099086999893188, -2.179478406906128]\n",
      "Grand sum of 515 tensor sets is: [199.09275817871094, 842.213623046875, -136.52772521972656, -241.20877075195312, -239.2355499267578]\n",
      "\n",
      "Instance 672 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 5: [0.25671353936195374, 2.6401424407958984, -1.0656551122665405, 0.19815677404403687, -0.9196621775627136]\n",
      "Grand sum of 516 tensor sets is: [199.34947204589844, 844.853759765625, -137.5933837890625, -241.0106201171875, -240.15521240234375]\n",
      "\n",
      "Instance 673 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [307]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 307: [0.7312191724777222, 1.6813244819641113, -0.7760120630264282, -1.8049888610839844, 0.655742883682251]\n",
      "Grand sum of 517 tensor sets is: [200.0806884765625, 846.5350952148438, -138.36940002441406, -242.81561279296875, -239.4994659423828]\n",
      "\n",
      "Instance 674 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9, 18]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 9: [1.3573932647705078, 2.2954444885253906, -1.3260290622711182, 0.9411196708679199, 0.21025092899799347]\n",
      "car at index 18: [1.016384482383728, 2.94150447845459, -0.22361822426319122, -0.7232492566108704, 0.9620930552482605]\n",
      "Grand sum of 518 tensor sets is: [201.267578125, 849.153564453125, -139.14422607421875, -242.70668029785156, -238.91329956054688]\n",
      "\n",
      "Instance 675 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 7: [0.9494540691375732, 3.1432888507843018, -0.49330073595046997, -0.4777389466762543, -2.485466718673706]\n",
      "Grand sum of 519 tensor sets is: [202.21702575683594, 852.296875, -139.6375274658203, -243.18441772460938, -241.39877319335938]\n",
      "\n",
      "Instance 676 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [57, 65]\n",
      "Size of token embeddings is torch.Size([78, 13, 768])\n",
      "Shape of summed layers is: 78 x 768\n",
      "car at index 57: [-0.7511461973190308, 0.2794322073459625, 0.480749249458313, -0.09062819182872772, -0.9968911409378052]\n",
      "car at index 65: [0.08894649147987366, 1.3425366878509521, 0.35313910245895386, -1.230826497077942, -0.72828209400177]\n",
      "Grand sum of 520 tensor sets is: [201.88592529296875, 853.1078491210938, -139.2205810546875, -243.8451385498047, -242.2613525390625]\n",
      "\n",
      "Instance 677 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 678 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 679 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 680 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 681 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 18: [-0.6916813254356384, 3.225731372833252, -0.3568764925003052, -0.8378341794013977, -1.5903493165969849]\n",
      "Grand sum of 521 tensor sets is: [201.19424438476562, 856.3335571289062, -139.57745361328125, -244.68296813964844, -243.85169982910156]\n",
      "\n",
      "Instance 682 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 29: [-0.08486465364694595, 1.9486091136932373, 0.29737553000450134, -1.7340372800827026, -1.1189316511154175]\n",
      "Grand sum of 522 tensor sets is: [201.109375, 858.2821655273438, -139.2800750732422, -246.41700744628906, -244.9706268310547]\n",
      "\n",
      "Instance 683 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 684 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 12: [0.34929507970809937, 2.0833616256713867, -1.0527029037475586, -0.5477137565612793, -0.41916221380233765]\n",
      "Grand sum of 523 tensor sets is: [201.4586639404297, 860.3655395507812, -140.33277893066406, -246.9647216796875, -245.38978576660156]\n",
      "\n",
      "Instance 685 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 17: [0.8383500576019287, 2.348914384841919, -0.746776819229126, -0.8713098168373108, -2.1721103191375732]\n",
      "Grand sum of 524 tensor sets is: [202.29701232910156, 862.7144775390625, -141.07955932617188, -247.83602905273438, -247.5618896484375]\n",
      "\n",
      "Instance 686 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 25: [1.3255059719085693, 1.5556515455245972, -0.42180272936820984, 1.3607722520828247, -1.497833013534546]\n",
      "Grand sum of 525 tensor sets is: [203.6225128173828, 864.2701416015625, -141.50135803222656, -246.47525024414062, -249.05972290039062]\n",
      "\n",
      "Instance 687 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 22: [0.5524098873138428, 1.127604603767395, -0.4607323408126831, -3.1238434314727783, 1.1127036809921265]\n",
      "Grand sum of 526 tensor sets is: [204.1749267578125, 865.3977661132812, -141.96209716796875, -249.59909057617188, -247.947021484375]\n",
      "\n",
      "Instance 688 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 689 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 12: [0.8896055817604065, 2.237107753753662, -0.11950629949569702, -0.7959626913070679, 0.6247702836990356]\n",
      "Grand sum of 527 tensor sets is: [205.0645294189453, 867.6348876953125, -142.08160400390625, -250.39505004882812, -247.32225036621094]\n",
      "\n",
      "Instance 690 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 16: [0.343478262424469, 2.2798521518707275, -0.0430365689098835, 1.5243773460388184, -0.9732587933540344]\n",
      "Grand sum of 528 tensor sets is: [205.4080047607422, 869.9147338867188, -142.1246337890625, -248.87066650390625, -248.29550170898438]\n",
      "\n",
      "Instance 691 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 15: [-0.028571411967277527, 2.1433639526367188, -0.3797268271446228, 2.7883975505828857, -3.2879867553710938]\n",
      "Grand sum of 529 tensor sets is: [205.3794403076172, 872.05810546875, -142.50436401367188, -246.082275390625, -251.58349609375]\n",
      "\n",
      "Instance 692 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 693 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "car at index 26: [0.9347615838050842, 0.31362384557724, -1.3513827323913574, 0.8898800611495972, 4.279651641845703]\n",
      "Grand sum of 530 tensor sets is: [206.314208984375, 872.3717041015625, -143.85574340820312, -245.19239807128906, -247.30384826660156]\n",
      "\n",
      "Instance 694 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 3: [0.013328376226127148, 1.1377696990966797, -0.6229856014251709, -2.2075939178466797, 0.7457956075668335]\n",
      "Grand sum of 531 tensor sets is: [206.32752990722656, 873.5094604492188, -144.47872924804688, -247.39999389648438, -246.5580596923828]\n",
      "\n",
      "Instance 695 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [461]\n",
      "Size of token embeddings is torch.Size([495, 13, 768])\n",
      "Shape of summed layers is: 495 x 768\n",
      "car at index 461: [1.2267630100250244, 0.6817327737808228, 1.4326261281967163, 1.692652702331543, -0.21697737276554108]\n",
      "Grand sum of 532 tensor sets is: [207.55429077148438, 874.1912231445312, -143.0460968017578, -245.70733642578125, -246.77503967285156]\n",
      "\n",
      "Instance 696 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 19: [0.693023145198822, 2.189836025238037, -0.7049984335899353, -0.555567741394043, 0.9819289445877075]\n",
      "Grand sum of 533 tensor sets is: [208.247314453125, 876.3810424804688, -143.7510986328125, -246.26290893554688, -245.79310607910156]\n",
      "\n",
      "Instance 697 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 15: [-0.08265749365091324, 1.0939297676086426, 0.29023343324661255, 3.0433642864227295, -4.098572731018066]\n",
      "Grand sum of 534 tensor sets is: [208.16465759277344, 877.4749755859375, -143.4608612060547, -243.21954345703125, -249.8916778564453]\n",
      "\n",
      "Instance 698 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 18: [-0.3146810531616211, 4.080533981323242, -0.01054673083126545, 0.7696316242218018, -1.3421213626861572]\n",
      "Grand sum of 535 tensor sets is: [207.8499755859375, 881.5554809570312, -143.47140502929688, -242.4499053955078, -251.23379516601562]\n",
      "\n",
      "Instance 699 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 700 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 5: [0.00017184019088745117, 3.289769172668457, 0.26604196429252625, -0.5383753776550293, -1.6211756467819214]\n",
      "Grand sum of 536 tensor sets is: [207.8501434326172, 884.8452758789062, -143.2053680419922, -242.98828125, -252.85496520996094]\n",
      "\n",
      "Instance 701 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 16: [0.45645102858543396, 1.7705141305923462, -0.11494648456573486, 1.7294702529907227, -2.102700710296631]\n",
      "Grand sum of 537 tensor sets is: [208.3065948486328, 886.6157836914062, -143.3203125, -241.25880432128906, -254.95767211914062]\n",
      "\n",
      "Instance 702 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 25: [0.5995705127716064, 2.9053537845611572, -0.42783743143081665, -0.1772812157869339, -1.9398088455200195]\n",
      "Grand sum of 538 tensor sets is: [208.90615844726562, 889.5211181640625, -143.74815368652344, -241.4360809326172, -256.8974914550781]\n",
      "\n",
      "Instance 703 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2, 15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 2: [-0.44729718565940857, 1.6294218301773071, -1.489087700843811, -1.4128814935684204, 0.9917943477630615]\n",
      "car at index 15: [-0.12345689535140991, 1.6258529424667358, -0.717807948589325, -1.3516814708709717, -2.0185582637786865]\n",
      "Grand sum of 539 tensor sets is: [208.62078857421875, 891.1487426757812, -144.8516082763672, -242.818359375, -257.410888671875]\n",
      "\n",
      "Instance 704 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 14: [-0.25993990898132324, 3.0347137451171875, 0.4189684987068176, -0.2655287981033325, -3.34912109375]\n",
      "Grand sum of 540 tensor sets is: [208.36085510253906, 894.1834716796875, -144.43263244628906, -243.08389282226562, -260.760009765625]\n",
      "\n",
      "Instance 705 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 706 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 707 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10, 19, 28]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 10: [0.07304120063781738, 1.955185055732727, -0.2050316035747528, -1.9242768287658691, -1.947622537612915]\n",
      "car at index 19: [-0.32244443893432617, 2.309366226196289, 0.2187211811542511, -1.730345368385315, -2.259608268737793]\n",
      "car at index 28: [-0.035626549273729324, 1.8469641208648682, -0.16865582764148712, -2.986508369445801, -0.8507656455039978]\n",
      "Grand sum of 541 tensor sets is: [208.26583862304688, 896.2206420898438, -144.48428344726562, -245.297607421875, -262.4460144042969]\n",
      "\n",
      "Instance 708 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "car at index 47: [1.0551875829696655, 3.088052272796631, 0.28949564695358276, -0.870477020740509, -1.973237156867981]\n",
      "Grand sum of 542 tensor sets is: [209.32102966308594, 899.3087158203125, -144.19479370117188, -246.1680908203125, -264.41925048828125]\n",
      "\n",
      "Instance 709 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 710 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 8: [0.20222164690494537, 2.3652870655059814, 0.4492703974246979, 0.7716031074523926, -3.5103955268859863]\n",
      "Grand sum of 543 tensor sets is: [209.52325439453125, 901.6740112304688, -143.7455291748047, -245.396484375, -267.9296569824219]\n",
      "\n",
      "Instance 711 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 28: [-0.2076764702796936, 1.5143957138061523, -0.47390028834342957, -1.0693769454956055, -2.1430869102478027]\n",
      "Grand sum of 544 tensor sets is: [209.31558227539062, 903.1884155273438, -144.2194366455078, -246.4658660888672, -270.07275390625]\n",
      "\n",
      "Instance 712 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 713 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 4: [0.07868524640798569, 2.2703499794006348, -0.009819280356168747, -0.9140396118164062, -0.72929847240448]\n",
      "Grand sum of 545 tensor sets is: [209.39427185058594, 905.458740234375, -144.22926330566406, -247.37991333007812, -270.80206298828125]\n",
      "\n",
      "Instance 714 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 23: [0.3065115213394165, 3.393723487854004, 0.4264409840106964, -1.409178376197815, -1.6333012580871582]\n",
      "Grand sum of 546 tensor sets is: [209.70079040527344, 908.8524780273438, -143.80282592773438, -248.78909301757812, -272.43536376953125]\n",
      "\n",
      "Instance 715 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [211]\n",
      "Size of token embeddings is torch.Size([233, 13, 768])\n",
      "Shape of summed layers is: 233 x 768\n",
      "car at index 211: [0.2100560963153839, 1.1689894199371338, -0.19846118986606598, -1.0720494985580444, 5.5187458992004395]\n",
      "Grand sum of 547 tensor sets is: [209.9108428955078, 910.021484375, -144.00128173828125, -249.86114501953125, -266.9166259765625]\n",
      "\n",
      "Instance 716 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 2: [1.005812168121338, 2.1658589839935303, -0.13054311275482178, -1.6877425909042358, -1.3419787883758545]\n",
      "Grand sum of 548 tensor sets is: [210.91665649414062, 912.1873168945312, -144.13182067871094, -251.54888916015625, -268.25860595703125]\n",
      "\n",
      "Instance 717 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 14: [0.17450928688049316, 1.6003130674362183, 0.1034054160118103, -1.1929514408111572, -1.8848347663879395]\n",
      "Grand sum of 549 tensor sets is: [211.09117126464844, 913.7876586914062, -144.02841186523438, -252.74183654785156, -270.1434326171875]\n",
      "\n",
      "Instance 718 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 21: [0.39375534653663635, 2.19535756111145, -0.6047046780586243, -1.2220470905303955, -1.4070587158203125]\n",
      "Grand sum of 550 tensor sets is: [211.48492431640625, 915.9830322265625, -144.63311767578125, -253.96388244628906, -271.55047607421875]\n",
      "\n",
      "Instance 719 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 4: [-0.15386371314525604, 1.051629662513733, -0.5439376831054688, 1.2930231094360352, -0.5480664968490601]\n",
      "Grand sum of 551 tensor sets is: [211.3310546875, 917.03466796875, -145.17706298828125, -252.6708526611328, -272.0985412597656]\n",
      "\n",
      "Instance 720 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6, 18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 6: [0.04515044391155243, 2.4475371837615967, -0.10422657430171967, 0.40515437722206116, -2.2934868335723877]\n",
      "car at index 18: [1.1231262683868408, 2.428551435470581, 0.003045007586479187, 1.475013256072998, -3.262284278869629]\n",
      "Grand sum of 552 tensor sets is: [211.91519165039062, 919.4727172851562, -145.2276611328125, -251.73077392578125, -274.8764343261719]\n",
      "\n",
      "Instance 721 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 722 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 723 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 18: [0.5813166499137878, 1.4811255931854248, -0.566653311252594, -3.5653762817382812, -1.3518714904785156]\n",
      "Grand sum of 553 tensor sets is: [212.4965057373047, 920.953857421875, -145.7943115234375, -255.296142578125, -276.2283020019531]\n",
      "\n",
      "Instance 724 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 24: [0.6595670580863953, 2.7760467529296875, -0.5790762305259705, -0.8722860813140869, -0.30109214782714844]\n",
      "Grand sum of 554 tensor sets is: [213.15606689453125, 923.7299194335938, -146.37338256835938, -256.1684265136719, -276.5293884277344]\n",
      "\n",
      "Instance 725 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 12: [0.3320962190628052, 1.7296240329742432, -0.20819050073623657, -2.4324989318847656, -1.3674041032791138]\n",
      "Grand sum of 555 tensor sets is: [213.4881591796875, 925.4595336914062, -146.58157348632812, -258.6009216308594, -277.89678955078125]\n",
      "\n",
      "Instance 726 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 26: [0.7427427768707275, 2.9687955379486084, -0.1445564329624176, -1.050440788269043, -2.5998775959014893]\n",
      "Grand sum of 556 tensor sets is: [214.23089599609375, 928.4283447265625, -146.72613525390625, -259.6513671875, -280.4966735839844]\n",
      "\n",
      "Instance 727 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 728 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [257]\n",
      "Size of token embeddings is torch.Size([364, 13, 768])\n",
      "Shape of summed layers is: 364 x 768\n",
      "car at index 257: [0.7665387988090515, 2.481435775756836, -1.262976884841919, -0.8886059522628784, 2.3114798069000244]\n",
      "Grand sum of 557 tensor sets is: [214.9974365234375, 930.9097900390625, -147.98910522460938, -260.53997802734375, -278.1851806640625]\n",
      "\n",
      "Instance 729 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 730 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 29: [0.2892540395259857, 1.6156728267669678, -0.3439558148384094, 2.589249610900879, -2.6584150791168213]\n",
      "Grand sum of 558 tensor sets is: [215.2866973876953, 932.5254516601562, -148.3330535888672, -257.9507141113281, -280.8435974121094]\n",
      "\n",
      "Instance 731 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 11: [0.3517549932003021, -0.6581895351409912, -0.06101110577583313, 2.193336009979248, 3.0243160724639893]\n",
      "Grand sum of 559 tensor sets is: [215.63845825195312, 931.8672485351562, -148.39405822753906, -255.75738525390625, -277.81927490234375]\n",
      "\n",
      "Instance 732 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 21: [0.9902534484863281, 1.4659576416015625, -0.33497849106788635, -0.5511354207992554, -1.0011405944824219]\n",
      "Grand sum of 560 tensor sets is: [216.6287078857422, 933.3331909179688, -148.72903442382812, -256.30853271484375, -278.8204040527344]\n",
      "\n",
      "Instance 733 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 734 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 735 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 15: [-0.5661631226539612, 2.588168144226074, 0.050586454570293427, -0.7971640825271606, -1.673185110092163]\n",
      "Grand sum of 561 tensor sets is: [216.0625457763672, 935.92138671875, -148.67845153808594, -257.1056823730469, -280.49359130859375]\n",
      "\n",
      "Instance 736 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 737 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 20: [-0.42689529061317444, 1.5952192544937134, -0.8818086385726929, -2.027200698852539, -1.482189416885376]\n",
      "Grand sum of 562 tensor sets is: [215.63565063476562, 937.5166015625, -149.5602569580078, -259.13287353515625, -281.97576904296875]\n",
      "\n",
      "Instance 738 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 739 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 740 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [55]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "car at index 55: [1.3658658266067505, 1.5963014364242554, 0.5157387256622314, -1.3229682445526123, -1.7940959930419922]\n",
      "Grand sum of 563 tensor sets is: [217.0015106201172, 939.1129150390625, -149.04452514648438, -260.4558410644531, -283.7698669433594]\n",
      "\n",
      "Instance 741 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3, 18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 3: [-0.8861275911331177, 0.8628280162811279, -0.24305523931980133, -0.12494019418954849, -0.9756209850311279]\n",
      "car at index 18: [0.13718228042125702, 1.8370897769927979, 0.6990962028503418, -0.26382753252983093, -1.224939227104187]\n",
      "Grand sum of 564 tensor sets is: [216.62704467773438, 940.462890625, -148.81649780273438, -260.6502380371094, -284.8701477050781]\n",
      "\n",
      "Instance 742 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 13: [-0.19496093690395355, 1.7732288837432861, 0.38286030292510986, 0.3804744482040405, -1.7397739887237549]\n",
      "Grand sum of 565 tensor sets is: [216.4320831298828, 942.2361450195312, -148.4336395263672, -260.269775390625, -286.60992431640625]\n",
      "\n",
      "Instance 743 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 744 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 745 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 746 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [39, 61]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "car at index 39: [0.22386345267295837, 0.9970278739929199, -0.46792834997177124, 2.300550699234009, -1.0454241037368774]\n",
      "car at index 61: [1.1719145774841309, 0.4850696921348572, -0.5615242123603821, 0.9000957012176514, 0.33993232250213623]\n",
      "Grand sum of 566 tensor sets is: [217.12997436523438, 942.9771728515625, -148.9483642578125, -258.6694641113281, -286.9626770019531]\n",
      "\n",
      "Instance 747 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 748 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 14: [0.4307355284690857, 1.9335790872573853, -0.21114671230316162, -0.5711144208908081, -1.2135361433029175]\n",
      "Grand sum of 567 tensor sets is: [217.5607147216797, 944.9107666015625, -149.15951538085938, -259.2405700683594, -288.17620849609375]\n",
      "\n",
      "Instance 749 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 11: [0.020597532391548157, 2.85455322265625, -0.630812406539917, 0.2401210367679596, 0.7793493270874023]\n",
      "Grand sum of 568 tensor sets is: [217.58131408691406, 947.7653198242188, -149.7903289794922, -259.0004577636719, -287.3968505859375]\n",
      "\n",
      "Instance 750 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 751 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [76]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "car at index 76: [-0.5340300798416138, -0.11858385056257248, -0.27428388595581055, 0.9193069934844971, 3.375516891479492]\n",
      "Grand sum of 569 tensor sets is: [217.0472869873047, 947.646728515625, -150.06460571289062, -258.0811462402344, -284.0213317871094]\n",
      "\n",
      "Instance 752 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 753 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 754 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 755 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 21: [-0.42529794573783875, 2.472318172454834, 1.3579174280166626, -2.3983957767486572, -2.17465877532959]\n",
      "Grand sum of 570 tensor sets is: [216.6219940185547, 950.1190185546875, -148.70669555664062, -260.47955322265625, -286.19598388671875]\n",
      "\n",
      "Instance 756 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 20: [0.5511884093284607, -0.9895195960998535, -0.5331336259841919, 2.2166926860809326, 3.699995517730713]\n",
      "Grand sum of 571 tensor sets is: [217.17318725585938, 949.1295166015625, -149.2398223876953, -258.2628479003906, -282.4960021972656]\n",
      "\n",
      "Instance 757 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 758 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 13: [1.3109315633773804, 2.179347038269043, -0.5346620678901672, -0.07471401244401932, 1.2910770177841187]\n",
      "Grand sum of 572 tensor sets is: [218.48411560058594, 951.308837890625, -149.7744903564453, -258.3375549316406, -281.2049255371094]\n",
      "\n",
      "Instance 759 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 18: [-0.8229405879974365, 3.3899970054626465, 0.44615089893341064, 1.5121769905090332, -2.3328194618225098]\n",
      "Grand sum of 573 tensor sets is: [217.6611785888672, 954.6988525390625, -149.32833862304688, -256.82537841796875, -283.5377502441406]\n",
      "\n",
      "Instance 760 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 761 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 762 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 7: [-0.7091009616851807, 2.038729190826416, -0.43942731618881226, 2.8967456817626953, -3.820302963256836]\n",
      "Grand sum of 574 tensor sets is: [216.9520721435547, 956.7376098632812, -149.76776123046875, -253.9286346435547, -287.3580627441406]\n",
      "\n",
      "Instance 763 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "car at index 9: [-0.054822251200675964, 3.0821752548217773, 0.01306663453578949, -1.624661922454834, 0.5996379852294922]\n",
      "Grand sum of 575 tensor sets is: [216.89724731445312, 959.8197631835938, -149.75469970703125, -255.5532989501953, -286.7584228515625]\n",
      "\n",
      "Instance 764 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([101, 13, 768])\n",
      "Shape of summed layers is: 101 x 768\n",
      "car at index 35: [-0.4730313718318939, -0.49637237191200256, 0.4684714674949646, -1.546970009803772, -0.02428550273180008]\n",
      "Grand sum of 576 tensor sets is: [216.42420959472656, 959.3233642578125, -149.28622436523438, -257.10028076171875, -286.78271484375]\n",
      "\n",
      "Instance 765 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 24: [0.41042208671569824, 0.9383482933044434, -0.14049144089221954, -1.7873034477233887, 1.0418306589126587]\n",
      "Grand sum of 577 tensor sets is: [216.83462524414062, 960.26171875, -149.4267120361328, -258.8875732421875, -285.7408752441406]\n",
      "\n",
      "Instance 766 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 767 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 8: [0.670302152633667, 2.098710536956787, -1.053723931312561, -0.06535312533378601, -0.3492371439933777]\n",
      "Grand sum of 578 tensor sets is: [217.5049285888672, 962.3604125976562, -150.48043823242188, -258.9529113769531, -286.0901184082031]\n",
      "\n",
      "Instance 768 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([110, 13, 768])\n",
      "Shape of summed layers is: 110 x 768\n",
      "car at index 39: [1.3740756511688232, -1.3339089155197144, -1.1250272989273071, 0.26141393184661865, 0.7742000222206116]\n",
      "Grand sum of 579 tensor sets is: [218.87899780273438, 961.0264892578125, -151.60546875, -258.6914978027344, -285.31591796875]\n",
      "\n",
      "Instance 769 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 8: [1.1278749704360962, -1.8849351406097412, -0.02664315700531006, 1.5910916328430176, 0.3895453214645386]\n",
      "Grand sum of 580 tensor sets is: [220.00686645507812, 959.1415405273438, -151.63211059570312, -257.10040283203125, -284.9263610839844]\n",
      "\n",
      "Instance 770 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 14: [0.4020773470401764, -0.646595299243927, -0.16139793395996094, 2.7450168132781982, 3.282970428466797]\n",
      "Grand sum of 581 tensor sets is: [220.40895080566406, 958.4949340820312, -151.7935028076172, -254.3553924560547, -281.6434020996094]\n",
      "\n",
      "Instance 771 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([66, 13, 768])\n",
      "Shape of summed layers is: 66 x 768\n",
      "car at index 23: [0.646335244178772, 2.8595128059387207, 0.825809121131897, -1.122340202331543, -2.1249232292175293]\n",
      "Grand sum of 582 tensor sets is: [221.05528259277344, 961.3544311523438, -150.9676971435547, -255.4777374267578, -283.768310546875]\n",
      "\n",
      "Instance 772 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 773 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "car at index 33: [0.8666285276412964, 2.312615394592285, -0.1648852825164795, -0.9190015196800232, 1.0076380968093872]\n",
      "Grand sum of 583 tensor sets is: [221.92190551757812, 963.6670532226562, -151.13258361816406, -256.396728515625, -282.76068115234375]\n",
      "\n",
      "Instance 774 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 775 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 14: [0.5551608204841614, 1.6927316188812256, 0.7071904540061951, 2.7669992446899414, -2.357438802719116]\n",
      "Grand sum of 584 tensor sets is: [222.47706604003906, 965.3598022460938, -150.42539978027344, -253.62973022460938, -285.1181335449219]\n",
      "\n",
      "Instance 776 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "car at index 28: [0.6401733160018921, 2.9498023986816406, 0.023379012942314148, -1.1741492748260498, 0.22896726429462433]\n",
      "Grand sum of 585 tensor sets is: [223.1172332763672, 968.3096313476562, -150.4020233154297, -254.8038787841797, -284.88916015625]\n",
      "\n",
      "Instance 777 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 778 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 3: [0.10834496468305588, 0.8977199792861938, -0.7361865043640137, -2.9000539779663086, 3.0029003620147705]\n",
      "Grand sum of 586 tensor sets is: [223.22557067871094, 969.2073364257812, -151.13821411132812, -257.70391845703125, -281.8862609863281]\n",
      "\n",
      "Instance 779 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 7: [-0.28024744987487793, 4.1618876457214355, 0.020440295338630676, 1.7258329391479492, -1.3291386365890503]\n",
      "Grand sum of 587 tensor sets is: [222.94532775878906, 973.3692016601562, -151.11776733398438, -255.97808837890625, -283.21539306640625]\n",
      "\n",
      "Instance 780 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 9: [0.19436073303222656, -0.9155778884887695, -1.0142713785171509, 0.8666796684265137, 3.301297426223755]\n",
      "Grand sum of 588 tensor sets is: [223.1396942138672, 972.45361328125, -152.1320343017578, -255.1114044189453, -279.9140930175781]\n",
      "\n",
      "Instance 781 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 782 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 32: [0.017162352800369263, 1.756672978401184, 1.6246798038482666, 2.0283501148223877, -1.9894473552703857]\n",
      "Grand sum of 589 tensor sets is: [223.1568603515625, 974.2102661132812, -150.50735473632812, -253.0830535888672, -281.9035339355469]\n",
      "\n",
      "Instance 783 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 13: [0.389599084854126, 1.259628415107727, -0.27556583285331726, -0.3295323848724365, -0.10948444902896881]\n",
      "Grand sum of 590 tensor sets is: [223.5464630126953, 975.4699096679688, -150.7829132080078, -253.41258239746094, -282.0130310058594]\n",
      "\n",
      "Instance 784 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 7: [1.164119005203247, 3.1059114933013916, 0.006632208824157715, 0.19680702686309814, -2.0566813945770264]\n",
      "Grand sum of 591 tensor sets is: [224.71058654785156, 978.5758056640625, -150.77627563476562, -253.2157745361328, -284.0697021484375]\n",
      "\n",
      "Instance 785 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 786 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 27: [0.0748453289270401, 1.0451675653457642, -0.2775770425796509, -1.0577627420425415, -1.9192299842834473]\n",
      "Grand sum of 592 tensor sets is: [224.78543090820312, 979.6209716796875, -151.05384826660156, -254.27354431152344, -285.9889221191406]\n",
      "\n",
      "Instance 787 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 788 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 29: [0.13921673595905304, 2.7259914875030518, -1.379252552986145, -1.3692072629928589, -1.3965610265731812]\n",
      "Grand sum of 593 tensor sets is: [224.92465209960938, 982.3469848632812, -152.43310546875, -255.6427459716797, -287.385498046875]\n",
      "\n",
      "Instance 789 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "car at index 19: [-0.5462314486503601, 2.926382541656494, 1.031614899635315, -1.2718497514724731, -0.9085076451301575]\n",
      "Grand sum of 594 tensor sets is: [224.37841796875, 985.2733764648438, -151.4014892578125, -256.9145812988281, -288.29400634765625]\n",
      "\n",
      "Instance 790 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 19: [0.5079355239868164, 1.5317438840866089, -0.5502986907958984, 0.33457162976264954, -0.2252548187971115]\n",
      "Grand sum of 595 tensor sets is: [224.8863525390625, 986.8051147460938, -151.9517822265625, -256.58001708984375, -288.5192565917969]\n",
      "\n",
      "Instance 791 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 19: [0.33616456389427185, 0.44848620891571045, -0.30473533272743225, 1.763178825378418, -2.6736884117126465]\n",
      "Grand sum of 596 tensor sets is: [225.22251892089844, 987.2536010742188, -152.2565155029297, -254.81683349609375, -291.19293212890625]\n",
      "\n",
      "Instance 792 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 4: [0.8199619054794312, 0.42990249395370483, 0.271974116563797, -1.7908215522766113, -1.3280484676361084]\n",
      "Grand sum of 597 tensor sets is: [226.04248046875, 987.6835327148438, -151.9845428466797, -256.607666015625, -292.5209655761719]\n",
      "\n",
      "Instance 793 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 794 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 795 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 25: [0.45746785402297974, 2.7641401290893555, -1.0341007709503174, -0.36641669273376465, 2.0475101470947266]\n",
      "Grand sum of 598 tensor sets is: [226.4999542236328, 990.4476928710938, -153.01864624023438, -256.9740905761719, -290.47344970703125]\n",
      "\n",
      "Instance 796 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 797 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 798 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 4: [0.2848854660987854, 1.5467735528945923, -0.9097987413406372, -1.7735246419906616, -0.6173120141029358]\n",
      "Grand sum of 599 tensor sets is: [226.7848358154297, 991.9944458007812, -153.92845153808594, -258.74761962890625, -291.09075927734375]\n",
      "\n",
      "Instance 799 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 800 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 20: [-0.2809569835662842, 2.1121256351470947, -0.34707942605018616, 0.44344329833984375, -0.9811021685600281]\n",
      "Grand sum of 600 tensor sets is: [226.50387573242188, 994.1065673828125, -154.27552795410156, -258.3041687011719, -292.0718688964844]\n",
      "\n",
      "Instance 801 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 20: [1.114341378211975, 0.4422489404678345, -0.7516565322875977, 0.6944334506988525, 1.807746171951294]\n",
      "Grand sum of 601 tensor sets is: [227.6182098388672, 994.548828125, -155.02719116210938, -257.6097412109375, -290.2641296386719]\n",
      "\n",
      "Instance 802 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 803 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 804 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 805 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 806 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 807 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 10: [0.46507930755615234, 2.7291958332061768, -0.10522942245006561, -1.2737356424331665, -1.282568097114563]\n",
      "Grand sum of 602 tensor sets is: [228.08328247070312, 997.2780151367188, -155.13241577148438, -258.88348388671875, -291.54669189453125]\n",
      "\n",
      "Instance 808 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 11: [0.2416042536497116, 1.392746925354004, 0.4360382556915283, 2.092092990875244, -0.6492392420768738]\n",
      "Grand sum of 603 tensor sets is: [228.32489013671875, 998.6707763671875, -154.69638061523438, -256.7913818359375, -292.1959228515625]\n",
      "\n",
      "Instance 809 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([114, 13, 768])\n",
      "Shape of summed layers is: 114 x 768\n",
      "car at index 24: [1.3349733352661133, 1.9686176776885986, -0.5168289542198181, -1.3326306343078613, -0.6583569049835205]\n",
      "Grand sum of 604 tensor sets is: [229.6598663330078, 1000.639404296875, -155.2132110595703, -258.1240234375, -292.8542785644531]\n",
      "\n",
      "Instance 810 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 13: [0.5469253063201904, 2.283252000808716, -1.1983590126037598, -2.55601167678833, -0.614571750164032]\n",
      "Grand sum of 605 tensor sets is: [230.206787109375, 1002.9226684570312, -156.4115753173828, -260.6800231933594, -293.4688415527344]\n",
      "\n",
      "Instance 811 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 21: [1.494350790977478, -1.2355190515518188, 0.0163995623588562, 0.5556299686431885, 3.252105712890625]\n",
      "Grand sum of 606 tensor sets is: [231.70114135742188, 1001.6871337890625, -156.39517211914062, -260.1243896484375, -290.21673583984375]\n",
      "\n",
      "Instance 812 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 8: [0.14796970784664154, 1.442084789276123, 0.27861857414245605, 0.3778645694255829, -1.1832760572433472]\n",
      "Grand sum of 607 tensor sets is: [231.84910583496094, 1003.1292114257812, -156.11654663085938, -259.74652099609375, -291.4000244140625]\n",
      "\n",
      "Instance 813 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 814 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 12: [0.5166923403739929, 2.480187177658081, 1.170747995376587, 0.0841490626335144, 0.07669326663017273]\n",
      "Grand sum of 608 tensor sets is: [232.3657989501953, 1005.609375, -154.94580078125, -259.6623840332031, -291.3233337402344]\n",
      "\n",
      "Instance 815 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6, 15, 27, 38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 6: [0.6954249143600464, 2.84995174407959, -1.2888883352279663, -0.999531626701355, 0.15807244181632996]\n",
      "car at index 15: [0.7523280382156372, 3.2437963485717773, -0.772942304611206, -0.05275876820087433, -2.1808204650878906]\n",
      "car at index 27: [1.2711628675460815, 4.035166263580322, -0.8013771772384644, -1.488896369934082, -2.0943026542663574]\n",
      "car at index 38: [0.8968866467475891, 4.225948810577393, -1.2052953243255615, -1.055738091468811, 0.7644641995429993]\n",
      "Grand sum of 609 tensor sets is: [233.26974487304688, 1009.1981201171875, -155.96292114257812, -260.5616149902344, -292.1614685058594]\n",
      "\n",
      "Instance 816 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 26: [1.070615291595459, -0.6029699444770813, -0.7993477582931519, 1.622521162033081, 1.835673213005066]\n",
      "Grand sum of 610 tensor sets is: [234.34036254882812, 1008.5951538085938, -156.76226806640625, -258.9390869140625, -290.3258056640625]\n",
      "\n",
      "Instance 817 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 818 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [52]\n",
      "Size of token embeddings is torch.Size([99, 13, 768])\n",
      "Shape of summed layers is: 99 x 768\n",
      "car at index 52: [1.3569318056106567, -2.5407819747924805, -0.5047142505645752, 0.23982122540473938, 1.623666524887085]\n",
      "Grand sum of 611 tensor sets is: [235.69729614257812, 1006.0543823242188, -157.26698303222656, -258.69927978515625, -288.7021484375]\n",
      "\n",
      "Instance 819 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 13: [-0.5119667649269104, -0.2344742864370346, 0.42068517208099365, 0.13541832566261292, -1.5315418243408203]\n",
      "Grand sum of 612 tensor sets is: [235.18533325195312, 1005.8198852539062, -156.84629821777344, -258.5638732910156, -290.23370361328125]\n",
      "\n",
      "Instance 820 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([129, 13, 768])\n",
      "Shape of summed layers is: 129 x 768\n",
      "car at index 9: [0.9030309319496155, -0.9666458368301392, -0.25053292512893677, 1.8570340871810913, 3.6143229007720947]\n",
      "Grand sum of 613 tensor sets is: [236.08836364746094, 1004.8532104492188, -157.09683227539062, -256.70684814453125, -286.619384765625]\n",
      "\n",
      "Instance 821 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 20: [0.6775176525115967, 2.924215793609619, -0.3234206438064575, -2.528247833251953, -0.932220995426178]\n",
      "Grand sum of 614 tensor sets is: [236.76588439941406, 1007.7774047851562, -157.42025756835938, -259.235107421875, -287.5516052246094]\n",
      "\n",
      "Instance 822 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 823 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 12: [0.32871800661087036, 1.6481705904006958, -0.16984635591506958, -0.510445773601532, -0.057156600058078766]\n",
      "Grand sum of 615 tensor sets is: [237.0946044921875, 1009.4255981445312, -157.59010314941406, -259.74554443359375, -287.6087646484375]\n",
      "\n",
      "Instance 824 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3, 14]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 3: [0.08801941573619843, -0.3482816219329834, -1.0189141035079956, -1.9499120712280273, -1.0870258808135986]\n",
      "car at index 14: [-0.10120189189910889, 1.409983515739441, -0.8500165939331055, -2.5106215476989746, -2.55578351020813]\n",
      "Grand sum of 616 tensor sets is: [237.0880126953125, 1009.9564208984375, -158.52456665039062, -261.9757995605469, -289.43017578125]\n",
      "\n",
      "Instance 825 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 826 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15, 21, 28]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 15: [1.1031277179718018, 1.2724249362945557, -0.6683585047721863, 0.4940239191055298, -0.6098527908325195]\n",
      "car at index 21: [-0.8134704232215881, 2.8927478790283203, 0.49206435680389404, -1.0426340103149414, -1.5838797092437744]\n",
      "car at index 28: [-0.523963212966919, 3.162092685699463, 0.3432149291038513, -0.7938400506973267, -2.6971020698547363]\n",
      "Grand sum of 617 tensor sets is: [237.00991821289062, 1012.3988647460938, -158.46893310546875, -262.42327880859375, -291.0604553222656]\n",
      "\n",
      "Instance 827 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 10: [-0.2597399055957794, 2.6358158588409424, 0.048365939408540726, -1.3233625888824463, -1.670472264289856]\n",
      "Grand sum of 618 tensor sets is: [236.75018310546875, 1015.03466796875, -158.42056274414062, -263.74664306640625, -292.7309265136719]\n",
      "\n",
      "Instance 828 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 619 tensor sets is: [238.8678436279297, 1016.7110595703125, -159.65484619140625, -265.41217041015625, -292.87542724609375]\n",
      "\n",
      "Instance 829 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 830 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 831 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 832 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [115]\n",
      "Size of token embeddings is torch.Size([335, 13, 768])\n",
      "Shape of summed layers is: 335 x 768\n",
      "car at index 115: [0.8678020238876343, -1.0731654167175293, -0.767815351486206, 1.2371493577957153, 3.832540512084961]\n",
      "Grand sum of 620 tensor sets is: [239.7356414794922, 1015.6378784179688, -160.42266845703125, -264.1750183105469, -289.0428771972656]\n",
      "\n",
      "Instance 833 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "car at index 11: [-0.036107130348682404, 2.1226894855499268, 0.07696239650249481, -0.6954665780067444, -0.4236173629760742]\n",
      "Grand sum of 621 tensor sets is: [239.6995391845703, 1017.7605590820312, -160.345703125, -264.8704833984375, -289.46649169921875]\n",
      "\n",
      "Instance 834 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 14: [0.5469135046005249, 1.7605221271514893, -0.20588263869285583, 2.216726779937744, -1.0111238956451416]\n",
      "Grand sum of 622 tensor sets is: [240.2464599609375, 1019.5210571289062, -160.5515899658203, -262.65374755859375, -290.4776306152344]\n",
      "\n",
      "Instance 835 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 30: [0.39530858397483826, 1.7583057880401611, 0.20699751377105713, -2.0309255123138428, -1.667177677154541]\n",
      "Grand sum of 623 tensor sets is: [240.6417694091797, 1021.2793579101562, -160.34458923339844, -264.6846618652344, -292.1448059082031]\n",
      "\n",
      "Instance 836 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 10: [0.6345304846763611, 1.749498724937439, -0.10216442495584488, -0.3448891043663025, -0.5732999444007874]\n",
      "Grand sum of 624 tensor sets is: [241.27630615234375, 1023.0288696289062, -160.44674682617188, -265.029541015625, -292.7181091308594]\n",
      "\n",
      "Instance 837 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 8: [0.786366879940033, 1.8531209230422974, -0.5038343071937561, -1.2431907653808594, -0.538331151008606]\n",
      "Grand sum of 625 tensor sets is: [242.0626678466797, 1024.8819580078125, -160.95057678222656, -266.2727355957031, -293.2564392089844]\n",
      "\n",
      "Instance 838 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([133, 13, 768])\n",
      "Shape of summed layers is: 133 x 768\n",
      "car at index 28: [1.035080075263977, 1.5633516311645508, 0.9811044335365295, 0.13432222604751587, 2.1215362548828125]\n",
      "Grand sum of 626 tensor sets is: [243.09774780273438, 1026.4453125, -159.96946716308594, -266.138427734375, -291.1348876953125]\n",
      "\n",
      "Instance 839 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 840 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 7: [0.1400851607322693, 0.1719280481338501, 0.5156516432762146, -0.5365768074989319, 1.6939703226089478]\n",
      "Grand sum of 627 tensor sets is: [243.2378387451172, 1026.6171875, -159.4538116455078, -266.6750183105469, -289.44091796875]\n",
      "\n",
      "Instance 841 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 26: [1.1003241539001465, 2.7958316802978516, 0.9020030498504639, 0.271038681268692, -3.2501754760742188]\n",
      "Grand sum of 628 tensor sets is: [244.33816528320312, 1029.4129638671875, -158.5518035888672, -266.40399169921875, -292.69110107421875]\n",
      "\n",
      "Instance 842 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 15: [-0.9194604158401489, 0.6618155241012573, -0.38445255160331726, 0.04982329532504082, -2.4462342262268066]\n",
      "Grand sum of 629 tensor sets is: [243.418701171875, 1030.0748291015625, -158.93624877929688, -266.3541564941406, -295.1373291015625]\n",
      "\n",
      "Instance 843 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 2: [0.5054959654808044, 1.3924981355667114, 0.44301319122314453, -1.3699108362197876, 0.6656923890113831]\n",
      "Grand sum of 630 tensor sets is: [243.9241943359375, 1031.46728515625, -158.4932403564453, -267.72406005859375, -294.4716491699219]\n",
      "\n",
      "Instance 844 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 845 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 9: [0.6954032778739929, 3.2024993896484375, 0.06527813524007797, -1.1648634672164917, -0.8348920941352844]\n",
      "Grand sum of 631 tensor sets is: [244.61959838867188, 1034.6697998046875, -158.42796325683594, -268.888916015625, -295.3065490722656]\n",
      "\n",
      "Instance 846 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 32]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 11: [0.5581645369529724, 1.0527828931808472, -0.7464016079902649, -1.73806893825531, -1.5348613262176514]\n",
      "car at index 32: [0.6355673670768738, 0.5753507614135742, -0.033690132200717926, -1.1797679662704468, -0.9980483055114746]\n",
      "Grand sum of 632 tensor sets is: [245.21646118164062, 1035.48388671875, -158.81800842285156, -270.34783935546875, -296.572998046875]\n",
      "\n",
      "Instance 847 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4, 13, 22]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 4: [1.0526138544082642, 0.7236846089363098, -0.39295095205307007, -1.01686429977417, -1.5079560279846191]\n",
      "car at index 13: [0.7681447863578796, 1.1931349039077759, 0.5302462577819824, -0.6626686453819275, -1.8150690793991089]\n",
      "car at index 22: [0.668434202671051, 1.0252097845077515, 0.4850345849990845, -0.3860560953617096, -1.3806430101394653]\n",
      "Grand sum of 633 tensor sets is: [246.0461883544922, 1036.464599609375, -158.61056518554688, -271.036376953125, -298.1408996582031]\n",
      "\n",
      "Instance 848 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6, 18]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 6: [0.43940848112106323, 2.0049962997436523, -0.6436150074005127, 0.4507358968257904, 1.209332823753357]\n",
      "car at index 18: [0.0374690443277359, 1.9289454221725464, -0.8663867712020874, 0.0413145087659359, -2.349728584289551]\n",
      "Grand sum of 634 tensor sets is: [246.2846221923828, 1038.4315185546875, -159.36557006835938, -270.79034423828125, -298.7110900878906]\n",
      "\n",
      "Instance 849 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 14: [-0.6703935265541077, 0.18844753503799438, 1.2675602436065674, -1.0461080074310303, 0.11019529402256012]\n",
      "Grand sum of 635 tensor sets is: [245.61422729492188, 1038.6199951171875, -158.09800720214844, -271.8364562988281, -298.60089111328125]\n",
      "\n",
      "Instance 850 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 851 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 24]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "car at index 7: [0.5900665521621704, 2.3384485244750977, -0.7463645339012146, -0.47679051756858826, 1.9435182809829712]\n",
      "car at index 24: [0.614408016204834, 1.9992884397506714, -0.08982053399085999, -0.36607101559638977, 1.1045712232589722]\n",
      "Grand sum of 636 tensor sets is: [246.21646118164062, 1040.788818359375, -158.51609802246094, -272.25787353515625, -297.07684326171875]\n",
      "\n",
      "Instance 852 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 31: [0.18157632648944855, 1.0155770778656006, 0.2802017629146576, 0.7616165280342102, -0.9559684991836548]\n",
      "Grand sum of 637 tensor sets is: [246.39804077148438, 1041.804443359375, -158.23590087890625, -271.4962463378906, -298.0328063964844]\n",
      "\n",
      "Instance 853 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 4: [0.12843483686447144, 3.086536407470703, 0.5667886734008789, -0.2913397550582886, -2.4701921939849854]\n",
      "Grand sum of 638 tensor sets is: [246.52647399902344, 1044.8909912109375, -157.6691131591797, -271.78759765625, -300.50299072265625]\n",
      "\n",
      "Instance 854 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 855 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24, 36]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 24: [0.8527138829231262, 1.0018796920776367, -0.8299533128738403, -1.0936906337738037, 0.8105202913284302]\n",
      "car at index 36: [0.7266370058059692, 1.1668835878372192, -0.9619532823562622, -1.6231282949447632, -1.1290922164916992]\n",
      "Grand sum of 639 tensor sets is: [247.31614685058594, 1045.975341796875, -158.5650634765625, -273.14599609375, -300.6622619628906]\n",
      "\n",
      "Instance 856 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 857 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 858 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 14: [0.21240976452827454, 1.4119212627410889, -2.0015242099761963, -1.264151692390442, -0.7974288463592529]\n",
      "Grand sum of 640 tensor sets is: [247.52854919433594, 1047.38720703125, -160.56658935546875, -274.41015625, -301.4596862792969]\n",
      "\n",
      "Instance 859 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3, 26]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 3: [-1.1954212188720703, 1.4889729022979736, 0.25833624601364136, -1.7466486692428589, 0.6805431842803955]\n",
      "car at index 26: [-0.8694864511489868, 1.0592790842056274, 0.5499971508979797, -0.9216609597206116, 1.2032136917114258]\n",
      "Grand sum of 641 tensor sets is: [246.49609375, 1048.661376953125, -160.1624298095703, -275.74432373046875, -300.517822265625]\n",
      "\n",
      "Instance 860 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 10: [1.2650172710418701, -1.1878795623779297, -0.4710999131202698, 1.3314765691757202, 2.3562374114990234]\n",
      "Grand sum of 642 tensor sets is: [247.7611083984375, 1047.4735107421875, -160.63352966308594, -274.412841796875, -298.1615905761719]\n",
      "\n",
      "Instance 861 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 862 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 4: [0.5729762315750122, 2.1777961254119873, -0.13794833421707153, -0.9577749967575073, 0.2123258411884308]\n",
      "Grand sum of 643 tensor sets is: [248.33409118652344, 1049.6513671875, -160.771484375, -275.37060546875, -297.94927978515625]\n",
      "\n",
      "Instance 863 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 9: [0.42078322172164917, 3.0551719665527344, -0.7200517058372498, 0.7107909917831421, -2.1429741382598877]\n",
      "Grand sum of 644 tensor sets is: [248.75486755371094, 1052.70654296875, -161.4915313720703, -274.6598205566406, -300.0922546386719]\n",
      "\n",
      "Instance 864 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9, 20]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 9: [0.1720060259103775, 1.8648186922073364, -0.8277904987335205, -1.918168544769287, -0.36662283539772034]\n",
      "car at index 20: [0.179336816072464, 2.366262912750244, -0.04482391104102135, -1.6988232135772705, -0.4095330238342285]\n",
      "Grand sum of 645 tensor sets is: [248.9305419921875, 1054.8221435546875, -161.92784118652344, -276.46832275390625, -300.4803466796875]\n",
      "\n",
      "Instance 865 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 12: [0.2551746368408203, 2.2352635860443115, -0.7588742971420288, 1.7764467000961304, -0.3613584041595459]\n",
      "Grand sum of 646 tensor sets is: [249.1857147216797, 1057.057373046875, -162.6867218017578, -274.6918640136719, -300.8417053222656]\n",
      "\n",
      "Instance 866 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 867 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "car at index 29: [0.5877522230148315, 0.40183666348457336, -1.035367727279663, 0.34483617544174194, 2.892930030822754]\n",
      "Grand sum of 647 tensor sets is: [249.77346801757812, 1057.459228515625, -163.7220916748047, -274.3470153808594, -297.9487609863281]\n",
      "\n",
      "Instance 868 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "car at index 8: [0.475509375333786, 1.9806963205337524, -0.20100867748260498, -1.4495251178741455, -0.9354661703109741]\n",
      "Grand sum of 648 tensor sets is: [250.2489776611328, 1059.43994140625, -163.923095703125, -275.7965393066406, -298.88421630859375]\n",
      "\n",
      "Instance 869 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 870 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([81, 13, 768])\n",
      "Shape of summed layers is: 81 x 768\n",
      "car at index 37: [0.6232802271842957, 2.7149579524993896, -0.2401161938905716, -1.316931128501892, 0.03852538764476776]\n",
      "Grand sum of 649 tensor sets is: [250.87225341796875, 1062.1549072265625, -164.1632080078125, -277.11346435546875, -298.845703125]\n",
      "\n",
      "Instance 871 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 19: [-0.23651403188705444, 1.2993446588516235, 0.8693460822105408, -1.4531471729278564, 0.01819467544555664]\n",
      "Grand sum of 650 tensor sets is: [250.6357421875, 1063.4542236328125, -163.2938690185547, -278.5666198730469, -298.8275146484375]\n",
      "\n",
      "Instance 872 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 9: [0.5552121996879578, 1.6514670848846436, -1.3366076946258545, -1.7735307216644287, 0.5621871948242188]\n",
      "Grand sum of 651 tensor sets is: [251.19094848632812, 1065.105712890625, -164.63047790527344, -280.34014892578125, -298.26531982421875]\n",
      "\n",
      "Instance 873 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 2: [-0.08439549058675766, 3.0904135704040527, 0.013348191976547241, -0.8386803865432739, -1.7551146745681763]\n",
      "Grand sum of 652 tensor sets is: [251.10655212402344, 1068.1961669921875, -164.61712646484375, -281.1788330078125, -300.02044677734375]\n",
      "\n",
      "Instance 874 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [118]\n",
      "Size of token embeddings is torch.Size([155, 13, 768])\n",
      "Shape of summed layers is: 155 x 768\n",
      "car at index 118: [0.9511698484420776, -1.2583673000335693, -0.6359943747520447, 1.1629407405853271, 5.023273468017578]\n",
      "Grand sum of 653 tensor sets is: [252.05772399902344, 1066.937744140625, -165.2531280517578, -280.0158996582031, -294.9971618652344]\n",
      "\n",
      "Instance 875 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 7: [-0.14574238657951355, 2.154967784881592, -0.8403400182723999, -1.4517253637313843, -1.9862351417541504]\n",
      "Grand sum of 654 tensor sets is: [251.9119873046875, 1069.0926513671875, -166.09347534179688, -281.4676208496094, -296.9833984375]\n",
      "\n",
      "Instance 876 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 17]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 11: [0.0027423352003097534, 1.9003942012786865, -1.034591555595398, -1.5672049522399902, -0.3493903875350952]\n",
      "car at index 17: [-0.003763992339372635, 1.588529348373413, -0.09214400500059128, -0.09062206745147705, 0.31637221574783325]\n",
      "Grand sum of 655 tensor sets is: [251.91148376464844, 1070.837158203125, -166.65684509277344, -282.2965393066406, -296.9999084472656]\n",
      "\n",
      "Instance 877 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [212]\n",
      "Size of token embeddings is torch.Size([465, 13, 768])\n",
      "Shape of summed layers is: 465 x 768\n",
      "car at index 212: [0.7571089267730713, 2.037895441055298, -0.3120841681957245, -2.6231768131256104, 2.0964195728302]\n",
      "Grand sum of 656 tensor sets is: [252.66859436035156, 1072.875, -166.96893310546875, -284.9197082519531, -294.90350341796875]\n",
      "\n",
      "Instance 878 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 879 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33, 68]\n",
      "Size of token embeddings is torch.Size([81, 13, 768])\n",
      "Shape of summed layers is: 81 x 768\n",
      "car at index 33: [-0.09756647050380707, 2.390122413635254, -0.46842432022094727, -2.153531312942505, 0.7519474029541016]\n",
      "car at index 68: [0.43502315878868103, 0.8275442719459534, -1.2846394777297974, -0.49043744802474976, 0.7459242343902588]\n",
      "Grand sum of 657 tensor sets is: [252.8373260498047, 1074.48388671875, -167.845458984375, -286.24169921875, -294.1545715332031]\n",
      "\n",
      "Instance 880 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 881 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 2: [0.4281034469604492, 1.017027735710144, -0.22914288938045502, 0.12074311822652817, -0.08869946748018265]\n",
      "Grand sum of 658 tensor sets is: [253.2654266357422, 1075.5008544921875, -168.07460021972656, -286.1209411621094, -294.2432861328125]\n",
      "\n",
      "Instance 882 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 883 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 5: [0.45840680599212646, 2.038515090942383, -1.1753509044647217, -0.519013524055481, 1.2964156866073608]\n",
      "Grand sum of 659 tensor sets is: [253.7238311767578, 1077.5394287109375, -169.2499542236328, -286.63995361328125, -292.9468688964844]\n",
      "\n",
      "Instance 884 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 10: [-0.3521457612514496, 1.6671596765518188, -0.3713957965373993, 0.30796608328819275, -2.0589828491210938]\n",
      "Grand sum of 660 tensor sets is: [253.37168884277344, 1079.20654296875, -169.62135314941406, -286.3320007324219, -295.005859375]\n",
      "\n",
      "Instance 885 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 15: [0.6916458606719971, 1.855867862701416, -0.7830523252487183, -1.3887507915496826, -0.19556471705436707]\n",
      "Grand sum of 661 tensor sets is: [254.06333923339844, 1081.0623779296875, -170.40440368652344, -287.72076416015625, -295.201416015625]\n",
      "\n",
      "Instance 886 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 11: [-0.10461954027414322, 0.2789139747619629, -1.5012441873550415, -0.6502118110656738, -1.1968798637390137]\n",
      "Grand sum of 662 tensor sets is: [253.95872497558594, 1081.34130859375, -171.90565490722656, -288.3709716796875, -296.3982849121094]\n",
      "\n",
      "Instance 887 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "car at index 34: [-0.2148101031780243, 0.7302997708320618, -1.2390809059143066, -0.25942039489746094, -0.022990435361862183]\n",
      "Grand sum of 663 tensor sets is: [253.74391174316406, 1082.0716552734375, -173.1447296142578, -288.6304016113281, -296.4212646484375]\n",
      "\n",
      "Instance 888 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 19: [0.26483455300331116, 2.1175928115844727, 0.9552001953125, 1.670500636100769, -2.4910430908203125]\n",
      "Grand sum of 664 tensor sets is: [254.0087432861328, 1084.189208984375, -172.1895294189453, -286.95989990234375, -298.91229248046875]\n",
      "\n",
      "Instance 889 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 890 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 131: [0.23582568764686584, 1.6909043788909912, -0.25876644253730774, -1.1283290386199951, -0.12899576127529144]\n",
      "Grand sum of 665 tensor sets is: [254.24456787109375, 1085.880126953125, -172.44830322265625, -288.0882263183594, -299.0412902832031]\n",
      "\n",
      "Instance 891 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "car at index 17: [0.7825833559036255, -0.665968656539917, -0.02541375160217285, 0.9710811376571655, 1.5269471406936646]\n",
      "Grand sum of 666 tensor sets is: [255.0271453857422, 1085.214111328125, -172.47372436523438, -287.1171569824219, -297.51434326171875]\n",
      "\n",
      "Instance 892 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 18: [0.6018051505088806, 2.935736894607544, -0.9934250116348267, 0.4443986117839813, 1.117485523223877]\n",
      "Grand sum of 667 tensor sets is: [255.6289520263672, 1088.14990234375, -173.46714782714844, -286.6727600097656, -296.3968505859375]\n",
      "\n",
      "Instance 893 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 20: [0.6791349649429321, 1.6902790069580078, 0.26752620935440063, -1.712896704673767, -1.554172158241272]\n",
      "Grand sum of 668 tensor sets is: [256.3080749511719, 1089.8402099609375, -173.19961547851562, -288.3856506347656, -297.9510192871094]\n",
      "\n",
      "Instance 894 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 4: [0.5680947303771973, 2.115797758102417, -1.3507486581802368, -2.0081257820129395, -0.18998584151268005]\n",
      "Grand sum of 669 tensor sets is: [256.87615966796875, 1091.9560546875, -174.5503692626953, -290.3937683105469, -298.1409912109375]\n",
      "\n",
      "Instance 895 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 5: [0.40530306100845337, 2.618180513381958, 0.00752074271440506, -2.154550075531006, -0.4564259350299835]\n",
      "Grand sum of 670 tensor sets is: [257.2814636230469, 1094.57421875, -174.5428466796875, -292.5483093261719, -298.597412109375]\n",
      "\n",
      "Instance 896 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 9: [-0.23976118862628937, 1.9163811206817627, -0.8665618300437927, -1.5010960102081299, -2.5892271995544434]\n",
      "Grand sum of 671 tensor sets is: [257.0417175292969, 1096.4906005859375, -175.40940856933594, -294.0494079589844, -301.1866455078125]\n",
      "\n",
      "Instance 897 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 9: [-0.4145895838737488, 2.8331520557403564, -0.44365036487579346, -0.3851886987686157, -1.4857221841812134]\n",
      "Grand sum of 672 tensor sets is: [256.62713623046875, 1099.32373046875, -175.85305786132812, -294.4346008300781, -302.67236328125]\n",
      "\n",
      "Instance 898 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [202]\n",
      "Size of token embeddings is torch.Size([253, 13, 768])\n",
      "Shape of summed layers is: 253 x 768\n",
      "car at index 202: [0.40645453333854675, 0.47038304805755615, -0.667731523513794, -1.072697401046753, -0.00030425190925598145]\n",
      "Grand sum of 673 tensor sets is: [257.0335998535156, 1099.7940673828125, -176.52078247070312, -295.5072937011719, -302.67266845703125]\n",
      "\n",
      "Instance 899 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 900 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 3: [0.15312203764915466, 3.83541202545166, -1.344545602798462, -1.4252779483795166, -0.6630294919013977]\n",
      "Grand sum of 674 tensor sets is: [257.1867370605469, 1103.6295166015625, -177.86532592773438, -296.9325866699219, -303.335693359375]\n",
      "\n",
      "Instance 901 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "car at index 28: [0.0007164925336837769, 2.616739273071289, -0.026572201400995255, -2.8673481941223145, -0.8368910551071167]\n",
      "Grand sum of 675 tensor sets is: [257.18743896484375, 1106.2462158203125, -177.8918914794922, -299.7999267578125, -304.1725769042969]\n",
      "\n",
      "Instance 902 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 7: [-0.20452842116355896, 2.7011733055114746, -0.48079198598861694, -2.3591883182525635, -0.8604613542556763]\n",
      "Grand sum of 676 tensor sets is: [256.98291015625, 1108.9473876953125, -178.3726806640625, -302.15911865234375, -305.0330505371094]\n",
      "\n",
      "Instance 903 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 11: [-0.019055604934692383, 3.387754201889038, 0.18698501586914062, -1.7159866094589233, -1.5500067472457886]\n",
      "Grand sum of 677 tensor sets is: [256.9638671875, 1112.3350830078125, -178.18569946289062, -303.8750915527344, -306.58306884765625]\n",
      "\n",
      "Instance 904 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 905 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 906 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 6: [-0.2895980477333069, 0.8395588994026184, -0.9704111814498901, -1.4470829963684082, -0.6394685506820679]\n",
      "Grand sum of 678 tensor sets is: [256.67425537109375, 1113.1746826171875, -179.15611267089844, -305.3221740722656, -307.2225341796875]\n",
      "\n",
      "Instance 907 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 25: [0.7092103958129883, 1.2081924676895142, 0.8863035440444946, 2.9182841777801514, -2.6574866771698]\n",
      "Grand sum of 679 tensor sets is: [257.3834533691406, 1114.3829345703125, -178.26980590820312, -302.4039001464844, -309.8800354003906]\n",
      "\n",
      "Instance 908 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 4: [1.6593120098114014, 1.3872970342636108, -0.9011659026145935, -1.5839519500732422, 0.9861806631088257]\n",
      "Grand sum of 680 tensor sets is: [259.0427551269531, 1115.770263671875, -179.1709747314453, -303.98785400390625, -308.89385986328125]\n",
      "\n",
      "Instance 909 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 6: [-0.14276400208473206, 0.026363033801317215, 0.17090697586536407, 2.171374797821045, -4.3675079345703125]\n",
      "Grand sum of 681 tensor sets is: [258.8999938964844, 1115.796630859375, -179.00006103515625, -301.81646728515625, -313.2613525390625]\n",
      "\n",
      "Instance 910 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2, 21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 2: [0.15511572360992432, 1.3472498655319214, 0.2530618906021118, -2.3625195026397705, -0.415825754404068]\n",
      "car at index 21: [0.18424929678440094, 1.4959746599197388, 1.2955787181854248, -2.840562105178833, -1.5810743570327759]\n",
      "Grand sum of 682 tensor sets is: [259.0696716308594, 1117.21826171875, -178.22573852539062, -304.4179992675781, -314.2597961425781]\n",
      "\n",
      "Instance 911 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [124]\n",
      "Size of token embeddings is torch.Size([151, 13, 768])\n",
      "Shape of summed layers is: 151 x 768\n",
      "car at index 124: [0.5515617728233337, 0.0981915295124054, -1.277684211730957, 0.6609699726104736, 3.783843517303467]\n",
      "Grand sum of 683 tensor sets is: [259.6212463378906, 1117.31640625, -179.50341796875, -303.75701904296875, -310.4759521484375]\n",
      "\n",
      "Instance 912 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 5: [0.8249891996383667, 0.3479022681713104, -0.4612407088279724, 0.6343589425086975, -0.9200606942176819]\n",
      "Grand sum of 684 tensor sets is: [260.44622802734375, 1117.664306640625, -179.96466064453125, -303.1226501464844, -311.3960266113281]\n",
      "\n",
      "Instance 913 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 4: [0.5648154020309448, 1.6549850702285767, -0.9758009910583496, -0.7928694486618042, 1.9265964031219482]\n",
      "Grand sum of 685 tensor sets is: [261.01104736328125, 1119.3193359375, -180.94046020507812, -303.91552734375, -309.46942138671875]\n",
      "\n",
      "Instance 914 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 38: [1.100177526473999, 3.248154401779175, -0.29698705673217773, -2.169959545135498, -0.46613895893096924]\n",
      "Grand sum of 686 tensor sets is: [262.1112365722656, 1122.5675048828125, -181.23744201660156, -306.0854797363281, -309.935546875]\n",
      "\n",
      "Instance 915 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [129, 136]\n",
      "Size of token embeddings is torch.Size([397, 13, 768])\n",
      "Shape of summed layers is: 397 x 768\n",
      "car at index 129: [0.9031996726989746, 1.8111693859100342, 0.1260238140821457, 0.24954237043857574, -0.2954513728618622]\n",
      "car at index 136: [0.8173167109489441, 1.4433810710906982, -0.6057784557342529, -0.87981116771698, 0.8632596135139465]\n",
      "Grand sum of 687 tensor sets is: [262.97149658203125, 1124.19482421875, -181.47732543945312, -306.4006042480469, -309.6516418457031]\n",
      "\n",
      "Instance 916 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 7: [0.3998847007751465, 3.1280603408813477, -0.7933928370475769, -0.02286538854241371, -2.0155324935913086]\n",
      "Grand sum of 688 tensor sets is: [263.3713684082031, 1127.3228759765625, -182.27072143554688, -306.4234619140625, -311.66717529296875]\n",
      "\n",
      "Instance 917 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 918 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 919 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "car at index 14: [1.0543056726455688, 3.459993600845337, 0.13652552664279938, -1.6441311836242676, -1.4155255556106567]\n",
      "Grand sum of 689 tensor sets is: [264.4256591796875, 1130.7828369140625, -182.1342010498047, -308.0675964355469, -313.08270263671875]\n",
      "\n",
      "Instance 920 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "car at index 28: [0.9039500951766968, 2.1536405086517334, -0.130272775888443, -0.70328688621521, -2.125777244567871]\n",
      "Grand sum of 690 tensor sets is: [265.3296203613281, 1132.9365234375, -182.2644805908203, -308.7708740234375, -315.2084655761719]\n",
      "\n",
      "Instance 921 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 922 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 5: [0.633280336856842, 2.008974313735962, -0.015917859971523285, -0.8826419115066528, -2.174132823944092]\n",
      "Grand sum of 691 tensor sets is: [265.962890625, 1134.945556640625, -182.2803955078125, -309.65350341796875, -317.3825988769531]\n",
      "\n",
      "Instance 923 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 4: [0.47353243827819824, 1.8235440254211426, -0.980283260345459, -2.2076261043548584, 0.047054823487997055]\n",
      "Grand sum of 692 tensor sets is: [266.4364318847656, 1136.76904296875, -183.26068115234375, -311.8611145019531, -317.3355407714844]\n",
      "\n",
      "Instance 924 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 16: [-0.51680988073349, 3.9454076290130615, 0.26078054308891296, 2.3971729278564453, -2.3276021480560303]\n",
      "Grand sum of 693 tensor sets is: [265.91961669921875, 1140.7144775390625, -182.99989318847656, -309.46392822265625, -319.66314697265625]\n",
      "\n",
      "Instance 925 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 24: [1.1149406433105469, 2.049854278564453, -0.9747323393821716, -0.9285154342651367, 2.2649683952331543]\n",
      "Grand sum of 694 tensor sets is: [267.0345458984375, 1142.7642822265625, -183.97462463378906, -310.3924560546875, -317.398193359375]\n",
      "\n",
      "Instance 926 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 10: [-0.2924940884113312, 1.8148573637008667, -0.9982732534408569, -1.560268759727478, -0.7672444581985474]\n",
      "Grand sum of 695 tensor sets is: [266.7420654296875, 1144.5791015625, -184.972900390625, -311.9527282714844, -318.1654357910156]\n",
      "\n",
      "Instance 927 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 14: [0.36484771966934204, 2.763648509979248, -0.08144360780715942, 1.2191030979156494, -2.798837900161743]\n",
      "Grand sum of 696 tensor sets is: [267.1069030761719, 1147.3427734375, -185.05433654785156, -310.7336120605469, -320.9642639160156]\n",
      "\n",
      "Instance 928 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 43: [0.7620043754577637, 0.788540244102478, -1.0996049642562866, 0.46423184871673584, 1.2340219020843506]\n",
      "Grand sum of 697 tensor sets is: [267.868896484375, 1148.13134765625, -186.15394592285156, -310.2693786621094, -319.7302551269531]\n",
      "\n",
      "Instance 929 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 930 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 22: [0.02173047885298729, 2.371593952178955, -0.39743807911872864, 1.1555523872375488, 0.7603363990783691]\n",
      "Grand sum of 698 tensor sets is: [267.890625, 1150.5029296875, -186.5513916015625, -309.11383056640625, -318.96990966796875]\n",
      "\n",
      "Instance 931 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [51]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "car at index 51: [0.4659883379936218, 2.6448190212249756, -0.4135255515575409, -1.2934777736663818, -0.12893104553222656]\n",
      "Grand sum of 699 tensor sets is: [268.35662841796875, 1153.147705078125, -186.9649200439453, -310.4073181152344, -319.0988464355469]\n",
      "\n",
      "Instance 932 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 5: [1.1488878726959229, 1.7717435359954834, -1.3922368288040161, -0.6270546913146973, -1.7157796621322632]\n",
      "Grand sum of 700 tensor sets is: [269.5055236816406, 1154.91943359375, -188.35716247558594, -311.03436279296875, -320.81463623046875]\n",
      "\n",
      "Instance 933 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [211]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 211: [1.1001183986663818, 1.2305692434310913, -0.6856818199157715, -0.7824596166610718, 1.524458646774292]\n",
      "Grand sum of 701 tensor sets is: [270.60565185546875, 1156.1500244140625, -189.0428466796875, -311.81683349609375, -319.2901916503906]\n",
      "\n",
      "Instance 934 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 19: [-0.6723566055297852, 1.972905158996582, -0.5395227670669556, 2.1574254035949707, -1.117830514907837]\n",
      "Grand sum of 702 tensor sets is: [269.93328857421875, 1158.1229248046875, -189.58236694335938, -309.6593933105469, -320.40802001953125]\n",
      "\n",
      "Instance 935 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 936 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "car at index 27: [-0.8065916299819946, 2.451918363571167, 1.1250712871551514, -0.910750150680542, -1.3820688724517822]\n",
      "Grand sum of 703 tensor sets is: [269.126708984375, 1160.5748291015625, -188.45729064941406, -310.57012939453125, -321.79010009765625]\n",
      "\n",
      "Instance 937 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 24]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 11: [-0.2817177474498749, 0.8803664445877075, -0.1964840590953827, -0.4163169860839844, 0.3083180785179138]\n",
      "car at index 24: [0.7626160383224487, 1.5031837224960327, -0.693655252456665, -0.25818702578544617, 1.365762710571289]\n",
      "Grand sum of 704 tensor sets is: [269.3671569824219, 1161.7666015625, -188.90235900878906, -310.9073791503906, -320.95306396484375]\n",
      "\n",
      "Instance 938 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([5, 13, 768])\n",
      "Shape of summed layers is: 5 x 768\n",
      "car at index 2: [-0.5565102696418762, 1.4755359888076782, -0.39782649278640747, 0.0991508886218071, 0.2940862774848938]\n",
      "Grand sum of 705 tensor sets is: [268.8106384277344, 1163.2421875, -189.30018615722656, -310.8082275390625, -320.6589660644531]\n",
      "\n",
      "Instance 939 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 16: [0.20999431610107422, 1.6573562622070312, -0.49977052211761475, -2.2879748344421387, -1.8302005529403687]\n",
      "Grand sum of 706 tensor sets is: [269.0206298828125, 1164.8995361328125, -189.79995727539062, -313.09619140625, -322.4891662597656]\n",
      "\n",
      "Instance 940 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 17: [1.0022732019424438, 1.4734182357788086, -0.06453973799943924, -0.7506327629089355, -2.2859225273132324]\n",
      "Grand sum of 707 tensor sets is: [270.02288818359375, 1166.3729248046875, -189.864501953125, -313.8468322753906, -324.77508544921875]\n",
      "\n",
      "Instance 941 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 942 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 943 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 944 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 15: [-0.012819677591323853, 1.6784168481826782, -0.10362974554300308, 0.35989463329315186, -2.0775399208068848]\n",
      "Grand sum of 708 tensor sets is: [270.01007080078125, 1168.0513916015625, -189.96812438964844, -313.4869384765625, -326.8526306152344]\n",
      "\n",
      "Instance 945 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([80, 13, 768])\n",
      "Shape of summed layers is: 80 x 768\n",
      "car at index 14: [2.357844829559326, -2.521080493927002, 0.6453008055686951, 0.2700757682323456, 0.19213014841079712]\n",
      "Grand sum of 709 tensor sets is: [272.367919921875, 1165.5302734375, -189.3228302001953, -313.21685791015625, -326.6604919433594]\n",
      "\n",
      "Instance 946 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 17: [0.31262272596359253, 2.3295066356658936, 0.13251690566539764, -2.336591958999634, -0.48341143131256104]\n",
      "Grand sum of 710 tensor sets is: [272.6805419921875, 1167.8597412109375, -189.1903076171875, -315.5534362792969, -327.1438903808594]\n",
      "\n",
      "Instance 947 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 3: [0.5729539394378662, 2.840810775756836, -0.2060621827840805, -1.7556896209716797, -1.1517412662506104]\n",
      "Grand sum of 711 tensor sets is: [273.2535095214844, 1170.7005615234375, -189.3963623046875, -317.3091125488281, -328.2956237792969]\n",
      "\n",
      "Instance 948 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 949 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14, 20]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 14: [1.1178797483444214, 2.5072624683380127, -0.2644965946674347, 0.7220214009284973, -2.0910685062408447]\n",
      "car at index 20: [-0.2067137360572815, 3.388646364212036, -0.6551417112350464, 0.6890284419059753, -2.6584129333496094]\n",
      "Grand sum of 712 tensor sets is: [273.7091064453125, 1173.6485595703125, -189.85618591308594, -316.60357666015625, -330.67034912109375]\n",
      "\n",
      "Instance 950 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 2: [0.5848053693771362, 2.656128406524658, 0.48885875940322876, -1.5616482496261597, -1.096427321434021]\n",
      "Grand sum of 713 tensor sets is: [274.2939147949219, 1176.3046875, -189.36732482910156, -318.16522216796875, -331.76678466796875]\n",
      "\n",
      "Instance 951 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "car at index 50: [1.6306655406951904, -1.3010011911392212, -1.1782548427581787, 1.2703593969345093, 2.199537515640259]\n",
      "Grand sum of 714 tensor sets is: [275.9245910644531, 1175.003662109375, -190.5455780029297, -316.8948669433594, -329.5672607421875]\n",
      "\n",
      "Instance 952 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 30: [0.8159289360046387, 0.1700512319803238, -0.694212019443512, -0.5794410109519958, 1.5378761291503906]\n",
      "Grand sum of 715 tensor sets is: [276.7405090332031, 1175.1737060546875, -191.2397918701172, -317.47430419921875, -328.0293884277344]\n",
      "\n",
      "Instance 953 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 29: [-0.10378283262252808, 4.382467746734619, -0.08186671137809753, 0.4547976851463318, 0.16177316009998322]\n",
      "Grand sum of 716 tensor sets is: [276.63671875, 1179.55615234375, -191.3216552734375, -317.0195007324219, -327.86761474609375]\n",
      "\n",
      "Instance 954 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "car at index 41: [1.1994974613189697, -0.977837324142456, 0.1808488965034485, 1.439910650253296, 0.9157450795173645]\n",
      "Grand sum of 717 tensor sets is: [277.8362121582031, 1178.578369140625, -191.14080810546875, -315.57958984375, -326.9518737792969]\n",
      "\n",
      "Instance 955 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 956 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 24: [-0.08510293066501617, 1.2238764762878418, 0.8745250701904297, -1.0454038381576538, -1.5989444255828857]\n",
      "Grand sum of 718 tensor sets is: [277.7510986328125, 1179.80224609375, -190.2662811279297, -316.625, -328.5508117675781]\n",
      "\n",
      "Instance 957 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 958 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 959 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 960 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 17: [0.8657232522964478, 3.3362863063812256, -0.3487895131111145, 0.6996974945068359, -2.7054989337921143]\n",
      "Grand sum of 719 tensor sets is: [278.6168212890625, 1183.1385498046875, -190.6150665283203, -315.92529296875, -331.2563171386719]\n",
      "\n",
      "Instance 961 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 18: [-0.11343228816986084, 1.18161940574646, -1.1620750427246094, -0.19464872777462006, -1.1667084693908691]\n",
      "Grand sum of 720 tensor sets is: [278.5033874511719, 1184.3201904296875, -191.7771453857422, -316.11993408203125, -332.42303466796875]\n",
      "\n",
      "Instance 962 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 8: [-0.7169988751411438, 3.286092519760132, -0.6478524208068848, -1.8384649753570557, 1.1022560596466064]\n",
      "Grand sum of 721 tensor sets is: [277.786376953125, 1187.6063232421875, -192.4250030517578, -317.9584045410156, -331.3207702636719]\n",
      "\n",
      "Instance 963 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 964 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8, 10]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "car at index 8: [-0.18739338219165802, 1.9482510089874268, 0.27007856965065, -2.937643051147461, -1.6925864219665527]\n",
      "car at index 10: [0.19126003980636597, 2.288095712661743, -0.18068578839302063, -0.04997163265943527, -0.6902673840522766]\n",
      "Grand sum of 722 tensor sets is: [277.7882995605469, 1189.7244873046875, -192.38031005859375, -319.45220947265625, -332.51220703125]\n",
      "\n",
      "Instance 965 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "car at index 40: [1.39853036403656, -1.394228458404541, -0.7307286262512207, 0.37927117943763733, 0.03633130341768265]\n",
      "Grand sum of 723 tensor sets is: [279.18682861328125, 1188.3302001953125, -193.1110382080078, -319.07293701171875, -332.4758605957031]\n",
      "\n",
      "Instance 966 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 967 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 14: [-0.2080678641796112, 2.3473644256591797, -0.6097249388694763, -0.10927148163318634, -2.3311243057250977]\n",
      "Grand sum of 724 tensor sets is: [278.978759765625, 1190.6776123046875, -193.72076416015625, -319.1822204589844, -334.8069763183594]\n",
      "\n",
      "Instance 968 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [318]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 318: [0.18047203123569489, 2.3846688270568848, -0.5309341549873352, -1.0608117580413818, 1.8781602382659912]\n",
      "Grand sum of 725 tensor sets is: [279.15924072265625, 1193.062255859375, -194.25169372558594, -320.2430419921875, -332.9288024902344]\n",
      "\n",
      "Instance 969 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "car at index 7: [0.8466371297836304, -1.0580922365188599, -0.047322750091552734, 0.648051381111145, 3.156517744064331]\n",
      "Grand sum of 726 tensor sets is: [280.0058898925781, 1192.004150390625, -194.29901123046875, -319.5950012207031, -329.77227783203125]\n",
      "\n",
      "Instance 970 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 971 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "car at index 10: [0.2564483880996704, -1.439337134361267, -0.18128372728824615, 1.2938330173492432, 3.6634464263916016]\n",
      "Grand sum of 727 tensor sets is: [280.2623291015625, 1190.5648193359375, -194.4803009033203, -318.3011779785156, -326.10882568359375]\n",
      "\n",
      "Instance 972 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 8: [1.0161104202270508, 1.03487229347229, -1.1226962804794312, -1.298157811164856, 0.49782872200012207]\n",
      "Grand sum of 728 tensor sets is: [281.2784423828125, 1191.5997314453125, -195.60299682617188, -319.5993347167969, -325.6109924316406]\n",
      "\n",
      "Instance 973 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 7: [0.3007237911224365, 2.0584723949432373, -0.9672141075134277, 1.502176284790039, -2.507525682449341]\n",
      "Grand sum of 729 tensor sets is: [281.57916259765625, 1193.658203125, -196.57020568847656, -318.09716796875, -328.1185302734375]\n",
      "\n",
      "Instance 974 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [55]\n",
      "Size of token embeddings is torch.Size([70, 13, 768])\n",
      "Shape of summed layers is: 70 x 768\n",
      "car at index 55: [-0.16436922550201416, -0.8856229782104492, -1.5821499824523926, 1.2919176816940308, 3.4016246795654297]\n",
      "Grand sum of 730 tensor sets is: [281.414794921875, 1192.7725830078125, -198.15235900878906, -316.80523681640625, -324.7169189453125]\n",
      "\n",
      "Instance 975 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 36: [1.1631666421890259, 1.7591549158096313, -0.6227055191993713, 0.387153685092926, 0.19060835242271423]\n",
      "Grand sum of 731 tensor sets is: [282.5779724121094, 1194.53173828125, -198.7750701904297, -316.4180908203125, -324.52630615234375]\n",
      "\n",
      "Instance 976 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 977 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 14: [0.34221285581588745, 1.7619459629058838, 0.9377101063728333, -1.059342384338379, -0.36982548236846924]\n",
      "Grand sum of 732 tensor sets is: [282.9201965332031, 1196.293701171875, -197.8373565673828, -317.4774475097656, -324.8961181640625]\n",
      "\n",
      "Instance 978 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([248, 13, 768])\n",
      "Shape of summed layers is: 248 x 768\n",
      "car at index 33: [0.4421585500240326, 1.581210732460022, -1.6743088960647583, -0.8880506753921509, 1.5066405534744263]\n",
      "Grand sum of 733 tensor sets is: [283.36236572265625, 1197.8748779296875, -199.5116729736328, -318.3655090332031, -323.38946533203125]\n",
      "\n",
      "Instance 979 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 6: [0.5586664080619812, 3.569434404373169, -1.043493390083313, -0.6436914801597595, 1.5123167037963867]\n",
      "Grand sum of 734 tensor sets is: [283.9210205078125, 1201.4443359375, -200.55516052246094, -319.0091857910156, -321.87713623046875]\n",
      "\n",
      "Instance 980 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 15: [-0.08361336588859558, 1.978757619857788, 0.5842941403388977, 2.2319576740264893, -3.304997682571411]\n",
      "Grand sum of 735 tensor sets is: [283.83740234375, 1203.423095703125, -199.9708709716797, -316.7772216796875, -325.18212890625]\n",
      "\n",
      "Instance 981 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 27: [0.3578695058822632, 1.923293948173523, 0.1365751177072525, -0.5669785737991333, -0.4651467204093933]\n",
      "Grand sum of 736 tensor sets is: [284.1952819824219, 1205.346435546875, -199.83428955078125, -317.3442077636719, -325.64727783203125]\n",
      "\n",
      "Instance 982 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12, 14, 16, 53, 97, 111]\n",
      "Size of token embeddings is torch.Size([150, 13, 768])\n",
      "Shape of summed layers is: 150 x 768\n",
      "car at index 12: [0.3908247947692871, 2.150573968887329, 0.44996267557144165, 0.1463094800710678, -0.47966471314430237]\n",
      "car at index 14: [-0.00611693412065506, 1.1778807640075684, -0.3676908314228058, 0.8682859539985657, 0.48844802379608154]\n",
      "car at index 16: [-0.2871348261833191, 1.158850073814392, -0.15370504558086395, 0.043585263192653656, 0.2454674243927002]\n",
      "car at index 53: [0.44574642181396484, 3.5511701107025146, 0.3932766914367676, 0.21129722893238068, 2.1418511867523193]\n",
      "car at index 97: [0.5169853568077087, 2.3096768856048584, 0.5163005590438843, 2.2856714725494385, -2.169637441635132]\n",
      "car at index 111: [0.8814795017242432, 1.2544262409210205, -0.15205633640289307, -1.523364782333374, -0.8422314524650574]\n",
      "Grand sum of 737 tensor sets is: [284.5189208984375, 1207.2801513671875, -199.71994018554688, -317.0055847167969, -325.7499084472656]\n",
      "\n",
      "Instance 983 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 38: [1.2090930938720703, 1.914389967918396, -0.9967748522758484, -2.6895580291748047, 2.6631245613098145]\n",
      "Grand sum of 738 tensor sets is: [285.72802734375, 1209.194580078125, -200.7167205810547, -319.69512939453125, -323.0867919921875]\n",
      "\n",
      "Instance 984 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 19: [0.20564359426498413, 1.2942328453063965, 0.21796758472919464, 2.888362407684326, -3.3745789527893066]\n",
      "Grand sum of 739 tensor sets is: [285.9336853027344, 1210.48876953125, -200.49874877929688, -316.8067626953125, -326.46136474609375]\n",
      "\n",
      "Instance 985 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 986 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 10: [-0.6619921922683716, 2.7664589881896973, -0.752364456653595, -0.9115634560585022, -1.4819563627243042]\n",
      "Grand sum of 740 tensor sets is: [285.2716979980469, 1213.2552490234375, -201.25111389160156, -317.71832275390625, -327.9433288574219]\n",
      "\n",
      "Instance 987 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 12: [0.37241804599761963, 1.2237815856933594, -0.3548685908317566, 1.4668785333633423, -0.04381382465362549]\n",
      "Grand sum of 741 tensor sets is: [285.64410400390625, 1214.47900390625, -201.60598754882812, -316.2514343261719, -327.9871520996094]\n",
      "\n",
      "Instance 988 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 31: [1.3244630098342896, -0.19325104355812073, -0.10115544497966766, 0.24117028713226318, -0.04624977707862854]\n",
      "Grand sum of 742 tensor sets is: [286.96856689453125, 1214.2857666015625, -201.70713806152344, -316.01025390625, -328.0334167480469]\n",
      "\n",
      "Instance 989 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 990 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 991 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 10: [0.9126923084259033, 1.9413419961929321, -0.06563809514045715, -2.4885220527648926, -0.805161714553833]\n",
      "Grand sum of 743 tensor sets is: [287.8812561035156, 1216.22705078125, -201.7727813720703, -318.498779296875, -328.8385925292969]\n",
      "\n",
      "Instance 992 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 993 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 994 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 995 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [42, 44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([175, 13, 768])\n",
      "Shape of summed layers is: 175 x 768\n",
      "car at index 42: [0.05606823414564133, -0.4788869321346283, -0.7205262780189514, 1.090325117111206, 0.5285754203796387]\n",
      "car at index 44: [-0.07784322649240494, 0.025381848216056824, 0.10027061402797699, -0.5371129512786865, -2.2005465030670166]\n",
      "Grand sum of 744 tensor sets is: [287.870361328125, 1216.000244140625, -202.08291625976562, -318.22216796875, -329.6745910644531]\n",
      "\n",
      "Instance 996 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 38: [0.5135715007781982, 1.0346862077713013, 0.8111996054649353, 0.25117868185043335, -1.350477933883667]\n",
      "Grand sum of 745 tensor sets is: [288.3839416503906, 1217.034912109375, -201.27171325683594, -317.9709777832031, -331.0250549316406]\n",
      "\n",
      "Instance 997 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 998 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 19: [0.25492268800735474, 1.116609811782837, 0.4798164367675781, 3.559464693069458, -1.8482143878936768]\n",
      "Grand sum of 746 tensor sets is: [288.63885498046875, 1218.1514892578125, -200.79190063476562, -314.4114990234375, -332.8732604980469]\n",
      "\n",
      "Instance 999 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 16: [-0.17018350958824158, 2.5256857872009277, -0.722618818283081, -0.8696762323379517, -1.485948920249939]\n",
      "Grand sum of 747 tensor sets is: [288.4686584472656, 1220.6771240234375, -201.5145263671875, -315.28118896484375, -334.3592224121094]\n",
      "\n",
      "Instance 1000 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 2: [-0.3947651982307434, 0.6377201080322266, 0.4841955602169037, -3.474386692047119, 0.13462159037590027]\n",
      "Grand sum of 748 tensor sets is: [288.0738830566406, 1221.3148193359375, -201.03033447265625, -318.7555847167969, -334.224609375]\n",
      "\n",
      "Instance 1001 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [60]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "car at index 60: [0.009641056880354881, -0.4755682349205017, -2.015753746032715, 0.3214326798915863, 2.2987494468688965]\n",
      "Grand sum of 749 tensor sets is: [288.0835266113281, 1220.8392333984375, -203.04608154296875, -318.43414306640625, -331.9258728027344]\n",
      "\n",
      "Instance 1002 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16, 57]\n",
      "Size of token embeddings is torch.Size([86, 13, 768])\n",
      "Shape of summed layers is: 86 x 768\n",
      "car at index 16: [0.4775409400463104, -0.7143059968948364, 1.022050142288208, 0.6750816702842712, 0.34015947580337524]\n",
      "car at index 57: [0.4596613049507141, 0.7939256429672241, -0.11393768340349197, -1.4502909183502197, -0.5734044313430786]\n",
      "Grand sum of 750 tensor sets is: [288.5521240234375, 1220.8790283203125, -202.59202575683594, -318.8217468261719, -332.04248046875]\n",
      "\n",
      "Instance 1003 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 2: [1.4101109504699707, 0.7373690009117126, -0.2565644383430481, -0.26987767219543457, 1.3633636236190796]\n",
      "Grand sum of 751 tensor sets is: [289.9622497558594, 1221.616455078125, -202.8485870361328, -319.09161376953125, -330.6791076660156]\n",
      "\n",
      "Instance 1004 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1005 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 47: [-0.11333195865154266, 0.688657283782959, 0.21477162837982178, -2.9910364151000977, -1.2309348583221436]\n",
      "Grand sum of 752 tensor sets is: [289.8489074707031, 1222.3050537109375, -202.63381958007812, -322.0826416015625, -331.9100341796875]\n",
      "\n",
      "Instance 1006 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "car at index 3: [0.4616754949092865, -0.4986130893230438, -1.6596060991287231, -0.9657891392707825, 0.27015382051467896]\n",
      "Grand sum of 753 tensor sets is: [290.3105773925781, 1221.806396484375, -204.29342651367188, -323.0484313964844, -331.639892578125]\n",
      "\n",
      "Instance 1007 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 10: [-0.15116477012634277, 1.818198561668396, -0.3682977855205536, -1.1964439153671265, -2.5633699893951416]\n",
      "Grand sum of 754 tensor sets is: [290.159423828125, 1223.6246337890625, -204.66172790527344, -324.244873046875, -334.2032775878906]\n",
      "\n",
      "Instance 1008 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1009 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13, 19]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 13: [0.1868550330400467, 0.9922882318496704, -0.3055322468280792, -1.2819632291793823, -0.5632621645927429]\n",
      "car at index 19: [0.13859857618808746, 1.1623746156692505, -0.11977919936180115, -2.136349678039551, -1.9469432830810547]\n",
      "Grand sum of 755 tensor sets is: [290.3221435546875, 1224.701904296875, -204.8743896484375, -325.95404052734375, -335.4583740234375]\n",
      "\n",
      "Instance 1010 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1011 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 15: [-0.40002769231796265, 1.5390534400939941, 0.7401081919670105, 0.3740425109863281, -3.8788256645202637]\n",
      "Grand sum of 756 tensor sets is: [289.922119140625, 1226.240966796875, -204.13427734375, -325.5799865722656, -339.3371887207031]\n",
      "\n",
      "Instance 1012 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 24: [0.18266716599464417, 3.576262950897217, 0.04161466285586357, -0.6801292896270752, -1.5284106731414795]\n",
      "Grand sum of 757 tensor sets is: [290.10479736328125, 1229.8172607421875, -204.09266662597656, -326.2601013183594, -340.8656005859375]\n",
      "\n",
      "Instance 1013 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 2: [-0.3991358280181885, 1.9837183952331543, 0.3573395609855652, -1.2237355709075928, -1.5121296644210815]\n",
      "Grand sum of 758 tensor sets is: [289.7056579589844, 1231.801025390625, -203.73532104492188, -327.48382568359375, -342.3777160644531]\n",
      "\n",
      "Instance 1014 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 10: [0.47690823674201965, 0.7279856204986572, 0.5307187438011169, -0.3161773979663849, -1.4027396440505981]\n",
      "Grand sum of 759 tensor sets is: [290.18255615234375, 1232.529052734375, -203.20460510253906, -327.8000183105469, -343.78045654296875]\n",
      "\n",
      "Instance 1015 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 5: [0.4059167206287384, 1.25619637966156, -0.9309722781181335, -1.2481660842895508, 1.1299288272857666]\n",
      "Grand sum of 760 tensor sets is: [290.5884704589844, 1233.7852783203125, -204.1355743408203, -329.0481872558594, -342.6505126953125]\n",
      "\n",
      "Instance 1016 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 14: [0.31761011481285095, -0.015914373099803925, 0.47949549555778503, -1.8685110807418823, 0.4683341681957245]\n",
      "Grand sum of 761 tensor sets is: [290.90606689453125, 1233.7694091796875, -203.6560821533203, -330.91668701171875, -342.18218994140625]\n",
      "\n",
      "Instance 1017 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "car at index 14: [0.45127546787261963, 2.4388277530670166, -0.14279162883758545, -1.9821218252182007, -1.728885531425476]\n",
      "Grand sum of 762 tensor sets is: [291.3573303222656, 1236.208251953125, -203.7988739013672, -332.8988037109375, -343.91107177734375]\n",
      "\n",
      "Instance 1018 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 11: [0.4528108835220337, 1.5775015354156494, -1.2547053098678589, -0.6710890531539917, 0.7143005728721619]\n",
      "Grand sum of 763 tensor sets is: [291.8101501464844, 1237.7857666015625, -205.05357360839844, -333.56988525390625, -343.19677734375]\n",
      "\n",
      "Instance 1019 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1020 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 12: [1.039631962776184, 2.578134059906006, 0.024494215846061707, -0.883003830909729, -1.106382966041565]\n",
      "Grand sum of 764 tensor sets is: [292.84979248046875, 1240.3638916015625, -205.02908325195312, -334.452880859375, -344.30316162109375]\n",
      "\n",
      "Instance 1021 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 6: [0.6243773698806763, 1.3885111808776855, -0.48635926842689514, 0.04490445554256439, 0.3521992564201355]\n",
      "Grand sum of 765 tensor sets is: [293.47418212890625, 1241.75244140625, -205.51544189453125, -334.4079895019531, -343.9509582519531]\n",
      "\n",
      "Instance 1022 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "car at index 42: [1.4123419523239136, -1.4974327087402344, -0.5254272222518921, 1.5234460830688477, 2.6648051738739014]\n",
      "Grand sum of 766 tensor sets is: [294.88653564453125, 1240.2550048828125, -206.04086303710938, -332.8845520019531, -341.2861633300781]\n",
      "\n",
      "Instance 1023 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1024 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 15: [0.4338638484477997, -0.24445468187332153, -1.302423357963562, 1.2879890203475952, 2.7001092433929443]\n",
      "Grand sum of 767 tensor sets is: [295.3204040527344, 1240.010498046875, -207.34329223632812, -331.5965576171875, -338.5860595703125]\n",
      "\n",
      "Instance 1025 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1026 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1027 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2, 20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 2: [-0.874573826789856, 1.5511702299118042, 0.2904704809188843, -1.991980791091919, -0.7499173879623413]\n",
      "car at index 20: [0.6348726749420166, 1.2310582399368286, -1.2489985227584839, 0.221152201294899, 1.0638525485992432]\n",
      "Grand sum of 768 tensor sets is: [295.2005615234375, 1241.401611328125, -207.8225555419922, -332.4819641113281, -338.4290771484375]\n",
      "\n",
      "Instance 1028 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1029 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 8: [-0.00045646727085113525, 1.6595525741577148, -0.6009309887886047, -2.8101937770843506, -1.4558138847351074]\n",
      "Grand sum of 769 tensor sets is: [295.2001037597656, 1243.0611572265625, -208.42349243164062, -335.2921447753906, -339.8848876953125]\n",
      "\n",
      "Instance 1030 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1031 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 7: [0.12947978079319, 2.8370025157928467, -0.5494190454483032, -2.2117459774017334, -2.448026180267334]\n",
      "Grand sum of 770 tensor sets is: [295.32958984375, 1245.898193359375, -208.97291564941406, -337.5038757324219, -342.3329162597656]\n",
      "\n",
      "Instance 1032 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 5: [0.3353143334388733, 1.0688008069992065, -0.5516824722290039, 2.224562644958496, -1.2972028255462646]\n",
      "Grand sum of 771 tensor sets is: [295.6649169921875, 1246.967041015625, -209.52459716796875, -335.2793273925781, -343.630126953125]\n",
      "\n",
      "Instance 1033 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 19: [0.5486211180686951, 2.6117606163024902, -0.6465668678283691, -0.3547297418117523, -1.5821599960327148]\n",
      "Grand sum of 772 tensor sets is: [296.2135314941406, 1249.578857421875, -210.17115783691406, -335.6340637207031, -345.2122802734375]\n",
      "\n",
      "Instance 1034 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [146, 234, 356, 362]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 146: [-0.3094575107097626, 2.3568806648254395, -0.20935194194316864, 1.0183615684509277, -0.047270700335502625]\n",
      "car at index 234: [0.18685361742973328, 2.814922571182251, -0.1488889455795288, -1.5846632719039917, 1.2123830318450928]\n",
      "car at index 356: [0.47992658615112305, 1.6479618549346924, -0.5777234435081482, -0.39899981021881104, 2.0432379245758057]\n",
      "car at index 362: [1.1849088668823242, 2.2716925144195557, 0.1493094116449356, -0.8131417036056519, -1.2664774656295776]\n",
      "Grand sum of 773 tensor sets is: [296.5990905761719, 1251.8516845703125, -210.36782836914062, -336.07867431640625, -344.726806640625]\n",
      "\n",
      "Instance 1035 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 10: [0.42411062121391296, 5.16584587097168, -0.23973986506462097, 1.045638084411621, 2.7495522499084473]\n",
      "Grand sum of 774 tensor sets is: [297.023193359375, 1257.017578125, -210.60757446289062, -335.0330505371094, -341.9772644042969]\n",
      "\n",
      "Instance 1036 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "car at index 21: [1.2374922037124634, -1.0200961828231812, -0.40685731172561646, 2.5245275497436523, 3.8967432975769043]\n",
      "Grand sum of 775 tensor sets is: [298.26068115234375, 1255.9974365234375, -211.01443481445312, -332.5085144042969, -338.0805358886719]\n",
      "\n",
      "Instance 1037 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 6: [0.676869809627533, 1.0847316980361938, 1.2361578941345215, 3.264538049697876, -3.236382484436035]\n",
      "Grand sum of 776 tensor sets is: [298.93756103515625, 1257.0821533203125, -209.7782745361328, -329.2439880371094, -341.3169250488281]\n",
      "\n",
      "Instance 1038 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [59]\n",
      "Size of token embeddings is torch.Size([129, 13, 768])\n",
      "Shape of summed layers is: 129 x 768\n",
      "car at index 59: [-0.06225466728210449, 1.3751649856567383, -0.3683876693248749, 0.5824962258338928, -0.7882405519485474]\n",
      "Grand sum of 777 tensor sets is: [298.87530517578125, 1258.457275390625, -210.14666748046875, -328.6614990234375, -342.10516357421875]\n",
      "\n",
      "Instance 1039 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 5: [-0.32132503390312195, 1.544040560722351, -0.9027293920516968, -1.8006601333618164, -1.7626566886901855]\n",
      "Grand sum of 778 tensor sets is: [298.5539855957031, 1260.0013427734375, -211.0493927001953, -330.462158203125, -343.8678283691406]\n",
      "\n",
      "Instance 1040 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1041 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [76]\n",
      "Size of token embeddings is torch.Size([121, 13, 768])\n",
      "Shape of summed layers is: 121 x 768\n",
      "car at index 76: [-0.360916405916214, 1.1502865552902222, -0.34115728735923767, -0.23433244228363037, -0.5337107181549072]\n",
      "Grand sum of 779 tensor sets is: [298.19305419921875, 1261.151611328125, -211.3905487060547, -330.6965026855469, -344.40155029296875]\n",
      "\n",
      "Instance 1042 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 22: [0.32106468081474304, -0.5504107475280762, -0.50154709815979, 1.2577781677246094, 3.993231773376465]\n",
      "Grand sum of 780 tensor sets is: [298.5141296386719, 1260.6011962890625, -211.89208984375, -329.438720703125, -340.4083251953125]\n",
      "\n",
      "Instance 1043 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 3: [0.2693939208984375, 1.0893102884292603, -0.3339860439300537, -2.1070261001586914, -0.3813701868057251]\n",
      "Grand sum of 781 tensor sets is: [298.78350830078125, 1261.6905517578125, -212.22607421875, -331.5457458496094, -340.7897033691406]\n",
      "\n",
      "Instance 1044 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [395]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 395: [1.0304489135742188, 2.202329158782959, 0.5035207271575928, 0.9473627805709839, -0.08122904598712921]\n",
      "Grand sum of 782 tensor sets is: [299.81396484375, 1263.892822265625, -211.72254943847656, -330.598388671875, -340.8709411621094]\n",
      "\n",
      "Instance 1045 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 8: [0.0026552006602287292, 4.037240028381348, 0.09445716440677643, -0.8346675634384155, -0.9356411695480347]\n",
      "Grand sum of 783 tensor sets is: [299.8166198730469, 1267.9300537109375, -211.6280975341797, -331.43304443359375, -341.80657958984375]\n",
      "\n",
      "Instance 1046 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1047 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1048 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "car at index 5: [0.7093641757965088, 1.6342977285385132, -0.3366720974445343, -1.059912919998169, -1.2560056447982788]\n",
      "Grand sum of 784 tensor sets is: [300.5259704589844, 1269.5643310546875, -211.9647674560547, -332.4929504394531, -343.0625915527344]\n",
      "\n",
      "Instance 1049 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "car at index 22: [0.3787365257740021, 3.0956180095672607, 0.19755396246910095, -1.8524229526519775, 0.6015352606773376]\n",
      "Grand sum of 785 tensor sets is: [300.9046936035156, 1272.659912109375, -211.7672119140625, -334.3453674316406, -342.4610595703125]\n",
      "\n",
      "Instance 1050 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 4: [0.5290701985359192, 2.738157272338867, -0.4710829555988312, -2.546147584915161, 0.0067072659730911255]\n",
      "Grand sum of 786 tensor sets is: [301.43377685546875, 1275.3980712890625, -212.23829650878906, -336.8915100097656, -342.454345703125]\n",
      "\n",
      "Instance 1051 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 10: [0.2870299816131592, 0.7512590289115906, -0.34177303314208984, 0.8584110736846924, 1.26841139793396]\n",
      "Grand sum of 787 tensor sets is: [301.7207946777344, 1276.1492919921875, -212.58006286621094, -336.0331115722656, -341.1859436035156]\n",
      "\n",
      "Instance 1052 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 12: [-0.22707386314868927, 2.964240789413452, -0.8789675831794739, 0.45182332396507263, -1.630960464477539]\n",
      "Grand sum of 788 tensor sets is: [301.49371337890625, 1279.113525390625, -213.4590301513672, -335.581298828125, -342.81689453125]\n",
      "\n",
      "Instance 1053 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 5: [0.8520771265029907, 1.2298204898834229, -0.34209808707237244, -0.8216565847396851, 0.7910608053207397]\n",
      "Grand sum of 789 tensor sets is: [302.3457946777344, 1280.3433837890625, -213.80113220214844, -336.4029541015625, -342.0258483886719]\n",
      "\n",
      "Instance 1054 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 6: [0.41995275020599365, 1.710693359375, -0.1553262323141098, -2.220996618270874, 0.6020206212997437]\n",
      "Grand sum of 790 tensor sets is: [302.7657470703125, 1282.0540771484375, -213.95645141601562, -338.62396240234375, -341.423828125]\n",
      "\n",
      "Instance 1055 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 6: [0.13674716651439667, 2.824680805206299, -0.9110108613967896, 0.9897992014884949, -2.4126205444335938]\n",
      "Grand sum of 791 tensor sets is: [302.9024963378906, 1284.8787841796875, -214.86746215820312, -337.6341552734375, -343.8364562988281]\n",
      "\n",
      "Instance 1056 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 12: [0.7255674004554749, 1.29860520362854, -1.15579092502594, -0.7861579656600952, 0.8617494702339172]\n",
      "Grand sum of 792 tensor sets is: [303.6280517578125, 1286.1773681640625, -216.02325439453125, -338.4203186035156, -342.9747009277344]\n",
      "\n",
      "Instance 1057 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "car at index 18: [-0.0035255178809165955, 1.283201813697815, -0.7718402147293091, -1.0151468515396118, -0.4178820550441742]\n",
      "Grand sum of 793 tensor sets is: [303.62451171875, 1287.4605712890625, -216.7950897216797, -339.4354553222656, -343.392578125]\n",
      "\n",
      "Instance 1058 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1059 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 7: [0.9196664094924927, 1.4355454444885254, -1.4045922756195068, -0.7234430313110352, 0.30205920338630676]\n",
      "Grand sum of 794 tensor sets is: [304.544189453125, 1288.8961181640625, -218.19967651367188, -340.1589050292969, -343.09051513671875]\n",
      "\n",
      "Instance 1060 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 2: [0.4684339761734009, 2.1647191047668457, -0.5572831034660339, -2.236077070236206, -1.0189673900604248]\n",
      "Grand sum of 795 tensor sets is: [305.01263427734375, 1291.060791015625, -218.7569580078125, -342.3949890136719, -344.1094970703125]\n",
      "\n",
      "Instance 1061 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 10: [-0.13177207112312317, 1.8759640455245972, -1.1504744291305542, -1.516178011894226, -0.8380656838417053]\n",
      "Grand sum of 796 tensor sets is: [304.880859375, 1292.936767578125, -219.9074249267578, -343.9111633300781, -344.94757080078125]\n",
      "\n",
      "Instance 1062 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1063 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 18: [0.354092538356781, 2.0073611736297607, -0.2633002996444702, -2.869870901107788, -0.33562782406806946]\n",
      "Grand sum of 797 tensor sets is: [305.2349548339844, 1294.944091796875, -220.1707305908203, -346.7810363769531, -345.283203125]\n",
      "\n",
      "Instance 1064 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 8: [1.050398588180542, 1.736893892288208, 0.0692649781703949, 0.2897985279560089, 1.1652458906173706]\n",
      "Grand sum of 798 tensor sets is: [306.28533935546875, 1296.6810302734375, -220.10147094726562, -346.4912414550781, -344.1179504394531]\n",
      "\n",
      "Instance 1065 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6, 10]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 6: [0.6326581835746765, 1.8948811292648315, -0.5326336026191711, 0.946200430393219, -1.2851593494415283]\n",
      "car at index 10: [-0.35039597749710083, 3.803614377975464, 0.996628999710083, -0.6422829031944275, -2.798175573348999]\n",
      "Grand sum of 799 tensor sets is: [306.4264831542969, 1299.5302734375, -219.86947631835938, -346.33929443359375, -346.15960693359375]\n",
      "\n",
      "Instance 1066 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "car at index 8: [1.0282623767852783, 1.3076691627502441, -0.43649300932884216, 0.17320218682289124, -0.252623975276947]\n",
      "Grand sum of 800 tensor sets is: [307.4547424316406, 1300.837890625, -220.30596923828125, -346.1661071777344, -346.4122314453125]\n",
      "\n",
      "Instance 1067 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 6: [0.29001396894454956, 1.6740344762802124, 0.7663494944572449, -0.2460286021232605, -1.6270116567611694]\n",
      "Grand sum of 801 tensor sets is: [307.7447509765625, 1302.511962890625, -219.5396270751953, -346.4121398925781, -348.03924560546875]\n",
      "\n",
      "Instance 1068 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 20: [0.47615259885787964, 3.499335527420044, -0.5427246689796448, 1.1908338069915771, 1.3730683326721191]\n",
      "Grand sum of 802 tensor sets is: [308.2209167480469, 1306.0113525390625, -220.0823516845703, -345.2213134765625, -346.6661682128906]\n",
      "\n",
      "Instance 1069 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 14: [0.6924759745597839, 1.2144380807876587, -0.14398831129074097, 2.87442946434021, -0.002928313799202442]\n",
      "Grand sum of 803 tensor sets is: [308.91339111328125, 1307.225830078125, -220.22633361816406, -342.3468933105469, -346.6690979003906]\n",
      "\n",
      "Instance 1070 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 4: [0.07368601858615875, 2.194857120513916, -0.7414173483848572, -1.7068345546722412, -0.9747359752655029]\n",
      "Grand sum of 804 tensor sets is: [308.9870910644531, 1309.420654296875, -220.96775817871094, -344.0537414550781, -347.6438293457031]\n",
      "\n",
      "Instance 1071 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 13: [1.0702153444290161, 1.6451973915100098, -0.8035396337509155, -1.4347954988479614, -0.9162076711654663]\n",
      "Grand sum of 805 tensor sets is: [310.05731201171875, 1311.0657958984375, -221.77130126953125, -345.488525390625, -348.5600280761719]\n",
      "\n",
      "Instance 1072 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1073 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1074 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 10: [0.4784921407699585, 3.148029327392578, 0.21161645650863647, -1.0240545272827148, -0.5044302344322205]\n",
      "Grand sum of 806 tensor sets is: [310.5357971191406, 1314.2138671875, -221.5596923828125, -346.5125732421875, -349.064453125]\n",
      "\n",
      "Instance 1075 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 11: [2.280860662460327, -1.6173481941223145, 0.2394314408302307, 0.8318631052970886, 0.11044175922870636]\n",
      "Grand sum of 807 tensor sets is: [312.816650390625, 1312.5965576171875, -221.3202667236328, -345.68072509765625, -348.9540100097656]\n",
      "\n",
      "Instance 1076 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 4: [-0.288270503282547, 2.8629279136657715, -0.6643297672271729, -0.6465123891830444, -1.6927961111068726]\n",
      "Grand sum of 808 tensor sets is: [312.52838134765625, 1315.45947265625, -221.98460388183594, -346.3272399902344, -350.6468200683594]\n",
      "\n",
      "Instance 1077 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 33: [-0.7164350152015686, 2.624427080154419, -0.31373122334480286, -2.1532793045043945, -2.11177134513855]\n",
      "Grand sum of 809 tensor sets is: [311.81195068359375, 1318.0838623046875, -222.29833984375, -348.48052978515625, -352.75860595703125]\n",
      "\n",
      "Instance 1078 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 13: [-0.24682392179965973, 0.8204412460327148, -0.9249388575553894, 0.29314327239990234, -0.9145730137825012]\n",
      "Grand sum of 810 tensor sets is: [311.56512451171875, 1318.904296875, -223.22328186035156, -348.1873779296875, -353.6731872558594]\n",
      "\n",
      "Instance 1079 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 17: [-0.22349216043949127, 1.5052335262298584, 0.0323757603764534, 2.662442207336426, -1.3148337602615356]\n",
      "Grand sum of 811 tensor sets is: [311.3416442871094, 1320.4095458984375, -223.19090270996094, -345.5249328613281, -354.9880065917969]\n",
      "\n",
      "Instance 1080 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [105]\n",
      "Size of token embeddings is torch.Size([109, 13, 768])\n",
      "Shape of summed layers is: 109 x 768\n",
      "car at index 105: [0.5971429347991943, 2.168212890625, -0.3150348663330078, 3.6164228916168213, -0.36498647928237915]\n",
      "Grand sum of 812 tensor sets is: [311.93878173828125, 1322.5777587890625, -223.5059356689453, -341.90850830078125, -355.3529968261719]\n",
      "\n",
      "Instance 1081 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 9: [0.30827081203460693, 1.9121109247207642, -1.0149860382080078, -1.2616424560546875, -0.1996595412492752]\n",
      "Grand sum of 813 tensor sets is: [312.2470397949219, 1324.4898681640625, -224.5209197998047, -343.170166015625, -355.5526428222656]\n",
      "\n",
      "Instance 1082 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1083 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "car at index 18: [0.061850763857364655, 1.6776723861694336, -0.6461219787597656, -1.6579195261001587, -2.7542285919189453]\n",
      "Grand sum of 814 tensor sets is: [312.30889892578125, 1326.16748046875, -225.1670379638672, -344.8280944824219, -358.306884765625]\n",
      "\n",
      "Instance 1084 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 11: [-0.20137351751327515, 2.520935535430908, 0.012635434977710247, -1.1561384201049805, -1.5658427476882935]\n",
      "Grand sum of 815 tensor sets is: [312.1075134277344, 1328.6884765625, -225.15440368652344, -345.9842224121094, -359.87274169921875]\n",
      "\n",
      "Instance 1085 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 29: [0.4581116735935211, 1.7769594192504883, -0.19264563918113708, -0.8553175926208496, -0.8253231644630432]\n",
      "Grand sum of 816 tensor sets is: [312.56561279296875, 1330.4654541015625, -225.3470458984375, -346.83953857421875, -360.69805908203125]\n",
      "\n",
      "Instance 1086 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1087 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1088 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 16: [0.7674113512039185, -0.5808163285255432, -0.49804648756980896, 1.2350449562072754, 1.9176307916641235]\n",
      "Grand sum of 817 tensor sets is: [313.3330383300781, 1329.8846435546875, -225.8450927734375, -345.6044921875, -358.7804260253906]\n",
      "\n",
      "Instance 1089 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5, 16]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 5: [-0.5712722539901733, 1.4556255340576172, -1.1733945608139038, -2.5104548931121826, -1.305160403251648]\n",
      "car at index 16: [0.7612929344177246, 2.1085216999053955, -0.10300995409488678, -2.1763017177581787, -0.6820272207260132]\n",
      "Grand sum of 818 tensor sets is: [313.42803955078125, 1331.666748046875, -226.48329162597656, -347.9478759765625, -359.7740173339844]\n",
      "\n",
      "Instance 1090 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 14: [1.2832410335540771, 2.0118348598480225, -0.180343359708786, -1.3001447916030884, -1.5794569253921509]\n",
      "Grand sum of 819 tensor sets is: [314.7112731933594, 1333.6785888671875, -226.66363525390625, -349.2480163574219, -361.3534851074219]\n",
      "\n",
      "Instance 1091 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 22: [0.13453349471092224, 1.5517665147781372, -0.3043487071990967, -2.333850383758545, 1.1962273120880127]\n",
      "Grand sum of 820 tensor sets is: [314.8457946777344, 1335.2303466796875, -226.96798706054688, -351.5818786621094, -360.1572570800781]\n",
      "\n",
      "Instance 1092 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 7: [0.47955113649368286, 2.0466384887695312, -1.2687828540802002, 0.6247337460517883, -0.7077755331993103]\n",
      "Grand sum of 821 tensor sets is: [315.3253479003906, 1337.2769775390625, -228.2367706298828, -350.9571533203125, -360.8650207519531]\n",
      "\n",
      "Instance 1093 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 3: [-0.16199396550655365, 2.217926263809204, -0.946349024772644, -1.9943650960922241, -0.6154433488845825]\n",
      "Grand sum of 822 tensor sets is: [315.1633605957031, 1339.494873046875, -229.18312072753906, -352.9515075683594, -361.48046875]\n",
      "\n",
      "Instance 1094 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 4: [-0.09752657264471054, 0.5821897983551025, 0.0027724504470825195, 0.05264207720756531, 0.05677124857902527]\n",
      "Grand sum of 823 tensor sets is: [315.0658264160156, 1340.0770263671875, -229.1803436279297, -352.89886474609375, -361.4237060546875]\n",
      "\n",
      "Instance 1095 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "car at index 19: [0.31700387597084045, 2.753800868988037, 1.4164977073669434, -1.0118368864059448, -2.2430648803710938]\n",
      "Grand sum of 824 tensor sets is: [315.3828430175781, 1342.830810546875, -227.7638397216797, -353.91070556640625, -363.6667785644531]\n",
      "\n",
      "Instance 1096 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1097 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1098 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "car at index 40: [1.562659502029419, -0.9566534757614136, -0.915805459022522, 0.24641484022140503, 1.933640956878662]\n",
      "Grand sum of 825 tensor sets is: [316.94549560546875, 1341.8741455078125, -228.6796417236328, -353.6642761230469, -361.7331237792969]\n",
      "\n",
      "Instance 1099 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1100 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 33: [-0.10562869906425476, 4.829427242279053, 0.15813061594963074, 0.2167697548866272, -0.19541996717453003]\n",
      "Grand sum of 826 tensor sets is: [316.8398742675781, 1346.70361328125, -228.52151489257812, -353.447509765625, -361.9285583496094]\n",
      "\n",
      "Instance 1101 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1102 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 22: [1.503036379814148, 2.166557788848877, -1.1047313213348389, -0.3426389694213867, -0.321804940700531]\n",
      "Grand sum of 827 tensor sets is: [318.3428955078125, 1348.8701171875, -229.62625122070312, -353.7901611328125, -362.2503662109375]\n",
      "\n",
      "Instance 1103 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1104 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1105 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "car at index 49: [-0.1908620297908783, 0.3380451202392578, 0.6066602468490601, 1.4390006065368652, 4.194987773895264]\n",
      "Grand sum of 828 tensor sets is: [318.15203857421875, 1349.2081298828125, -229.01959228515625, -352.3511657714844, -358.0553894042969]\n",
      "\n",
      "Instance 1106 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 4: [1.2003737688064575, 1.061032772064209, -0.7879527807235718, -0.36840951442718506, 1.5192933082580566]\n",
      "Grand sum of 829 tensor sets is: [319.3524169921875, 1350.2691650390625, -229.8075408935547, -352.7195739746094, -356.5361022949219]\n",
      "\n",
      "Instance 1107 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 13: [0.08752600103616714, 2.20503306388855, -1.1515151262283325, 0.221675843000412, -1.4827532768249512]\n",
      "Grand sum of 830 tensor sets is: [319.43994140625, 1352.4742431640625, -230.9590606689453, -352.4978942871094, -358.01885986328125]\n",
      "\n",
      "Instance 1108 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "car at index 26: [0.6380051374435425, -0.7421259880065918, -0.764467716217041, 0.07713047415018082, 1.051002860069275]\n",
      "Grand sum of 831 tensor sets is: [320.07794189453125, 1351.732177734375, -231.72352600097656, -352.4207763671875, -356.9678649902344]\n",
      "\n",
      "Instance 1109 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 11: [0.7758456468582153, 2.5351521968841553, -0.47935521602630615, -1.9835615158081055, -2.3800508975982666]\n",
      "Grand sum of 832 tensor sets is: [320.8537902832031, 1354.267333984375, -232.202880859375, -354.4043273925781, -359.3479309082031]\n",
      "\n",
      "Instance 1110 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 4: [-1.2434757947921753, 2.830349922180176, -0.512026846408844, 0.857546329498291, -1.9582964181900024]\n",
      "Grand sum of 833 tensor sets is: [319.6103210449219, 1357.09765625, -232.71490478515625, -353.5467834472656, -361.30621337890625]\n",
      "\n",
      "Instance 1111 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 32: [1.0560765266418457, 0.523899495601654, -1.360561728477478, -1.2652359008789062, 0.6555765271186829]\n",
      "Grand sum of 834 tensor sets is: [320.6664123535156, 1357.62158203125, -234.07546997070312, -354.81201171875, -360.650634765625]\n",
      "\n",
      "Instance 1112 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 23: [0.3017866611480713, 1.624938726425171, 0.8437195420265198, -0.3584985136985779, -1.3127899169921875]\n",
      "Grand sum of 835 tensor sets is: [320.96820068359375, 1359.2464599609375, -233.23175048828125, -355.1705017089844, -361.96343994140625]\n",
      "\n",
      "Instance 1113 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 11: [0.005695372819900513, 2.0192906856536865, -1.0554203987121582, 0.664863646030426, 0.8172110319137573]\n",
      "Grand sum of 836 tensor sets is: [320.9739074707031, 1361.2657470703125, -234.28717041015625, -354.5056457519531, -361.146240234375]\n",
      "\n",
      "Instance 1114 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 13: [-0.334954172372818, 1.4778200387954712, 1.5369250774383545, 1.3469197750091553, -3.6789934635162354]\n",
      "Grand sum of 837 tensor sets is: [320.6389465332031, 1362.7435302734375, -232.750244140625, -353.1587219238281, -364.8252258300781]\n",
      "\n",
      "Instance 1115 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 11: [0.7824639678001404, 2.6634562015533447, -0.31238457560539246, -2.433605194091797, 0.3695884346961975]\n",
      "Grand sum of 838 tensor sets is: [321.4214172363281, 1365.406982421875, -233.0626220703125, -355.5923156738281, -364.45562744140625]\n",
      "\n",
      "Instance 1116 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 9: [0.6994017362594604, 1.537994384765625, -0.4027705490589142, -0.7178373336791992, -0.7884904146194458]\n",
      "Grand sum of 839 tensor sets is: [322.1208190917969, 1366.9449462890625, -233.46539306640625, -356.3101501464844, -365.2441101074219]\n",
      "\n",
      "Instance 1117 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 9: [0.32204753160476685, 0.1708817332983017, -0.6895492076873779, 1.3126238584518433, 4.640415668487549]\n",
      "Grand sum of 840 tensor sets is: [322.44287109375, 1367.1158447265625, -234.15493774414062, -354.9975280761719, -360.60369873046875]\n",
      "\n",
      "Instance 1118 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 10: [0.1876697838306427, 1.3973976373672485, 0.31936588883399963, 0.3998466432094574, -0.7585580945014954]\n",
      "Grand sum of 841 tensor sets is: [322.63055419921875, 1368.51318359375, -233.8355712890625, -354.5976867675781, -361.36224365234375]\n",
      "\n",
      "Instance 1119 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 3: [0.6233973503112793, 3.159303665161133, -0.7139065861701965, -2.253842353820801, -0.616908609867096]\n",
      "Grand sum of 842 tensor sets is: [323.2539367675781, 1371.6724853515625, -234.5494842529297, -356.8515319824219, -361.9791564941406]\n",
      "\n",
      "Instance 1120 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 7: [-0.17938987910747528, -0.49376246333122253, 0.6519418954849243, 0.1449979543685913, 0.49142831563949585]\n",
      "Grand sum of 843 tensor sets is: [323.0745544433594, 1371.1787109375, -233.8975372314453, -356.70654296875, -361.48773193359375]\n",
      "\n",
      "Instance 1121 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 35: [1.0259367227554321, 1.4390325546264648, -1.7086251974105835, 0.09049280732870102, 0.7478312253952026]\n",
      "Grand sum of 844 tensor sets is: [324.1004943847656, 1372.6177978515625, -235.6061553955078, -356.6160583496094, -360.7398986816406]\n",
      "\n",
      "Instance 1122 of car.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1123 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 17: [-0.404757022857666, 1.9926183223724365, -0.7610992789268494, -1.5280863046646118, -2.539263963699341]\n",
      "Grand sum of 845 tensor sets is: [323.69573974609375, 1374.6104736328125, -236.36724853515625, -358.1441345214844, -363.2791748046875]\n",
      "\n",
      "Instance 1124 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "car at index 30: [0.12682494521141052, 2.4403181076049805, 0.8881158828735352, -0.0076575614511966705, -2.0222833156585693]\n",
      "Grand sum of 846 tensor sets is: [323.82257080078125, 1377.05078125, -235.4791259765625, -358.15179443359375, -365.30145263671875]\n",
      "\n",
      "Instance 1125 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1126 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 2: [-0.1908298283815384, 2.393700361251831, -0.2950473129749298, -2.1106786727905273, -1.182741403579712]\n",
      "Grand sum of 847 tensor sets is: [323.6317443847656, 1379.4444580078125, -235.774169921875, -360.2624816894531, -366.48419189453125]\n",
      "\n",
      "Instance 1127 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 25: [0.3419247269630432, 0.5135349631309509, -0.6638181209564209, -0.649718701839447, -0.6312430500984192]\n",
      "Grand sum of 848 tensor sets is: [323.9736633300781, 1379.9580078125, -236.43798828125, -360.9122009277344, -367.1154479980469]\n",
      "\n",
      "Instance 1128 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 26: [0.7455386519432068, 0.6227813959121704, -0.7239871621131897, -0.9892231822013855, -0.8571574091911316]\n",
      "Grand sum of 849 tensor sets is: [324.7192077636719, 1380.580810546875, -237.16197204589844, -361.90142822265625, -367.97259521484375]\n",
      "\n",
      "Instance 1129 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 7: [0.7539315819740295, 1.383730411529541, -0.6750296950340271, -0.6731977462768555, -1.478553056716919]\n",
      "Grand sum of 850 tensor sets is: [325.47314453125, 1381.964599609375, -237.83700561523438, -362.5746154785156, -369.4511413574219]\n",
      "\n",
      "Instance 1130 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "car at index 3: [1.2957600355148315, 2.1305506229400635, -0.778485894203186, -0.5674542188644409, 0.27756187319755554]\n",
      "Grand sum of 851 tensor sets is: [326.7688903808594, 1384.0950927734375, -238.61549377441406, -363.1420593261719, -369.173583984375]\n",
      "\n",
      "Instance 1131 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 8: [0.2948170602321625, 1.6777615547180176, -1.2169057130813599, -1.794509768486023, 1.3417437076568604]\n",
      "Grand sum of 852 tensor sets is: [327.063720703125, 1385.7728271484375, -239.8323974609375, -364.9365539550781, -367.83184814453125]\n",
      "\n",
      "Instance 1132 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "car at index 8: [1.5423716306686401, -0.6441693902015686, -0.10319212824106216, 2.422441244125366, 3.095407724380493]\n",
      "Grand sum of 853 tensor sets is: [328.6060791015625, 1385.128662109375, -239.9355926513672, -362.51409912109375, -364.7364501953125]\n",
      "\n",
      "Instance 1133 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 40: [0.9394556879997253, -1.7121679782867432, 0.12234307825565338, 0.5932905077934265, -0.03016640990972519]\n",
      "Grand sum of 854 tensor sets is: [329.5455322265625, 1383.41650390625, -239.81324768066406, -361.9208068847656, -364.7666015625]\n",
      "\n",
      "Instance 1134 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1135 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1136 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 7: [-0.22899216413497925, 1.9731574058532715, -0.5741474628448486, -1.5184476375579834, -1.9773753881454468]\n",
      "Grand sum of 855 tensor sets is: [329.3165283203125, 1385.3896484375, -240.38739013671875, -363.4392395019531, -366.7439880371094]\n",
      "\n",
      "Instance 1137 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 6: [0.145914226770401, 1.6029131412506104, 0.6102800369262695, 2.4221715927124023, -1.032828688621521]\n",
      "Grand sum of 856 tensor sets is: [329.4624328613281, 1386.9925537109375, -239.77711486816406, -361.0170593261719, -367.7768249511719]\n",
      "\n",
      "Instance 1138 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 11: [1.095116138458252, 2.454763889312744, -0.5504835844039917, -1.2459121942520142, 0.3734399676322937]\n",
      "Grand sum of 857 tensor sets is: [330.55755615234375, 1389.447265625, -240.3275909423828, -362.2629699707031, -367.40338134765625]\n",
      "\n",
      "Instance 1139 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 8: [0.9032151699066162, -0.8194890022277832, -0.5981040596961975, 1.6833817958831787, 3.3452022075653076]\n",
      "Grand sum of 858 tensor sets is: [331.4607849121094, 1388.6278076171875, -240.92568969726562, -360.57958984375, -364.05816650390625]\n",
      "\n",
      "Instance 1140 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 15: [0.5275475978851318, 5.766360282897949, 0.5020514726638794, -1.2405186891555786, 0.3631860613822937]\n",
      "Grand sum of 859 tensor sets is: [331.98834228515625, 1394.3941650390625, -240.42364501953125, -361.8200988769531, -363.6949768066406]\n",
      "\n",
      "Instance 1141 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 26: [0.30693474411964417, 3.394623279571533, -0.8511172533035278, -1.9972761869430542, -0.6706262826919556]\n",
      "Grand sum of 860 tensor sets is: [332.2952880859375, 1397.788818359375, -241.27476501464844, -363.8173828125, -364.3656005859375]\n",
      "\n",
      "Instance 1142 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 17: [-0.08637544512748718, 1.6136690378189087, -0.023574218153953552, 2.377389907836914, -4.319117546081543]\n",
      "Grand sum of 861 tensor sets is: [332.20892333984375, 1399.4024658203125, -241.29833984375, -361.44000244140625, -368.6847229003906]\n",
      "\n",
      "Instance 1143 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 20: [0.726669192314148, 1.3877207040786743, 0.6351287961006165, -0.6888397932052612, -1.2495821714401245]\n",
      "Grand sum of 862 tensor sets is: [332.9355773925781, 1400.7901611328125, -240.6632080078125, -362.12884521484375, -369.9342956542969]\n",
      "\n",
      "Instance 1144 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 6: [0.513938307762146, 2.934400796890259, -0.6920554041862488, 0.08658018708229065, -0.1808900535106659]\n",
      "Grand sum of 863 tensor sets is: [333.44952392578125, 1403.724609375, -241.3552703857422, -362.0422668457031, -370.11517333984375]\n",
      "\n",
      "Instance 1145 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [128]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 128: [1.165639877319336, 2.0819027423858643, -0.8894450068473816, -0.9018943905830383, 0.006636392325162888]\n",
      "Grand sum of 864 tensor sets is: [334.61517333984375, 1405.8065185546875, -242.24472045898438, -362.94415283203125, -370.1085510253906]\n",
      "\n",
      "Instance 1146 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 11: [-0.11323854327201843, 0.10690171271562576, 1.3877978324890137, 0.9575490355491638, 0.41819149255752563]\n",
      "Grand sum of 865 tensor sets is: [334.5019226074219, 1405.9134521484375, -240.85691833496094, -361.9866027832031, -369.69036865234375]\n",
      "\n",
      "Instance 1147 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1148 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 8: [0.704657793045044, 1.4150419235229492, -0.2103143036365509, -0.7213841080665588, 0.3642233610153198]\n",
      "Grand sum of 866 tensor sets is: [335.2065734863281, 1407.3284912109375, -241.06723022460938, -362.7079772949219, -369.3261413574219]\n",
      "\n",
      "Instance 1149 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1150 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 14: [0.6310820579528809, 1.435206651687622, -0.55325847864151, -0.04243689030408859, -0.47183147072792053]\n",
      "Grand sum of 867 tensor sets is: [335.837646484375, 1408.763671875, -241.6204833984375, -362.75042724609375, -369.7979736328125]\n",
      "\n",
      "Instance 1151 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 44: [0.15537072718143463, 3.111154079437256, -1.0361799001693726, -0.11480734497308731, 0.11322516202926636]\n",
      "Grand sum of 868 tensor sets is: [335.9930114746094, 1411.8748779296875, -242.6566619873047, -362.865234375, -369.68475341796875]\n",
      "\n",
      "Instance 1152 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 11: [0.8200631141662598, 1.5581774711608887, -0.8402517437934875, 0.5598620176315308, 0.7623859643936157]\n",
      "Grand sum of 869 tensor sets is: [336.8130798339844, 1413.43310546875, -243.49691772460938, -362.30535888671875, -368.92236328125]\n",
      "\n",
      "Instance 1153 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2, 6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 2: [0.39561977982521057, 1.21811044216156, -0.7673172950744629, 0.7888554334640503, 0.09365344792604446]\n",
      "car at index 6: [-0.4628448784351349, 1.472689151763916, -0.6454706788063049, -1.851039171218872, -2.1680259704589844]\n",
      "Grand sum of 870 tensor sets is: [336.77947998046875, 1414.778564453125, -244.20330810546875, -362.8364562988281, -369.9595642089844]\n",
      "\n",
      "Instance 1154 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15, 24]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 15: [0.9622684717178345, 2.329355239868164, -1.371014952659607, -0.34257274866104126, -1.6150356531143188]\n",
      "car at index 24: [0.8810231685638428, 2.1886117458343506, -0.9562058448791504, -0.4555082321166992, -1.3541064262390137]\n",
      "Grand sum of 871 tensor sets is: [337.70111083984375, 1417.03759765625, -245.36691284179688, -363.2355041503906, -371.4441223144531]\n",
      "\n",
      "Instance 1155 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 16: [1.3328971862792969, 3.14489483833313, 0.09547606110572815, 0.8879058957099915, -3.2005882263183594]\n",
      "Grand sum of 872 tensor sets is: [339.03399658203125, 1420.1824951171875, -245.2714385986328, -362.34759521484375, -374.64471435546875]\n",
      "\n",
      "Instance 1156 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 35: [0.33900266885757446, 2.4310970306396484, -0.1833588182926178, 1.2817851305007935, -2.284252166748047]\n",
      "Grand sum of 873 tensor sets is: [339.37298583984375, 1422.6136474609375, -245.45480346679688, -361.0657958984375, -376.928955078125]\n",
      "\n",
      "Instance 1157 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1158 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([101, 13, 768])\n",
      "Shape of summed layers is: 101 x 768\n",
      "car at index 19: [-0.16277533769607544, -0.2921064794063568, -0.23105597496032715, 0.17027556896209717, 0.2666219472885132]\n",
      "Grand sum of 874 tensor sets is: [339.210205078125, 1422.321533203125, -245.68585205078125, -360.8955078125, -376.6623229980469]\n",
      "\n",
      "Instance 1159 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 3: [1.0516657829284668, 2.2087020874023438, -1.263996958732605, -0.20708876848220825, 0.7540186047554016]\n",
      "Grand sum of 875 tensor sets is: [340.2618713378906, 1424.5302734375, -246.94984436035156, -361.10260009765625, -375.9082946777344]\n",
      "\n",
      "Instance 1160 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1161 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 17: [0.12082041800022125, 2.320115327835083, 0.45447835326194763, 3.5148956775665283, -3.600860834121704]\n",
      "Grand sum of 876 tensor sets is: [340.3826904296875, 1426.850341796875, -246.495361328125, -357.58770751953125, -379.5091552734375]\n",
      "\n",
      "Instance 1162 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 31: [0.3975004255771637, 0.5565402507781982, 0.44080686569213867, 0.2171931266784668, -1.3637733459472656]\n",
      "Grand sum of 877 tensor sets is: [340.7801818847656, 1427.4068603515625, -246.05455017089844, -357.3705139160156, -380.8729248046875]\n",
      "\n",
      "Instance 1163 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "car at index 50: [0.24136243760585785, 1.8675978183746338, 0.4228055477142334, -1.4667129516601562, 0.21592190861701965]\n",
      "Grand sum of 878 tensor sets is: [341.02154541015625, 1429.2744140625, -245.63174438476562, -358.83721923828125, -380.6570129394531]\n",
      "\n",
      "Instance 1164 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20, 70]\n",
      "Size of token embeddings is torch.Size([83, 13, 768])\n",
      "Shape of summed layers is: 83 x 768\n",
      "car at index 20: [0.4519354999065399, 2.4738657474517822, -0.603669285774231, -0.24329116940498352, 1.0985636711120605]\n",
      "car at index 70: [0.09098272025585175, 3.6875085830688477, 0.17051464319229126, -0.4376939535140991, 2.143754005432129]\n",
      "Grand sum of 879 tensor sets is: [341.2929992675781, 1432.3551025390625, -245.84832763671875, -359.1777038574219, -379.0358581542969]\n",
      "\n",
      "Instance 1165 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 31: [1.3876960277557373, 1.501567006111145, -0.9947447180747986, -0.816152811050415, 0.6743448376655579]\n",
      "Grand sum of 880 tensor sets is: [342.6806945800781, 1433.856689453125, -246.84307861328125, -359.9938659667969, -378.36151123046875]\n",
      "\n",
      "Instance 1166 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 2: [-0.33574336767196655, 1.774110198020935, -0.5905501246452332, -0.8094196915626526, -1.2725062370300293]\n",
      "Grand sum of 881 tensor sets is: [342.3449401855469, 1435.630859375, -247.43362426757812, -360.80328369140625, -379.6340026855469]\n",
      "\n",
      "Instance 1167 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 9: [-0.5300343632698059, 4.156711578369141, -0.8251556158065796, -0.03103816881775856, -2.659092426300049]\n",
      "Grand sum of 882 tensor sets is: [341.8149108886719, 1439.78759765625, -248.25877380371094, -360.8343200683594, -382.2930908203125]\n",
      "\n",
      "Instance 1168 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 23: [0.5013584494590759, 3.0053837299346924, 0.6770957708358765, -1.0491615533828735, -2.7939460277557373]\n",
      "Grand sum of 883 tensor sets is: [342.3162841796875, 1442.79296875, -247.58168029785156, -361.88348388671875, -385.0870361328125]\n",
      "\n",
      "Instance 1169 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1170 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 15: [0.19618429243564606, -0.2805907726287842, -0.3290412425994873, 1.8819975852966309, 2.2204227447509766]\n",
      "Grand sum of 884 tensor sets is: [342.5124816894531, 1442.5123291015625, -247.9107208251953, -360.0014953613281, -382.8666076660156]\n",
      "\n",
      "Instance 1171 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 20: [0.9564158916473389, 2.5277748107910156, -0.39355671405792236, -1.098536491394043, 1.7938690185546875]\n",
      "Grand sum of 885 tensor sets is: [343.4689025878906, 1445.0401611328125, -248.3042755126953, -361.10003662109375, -381.07275390625]\n",
      "\n",
      "Instance 1172 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 19: [1.308095932006836, 1.526131272315979, 0.46426039934158325, -0.5907416939735413, -0.676311731338501]\n",
      "Grand sum of 886 tensor sets is: [344.7770080566406, 1446.5662841796875, -247.8400115966797, -361.6907653808594, -381.7490539550781]\n",
      "\n",
      "Instance 1173 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1174 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 8: [0.9070099592208862, 0.43070781230926514, -0.023425675928592682, -1.1248202323913574, 0.4188031852245331]\n",
      "Grand sum of 887 tensor sets is: [345.68402099609375, 1446.9969482421875, -247.86343383789062, -362.8155822753906, -381.33026123046875]\n",
      "\n",
      "Instance 1175 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([121, 13, 768])\n",
      "Shape of summed layers is: 121 x 768\n",
      "car at index 65: [0.38430583477020264, -1.5665233135223389, 0.3341633975505829, 2.664991855621338, 1.9120317697525024]\n",
      "Grand sum of 888 tensor sets is: [346.0683288574219, 1445.430419921875, -247.52926635742188, -360.1506042480469, -379.4182434082031]\n",
      "\n",
      "Instance 1176 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1177 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1178 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1179 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1180 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "car at index 17: [1.0152678489685059, -0.5534278750419617, -0.15321658551692963, 1.3751177787780762, 2.355975389480591]\n",
      "Grand sum of 889 tensor sets is: [347.0835876464844, 1444.876953125, -247.68247985839844, -358.7754821777344, -377.062255859375]\n",
      "\n",
      "Instance 1181 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 11: [0.7398982644081116, 1.4620599746704102, -0.27905988693237305, 0.21175093948841095, 1.2391221523284912]\n",
      "Grand sum of 890 tensor sets is: [347.823486328125, 1446.3389892578125, -247.96153259277344, -358.563720703125, -375.8231201171875]\n",
      "\n",
      "Instance 1182 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "car at index 19: [1.4102188348770142, -0.4512172341346741, -0.9077320694923401, -1.1883231401443481, 1.8454077243804932]\n",
      "Grand sum of 891 tensor sets is: [349.23370361328125, 1445.8878173828125, -248.8692626953125, -359.7520446777344, -373.97772216796875]\n",
      "\n",
      "Instance 1183 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3, 13, 30, 40]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "car at index 3: [0.40755704045295715, 1.5952647924423218, -0.07103347778320312, -0.7934514284133911, 0.2158501297235489]\n",
      "car at index 13: [0.8736334443092346, 2.575355291366577, 0.210514098405838, -0.02866424061357975, -0.45769938826560974]\n",
      "car at index 30: [0.3106754720211029, 2.435277223587036, 0.01425749808549881, -1.7289958000183105, 1.3934382200241089]\n",
      "car at index 40: [0.8694931268692017, 3.173640012741089, 0.16653934121131897, 0.08966813236474991, -0.25081613659858704]\n",
      "Grand sum of 892 tensor sets is: [349.8490295410156, 1448.3326416015625, -248.78919982910156, -360.3674011230469, -373.7525329589844]\n",
      "\n",
      "Instance 1184 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 13: [0.25415995717048645, 2.2169384956359863, -0.11490920186042786, -0.3655340075492859, -1.5431172847747803]\n",
      "Grand sum of 893 tensor sets is: [350.1031799316406, 1450.549560546875, -248.90411376953125, -360.7329406738281, -375.295654296875]\n",
      "\n",
      "Instance 1185 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1186 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 3: [0.5239540934562683, 1.374738097190857, -0.5921310782432556, -0.7618744969367981, 1.2212705612182617]\n",
      "Grand sum of 894 tensor sets is: [350.62713623046875, 1451.92431640625, -249.49624633789062, -361.49481201171875, -374.0743713378906]\n",
      "\n",
      "Instance 1187 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 4: [0.769290566444397, 2.684762477874756, -0.008760742843151093, -1.472039818763733, -1.8766602277755737]\n",
      "Grand sum of 895 tensor sets is: [351.39642333984375, 1454.609130859375, -249.5050048828125, -362.96685791015625, -375.9510192871094]\n",
      "\n",
      "Instance 1188 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 5: [-0.7514848113059998, 0.7806952595710754, 0.03720973804593086, 1.7562624216079712, -2.021038055419922]\n",
      "Grand sum of 896 tensor sets is: [350.6449279785156, 1455.3897705078125, -249.46778869628906, -361.2106018066406, -377.9720458984375]\n",
      "\n",
      "Instance 1189 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 15: [-0.6997829675674438, 1.7646684646606445, -0.4164643883705139, -2.5651469230651855, 0.10748445987701416]\n",
      "Grand sum of 897 tensor sets is: [349.9451599121094, 1457.1544189453125, -249.88424682617188, -363.7757568359375, -377.86456298828125]\n",
      "\n",
      "Instance 1190 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 17: [0.9235289096832275, 2.1895246505737305, -0.20033784210681915, -1.6326971054077148, 0.020103469491004944]\n",
      "Grand sum of 898 tensor sets is: [350.8686828613281, 1459.343994140625, -250.08457946777344, -365.408447265625, -377.8444519042969]\n",
      "\n",
      "Instance 1191 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 19: [0.34611040353775024, 1.3808499574661255, 0.9043959379196167, 0.3172665536403656, -0.8500298261642456]\n",
      "Grand sum of 899 tensor sets is: [351.21478271484375, 1460.724853515625, -249.18019104003906, -365.0911865234375, -378.6944885253906]\n",
      "\n",
      "Instance 1192 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [0.6523812413215637, 2.1518068313598633, -0.24427196383476257, 0.7836087346076965, 0.8163071870803833]\n",
      "Grand sum of 900 tensor sets is: [351.8671569824219, 1462.876708984375, -249.42446899414062, -364.3075866699219, -377.878173828125]\n",
      "\n",
      "Instance 1193 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1194 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 21: [0.1173793375492096, 3.774059295654297, -0.4683922529220581, 0.6626837849617004, -0.10685455799102783]\n",
      "Grand sum of 901 tensor sets is: [351.9845275878906, 1466.6507568359375, -249.8928680419922, -363.6448974609375, -377.9850158691406]\n",
      "\n",
      "Instance 1195 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 21: [0.5455515384674072, 2.7229316234588623, -0.3245050311088562, -2.599034547805786, -1.529799461364746]\n",
      "Grand sum of 902 tensor sets is: [352.53009033203125, 1469.3736572265625, -250.21737670898438, -366.2439270019531, -379.5148010253906]\n",
      "\n",
      "Instance 1196 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 33: [0.8274106979370117, -0.2727251648902893, -0.41389530897140503, 1.467855453491211, 2.240633249282837]\n",
      "Grand sum of 903 tensor sets is: [353.3575134277344, 1469.1009521484375, -250.6312713623047, -364.77606201171875, -377.274169921875]\n",
      "\n",
      "Instance 1197 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 3: [0.3755040466785431, 1.4475066661834717, -0.8443728089332581, -1.0402482748031616, -0.26973602175712585]\n",
      "Grand sum of 904 tensor sets is: [353.7330322265625, 1470.5484619140625, -251.47564697265625, -365.8163146972656, -377.5439147949219]\n",
      "\n",
      "Instance 1198 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 6: [0.4772299528121948, 1.7562674283981323, -0.8444303870201111, -0.9813055992126465, -1.7554348707199097]\n",
      "Grand sum of 905 tensor sets is: [354.21026611328125, 1472.3046875, -252.32008361816406, -366.797607421875, -379.2993469238281]\n",
      "\n",
      "Instance 1199 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1200 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 12: [0.87477046251297, 3.2487823963165283, -0.3231145739555359, -1.433237075805664, -1.0231821537017822]\n",
      "Grand sum of 906 tensor sets is: [355.08502197265625, 1475.553466796875, -252.64320373535156, -368.2308349609375, -380.3225402832031]\n",
      "\n",
      "Instance 1201 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 15: [0.4250984489917755, 2.1309335231781006, -0.10244004428386688, -2.0996878147125244, -0.05611182749271393]\n",
      "Grand sum of 907 tensor sets is: [355.5101318359375, 1477.6844482421875, -252.7456512451172, -370.3305358886719, -380.378662109375]\n",
      "\n",
      "Instance 1202 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 12: [0.848359227180481, 0.48604726791381836, -0.3300594389438629, -0.7471178770065308, 2.2588489055633545]\n",
      "Grand sum of 908 tensor sets is: [356.3584899902344, 1478.1705322265625, -253.07571411132812, -371.0776672363281, -378.11981201171875]\n",
      "\n",
      "Instance 1203 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "car at index 2: [0.11213020980358124, 1.2035027742385864, -1.1420414447784424, -2.354570150375366, 1.4830948114395142]\n",
      "Grand sum of 909 tensor sets is: [356.4706115722656, 1479.3740234375, -254.21775817871094, -373.4322509765625, -376.63671875]\n",
      "\n",
      "Instance 1204 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "car at index 41: [0.5875970125198364, 0.2734065651893616, -1.4965119361877441, 0.47147780656814575, 1.435753345489502]\n",
      "Grand sum of 910 tensor sets is: [357.0581970214844, 1479.6474609375, -255.71426391601562, -372.9607849121094, -375.2009582519531]\n",
      "\n",
      "Instance 1205 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 9: [0.12027111649513245, 2.9902732372283936, -0.19908201694488525, 0.5825846791267395, -2.277261734008789]\n",
      "Grand sum of 911 tensor sets is: [357.178466796875, 1482.6376953125, -255.91334533691406, -372.3782043457031, -377.47821044921875]\n",
      "\n",
      "Instance 1206 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 8: [-0.0391438826918602, 1.863429307937622, -0.09472256153821945, -1.5486388206481934, -0.9816215634346008]\n",
      "Grand sum of 912 tensor sets is: [357.1393127441406, 1484.5010986328125, -256.008056640625, -373.9268493652344, -378.4598388671875]\n",
      "\n",
      "Instance 1207 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 18: [0.6036890149116516, 2.0575034618377686, -0.1296611726284027, 1.540535807609558, -1.3925528526306152]\n",
      "Grand sum of 913 tensor sets is: [357.7430114746094, 1486.55859375, -256.1377258300781, -372.3863220214844, -379.8523864746094]\n",
      "\n",
      "Instance 1208 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 9: [0.19558745622634888, 2.6938438415527344, -0.5749129056930542, 0.7013193964958191, -0.14200572669506073]\n",
      "Grand sum of 914 tensor sets is: [357.9385986328125, 1489.25244140625, -256.712646484375, -371.68499755859375, -379.994384765625]\n",
      "\n",
      "Instance 1209 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8, 18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 8: [0.48152634501457214, 1.7123013734817505, -0.8268367648124695, 0.03641538321971893, -1.8169927597045898]\n",
      "car at index 18: [0.8527211546897888, 0.6515230536460876, -0.1667584925889969, 0.6843562722206116, -1.9521974325180054]\n",
      "Grand sum of 915 tensor sets is: [358.605712890625, 1490.434326171875, -257.2094421386719, -371.3246154785156, -381.87896728515625]\n",
      "\n",
      "Instance 1210 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([167, 13, 768])\n",
      "Shape of summed layers is: 167 x 768\n",
      "car at index 36: [1.3584723472595215, 0.7657484412193298, -0.06704919040203094, 0.21383538842201233, -1.4879825115203857]\n",
      "Grand sum of 916 tensor sets is: [359.96417236328125, 1491.2000732421875, -257.2764892578125, -371.11077880859375, -383.366943359375]\n",
      "\n",
      "Instance 1211 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1212 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1213 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 38: [-0.33396849036216736, 2.461909770965576, -0.9734383225440979, -1.67365300655365, -0.20041732490062714]\n",
      "Grand sum of 917 tensor sets is: [359.6302185058594, 1493.6619873046875, -258.24993896484375, -372.784423828125, -383.5673522949219]\n",
      "\n",
      "Instance 1214 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 2: [-0.8460162878036499, 3.119849920272827, 0.0589769147336483, -0.21546439826488495, -0.9787856936454773]\n",
      "Grand sum of 918 tensor sets is: [358.7842102050781, 1496.7818603515625, -258.1909484863281, -372.9998779296875, -384.546142578125]\n",
      "\n",
      "Instance 1215 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1216 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 25: [-0.24250176548957825, 2.1299641132354736, 0.2430294305086136, 0.49630364775657654, -1.9018800258636475]\n",
      "Grand sum of 919 tensor sets is: [358.5417175292969, 1498.911865234375, -257.9479064941406, -372.5035705566406, -386.4480285644531]\n",
      "\n",
      "Instance 1217 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1218 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [95]\n",
      "Size of token embeddings is torch.Size([258, 13, 768])\n",
      "Shape of summed layers is: 258 x 768\n",
      "car at index 95: [1.2650060653686523, -0.6151520609855652, -0.8987356424331665, -1.2951409816741943, 0.1555134356021881]\n",
      "Grand sum of 920 tensor sets is: [359.8067321777344, 1498.2967529296875, -258.8466491699219, -373.7987060546875, -386.2925109863281]\n",
      "\n",
      "Instance 1219 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 11: [-0.48933613300323486, 2.6625800132751465, 0.06344771385192871, -1.2252849340438843, -2.561457395553589]\n",
      "Grand sum of 921 tensor sets is: [359.3173828125, 1500.9593505859375, -258.783203125, -375.02398681640625, -388.8539733886719]\n",
      "\n",
      "Instance 1220 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([5, 13, 768])\n",
      "Shape of summed layers is: 5 x 768\n",
      "car at index 2: [-0.6175329685211182, 2.219517946243286, -0.08392752707004547, 1.7444956302642822, 0.3877830505371094]\n",
      "Grand sum of 922 tensor sets is: [358.6998596191406, 1503.1788330078125, -258.86712646484375, -373.27947998046875, -388.4661865234375]\n",
      "\n",
      "Instance 1221 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 10: [0.38454359769821167, 2.012342929840088, 0.14637219905853271, -0.1576113998889923, -0.6735266447067261]\n",
      "Grand sum of 923 tensor sets is: [359.08441162109375, 1505.191162109375, -258.72076416015625, -373.4371032714844, -389.13970947265625]\n",
      "\n",
      "Instance 1222 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 6: [1.2737339735031128, 2.566879987716675, -0.07686196267604828, -0.8741852641105652, 1.7125523090362549]\n",
      "Grand sum of 924 tensor sets is: [360.358154296875, 1507.758056640625, -258.7976379394531, -374.311279296875, -387.4271545410156]\n",
      "\n",
      "Instance 1223 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 4: [0.2734408974647522, 2.07535719871521, -0.03168701380491257, -1.9483243227005005, 0.4635814428329468]\n",
      "Grand sum of 925 tensor sets is: [360.631591796875, 1509.8333740234375, -258.8293151855469, -376.2596130371094, -386.96356201171875]\n",
      "\n",
      "Instance 1224 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 23]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 7: [-0.6667147278785706, 2.679666519165039, -1.0973069667816162, 0.2614239454269409, 1.9073612689971924]\n",
      "car at index 23: [1.0151177644729614, 0.18946398794651031, 0.7494845390319824, 1.4436888694763184, -2.2739226818084717]\n",
      "Grand sum of 926 tensor sets is: [360.8057861328125, 1511.2679443359375, -259.00323486328125, -375.40704345703125, -387.1468505859375]\n",
      "\n",
      "Instance 1225 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 4: [0.6069424748420715, 2.9906163215637207, -0.7922398447990417, -1.2311862707138062, -0.98432856798172]\n",
      "Grand sum of 927 tensor sets is: [361.4127197265625, 1514.258544921875, -259.79547119140625, -376.63824462890625, -388.13116455078125]\n",
      "\n",
      "Instance 1226 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1227 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14, 35, 57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "car at index 14: [0.8667258024215698, 2.440070390701294, -0.866095244884491, -1.9173023700714111, -0.7433925271034241]\n",
      "car at index 35: [0.96771240234375, 3.096869945526123, -0.9534326791763306, -0.5397730469703674, -0.24831032752990723]\n",
      "car at index 57: [0.5007349848747253, 2.3811779022216797, -1.9833295345306396, -0.7163136601448059, -0.31738489866256714]\n",
      "Grand sum of 928 tensor sets is: [362.19110107421875, 1516.89794921875, -261.0630798339844, -377.696044921875, -388.5675354003906]\n",
      "\n",
      "Instance 1228 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1229 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 7: [0.2417399287223816, 1.4599905014038086, -1.1284949779510498, -0.9666518568992615, 2.3006725311279297]\n",
      "Grand sum of 929 tensor sets is: [362.4328308105469, 1518.35791015625, -262.19158935546875, -378.6626892089844, -386.2668762207031]\n",
      "\n",
      "Instance 1230 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 21: [1.8418794870376587, 2.209219455718994, 0.318646103143692, 0.05823821946978569, -0.24008062481880188]\n",
      "Grand sum of 930 tensor sets is: [364.27471923828125, 1520.567138671875, -261.8729553222656, -378.6044616699219, -386.5069580078125]\n",
      "\n",
      "Instance 1231 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1232 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 4: [0.027515918016433716, 0.7907953262329102, -0.23283788561820984, -0.5398192405700684, -0.0848771184682846]\n",
      "Grand sum of 931 tensor sets is: [364.30224609375, 1521.35791015625, -262.1058044433594, -379.144287109375, -386.5918273925781]\n",
      "\n",
      "Instance 1233 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "car at index 32: [0.37262582778930664, 2.1726019382476807, 0.21002086997032166, 1.958493709564209, -1.7406433820724487]\n",
      "Grand sum of 932 tensor sets is: [364.67486572265625, 1523.530517578125, -261.8957824707031, -377.185791015625, -388.33245849609375]\n",
      "\n",
      "Instance 1234 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1235 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 15: [0.6801782846450806, 2.061168670654297, -0.3379039168357849, -0.4025309681892395, -2.0362398624420166]\n",
      "Grand sum of 933 tensor sets is: [365.35504150390625, 1525.5916748046875, -262.2336730957031, -377.58831787109375, -390.36871337890625]\n",
      "\n",
      "Instance 1236 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [1.0667675733566284, 2.571070432662964, -1.0282323360443115, -0.44038838148117065, 0.3440927267074585]\n",
      "Grand sum of 934 tensor sets is: [366.42181396484375, 1528.1627197265625, -263.26190185546875, -378.0287170410156, -390.0246276855469]\n",
      "\n",
      "Instance 1237 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1238 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1239 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1240 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 18: [0.6050310134887695, 2.1155526638031006, -0.02733391337096691, -0.04688935726881027, 0.45441487431526184]\n",
      "Grand sum of 935 tensor sets is: [367.02685546875, 1530.2783203125, -263.28924560546875, -378.0755920410156, -389.5702209472656]\n",
      "\n",
      "Instance 1241 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([88, 13, 768])\n",
      "Shape of summed layers is: 88 x 768\n",
      "car at index 23: [0.2945571541786194, 0.45331430435180664, -0.4774228632450104, -0.71760094165802, 2.49507212638855]\n",
      "Grand sum of 936 tensor sets is: [367.3214111328125, 1530.731689453125, -263.76666259765625, -378.7931823730469, -387.07513427734375]\n",
      "\n",
      "Instance 1242 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 7: [0.17385897040367126, 1.6279374361038208, 0.7484920620918274, -1.3160284757614136, -1.5931062698364258]\n",
      "Grand sum of 937 tensor sets is: [367.4952697753906, 1532.359619140625, -263.0181579589844, -380.1092224121094, -388.6682434082031]\n",
      "\n",
      "Instance 1243 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 10: [0.3296903669834137, 1.505739450454712, -1.318401575088501, -1.3769387006759644, -0.3415885269641876]\n",
      "Grand sum of 938 tensor sets is: [367.824951171875, 1533.8653564453125, -264.3365478515625, -381.4861755371094, -389.00982666015625]\n",
      "\n",
      "Instance 1244 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1245 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 939 tensor sets is: [369.9425964355469, 1535.541748046875, -265.5708312988281, -383.1517028808594, -389.1543273925781]\n",
      "\n",
      "Instance 1246 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 12: [1.5721989870071411, 1.6795893907546997, -0.35309192538261414, -1.1451561450958252, -1.9941177368164062]\n",
      "Grand sum of 940 tensor sets is: [371.5148010253906, 1537.2213134765625, -265.9239196777344, -384.2968444824219, -391.1484375]\n",
      "\n",
      "Instance 1247 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1248 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([101, 13, 768])\n",
      "Shape of summed layers is: 101 x 768\n",
      "car at index 37: [0.5319408178329468, 3.234926462173462, -1.529449462890625, -2.3209662437438965, 0.39667317271232605]\n",
      "Grand sum of 941 tensor sets is: [372.0467529296875, 1540.456298828125, -267.453369140625, -386.6177978515625, -390.75177001953125]\n",
      "\n",
      "Instance 1249 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1250 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 11: [0.14037427306175232, 1.1961212158203125, -0.3361351788043976, 0.12903934717178345, -0.33637335896492004]\n",
      "Grand sum of 942 tensor sets is: [372.1871337890625, 1541.6524658203125, -267.78948974609375, -386.48876953125, -391.088134765625]\n",
      "\n",
      "Instance 1251 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 20]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 7: [-0.03151465952396393, 2.1776552200317383, -0.8070635795593262, -2.0857291221618652, -0.5895825624465942]\n",
      "car at index 20: [0.13737718760967255, 1.601938009262085, 0.4143539071083069, -1.4610481262207031, 0.030356310307979584]\n",
      "Grand sum of 943 tensor sets is: [372.24005126953125, 1543.542236328125, -267.98583984375, -388.26214599609375, -391.36773681640625]\n",
      "\n",
      "Instance 1252 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 42: [0.10572993755340576, 2.9385998249053955, -0.7360514998435974, -0.5394704937934875, -0.2603914141654968]\n",
      "Grand sum of 944 tensor sets is: [372.3457946777344, 1546.4808349609375, -268.7218933105469, -388.8016052246094, -391.6281433105469]\n",
      "\n",
      "Instance 1253 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1254 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 4: [-1.3065344095230103, 1.4687517881393433, -0.15884318947792053, -0.4162559509277344, -0.04809728264808655]\n",
      "Grand sum of 945 tensor sets is: [371.03924560546875, 1547.9495849609375, -268.8807373046875, -389.2178649902344, -391.6762390136719]\n",
      "\n",
      "Instance 1255 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([113, 13, 768])\n",
      "Shape of summed layers is: 113 x 768\n",
      "car at index 74: [0.2534654438495636, -1.4752908945083618, -0.6703392267227173, 1.896620273590088, 1.6126724481582642]\n",
      "Grand sum of 946 tensor sets is: [371.292724609375, 1546.4742431640625, -269.55108642578125, -387.3212585449219, -390.0635681152344]\n",
      "\n",
      "Instance 1256 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1257 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 10: [0.33926844596862793, -0.4132936894893646, 0.6150557994842529, 0.31619542837142944, -0.33855369687080383]\n",
      "Grand sum of 947 tensor sets is: [371.6319885253906, 1546.0609130859375, -268.93603515625, -387.00506591796875, -390.4021301269531]\n",
      "\n",
      "Instance 1258 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1259 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1260 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1261 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 13: [-0.2829970717430115, 1.2260138988494873, -0.32158994674682617, -1.649915099143982, -0.13339708745479584]\n",
      "Grand sum of 948 tensor sets is: [371.3489990234375, 1547.2869873046875, -269.25762939453125, -388.65496826171875, -390.5355224609375]\n",
      "\n",
      "Instance 1262 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 23: [0.1986686736345291, 1.5219212770462036, -0.2711332142353058, -0.3658176064491272, 0.3730263113975525]\n",
      "Grand sum of 949 tensor sets is: [371.54766845703125, 1548.8089599609375, -269.52874755859375, -389.0207824707031, -390.1625061035156]\n",
      "\n",
      "Instance 1263 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1264 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1265 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 5: [0.30419814586639404, 2.0011422634124756, -0.7789962291717529, -2.2677485942840576, -1.4705243110656738]\n",
      "Grand sum of 950 tensor sets is: [371.85186767578125, 1550.81005859375, -270.3077392578125, -391.2885437011719, -391.6330261230469]\n",
      "\n",
      "Instance 1266 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([167, 13, 768])\n",
      "Shape of summed layers is: 167 x 768\n",
      "car at index 36: [1.3584723472595215, 0.7657484412193298, -0.06704919040203094, 0.21383538842201233, -1.4879825115203857]\n",
      "Grand sum of 951 tensor sets is: [373.2103271484375, 1551.5758056640625, -270.3747863769531, -391.07470703125, -393.1210021972656]\n",
      "\n",
      "Instance 1267 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1268 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [52]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "car at index 52: [0.5326358079910278, 2.083246946334839, -1.8036363124847412, -1.4324654340744019, -1.2282524108886719]\n",
      "Grand sum of 952 tensor sets is: [373.7429504394531, 1553.6590576171875, -272.1784362792969, -392.5071716308594, -394.3492431640625]\n",
      "\n",
      "Instance 1269 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 26: [0.5307669639587402, 0.9877387881278992, -0.2954094111919403, -0.2820255756378174, -1.3805079460144043]\n",
      "Grand sum of 953 tensor sets is: [374.2737121582031, 1554.6468505859375, -272.4738464355469, -392.7891845703125, -395.729736328125]\n",
      "\n",
      "Instance 1270 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1271 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 4: [0.5832098722457886, 1.4800621271133423, -0.8479036092758179, -0.886961042881012, 0.4809289574623108]\n",
      "Grand sum of 954 tensor sets is: [374.85693359375, 1556.126953125, -273.3217468261719, -393.6761474609375, -395.2488098144531]\n",
      "\n",
      "Instance 1272 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [225]\n",
      "Size of token embeddings is torch.Size([432, 13, 768])\n",
      "Shape of summed layers is: 432 x 768\n",
      "car at index 225: [1.3601868152618408, -1.3174974918365479, -0.5940607190132141, -0.7777456045150757, -0.5675168633460999]\n",
      "Grand sum of 955 tensor sets is: [376.2171325683594, 1554.8094482421875, -273.9158020019531, -394.4538879394531, -395.8163146972656]\n",
      "\n",
      "Instance 1273 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "car at index 48: [0.7352835536003113, 1.9958446025848389, -0.7780970335006714, 0.7844033241271973, 0.3673953115940094]\n",
      "Grand sum of 956 tensor sets is: [376.9524230957031, 1556.8052978515625, -274.69390869140625, -393.66949462890625, -395.44891357421875]\n",
      "\n",
      "Instance 1274 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1275 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10, 114, 122]\n",
      "Size of token embeddings is torch.Size([149, 13, 768])\n",
      "Shape of summed layers is: 149 x 768\n",
      "car at index 10: [0.35280248522758484, 1.0992330312728882, -0.8382885456085205, -0.920535683631897, 0.0012429580092430115]\n",
      "car at index 114: [0.4898897111415863, 1.3977605104446411, -0.8401210904121399, -2.203327178955078, 0.655929684638977]\n",
      "car at index 122: [-1.1245007514953613, -0.1827085167169571, -1.3572715520858765, 0.6663739085197449, -3.4171321392059326]\n",
      "Grand sum of 957 tensor sets is: [376.8584899902344, 1557.5767822265625, -275.705810546875, -394.4886474609375, -396.368896484375]\n",
      "\n",
      "Instance 1276 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 20: [1.2326157093048096, 2.620588779449463, -0.5372310280799866, -0.09018875658512115, 1.4477275609970093]\n",
      "Grand sum of 958 tensor sets is: [378.0910949707031, 1560.1973876953125, -276.2430419921875, -394.5788269042969, -394.9211730957031]\n",
      "\n",
      "Instance 1277 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1278 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 5: [0.5852296352386475, 1.9745255708694458, -0.31997621059417725, -1.3727902173995972, 1.8278467655181885]\n",
      "Grand sum of 959 tensor sets is: [378.67633056640625, 1562.171875, -276.5630187988281, -395.9516296386719, -393.09332275390625]\n",
      "\n",
      "Instance 1279 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 17: [0.0716923177242279, 2.2820067405700684, -0.3411908447742462, 0.18321841955184937, -1.25572669506073]\n",
      "Grand sum of 960 tensor sets is: [378.7480163574219, 1564.453857421875, -276.9042053222656, -395.7684020996094, -394.34906005859375]\n",
      "\n",
      "Instance 1280 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 961 tensor sets is: [380.86566162109375, 1566.1302490234375, -278.13848876953125, -397.4339294433594, -394.4935607910156]\n",
      "\n",
      "Instance 1281 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 2: [0.05022667348384857, 1.998404622077942, -1.294228196144104, -1.0464198589324951, 0.17866401374340057]\n",
      "Grand sum of 962 tensor sets is: [380.9158935546875, 1568.128662109375, -279.4327087402344, -398.4803466796875, -394.3149108886719]\n",
      "\n",
      "Instance 1282 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [39, 61]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "car at index 39: [0.22386345267295837, 0.9970278739929199, -0.46792834997177124, 2.300550699234009, -1.0454241037368774]\n",
      "car at index 61: [1.1719145774841309, 0.4850696921348572, -0.5615242123603821, 0.9000957012176514, 0.33993232250213623]\n",
      "Grand sum of 963 tensor sets is: [381.61376953125, 1568.8697509765625, -279.94744873046875, -396.8800354003906, -394.66766357421875]\n",
      "\n",
      "Instance 1283 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 14, 28]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 7: [0.023500248789787292, 1.2059868574142456, 0.014075346291065216, -1.8054945468902588, -3.1675772666931152]\n",
      "car at index 14: [-0.4787173271179199, 2.71720027923584, 0.12093529105186462, -1.403619647026062, -4.289303779602051]\n",
      "car at index 28: [-0.6147422790527344, 1.651695966720581, -0.31443876028060913, -0.8641238212585449, -3.5604617595672607]\n",
      "Grand sum of 964 tensor sets is: [381.2571105957031, 1570.72802734375, -280.00726318359375, -398.23779296875, -398.3401184082031]\n",
      "\n",
      "Instance 1284 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1285 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 7: [0.34403103590011597, 0.8713545799255371, 0.3400403559207916, -1.8039734363555908, -0.10868741571903229]\n",
      "Grand sum of 965 tensor sets is: [381.60113525390625, 1571.599365234375, -279.667236328125, -400.0417785644531, -398.44879150390625]\n",
      "\n",
      "Instance 1286 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1287 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [80, 94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([274, 13, 768])\n",
      "Shape of summed layers is: 274 x 768\n",
      "car at index 80: [0.5025431513786316, -1.0273724794387817, -0.09257329255342484, -0.04367198795080185, 1.973146915435791]\n",
      "car at index 94: [1.3985908031463623, -0.7751449346542358, 1.606404423713684, -1.0461769104003906, 0.5909720659255981]\n",
      "Grand sum of 966 tensor sets is: [382.55169677734375, 1570.6981201171875, -278.9103088378906, -400.5867004394531, -397.1667175292969]\n",
      "\n",
      "Instance 1288 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 4: [0.06975339353084564, 1.6700036525726318, -1.2648662328720093, 0.5316555500030518, -1.0308880805969238]\n",
      "Grand sum of 967 tensor sets is: [382.6214599609375, 1572.3681640625, -280.1751708984375, -400.0550537109375, -398.1976013183594]\n",
      "\n",
      "Instance 1289 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 2: [0.49631139636039734, 1.4233014583587646, -1.125031590461731, -0.19721822440624237, 1.0332691669464111]\n",
      "Grand sum of 968 tensor sets is: [383.1177673339844, 1573.79150390625, -281.3002014160156, -400.25225830078125, -397.1643371582031]\n",
      "\n",
      "Instance 1290 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 9: [0.5493124127388, -0.11549434810876846, -0.47745463252067566, 1.6051180362701416, 3.160454034805298]\n",
      "Grand sum of 969 tensor sets is: [383.6670837402344, 1573.676025390625, -281.77764892578125, -398.6471252441406, -394.0038757324219]\n",
      "\n",
      "Instance 1291 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [96]\n",
      "Size of token embeddings is torch.Size([121, 13, 768])\n",
      "Shape of summed layers is: 121 x 768\n",
      "car at index 96: [-0.4416595697402954, 2.3742010593414307, 1.0373603105545044, 1.844473123550415, -4.9779815673828125]\n",
      "Grand sum of 970 tensor sets is: [383.2254333496094, 1576.0501708984375, -280.74029541015625, -396.8026428222656, -398.98187255859375]\n",
      "\n",
      "Instance 1292 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1293 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 11: [-0.2829531133174896, 2.125767469406128, -0.09395226836204529, -1.0412194728851318, -0.09536530822515488]\n",
      "Grand sum of 971 tensor sets is: [382.9424743652344, 1578.1759033203125, -280.8342590332031, -397.8438720703125, -399.0772399902344]\n",
      "\n",
      "Instance 1294 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 7: [-0.007758535444736481, 1.994301438331604, -0.6786626577377319, 0.5409821271896362, 0.1489735096693039]\n",
      "Grand sum of 972 tensor sets is: [382.9347229003906, 1580.170166015625, -281.5129089355469, -397.3028869628906, -398.9282531738281]\n",
      "\n",
      "Instance 1295 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1296 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 6: [-0.18239983916282654, 1.5679340362548828, 0.0009747818112373352, 0.6144136190414429, -2.3083300590515137]\n",
      "Grand sum of 973 tensor sets is: [382.7523193359375, 1581.7381591796875, -281.5119323730469, -396.6884765625, -401.236572265625]\n",
      "\n",
      "Instance 1297 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 4: [0.7241700291633606, 2.9286928176879883, 0.045098863542079926, -1.132136344909668, 0.0698268711566925]\n",
      "Grand sum of 974 tensor sets is: [383.47650146484375, 1584.6668701171875, -281.4668273925781, -397.82061767578125, -401.166748046875]\n",
      "\n",
      "Instance 1298 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 22: [0.11487454175949097, 1.8804062604904175, 0.2629193663597107, -2.985327959060669, -0.48943445086479187]\n",
      "Grand sum of 975 tensor sets is: [383.59136962890625, 1586.5472412109375, -281.20391845703125, -400.8059387207031, -401.65618896484375]\n",
      "\n",
      "Instance 1299 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 14: [0.7208091020584106, 1.720176339149475, 0.4074985384941101, -0.22463271021842957, -1.5719000101089478]\n",
      "Grand sum of 976 tensor sets is: [384.3121643066406, 1588.2674560546875, -280.7964172363281, -401.03057861328125, -403.22808837890625]\n",
      "\n",
      "Instance 1300 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 13: [-0.2829970717430115, 1.2260138988494873, -0.32158994674682617, -1.649915099143982, -0.13339708745479584]\n",
      "Grand sum of 977 tensor sets is: [384.0291748046875, 1589.4935302734375, -281.1180114746094, -402.68048095703125, -403.3614807128906]\n",
      "\n",
      "Instance 1301 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 6: [0.7381648421287537, 1.680866003036499, -0.7403185367584229, -2.4077460765838623, -0.07274174690246582]\n",
      "Grand sum of 978 tensor sets is: [384.767333984375, 1591.1744384765625, -281.85833740234375, -405.0882263183594, -403.4342346191406]\n",
      "\n",
      "Instance 1302 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [260]\n",
      "Size of token embeddings is torch.Size([490, 13, 768])\n",
      "Shape of summed layers is: 490 x 768\n",
      "car at index 260: [0.6893485188484192, 3.383143901824951, -0.27962827682495117, 0.8215711712837219, -2.3096141815185547]\n",
      "Grand sum of 979 tensor sets is: [385.4566955566406, 1594.5576171875, -282.1379699707031, -404.26666259765625, -405.74383544921875]\n",
      "\n",
      "Instance 1303 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 9: [0.979623556137085, 1.969026803970337, -0.6400845050811768, -0.09065916389226913, -2.077043294906616]\n",
      "Grand sum of 980 tensor sets is: [386.4363098144531, 1596.526611328125, -282.7780456542969, -404.3573303222656, -407.8208923339844]\n",
      "\n",
      "Instance 1304 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1305 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1306 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1307 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18, 25]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 18: [0.34252917766571045, 1.255693793296814, -0.5815412998199463, -1.7503958940505981, -0.42578575015068054]\n",
      "car at index 25: [0.3519781529903412, 1.0586520433425903, -0.26443037390708923, -1.967570424079895, -0.7484846711158752]\n",
      "Grand sum of 981 tensor sets is: [386.7835693359375, 1597.683837890625, -283.2010192871094, -406.21630859375, -408.40802001953125]\n",
      "\n",
      "Instance 1308 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 23: [0.6816549301147461, 3.8136770725250244, 0.028136011213064194, -1.4050703048706055, -1.302066683769226]\n",
      "Grand sum of 982 tensor sets is: [387.4652099609375, 1601.49755859375, -283.1728820800781, -407.6213684082031, -409.7100830078125]\n",
      "\n",
      "Instance 1309 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 7: [-0.2542321979999542, 1.820481538772583, -0.2166399508714676, -0.442831814289093, -2.355376720428467]\n",
      "Grand sum of 983 tensor sets is: [387.2109680175781, 1603.3179931640625, -283.3895263671875, -408.064208984375, -412.0654602050781]\n",
      "\n",
      "Instance 1310 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([66, 13, 768])\n",
      "Shape of summed layers is: 66 x 768\n",
      "car at index 42: [1.7290922403335571, 1.1459035873413086, -0.2938343584537506, 0.10304399579763412, 2.358682155609131]\n",
      "Grand sum of 984 tensor sets is: [388.9400634765625, 1604.4638671875, -283.683349609375, -407.9611511230469, -409.706787109375]\n",
      "\n",
      "Instance 1311 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 11: [0.22926032543182373, 1.2018401622772217, -0.2676224112510681, -0.6471755504608154, -0.7478066086769104]\n",
      "Grand sum of 985 tensor sets is: [389.1693115234375, 1605.6656494140625, -283.9509582519531, -408.60833740234375, -410.45458984375]\n",
      "\n",
      "Instance 1312 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3, 9, 21, 35]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 3: [-0.013118330389261246, 0.6544795036315918, -0.09556905925273895, -0.6671635508537292, 1.2048643827438354]\n",
      "car at index 9: [0.05653807520866394, 0.9916037321090698, 0.0278022438287735, -0.5686174035072327, -1.3325793743133545]\n",
      "car at index 21: [0.0166825819760561, 1.5205129384994507, 0.3296147584915161, -0.7580147981643677, -0.04742315411567688]\n",
      "car at index 35: [-0.46966660022735596, 0.8420253396034241, -0.33973538875579834, -1.6157360076904297, -0.9631968140602112]\n",
      "Grand sum of 986 tensor sets is: [389.0669250488281, 1606.6678466796875, -283.9704284667969, -409.5107116699219, -410.7391662597656]\n",
      "\n",
      "Instance 1313 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6, 26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 6: [0.5329548716545105, 1.3844490051269531, -1.123403787612915, -1.6522526741027832, 1.1459019184112549]\n",
      "car at index 26: [-0.5000650882720947, 2.840832471847534, 0.3368627429008484, -2.9779772758483887, -2.018280029296875]\n",
      "Grand sum of 987 tensor sets is: [389.0833740234375, 1608.780517578125, -284.36370849609375, -411.8258361816406, -411.17535400390625]\n",
      "\n",
      "Instance 1314 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1315 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [127]\n",
      "Size of token embeddings is torch.Size([302, 13, 768])\n",
      "Shape of summed layers is: 302 x 768\n",
      "car at index 127: [0.035781241953372955, 0.8230722546577454, 0.03700625151395798, 0.5959447622299194, -2.459000587463379]\n",
      "Grand sum of 988 tensor sets is: [389.119140625, 1609.6036376953125, -284.3266906738281, -411.2298889160156, -413.6343688964844]\n",
      "\n",
      "Instance 1316 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 19: [-0.526523232460022, 2.8466343879699707, 1.0400912761688232, -1.8356895446777344, -2.17077374458313]\n",
      "Grand sum of 989 tensor sets is: [388.5926208496094, 1612.4503173828125, -283.2865905761719, -413.0655822753906, -415.8051452636719]\n",
      "\n",
      "Instance 1317 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 14: [-0.2596132755279541, -0.23518043756484985, -0.10633734613656998, -1.6538268327713013, -1.2486006021499634]\n",
      "Grand sum of 990 tensor sets is: [388.3330078125, 1612.215087890625, -283.3929138183594, -414.71942138671875, -417.0537414550781]\n",
      "\n",
      "Instance 1318 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 9: [-0.1376855969429016, 1.1063123941421509, -0.5579023361206055, -0.14700452983379364, 0.13034462928771973]\n",
      "Grand sum of 991 tensor sets is: [388.1953125, 1613.3214111328125, -283.9508056640625, -414.8664245605469, -416.92340087890625]\n",
      "\n",
      "Instance 1319 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1320 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 11: [0.09282325208187103, 0.9941189289093018, -0.3670152425765991, 0.23920339345932007, 2.075737714767456]\n",
      "Grand sum of 992 tensor sets is: [388.28814697265625, 1614.3155517578125, -284.31781005859375, -414.6272277832031, -414.84765625]\n",
      "\n",
      "Instance 1321 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1322 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1323 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 21: [1.0376849174499512, 2.67321515083313, -0.37821057438850403, -1.2278012037277222, -1.2795058488845825]\n",
      "Grand sum of 993 tensor sets is: [389.3258361816406, 1616.98876953125, -284.6960144042969, -415.85504150390625, -416.1271667480469]\n",
      "\n",
      "Instance 1324 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 6: [0.6523661613464355, 2.069697856903076, -0.21788522601127625, -2.185405969619751, -1.6560744047164917]\n",
      "Grand sum of 994 tensor sets is: [389.97821044921875, 1619.0584716796875, -284.9139099121094, -418.0404357910156, -417.7832336425781]\n",
      "\n",
      "Instance 1325 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [1]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 1: [0.5595908164978027, 2.371506929397583, -1.2931103706359863, -0.2518446147441864, 3.221534490585327]\n",
      "Grand sum of 995 tensor sets is: [390.5378112792969, 1621.429931640625, -286.20703125, -418.2922668457031, -414.56170654296875]\n",
      "\n",
      "Instance 1326 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "car at index 18: [-0.3523855209350586, 1.2132887840270996, 0.95245361328125, -2.244250774383545, -3.18977427482605]\n",
      "Grand sum of 996 tensor sets is: [390.1854248046875, 1622.6431884765625, -285.25457763671875, -420.5365295410156, -417.7514953613281]\n",
      "\n",
      "Instance 1327 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 19: [0.4265543520450592, 1.7122856378555298, -0.541701078414917, -0.7033510208129883, -1.1685888767242432]\n",
      "Grand sum of 997 tensor sets is: [390.6119689941406, 1624.35546875, -285.7962646484375, -421.2398681640625, -418.9200744628906]\n",
      "\n",
      "Instance 1328 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1329 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 7: [0.9239763617515564, 1.0756683349609375, -0.863368809223175, -0.469124972820282, -3.0195024013519287]\n",
      "Grand sum of 998 tensor sets is: [391.53594970703125, 1625.43115234375, -286.6596374511719, -421.708984375, -421.9395751953125]\n",
      "\n",
      "Instance 1330 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1331 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 4: [0.9040496349334717, 1.67525053024292, -0.6338682770729065, -0.5116368532180786, 0.08122670650482178]\n",
      "Grand sum of 999 tensor sets is: [392.44000244140625, 1627.1064453125, -287.29351806640625, -422.2206115722656, -421.85833740234375]\n",
      "\n",
      "Instance 1332 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 21: [0.7337831258773804, 0.46761325001716614, -0.0404244065284729, 0.29265347123146057, -0.21727421879768372]\n",
      "Grand sum of 1000 tensor sets is: [393.1737976074219, 1627.5740966796875, -287.3339538574219, -421.9279479980469, -422.07562255859375]\n",
      "\n",
      "Instance 1333 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 10: [0.22223320603370667, 3.9105257987976074, -0.6351830363273621, -0.22686296701431274, 2.123501777648926]\n",
      "Grand sum of 1001 tensor sets is: [393.3960266113281, 1631.484619140625, -287.9691467285156, -422.1548156738281, -419.9521179199219]\n",
      "\n",
      "Instance 1334 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 3: [1.6029154062271118, 2.288418769836426, -0.63340163230896, -1.356737732887268, 1.0885553359985352]\n",
      "Grand sum of 1002 tensor sets is: [394.9989318847656, 1633.7730712890625, -288.6025390625, -423.5115661621094, -418.8635559082031]\n",
      "\n",
      "Instance 1335 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [82]\n",
      "Size of token embeddings is torch.Size([160, 13, 768])\n",
      "Shape of summed layers is: 160 x 768\n",
      "car at index 82: [1.864961862564087, -1.4978711605072021, -0.44513052701950073, 1.3592616319656372, 2.3579070568084717]\n",
      "Grand sum of 1003 tensor sets is: [396.8638916015625, 1632.275146484375, -289.04766845703125, -422.1523132324219, -416.5056457519531]\n",
      "\n",
      "Instance 1336 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 27: [-0.18668466806411743, 1.3058888912200928, 0.726472020149231, -1.2453277111053467, -1.452702522277832]\n",
      "Grand sum of 1004 tensor sets is: [396.6772155761719, 1633.5810546875, -288.3211975097656, -423.39764404296875, -417.9583435058594]\n",
      "\n",
      "Instance 1337 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1338 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 24: [0.3558615744113922, -0.9881496429443359, -0.2885286211967468, 1.3270171880722046, -0.07288984209299088]\n",
      "Grand sum of 1005 tensor sets is: [397.0330810546875, 1632.5928955078125, -288.6097412109375, -422.07061767578125, -418.0312194824219]\n",
      "\n",
      "Instance 1339 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 17: [0.8090295195579529, 3.061614513397217, 0.41918936371803284, -0.9864220023155212, -2.20747971534729]\n",
      "Grand sum of 1006 tensor sets is: [397.84210205078125, 1635.654541015625, -288.1905517578125, -423.0570373535156, -420.23870849609375]\n",
      "\n",
      "Instance 1340 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 49: [0.9499427080154419, 2.375368356704712, -0.7383880615234375, -1.7949724197387695, 1.4898103475570679]\n",
      "Grand sum of 1007 tensor sets is: [398.79205322265625, 1638.0299072265625, -288.928955078125, -424.8520202636719, -418.7489013671875]\n",
      "\n",
      "Instance 1341 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [81]\n",
      "Size of token embeddings is torch.Size([86, 13, 768])\n",
      "Shape of summed layers is: 86 x 768\n",
      "car at index 81: [0.7483681440353394, 1.3843562602996826, -0.8579389452934265, -2.524015426635742, 1.2083512544631958]\n",
      "Grand sum of 1008 tensor sets is: [399.5404357910156, 1639.414306640625, -289.7868957519531, -427.37603759765625, -417.5405578613281]\n",
      "\n",
      "Instance 1342 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 11: [1.519521713256836, 1.2352970838546753, -0.4386972188949585, 0.0706293135881424, -0.4488763213157654]\n",
      "Grand sum of 1009 tensor sets is: [401.0599670410156, 1640.649658203125, -290.2255859375, -427.305419921875, -417.98944091796875]\n",
      "\n",
      "Instance 1343 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1344 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "car at index 13: [1.459181785583496, 2.2505104541778564, -1.0070823431015015, -0.30788654088974, 0.019214998930692673]\n",
      "Grand sum of 1010 tensor sets is: [402.5191345214844, 1642.900146484375, -291.232666015625, -427.6133117675781, -417.97021484375]\n",
      "\n",
      "Instance 1345 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 24: [0.25826793909072876, 1.4941802024841309, 0.8171412944793701, 1.5608187913894653, -1.0423983335494995]\n",
      "Grand sum of 1011 tensor sets is: [402.77740478515625, 1644.394287109375, -290.41552734375, -426.052490234375, -419.0126037597656]\n",
      "\n",
      "Instance 1346 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 19: [0.4353679418563843, -0.2126077115535736, -0.5315333604812622, 0.025084640830755234, -1.5997912883758545]\n",
      "Grand sum of 1012 tensor sets is: [403.2127685546875, 1644.181640625, -290.9470520019531, -426.02740478515625, -420.6123962402344]\n",
      "\n",
      "Instance 1347 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1348 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 18: [0.14337846636772156, 1.4848538637161255, 0.4071459472179413, 1.8223919868469238, -2.785036563873291]\n",
      "Grand sum of 1013 tensor sets is: [403.35614013671875, 1645.66650390625, -290.5399169921875, -424.20501708984375, -423.3974304199219]\n",
      "\n",
      "Instance 1349 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "car at index 32: [-0.5407795906066895, 1.5916558504104614, -0.32467329502105713, 3.3958306312561035, 0.33975106477737427]\n",
      "Grand sum of 1014 tensor sets is: [402.81536865234375, 1647.2581787109375, -290.8645935058594, -420.8091735839844, -423.05767822265625]\n",
      "\n",
      "Instance 1350 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 8: [-0.6332905292510986, 2.394123077392578, 0.9074233770370483, 0.026370584964752197, -1.0987924337387085]\n",
      "Grand sum of 1015 tensor sets is: [402.18206787109375, 1649.65234375, -289.9571838378906, -420.7828063964844, -424.1564636230469]\n",
      "\n",
      "Instance 1351 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 6: [-0.799343466758728, 1.2778387069702148, 0.3513734042644501, 1.2824667692184448, -2.139843702316284]\n",
      "Grand sum of 1016 tensor sets is: [401.3827209472656, 1650.93017578125, -289.6058044433594, -419.5003356933594, -426.2962951660156]\n",
      "\n",
      "Instance 1352 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 4: [0.7768595814704895, 2.57912278175354, -1.1687357425689697, -2.3046820163726807, 0.6446062922477722]\n",
      "Grand sum of 1017 tensor sets is: [402.1595764160156, 1653.50927734375, -290.7745361328125, -421.8050231933594, -425.6517028808594]\n",
      "\n",
      "Instance 1353 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 2: [0.08032821118831635, 1.5499165058135986, 0.6778404712677002, -3.160261392593384, -0.40191134810447693]\n",
      "Grand sum of 1018 tensor sets is: [402.2398986816406, 1655.0592041015625, -290.0967102050781, -424.96527099609375, -426.0536193847656]\n",
      "\n",
      "Instance 1354 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1355 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 11: [0.24463717639446259, 2.214959144592285, -1.072670578956604, -0.8665768504142761, -1.5548979043960571]\n",
      "Grand sum of 1019 tensor sets is: [402.4845275878906, 1657.274169921875, -291.16937255859375, -425.83184814453125, -427.6085205078125]\n",
      "\n",
      "Instance 1356 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "car at index 22: [2.018671751022339, -2.5852346420288086, -0.37044113874435425, 0.31662535667419434, 1.2845733165740967]\n",
      "Grand sum of 1020 tensor sets is: [404.5032043457031, 1654.68896484375, -291.5398254394531, -425.5152282714844, -426.3239440917969]\n",
      "\n",
      "Instance 1357 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 3: [0.5680574178695679, 0.4970622658729553, -1.2580536603927612, -1.749586820602417, 0.38060325384140015]\n",
      "Grand sum of 1021 tensor sets is: [405.0712585449219, 1655.18603515625, -292.7978820800781, -427.2648010253906, -425.9433288574219]\n",
      "\n",
      "Instance 1358 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1359 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 37: [0.8535083532333374, 1.3224271535873413, -1.3256492614746094, -0.19509489834308624, -3.503246307373047]\n",
      "Grand sum of 1022 tensor sets is: [405.9247741699219, 1656.5084228515625, -294.12353515625, -427.45989990234375, -429.4465637207031]\n",
      "\n",
      "Instance 1360 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 8: [0.29519203305244446, 2.491481065750122, -1.1077499389648438, -2.1263041496276855, -0.38519787788391113]\n",
      "Grand sum of 1023 tensor sets is: [406.219970703125, 1658.9998779296875, -295.2312927246094, -429.5862121582031, -429.8317565917969]\n",
      "\n",
      "Instance 1361 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 9: [-0.1293819695711136, 3.0309886932373047, 0.670059323310852, -1.099441647529602, -2.203136920928955]\n",
      "Grand sum of 1024 tensor sets is: [406.090576171875, 1662.0308837890625, -294.56121826171875, -430.6856689453125, -432.0348815917969]\n",
      "\n",
      "Instance 1362 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 18: [-0.27400535345077515, 2.6473727226257324, 0.6341058611869812, -0.4092217683792114, -3.370718479156494]\n",
      "Grand sum of 1025 tensor sets is: [405.8165588378906, 1664.67822265625, -293.9271240234375, -431.0948791503906, -435.4056091308594]\n",
      "\n",
      "Instance 1363 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 23: [0.38698744773864746, 1.363505244255066, -0.9206731915473938, -0.4651380479335785, -0.10253225266933441]\n",
      "Grand sum of 1026 tensor sets is: [406.20355224609375, 1666.041748046875, -294.8478088378906, -431.5600280761719, -435.5081481933594]\n",
      "\n",
      "Instance 1364 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 8: [0.08101996779441833, 2.3075382709503174, -0.48983171582221985, 0.3566422462463379, -1.4253809452056885]\n",
      "Grand sum of 1027 tensor sets is: [406.2845764160156, 1668.3492431640625, -295.337646484375, -431.2033996582031, -436.93353271484375]\n",
      "\n",
      "Instance 1365 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [90]\n",
      "Size of token embeddings is torch.Size([115, 13, 768])\n",
      "Shape of summed layers is: 115 x 768\n",
      "car at index 90: [0.16143934428691864, 0.10513054579496384, -1.1478967666625977, 0.6176873445510864, 4.668933391571045]\n",
      "Grand sum of 1028 tensor sets is: [406.4460144042969, 1668.454345703125, -296.48553466796875, -430.5857238769531, -432.26458740234375]\n",
      "\n",
      "Instance 1366 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 14: [-0.3880675137042999, 1.2986741065979004, 0.6024036407470703, -2.2231645584106445, -0.9994584321975708]\n",
      "Grand sum of 1029 tensor sets is: [406.0579528808594, 1669.7530517578125, -295.88311767578125, -432.80889892578125, -433.2640380859375]\n",
      "\n",
      "Instance 1367 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18, 26]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 18: [-0.0934733971953392, 1.7446225881576538, -0.42599809169769287, -1.3394391536712646, -1.9429144859313965]\n",
      "car at index 26: [-0.25350525975227356, 2.834381580352783, 1.5839580297470093, -2.9169600009918213, -3.4581046104431152]\n",
      "Grand sum of 1030 tensor sets is: [405.88446044921875, 1672.0426025390625, -295.30413818359375, -434.9371032714844, -435.96453857421875]\n",
      "\n",
      "Instance 1368 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1369 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1370 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 19: [1.4182071685791016, 2.5421290397644043, 0.3609886169433594, -0.14181123673915863, -1.6907546520233154]\n",
      "Grand sum of 1031 tensor sets is: [407.30267333984375, 1674.584716796875, -294.9431457519531, -435.07891845703125, -437.6553039550781]\n",
      "\n",
      "Instance 1371 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "car at index 20: [0.6974893808364868, 1.6126532554626465, -1.225385069847107, -1.040083885192871, 4.178042411804199]\n",
      "Grand sum of 1032 tensor sets is: [408.0001525878906, 1676.1973876953125, -296.16851806640625, -436.1189880371094, -433.4772644042969]\n",
      "\n",
      "Instance 1372 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1373 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [64]\n",
      "Size of token embeddings is torch.Size([107, 13, 768])\n",
      "Shape of summed layers is: 107 x 768\n",
      "car at index 64: [0.4332647919654846, 1.070173740386963, -0.8447754979133606, -0.5422810316085815, 0.605798065662384]\n",
      "Grand sum of 1033 tensor sets is: [408.43341064453125, 1677.267578125, -297.0133056640625, -436.6612548828125, -432.8714599609375]\n",
      "\n",
      "Instance 1374 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 6: [0.6575602889060974, 1.1479915380477905, -0.4797360599040985, 1.7155265808105469, -1.2720470428466797]\n",
      "Grand sum of 1034 tensor sets is: [409.0909729003906, 1678.41552734375, -297.4930419921875, -434.94573974609375, -434.14349365234375]\n",
      "\n",
      "Instance 1375 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 14: [-0.25993990898132324, 3.0347137451171875, 0.4189684987068176, -0.2655287981033325, -3.34912109375]\n",
      "Grand sum of 1035 tensor sets is: [408.8310241699219, 1681.4501953125, -297.0740661621094, -435.2112731933594, -437.49261474609375]\n",
      "\n",
      "Instance 1376 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 5: [0.6512555480003357, 1.8674588203430176, -0.527203381061554, -0.3012015223503113, -1.1491693258285522]\n",
      "Grand sum of 1036 tensor sets is: [409.4822692871094, 1683.317626953125, -297.60125732421875, -435.5124816894531, -438.64178466796875]\n",
      "\n",
      "Instance 1377 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 9: [-0.7169203162193298, 1.5978071689605713, -0.7575597763061523, 0.21043704450130463, -1.1417158842086792]\n",
      "Grand sum of 1037 tensor sets is: [408.7653503417969, 1684.9154052734375, -298.35882568359375, -435.3020324707031, -439.78350830078125]\n",
      "\n",
      "Instance 1378 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 5: [-0.2308293879032135, 1.1545833349227905, -0.8493346571922302, -0.9771389365196228, -1.6383073329925537]\n",
      "Grand sum of 1038 tensor sets is: [408.5345153808594, 1686.0699462890625, -299.2081604003906, -436.2791748046875, -441.42181396484375]\n",
      "\n",
      "Instance 1379 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 11: [-0.5712490081787109, 1.7486186027526855, -1.3120245933532715, -2.099252700805664, -0.9436187744140625]\n",
      "car at index 15: [-1.1396976709365845, 2.8101768493652344, 0.44345545768737793, -2.2842137813568115, -3.984623908996582]\n",
      "Grand sum of 1039 tensor sets is: [407.6790466308594, 1688.349365234375, -299.6424560546875, -438.4709167480469, -443.88592529296875]\n",
      "\n",
      "Instance 1380 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 22: [0.8971348404884338, 1.5998529195785522, -0.9195781350135803, -0.8451775908470154, 2.0612592697143555]\n",
      "Grand sum of 1040 tensor sets is: [408.576171875, 1689.94921875, -300.5620422363281, -439.31610107421875, -441.8246765136719]\n",
      "\n",
      "Instance 1381 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 4: [0.6358008980751038, 1.417345404624939, -0.6364266872406006, 1.6854945421218872, -4.223500728607178]\n",
      "Grand sum of 1041 tensor sets is: [409.21197509765625, 1691.3665771484375, -301.1984558105469, -437.630615234375, -446.0481872558594]\n",
      "\n",
      "Instance 1382 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1383 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 5: [-0.19675569236278534, 1.1162924766540527, -1.3369150161743164, -1.1914494037628174, -0.9717962741851807]\n",
      "Grand sum of 1042 tensor sets is: [409.0152282714844, 1692.48291015625, -302.5353698730469, -438.8220520019531, -447.0199890136719]\n",
      "\n",
      "Instance 1384 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [58]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "car at index 58: [0.9375700354576111, 1.6495327949523926, 0.3951617181301117, -2.072608232498169, 0.25216928124427795]\n",
      "Grand sum of 1043 tensor sets is: [409.9527893066406, 1694.1324462890625, -302.14019775390625, -440.8946533203125, -446.767822265625]\n",
      "\n",
      "Instance 1385 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 9: [0.24571508169174194, 2.0189361572265625, -0.17877192795276642, -0.5346785187721252, -1.4002090692520142]\n",
      "Grand sum of 1044 tensor sets is: [410.1985168457031, 1696.1513671875, -302.3189697265625, -441.4293212890625, -448.16802978515625]\n",
      "\n",
      "Instance 1386 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 14: [0.5434224009513855, 2.08260440826416, -0.41278648376464844, -1.1372212171554565, -1.2093340158462524]\n",
      "Grand sum of 1045 tensor sets is: [410.741943359375, 1698.2340087890625, -302.73175048828125, -442.5665283203125, -449.3773498535156]\n",
      "\n",
      "Instance 1387 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 6: [-0.11542820930480957, 1.1340031623840332, -0.3936815857887268, -1.5111501216888428, -1.5061334371566772]\n",
      "Grand sum of 1046 tensor sets is: [410.62652587890625, 1699.3680419921875, -303.12542724609375, -444.0776672363281, -450.88348388671875]\n",
      "\n",
      "Instance 1388 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 5: [0.17165660858154297, 0.8535754680633545, 0.1754821538925171, -3.254220724105835, -0.570440411567688]\n",
      "Grand sum of 1047 tensor sets is: [410.7981872558594, 1700.2215576171875, -302.949951171875, -447.3318786621094, -451.45391845703125]\n",
      "\n",
      "Instance 1389 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "car at index 25: [0.940932035446167, 2.5493602752685547, -0.33026137948036194, -2.5766963958740234, -2.5944998264312744]\n",
      "Grand sum of 1048 tensor sets is: [411.7391052246094, 1702.7708740234375, -303.28021240234375, -449.9085693359375, -454.0484313964844]\n",
      "\n",
      "Instance 1390 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 9: [1.049821376800537, 3.1345512866973877, -0.43558356165885925, -1.332920789718628, 1.3773143291473389]\n",
      "Grand sum of 1049 tensor sets is: [412.7889404296875, 1705.9053955078125, -303.7157897949219, -451.2414855957031, -452.6711120605469]\n",
      "\n",
      "Instance 1391 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 7: [-0.5650343894958496, 2.178339719772339, 0.3587284982204437, 1.6402734518051147, -1.8830146789550781]\n",
      "Grand sum of 1050 tensor sets is: [412.2239074707031, 1708.083740234375, -303.3570556640625, -449.6012268066406, -454.55413818359375]\n",
      "\n",
      "Instance 1392 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 10: [-0.7428777813911438, 4.7947916984558105, -0.5314459204673767, 2.9498538970947266, 0.7489922046661377]\n",
      "Grand sum of 1051 tensor sets is: [411.48101806640625, 1712.8785400390625, -303.88848876953125, -446.6513671875, -453.8051452636719]\n",
      "\n",
      "Instance 1393 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 25: [0.37636417150497437, 0.6758548021316528, -0.30948957800865173, -1.2986572980880737, -1.5654077529907227]\n",
      "Grand sum of 1052 tensor sets is: [411.8573913574219, 1713.554443359375, -304.1979675292969, -447.95001220703125, -455.37054443359375]\n",
      "\n",
      "Instance 1394 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1395 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1396 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1397 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [61]\n",
      "Size of token embeddings is torch.Size([496, 13, 768])\n",
      "Shape of summed layers is: 496 x 768\n",
      "car at index 61: [0.7709801197052002, 0.6144729852676392, 0.4550432860851288, 0.11089962720870972, 1.4750975370407104]\n",
      "Grand sum of 1053 tensor sets is: [412.62835693359375, 1714.1689453125, -303.742919921875, -447.839111328125, -453.89544677734375]\n",
      "\n",
      "Instance 1398 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1399 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 39: [0.5787360668182373, 1.7814347743988037, -1.2300578355789185, -1.4919240474700928, 0.592222273349762]\n",
      "Grand sum of 1054 tensor sets is: [413.20709228515625, 1715.950439453125, -304.9729919433594, -449.3310241699219, -453.30322265625]\n",
      "\n",
      "Instance 1400 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1401 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1402 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1403 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 35: [0.20746445655822754, 1.468298077583313, -0.06895758211612701, 0.33565497398376465, -4.319455146789551]\n",
      "Grand sum of 1055 tensor sets is: [413.41455078125, 1717.418701171875, -305.0419616699219, -448.995361328125, -457.6226806640625]\n",
      "\n",
      "Instance 1404 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 24: [0.015575990080833435, 2.1785802841186523, 0.8734874725341797, 2.1501927375793457, -2.308579683303833]\n",
      "Grand sum of 1056 tensor sets is: [413.43011474609375, 1719.5972900390625, -304.1684875488281, -446.84515380859375, -459.9312744140625]\n",
      "\n",
      "Instance 1405 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 2: [-0.5602876543998718, 2.2890312671661377, 0.21307384967803955, -0.6744570136070251, -1.8474830389022827]\n",
      "Grand sum of 1057 tensor sets is: [412.86981201171875, 1721.8863525390625, -303.9554138183594, -447.5196228027344, -461.77874755859375]\n",
      "\n",
      "Instance 1406 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1407 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1408 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 18: [-0.11343228816986084, 1.18161940574646, -1.1620750427246094, -0.19464872777462006, -1.1667084693908691]\n",
      "Grand sum of 1058 tensor sets is: [412.7563781738281, 1723.0679931640625, -305.11749267578125, -447.7142639160156, -462.9454650878906]\n",
      "\n",
      "Instance 1409 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 5: [-0.8874288201332092, 3.5882961750030518, -0.33625587821006775, -0.2945515513420105, -1.7816718816757202]\n",
      "Grand sum of 1059 tensor sets is: [411.86895751953125, 1726.65625, -305.4537353515625, -448.0088195800781, -464.7271423339844]\n",
      "\n",
      "Instance 1410 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1411 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1412 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 19: [0.6810255646705627, 2.264272689819336, -0.11019167304039001, -1.9809508323669434, -0.5875571370124817]\n",
      "Grand sum of 1060 tensor sets is: [412.54998779296875, 1728.9205322265625, -305.5639343261719, -449.9897766113281, -465.314697265625]\n",
      "\n",
      "Instance 1413 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1414 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4, 44, 55]\n",
      "Size of token embeddings is torch.Size([82, 13, 768])\n",
      "Shape of summed layers is: 82 x 768\n",
      "car at index 4: [0.2671714425086975, 1.3998100757598877, -1.0467655658721924, -3.7418408393859863, -0.107611745595932]\n",
      "car at index 44: [0.22134970128536224, 2.6244559288024902, -0.6423049569129944, -3.0227065086364746, -2.374337911605835]\n",
      "car at index 55: [0.7853505611419678, 1.180891990661621, -0.846541166305542, -3.45780611038208, -2.23190975189209]\n",
      "Grand sum of 1061 tensor sets is: [412.974609375, 1730.6556396484375, -306.4091491699219, -453.397216796875, -466.885986328125]\n",
      "\n",
      "Instance 1415 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1416 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 21: [0.957948625087738, 0.9216497540473938, -0.29232221841812134, -1.149204969406128, -0.07906274497509003]\n",
      "Grand sum of 1062 tensor sets is: [413.93255615234375, 1731.5772705078125, -306.70147705078125, -454.5464172363281, -466.9650573730469]\n",
      "\n",
      "Instance 1417 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 3: [-0.18694721162319183, 1.843418002128601, -1.070474624633789, -2.2402212619781494, 0.04379785805940628]\n",
      "Grand sum of 1063 tensor sets is: [413.74560546875, 1733.420654296875, -307.7719421386719, -456.7866516113281, -466.9212646484375]\n",
      "\n",
      "Instance 1418 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2, 10, 18]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 2: [1.0201207399368286, 1.949736475944519, -0.8594666719436646, -0.6903347969055176, -0.8508756756782532]\n",
      "car at index 10: [0.8493500351905823, 2.5152645111083984, -0.28440791368484497, -0.9329257011413574, -1.5095127820968628]\n",
      "car at index 18: [0.8592740297317505, 2.8490023612976074, 0.1243072897195816, -0.8897411823272705, -2.056320905685425]\n",
      "Grand sum of 1064 tensor sets is: [414.6551818847656, 1735.858642578125, -308.1117858886719, -457.62432861328125, -468.39349365234375]\n",
      "\n",
      "Instance 1419 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1420 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1421 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 459: [-0.4652632772922516, 2.773749828338623, 1.3511382341384888, -0.04579386115074158, -0.5501638650894165]\n",
      "Grand sum of 1065 tensor sets is: [414.1899108886719, 1738.6324462890625, -306.7606506347656, -457.6701354980469, -468.94366455078125]\n",
      "\n",
      "Instance 1422 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1423 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([75, 13, 768])\n",
      "Shape of summed layers is: 75 x 768\n",
      "car at index 72: [0.33823543787002563, 0.626387357711792, -0.525968611240387, 1.22672438621521, 2.551112651824951]\n",
      "Grand sum of 1066 tensor sets is: [414.52813720703125, 1739.2587890625, -307.28662109375, -456.44342041015625, -466.3925476074219]\n",
      "\n",
      "Instance 1424 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1425 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "car at index 49: [0.741061806678772, 3.0523250102996826, -0.7902631163597107, -0.3684437870979309, -1.04435133934021]\n",
      "Grand sum of 1067 tensor sets is: [415.2691955566406, 1742.3111572265625, -308.0768737792969, -456.8118591308594, -467.4368896484375]\n",
      "\n",
      "Instance 1426 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 27: [1.0585591793060303, 2.0579118728637695, -0.5080869197845459, 1.738524079322815, -1.8689407110214233]\n",
      "Grand sum of 1068 tensor sets is: [416.3277587890625, 1744.3690185546875, -308.5849609375, -455.0733337402344, -469.3058166503906]\n",
      "\n",
      "Instance 1427 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1428 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "car at index 12: [1.2426866292953491, -1.1085999011993408, -0.1658748984336853, 2.0489370822906494, 1.4947630167007446]\n",
      "Grand sum of 1069 tensor sets is: [417.5704345703125, 1743.2603759765625, -308.7508239746094, -453.0243835449219, -467.8110656738281]\n",
      "\n",
      "Instance 1429 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [261]\n",
      "Size of token embeddings is torch.Size([263, 13, 768])\n",
      "Shape of summed layers is: 263 x 768\n",
      "car at index 261: [0.7077099084854126, 3.177243709564209, -0.5958161950111389, -1.7887119054794312, 0.3319673538208008]\n",
      "Grand sum of 1070 tensor sets is: [418.27813720703125, 1746.4376220703125, -309.3466491699219, -454.8131103515625, -467.4790954589844]\n",
      "\n",
      "Instance 1430 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1431 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "car at index 2: [-0.5275874137878418, 1.4808013439178467, -0.05511040985584259, 0.06082453951239586, -1.1581735610961914]\n",
      "Grand sum of 1071 tensor sets is: [417.75054931640625, 1747.91845703125, -309.4017639160156, -454.7522888183594, -468.63726806640625]\n",
      "\n",
      "Instance 1432 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 20]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 11: [0.4627265930175781, 1.7150366306304932, -0.40817490220069885, -0.473058819770813, -2.0588090419769287]\n",
      "car at index 20: [0.11928894370794296, 1.1982465982437134, -1.2851619720458984, 0.49875617027282715, -2.514434337615967]\n",
      "Grand sum of 1072 tensor sets is: [418.04156494140625, 1749.3751220703125, -310.2484436035156, -454.73944091796875, -470.92388916015625]\n",
      "\n",
      "Instance 1433 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 12: [0.587202250957489, 0.8294914960861206, -0.7429020404815674, 0.5193163156509399, 1.060971736907959]\n",
      "Grand sum of 1073 tensor sets is: [418.6287536621094, 1750.20458984375, -310.9913330078125, -454.2201232910156, -469.8629150390625]\n",
      "\n",
      "Instance 1434 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1435 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 9: [-0.18220429122447968, 1.5571668148040771, 0.9847683310508728, -1.7754048109054565, -1.9173462390899658]\n",
      "Grand sum of 1074 tensor sets is: [418.4465637207031, 1751.76171875, -310.0065612792969, -455.9955139160156, -471.7802734375]\n",
      "\n",
      "Instance 1436 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 16: [0.19470201432704926, 0.30183517932891846, -0.8482152223587036, -0.5530477166175842, 0.5538218021392822]\n",
      "Grand sum of 1075 tensor sets is: [418.6412658691406, 1752.0635986328125, -310.8547668457031, -456.5485534667969, -471.2264404296875]\n",
      "\n",
      "Instance 1437 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 8: [-0.12755924463272095, 2.4078304767608643, 0.05130793899297714, -1.2937109470367432, -0.9431477785110474]\n",
      "Grand sum of 1076 tensor sets is: [418.5137023925781, 1754.471435546875, -310.803466796875, -457.8422546386719, -472.1695861816406]\n",
      "\n",
      "Instance 1438 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 3: [1.2475571632385254, 1.0343050956726074, -0.5926072597503662, -2.250967025756836, 0.06597819924354553]\n",
      "Grand sum of 1077 tensor sets is: [419.7612609863281, 1755.5057373046875, -311.3960876464844, -460.0932312011719, -472.1036071777344]\n",
      "\n",
      "Instance 1439 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 18: [0.15309110283851624, 0.5711170434951782, -0.4033070206642151, -0.8674005270004272, 1.540571689605713]\n",
      "Grand sum of 1078 tensor sets is: [419.9143371582031, 1756.076904296875, -311.7994079589844, -460.96063232421875, -470.56304931640625]\n",
      "\n",
      "Instance 1440 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 4: [-0.4948687255382538, 1.961804747581482, -0.3784745931625366, 0.6145002841949463, -2.0508475303649902]\n",
      "Grand sum of 1079 tensor sets is: [419.4194641113281, 1758.0386962890625, -312.1778869628906, -460.34613037109375, -472.6138916015625]\n",
      "\n",
      "Instance 1441 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1442 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1443 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 41: [0.8523919582366943, 0.9183406829833984, -1.0208271741867065, -0.4030054807662964, -1.2931737899780273]\n",
      "Grand sum of 1080 tensor sets is: [420.2718505859375, 1758.95703125, -313.1986999511719, -460.7491455078125, -473.9070739746094]\n",
      "\n",
      "Instance 1444 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 3: [1.1728765964508057, 1.188278317451477, 0.48746615648269653, -1.4753448963165283, 1.2715659141540527]\n",
      "Grand sum of 1081 tensor sets is: [421.4447326660156, 1760.145263671875, -312.71124267578125, -462.2244873046875, -472.635498046875]\n",
      "\n",
      "Instance 1445 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1446 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [52, 83]\n",
      "Size of token embeddings is torch.Size([120, 13, 768])\n",
      "Shape of summed layers is: 120 x 768\n",
      "car at index 52: [0.3395511209964752, 1.9404219388961792, -0.8169178366661072, -2.915989398956299, 1.076246976852417]\n",
      "car at index 83: [0.556155800819397, 1.917404294013977, -1.7434736490249634, -0.6942060589790344, -1.0428580045700073]\n",
      "Grand sum of 1082 tensor sets is: [421.892578125, 1762.07421875, -313.9914245605469, -464.0295715332031, -472.6188049316406]\n",
      "\n",
      "Instance 1447 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 18: [0.015015100128948689, 2.085437297821045, -0.6932209134101868, -0.7151398658752441, -1.861816644668579]\n",
      "Grand sum of 1083 tensor sets is: [421.9075927734375, 1764.15966796875, -314.68463134765625, -464.7447204589844, -474.4806213378906]\n",
      "\n",
      "Instance 1448 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 18: [0.41854387521743774, 1.902888536453247, 0.6239879727363586, -1.4997923374176025, -2.5629920959472656]\n",
      "Grand sum of 1084 tensor sets is: [422.3261413574219, 1766.0625, -314.0606384277344, -466.2445068359375, -477.0436096191406]\n",
      "\n",
      "Instance 1449 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 12: [0.3841753900051117, 1.6386327743530273, -0.14944973587989807, -1.0005924701690674, 0.5171059370040894]\n",
      "Grand sum of 1085 tensor sets is: [422.7103271484375, 1767.701171875, -314.2100830078125, -467.2450866699219, -476.5264892578125]\n",
      "\n",
      "Instance 1450 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1451 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1452 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 7: [0.1593928039073944, 2.0497043132781982, -0.5016520619392395, 1.537293791770935, -4.069248676300049]\n",
      "Grand sum of 1086 tensor sets is: [422.8697204589844, 1769.7508544921875, -314.71173095703125, -465.7077941894531, -480.5957336425781]\n",
      "\n",
      "Instance 1453 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10, 29]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 10: [0.6656649112701416, 1.3792531490325928, -0.5840100646018982, 1.808542251586914, -1.8128455877304077]\n",
      "car at index 29: [1.0589494705200195, 1.287179946899414, 0.40476685762405396, 1.3626540899276733, -2.21926212310791]\n",
      "Grand sum of 1087 tensor sets is: [423.7320251464844, 1771.0841064453125, -314.8013610839844, -464.1221923828125, -482.6117858886719]\n",
      "\n",
      "Instance 1454 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 8: [0.3977265954017639, 0.4729987382888794, -0.8435055613517761, -0.21571139991283417, -2.66347599029541]\n",
      "Grand sum of 1088 tensor sets is: [424.1297607421875, 1771.55712890625, -315.6448669433594, -464.337890625, -485.2752685546875]\n",
      "\n",
      "Instance 1455 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 13: [0.6628025770187378, 1.8939180374145508, -0.24122744798660278, 0.4627454876899719, 0.09254324436187744]\n",
      "Grand sum of 1089 tensor sets is: [424.7925720214844, 1773.4510498046875, -315.8861083984375, -463.8751525878906, -485.1827392578125]\n",
      "\n",
      "Instance 1456 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 1090 tensor sets is: [426.91021728515625, 1775.12744140625, -317.1203918457031, -465.5406799316406, -485.3272399902344]\n",
      "\n",
      "Instance 1457 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 12: [0.14803427457809448, 2.3478031158447266, -1.2417428493499756, 1.0036568641662598, -0.022236905992031097]\n",
      "Grand sum of 1091 tensor sets is: [427.0582580566406, 1777.4752197265625, -318.36212158203125, -464.5370178222656, -485.3494873046875]\n",
      "\n",
      "Instance 1458 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1459 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 8: [0.6365223526954651, 0.7604804635047913, -1.0134289264678955, -2.273601531982422, -0.4715498089790344]\n",
      "Grand sum of 1092 tensor sets is: [427.6947937011719, 1778.2357177734375, -319.37554931640625, -466.81060791015625, -485.821044921875]\n",
      "\n",
      "Instance 1460 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 27: [1.3654038906097412, -0.9638739824295044, -1.050445795059204, 1.0763649940490723, 0.5657666325569153]\n",
      "Grand sum of 1093 tensor sets is: [429.0602111816406, 1777.2718505859375, -320.4259948730469, -465.7342529296875, -485.2552795410156]\n",
      "\n",
      "Instance 1461 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1462 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 14: [0.6750843524932861, 1.9779300689697266, -0.22484467923641205, -1.3871246576309204, -1.2044676542282104]\n",
      "Grand sum of 1094 tensor sets is: [429.73529052734375, 1779.249755859375, -320.6508483886719, -467.1213684082031, -486.4597473144531]\n",
      "\n",
      "Instance 1463 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1464 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 12: [0.5834245681762695, 1.2518649101257324, -0.466291218996048, -1.9315624237060547, 0.23813524842262268]\n",
      "Grand sum of 1095 tensor sets is: [430.3187255859375, 1780.5015869140625, -321.11712646484375, -469.05291748046875, -486.22161865234375]\n",
      "\n",
      "Instance 1465 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 12: [-0.39383888244628906, -0.6234116554260254, 0.4150824546813965, -0.1467207372188568, 1.5575029850006104]\n",
      "Grand sum of 1096 tensor sets is: [429.9248962402344, 1779.878173828125, -320.7020568847656, -469.19964599609375, -484.66412353515625]\n",
      "\n",
      "Instance 1466 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 25: [0.6134775280952454, -0.9530048966407776, -1.2392207384109497, -0.005405731499195099, 1.8704521656036377]\n",
      "Grand sum of 1097 tensor sets is: [430.5383605957031, 1778.9251708984375, -321.9412841796875, -469.2050476074219, -482.7936706542969]\n",
      "\n",
      "Instance 1467 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 9: [-0.8285285234451294, 1.5629302263259888, 0.054517436772584915, -0.42096325755119324, -0.15514566004276276]\n",
      "Grand sum of 1098 tensor sets is: [429.7098388671875, 1780.4881591796875, -321.88677978515625, -469.6260070800781, -482.9488220214844]\n",
      "\n",
      "Instance 1468 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1469 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "car at index 44: [-0.5170104503631592, 0.912129282951355, -0.4226067066192627, 2.4319112300872803, -2.062164306640625]\n",
      "Grand sum of 1099 tensor sets is: [429.1928405761719, 1781.4002685546875, -322.30938720703125, -467.194091796875, -485.010986328125]\n",
      "\n",
      "Instance 1470 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 6: [0.17233455181121826, 2.4332621097564697, -0.11954531073570251, 0.45217254757881165, -1.501720666885376]\n",
      "Grand sum of 1100 tensor sets is: [429.36517333984375, 1783.83349609375, -322.4289245605469, -466.7419128417969, -486.5126953125]\n",
      "\n",
      "Instance 1471 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [55]\n",
      "Size of token embeddings is torch.Size([78, 13, 768])\n",
      "Shape of summed layers is: 78 x 768\n",
      "car at index 55: [-0.05758333206176758, 2.3187785148620605, -1.3301949501037598, -2.5946762561798096, -0.8799123764038086]\n",
      "Grand sum of 1101 tensor sets is: [429.3075866699219, 1786.1522216796875, -323.7591247558594, -469.3365783691406, -487.3926086425781]\n",
      "\n",
      "Instance 1472 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1473 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 4: [0.26584893465042114, 0.4960884749889374, 0.26009151339530945, 2.9183483123779297, -2.7349019050598145]\n",
      "Grand sum of 1102 tensor sets is: [429.57342529296875, 1786.6483154296875, -323.4990234375, -466.4182434082031, -490.12750244140625]\n",
      "\n",
      "Instance 1474 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "car at index 34: [0.6642407774925232, -1.0643956661224365, -0.6634783148765564, 1.5157482624053955, 1.5386241674423218]\n",
      "Grand sum of 1103 tensor sets is: [430.2376708984375, 1785.5838623046875, -324.1625061035156, -464.9024963378906, -488.5888671875]\n",
      "\n",
      "Instance 1475 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 21: [0.670086145401001, 2.437762498855591, 0.5942931175231934, -0.4071103632450104, -0.1711660772562027]\n",
      "Grand sum of 1104 tensor sets is: [430.9077453613281, 1788.0216064453125, -323.5682067871094, -465.3096008300781, -488.7600402832031]\n",
      "\n",
      "Instance 1476 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1477 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 4: [0.20266585052013397, 1.817635178565979, -0.4191519021987915, -1.447068214416504, 0.03199945390224457]\n",
      "Grand sum of 1105 tensor sets is: [431.11041259765625, 1789.8392333984375, -323.98736572265625, -466.7566833496094, -488.72802734375]\n",
      "\n",
      "Instance 1478 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 36: [0.383561372756958, 1.9685941934585571, 0.5448733568191528, -2.426558494567871, -1.1485029458999634]\n",
      "Grand sum of 1106 tensor sets is: [431.4939880371094, 1791.807861328125, -323.4425048828125, -469.1832275390625, -489.87652587890625]\n",
      "\n",
      "Instance 1479 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([138, 13, 768])\n",
      "Shape of summed layers is: 138 x 768\n",
      "car at index 46: [0.26758188009262085, 1.0013271570205688, 1.5575612783432007, -0.8806226253509521, -3.262868642807007]\n",
      "Grand sum of 1107 tensor sets is: [431.7615661621094, 1792.8092041015625, -321.88494873046875, -470.0638427734375, -493.139404296875]\n",
      "\n",
      "Instance 1480 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 13: [0.3083495795726776, 1.9607902765274048, -0.8015985488891602, -2.5519871711730957, -0.5193443298339844]\n",
      "Grand sum of 1108 tensor sets is: [432.0699157714844, 1794.77001953125, -322.6865539550781, -472.6158447265625, -493.65875244140625]\n",
      "\n",
      "Instance 1481 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13, 22]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 13: [0.4209729731082916, 1.5662682056427002, -0.8645517826080322, -1.4047132730484009, -0.7031136751174927]\n",
      "car at index 22: [0.808322012424469, 1.467612385749817, -0.30965155363082886, -1.8718229532241821, -0.43663039803504944]\n",
      "Grand sum of 1109 tensor sets is: [432.6845703125, 1796.2869873046875, -323.2736511230469, -474.2541198730469, -494.2286376953125]\n",
      "\n",
      "Instance 1482 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [223]\n",
      "Size of token embeddings is torch.Size([390, 13, 768])\n",
      "Shape of summed layers is: 390 x 768\n",
      "car at index 223: [1.24264395236969, 0.15786321461200714, -0.4466247260570526, -1.9167227745056152, 1.4927589893341064]\n",
      "Grand sum of 1110 tensor sets is: [433.9272155761719, 1796.44482421875, -323.72027587890625, -476.17083740234375, -492.7358703613281]\n",
      "\n",
      "Instance 1483 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 16: [0.7914508581161499, 1.5286166667938232, -0.9789313673973083, 0.15779387950897217, -0.2979107201099396]\n",
      "Grand sum of 1111 tensor sets is: [434.7186584472656, 1797.973388671875, -324.69921875, -476.0130310058594, -493.0337829589844]\n",
      "\n",
      "Instance 1484 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 5: [0.6163627505302429, 3.0398199558258057, -0.7879305481910706, -0.3102838695049286, -0.7691872715950012]\n",
      "Grand sum of 1112 tensor sets is: [435.33502197265625, 1801.01318359375, -325.4871520996094, -476.32330322265625, -493.802978515625]\n",
      "\n",
      "Instance 1485 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 11: [0.6311049461364746, 2.2901525497436523, -0.08812078088521957, 2.436300754547119, 0.14137202501296997]\n",
      "Grand sum of 1113 tensor sets is: [435.96612548828125, 1803.3033447265625, -325.5752868652344, -473.8869934082031, -493.66162109375]\n",
      "\n",
      "Instance 1486 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 5: [0.1461038887500763, 2.4098711013793945, -0.8626104593276978, -0.530426561832428, -1.3578873872756958]\n",
      "Grand sum of 1114 tensor sets is: [436.11224365234375, 1805.7132568359375, -326.4378967285156, -474.41741943359375, -495.0195007324219]\n",
      "\n",
      "Instance 1487 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [35, 41]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 35: [-0.15268369019031525, 0.39470356702804565, -1.4419217109680176, 0.18437039852142334, -2.8636362552642822]\n",
      "car at index 41: [-0.923523485660553, 1.3060814142227173, -0.6810199618339539, -2.155106782913208, -2.0950064659118652]\n",
      "Grand sum of 1115 tensor sets is: [435.5741271972656, 1806.5635986328125, -327.4993591308594, -475.4028015136719, -497.4988098144531]\n",
      "\n",
      "Instance 1488 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40, 45]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "car at index 40: [-0.8095459938049316, 2.733266592025757, -1.956412434577942, -0.7926996350288391, -2.245800018310547]\n",
      "car at index 45: [0.1484237015247345, 2.617934465408325, -0.6424301862716675, -1.3764568567276, -2.915928602218628]\n",
      "Grand sum of 1116 tensor sets is: [435.2435607910156, 1809.2392578125, -328.79876708984375, -476.48736572265625, -500.0796813964844]\n",
      "\n",
      "Instance 1489 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1490 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 4: [-0.7432435750961304, 1.0979571342468262, -1.4165862798690796, 0.4639458954334259, -2.162522554397583]\n",
      "Grand sum of 1117 tensor sets is: [434.50030517578125, 1810.337158203125, -330.2153625488281, -476.0234069824219, -502.2422180175781]\n",
      "\n",
      "Instance 1491 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1492 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 23: [1.1430563926696777, 1.7887359857559204, -0.3722459077835083, 0.3117537796497345, -0.675510048866272]\n",
      "Grand sum of 1118 tensor sets is: [435.64337158203125, 1812.1258544921875, -330.5876159667969, -475.7116394042969, -502.917724609375]\n",
      "\n",
      "Instance 1493 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 11: [0.1771734356880188, -0.4234974980354309, -0.7675796151161194, -0.7926471829414368, -0.17049163579940796]\n",
      "Grand sum of 1119 tensor sets is: [435.820556640625, 1811.702392578125, -331.3551940917969, -476.5042724609375, -503.0882263183594]\n",
      "\n",
      "Instance 1494 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1495 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [84]\n",
      "Size of token embeddings is torch.Size([141, 13, 768])\n",
      "Shape of summed layers is: 141 x 768\n",
      "car at index 84: [0.8337405920028687, 0.8491068482398987, -0.2782764136791229, -2.34755277633667, 1.6060127019882202]\n",
      "Grand sum of 1120 tensor sets is: [436.654296875, 1812.551513671875, -331.63348388671875, -478.8518371582031, -501.4822082519531]\n",
      "\n",
      "Instance 1496 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1497 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 5: [1.2634731531143188, 1.3137941360473633, -1.1801552772521973, -1.4206466674804688, -1.2082449197769165]\n",
      "Grand sum of 1121 tensor sets is: [437.9177551269531, 1813.8653564453125, -332.8136291503906, -480.2724914550781, -502.6904602050781]\n",
      "\n",
      "Instance 1498 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 15: [0.2732836604118347, 1.3740520477294922, -0.29384297132492065, 0.8425700068473816, -0.8421217203140259]\n",
      "Grand sum of 1122 tensor sets is: [438.1910400390625, 1815.2393798828125, -333.10748291015625, -479.429931640625, -503.5325927734375]\n",
      "\n",
      "Instance 1499 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1500 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1501 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 21: [0.4247601628303528, 1.8146169185638428, -0.7510989904403687, -2.9541800022125244, -0.2855592370033264]\n",
      "Grand sum of 1123 tensor sets is: [438.6158142089844, 1817.053955078125, -333.85858154296875, -482.3841247558594, -503.8181457519531]\n",
      "\n",
      "Instance 1502 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 7: [-0.37334346771240234, 0.5738538503646851, -0.47528940439224243, 1.9667625427246094, -2.5870895385742188]\n",
      "Grand sum of 1124 tensor sets is: [438.2424621582031, 1817.6278076171875, -334.3338623046875, -480.4173583984375, -506.4052429199219]\n",
      "\n",
      "Instance 1503 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 40: [0.4974491000175476, 1.464829921722412, -0.02411922812461853, -0.5746864080429077, -1.0142091512680054]\n",
      "Grand sum of 1125 tensor sets is: [438.7398986816406, 1819.0926513671875, -334.35797119140625, -480.9920349121094, -507.4194641113281]\n",
      "\n",
      "Instance 1504 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 28: [0.1436108648777008, 1.0054374933242798, -0.08675280213356018, -0.7029435634613037, -0.24037696421146393]\n",
      "Grand sum of 1126 tensor sets is: [438.8835144042969, 1820.09814453125, -334.4447326660156, -481.6949768066406, -507.65985107421875]\n",
      "\n",
      "Instance 1505 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 13: [0.6068465709686279, 1.9618035554885864, -0.057720016688108444, -1.2375280857086182, -1.7785131931304932]\n",
      "Grand sum of 1127 tensor sets is: [439.4903564453125, 1822.0599365234375, -334.50244140625, -482.9324951171875, -509.4383544921875]\n",
      "\n",
      "Instance 1506 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1507 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 21: [0.026048943400382996, 2.0947790145874023, 0.12075503170490265, -2.334592342376709, -1.5641191005706787]\n",
      "Grand sum of 1128 tensor sets is: [439.51641845703125, 1824.1546630859375, -334.3816833496094, -485.26708984375, -511.0024719238281]\n",
      "\n",
      "Instance 1508 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([103, 13, 768])\n",
      "Shape of summed layers is: 103 x 768\n",
      "car at index 50: [0.166998028755188, 2.149127960205078, 0.7182297110557556, 1.330775499343872, 5.752427101135254]\n",
      "Grand sum of 1129 tensor sets is: [439.68341064453125, 1826.3038330078125, -333.6634521484375, -483.9363098144531, -505.2500305175781]\n",
      "\n",
      "Instance 1509 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 18: [0.5035401582717896, 2.3821916580200195, -1.0780829191207886, -0.9311271905899048, -2.863330841064453]\n",
      "Grand sum of 1130 tensor sets is: [440.18695068359375, 1828.68603515625, -334.7415466308594, -484.867431640625, -508.1133728027344]\n",
      "\n",
      "Instance 1510 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 6: [-0.019050568342208862, 2.784052610397339, -0.11734752357006073, -1.618804931640625, -0.5001620650291443]\n",
      "Grand sum of 1131 tensor sets is: [440.16790771484375, 1831.4700927734375, -334.85888671875, -486.4862365722656, -508.613525390625]\n",
      "\n",
      "Instance 1511 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 9: [-0.23347744345664978, 1.753216028213501, -0.5090224146842957, -2.135859251022339, -2.1919333934783936]\n",
      "Grand sum of 1132 tensor sets is: [439.9344177246094, 1833.2232666015625, -335.367919921875, -488.6221008300781, -510.8054504394531]\n",
      "\n",
      "Instance 1512 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1513 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 11: [-0.5712490081787109, 1.7486186027526855, -1.3120245933532715, -2.099252700805664, -0.9436187744140625]\n",
      "car at index 15: [-1.1396976709365845, 2.8101768493652344, 0.44345545768737793, -2.2842137813568115, -3.984623908996582]\n",
      "Grand sum of 1133 tensor sets is: [439.0789489746094, 1835.502685546875, -335.8022155761719, -490.8138427734375, -513.2695922851562]\n",
      "\n",
      "Instance 1514 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 12: [-0.7119371891021729, 1.0561587810516357, -0.4499768912792206, 3.3769259452819824, -0.4603365957736969]\n",
      "Grand sum of 1134 tensor sets is: [438.36700439453125, 1836.558837890625, -336.252197265625, -487.4369201660156, -513.7299194335938]\n",
      "\n",
      "Instance 1515 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31, 46]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 31: [0.9355786442756653, 2.0236635208129883, 0.082097128033638, 0.12249542772769928, -0.22010010480880737]\n",
      "car at index 46: [-0.4660668969154358, 3.181319236755371, -0.8812173008918762, -1.3193340301513672, -2.483489513397217]\n",
      "Grand sum of 1135 tensor sets is: [438.60174560546875, 1839.161376953125, -336.6517639160156, -488.03533935546875, -515.0817260742188]\n",
      "\n",
      "Instance 1516 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 35: [1.336784839630127, 1.8923665285110474, -0.21899893879890442, 0.5795406103134155, -0.31969153881073]\n",
      "Grand sum of 1136 tensor sets is: [439.93853759765625, 1841.0537109375, -336.8707580566406, -487.455810546875, -515.4014282226562]\n",
      "\n",
      "Instance 1517 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1518 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1519 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 15: [0.011932745575904846, 1.4205431938171387, -0.36290794610977173, -1.4166316986083984, -1.540340542793274]\n",
      "Grand sum of 1137 tensor sets is: [439.9504699707031, 1842.4742431640625, -337.2336730957031, -488.8724365234375, -516.9417724609375]\n",
      "\n",
      "Instance 1520 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 19: [0.48683983087539673, 3.2758255004882812, 0.12625476717948914, -1.9173328876495361, -1.756067156791687]\n",
      "Grand sum of 1138 tensor sets is: [440.43731689453125, 1845.7501220703125, -337.107421875, -490.7897644042969, -518.6978149414062]\n",
      "\n",
      "Instance 1521 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 29: [1.2019258737564087, 1.3640207052230835, -0.346264123916626, -2.185328245162964, 3.6667919158935547]\n",
      "Grand sum of 1139 tensor sets is: [441.6392517089844, 1847.1141357421875, -337.45367431640625, -492.97509765625, -515.031005859375]\n",
      "\n",
      "Instance 1522 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1523 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1524 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 5: [0.1010606586933136, 1.530888319015503, -1.2376773357391357, -0.8855273723602295, 0.7877121567726135]\n",
      "Grand sum of 1140 tensor sets is: [441.7403259277344, 1848.64501953125, -338.69134521484375, -493.8606262207031, -514.2432861328125]\n",
      "\n",
      "Instance 1525 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17, 25]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "car at index 17: [0.37471142411231995, 1.0901285409927368, 0.7215791344642639, 2.0070810317993164, -2.3798668384552]\n",
      "car at index 25: [0.6022064685821533, 1.178432583808899, 0.5912970304489136, 1.2089208364486694, -1.4423943758010864]\n",
      "Grand sum of 1141 tensor sets is: [442.2287902832031, 1849.779296875, -338.034912109375, -492.25262451171875, -516.1544189453125]\n",
      "\n",
      "Instance 1526 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 8: [0.13990163803100586, 2.185699224472046, -0.8564146757125854, -2.601750612258911, -0.5750583410263062]\n",
      "Grand sum of 1142 tensor sets is: [442.3686828613281, 1851.9649658203125, -338.8913269042969, -494.8543701171875, -516.7294921875]\n",
      "\n",
      "Instance 1527 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "car at index 21: [1.6723122596740723, -1.4896745681762695, -0.9793219566345215, 1.6512264013290405, 1.1503009796142578]\n",
      "Grand sum of 1143 tensor sets is: [444.0409851074219, 1850.475341796875, -339.8706359863281, -493.2031555175781, -515.5791625976562]\n",
      "\n",
      "Instance 1528 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1529 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 30: [0.7901753187179565, 1.9740118980407715, 0.1609555184841156, -0.45861178636550903, 0.5607911944389343]\n",
      "Grand sum of 1144 tensor sets is: [444.8311462402344, 1852.4493408203125, -339.7096862792969, -493.6617736816406, -515.0183715820312]\n",
      "\n",
      "Instance 1530 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10, 15, 20]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 10: [0.07494210451841354, 2.156850576400757, -0.1915934681892395, -2.4719042778015137, -2.4290802478790283]\n",
      "car at index 15: [-0.31926190853118896, 2.424405336380005, 0.2700492739677429, -2.257429599761963, -3.4978132247924805]\n",
      "car at index 20: [0.2147510051727295, 1.662278175354004, -0.24055194854736328, -3.165405750274658, -1.3871326446533203]\n",
      "Grand sum of 1145 tensor sets is: [444.8212890625, 1854.530517578125, -339.76373291015625, -496.2933654785156, -517.4563598632812]\n",
      "\n",
      "Instance 1531 of car.\n",
      "Looking for vocab token: car\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 1532 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1533 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36, 46]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 36: [-0.19233198463916779, 1.9305931329727173, -0.8666231036186218, -1.0006709098815918, -1.8625242710113525]\n",
      "car at index 46: [0.4320848286151886, 1.5076696872711182, -0.6097153425216675, -1.5789165496826172, -1.3051002025604248]\n",
      "Grand sum of 1146 tensor sets is: [444.941162109375, 1856.2496337890625, -340.50189208984375, -497.5831604003906, -519.0401611328125]\n",
      "\n",
      "Instance 1534 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1535 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 10: [-0.23423828184604645, 1.7951310873031616, -0.11515160650014877, 2.090881824493408, -3.6260859966278076]\n",
      "Grand sum of 1147 tensor sets is: [444.7069091796875, 1858.0447998046875, -340.6170349121094, -495.4922790527344, -522.666259765625]\n",
      "\n",
      "Instance 1536 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 20: [1.3277544975280762, 2.0479791164398193, -0.4704386293888092, -0.9137246012687683, -0.7317419052124023]\n",
      "Grand sum of 1148 tensor sets is: [446.03466796875, 1860.0927734375, -341.08746337890625, -496.406005859375, -523.3980102539062]\n",
      "\n",
      "Instance 1537 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 13: [-0.6401762962341309, 2.3410191535949707, -0.13238683342933655, -0.11721041798591614, -2.2659881114959717]\n",
      "Grand sum of 1149 tensor sets is: [445.3945007324219, 1862.433837890625, -341.2198486328125, -496.5232238769531, -525.6640014648438]\n",
      "\n",
      "Instance 1538 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1539 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 5: [0.41137680411338806, 1.158987283706665, -0.05651228502392769, 2.1296980381011963, 0.5806394219398499]\n",
      "Grand sum of 1150 tensor sets is: [445.8058776855469, 1863.5927734375, -341.2763671875, -494.3935241699219, -525.0833740234375]\n",
      "\n",
      "Instance 1540 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1541 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 3: [-0.16744594275951385, 1.292358160018921, -0.027365773916244507, -4.7322001457214355, 3.0698893070220947]\n",
      "Grand sum of 1151 tensor sets is: [445.638427734375, 1864.8851318359375, -341.3037414550781, -499.125732421875, -522.0134887695312]\n",
      "\n",
      "Instance 1542 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1543 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9, 30]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 9: [-0.05957403779029846, 1.780531644821167, -0.35921788215637207, -1.5557249784469604, -1.3528220653533936]\n",
      "car at index 30: [0.06084775924682617, 2.206289052963257, -0.08642422407865524, -2.48211669921875, -1.1003172397613525]\n",
      "Grand sum of 1152 tensor sets is: [445.6390686035156, 1866.8785400390625, -341.52655029296875, -501.1446533203125, -523.2400512695312]\n",
      "\n",
      "Instance 1544 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 16: [0.1228426992893219, 1.3577381372451782, -0.6801146268844604, 0.4068198502063751, -3.069549798965454]\n",
      "Grand sum of 1153 tensor sets is: [445.76190185546875, 1868.236328125, -342.2066650390625, -500.7378234863281, -526.3096313476562]\n",
      "\n",
      "Instance 1545 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 16: [1.7510321140289307, 1.9402105808258057, 0.45603710412979126, -0.7110502123832703, -2.7631194591522217]\n",
      "Grand sum of 1154 tensor sets is: [447.512939453125, 1870.176513671875, -341.7506408691406, -501.4488830566406, -529.07275390625]\n",
      "\n",
      "Instance 1546 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 6: [-0.09569276869297028, 2.0011966228485107, 0.03500084578990936, -2.5253334045410156, -1.019953966140747]\n",
      "Grand sum of 1155 tensor sets is: [447.417236328125, 1872.177734375, -341.71563720703125, -503.9742126464844, -530.0927124023438]\n",
      "\n",
      "Instance 1547 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 3: [0.12431230396032333, 2.8277714252471924, -0.45566460490226746, -1.9490525722503662, -1.0375734567642212]\n",
      "Grand sum of 1156 tensor sets is: [447.5415344238281, 1875.0054931640625, -342.1712951660156, -505.92327880859375, -531.1303100585938]\n",
      "\n",
      "Instance 1548 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 9: [-0.6249355673789978, 1.9117166996002197, -0.6432992219924927, 0.06707108020782471, -0.8067374229431152]\n",
      "Grand sum of 1157 tensor sets is: [446.9165954589844, 1876.917236328125, -342.8146057128906, -505.856201171875, -531.9370727539062]\n",
      "\n",
      "Instance 1549 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 21: [1.1826847791671753, 2.1009325981140137, 0.5105448365211487, 0.4314265847206116, -0.6733417510986328]\n",
      "Grand sum of 1158 tensor sets is: [448.0992736816406, 1879.0181884765625, -342.3040466308594, -505.4247741699219, -532.6104125976562]\n",
      "\n",
      "Instance 1550 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 14: [1.2747018337249756, 1.3149820566177368, -1.248012900352478, -1.8200582265853882, -0.8936750888824463]\n",
      "Grand sum of 1159 tensor sets is: [449.37396240234375, 1880.3331298828125, -343.55206298828125, -507.2448425292969, -533.5040893554688]\n",
      "\n",
      "Instance 1551 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1552 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 21: [0.40339142084121704, 0.9924173951148987, -2.108159065246582, -0.03817390650510788, -0.47168323397636414]\n",
      "Grand sum of 1160 tensor sets is: [449.77734375, 1881.3255615234375, -345.66021728515625, -507.28302001953125, -533.9757690429688]\n",
      "\n",
      "Instance 1553 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1554 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 25: [-0.15438690781593323, 1.5091978311538696, 0.7645552158355713, 0.9676879644393921, -1.1009323596954346]\n",
      "Grand sum of 1161 tensor sets is: [449.6229553222656, 1882.834716796875, -344.8956604003906, -506.3153381347656, -535.0767211914062]\n",
      "\n",
      "Instance 1555 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 49: [0.786629319190979, 2.7035086154937744, -0.17778241634368896, -1.0104442834854126, -0.6422688961029053]\n",
      "Grand sum of 1162 tensor sets is: [450.4095764160156, 1885.5382080078125, -345.0734558105469, -507.3257751464844, -535.718994140625]\n",
      "\n",
      "Instance 1556 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 46: [0.7822127342224121, 2.021247625350952, -1.5097424983978271, -1.9358055591583252, -0.6747919321060181]\n",
      "Grand sum of 1163 tensor sets is: [451.1918029785156, 1887.5594482421875, -346.58319091796875, -509.2615661621094, -536.393798828125]\n",
      "\n",
      "Instance 1557 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 22: [0.9035001993179321, 1.8145508766174316, 0.19030222296714783, -0.5894806981086731, -0.9562070369720459]\n",
      "Grand sum of 1164 tensor sets is: [452.0953063964844, 1889.3740234375, -346.39288330078125, -509.8510437011719, -537.3499755859375]\n",
      "\n",
      "Instance 1558 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 10: [0.4573248028755188, 0.8158448934555054, -1.0394760370254517, -0.07740242034196854, 0.29143476486206055]\n",
      "Grand sum of 1165 tensor sets is: [452.5526428222656, 1890.1898193359375, -347.432373046875, -509.9284362792969, -537.0585327148438]\n",
      "\n",
      "Instance 1559 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1560 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 20: [0.2089410126209259, 1.297297716140747, -1.2435903549194336, -0.076203852891922, -0.047389835119247437]\n",
      "Grand sum of 1166 tensor sets is: [452.7615966796875, 1891.487060546875, -348.67596435546875, -510.004638671875, -537.1058959960938]\n",
      "\n",
      "Instance 1561 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 13: [0.2504124045372009, 1.1240999698638916, -0.6211676001548767, -0.06268028169870377, -2.6511125564575195]\n",
      "Grand sum of 1167 tensor sets is: [453.01202392578125, 1892.6112060546875, -349.297119140625, -510.06732177734375, -539.7570190429688]\n",
      "\n",
      "Instance 1562 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([131, 13, 768])\n",
      "Shape of summed layers is: 131 x 768\n",
      "car at index 40: [2.0078952312469482, -1.8596456050872803, -0.8745448589324951, 0.27914926409721375, 1.3407132625579834]\n",
      "Grand sum of 1168 tensor sets is: [455.0199279785156, 1890.7515869140625, -350.1716613769531, -509.7881774902344, -538.4163208007812]\n",
      "\n",
      "Instance 1563 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 30: [0.12345433980226517, 1.9095232486724854, 0.5111628174781799, 0.28385862708091736, 1.6069082021713257]\n",
      "Grand sum of 1169 tensor sets is: [455.14337158203125, 1892.6611328125, -349.6604919433594, -509.50433349609375, -536.8093872070312]\n",
      "\n",
      "Instance 1564 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 6: [0.48554328083992004, 1.2249054908752441, -1.203188180923462, -0.5093737244606018, -0.9980266690254211]\n",
      "Grand sum of 1170 tensor sets is: [455.62890625, 1893.885986328125, -350.8636779785156, -510.0137023925781, -537.8074340820312]\n",
      "\n",
      "Instance 1565 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 40: [1.1088650226593018, 0.0904049426317215, -0.3917607069015503, -0.5534622669219971, 2.4784886837005615]\n",
      "Grand sum of 1171 tensor sets is: [456.7377624511719, 1893.9764404296875, -351.25543212890625, -510.5671691894531, -535.3289184570312]\n",
      "\n",
      "Instance 1566 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "car at index 43: [0.3250536024570465, 1.4981157779693604, 0.5994746088981628, -1.5371768474578857, -0.9181091785430908]\n",
      "Grand sum of 1172 tensor sets is: [457.06280517578125, 1895.474609375, -350.65594482421875, -512.1043701171875, -536.2470092773438]\n",
      "\n",
      "Instance 1567 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3, 13, 21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 3: [0.2737436592578888, 0.11386226862668991, -1.2684056758880615, -1.6546673774719238, 3.4528307914733887]\n",
      "car at index 13: [0.20176638662815094, -0.15731996297836304, -0.9898746013641357, -0.715156078338623, 2.5540080070495605]\n",
      "car at index 21: [0.20285695791244507, 0.652454137802124, -0.7851296663284302, -0.5757120251655579, -0.05036705732345581]\n",
      "Grand sum of 1173 tensor sets is: [457.2889404296875, 1895.6776123046875, -351.67041015625, -513.0862426757812, -534.2615356445312]\n",
      "\n",
      "Instance 1568 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 7: [-0.3481261134147644, 3.5744853019714355, -0.03626371547579765, -0.5072977542877197, -0.5695205330848694]\n",
      "Grand sum of 1174 tensor sets is: [456.9408264160156, 1899.2520751953125, -351.7066650390625, -513.5935668945312, -534.8310546875]\n",
      "\n",
      "Instance 1569 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1570 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 18: [-0.8341937065124512, 3.880402088165283, -0.18172383308410645, -0.43866512179374695, -3.453993082046509]\n",
      "Grand sum of 1175 tensor sets is: [456.10662841796875, 1903.1324462890625, -351.8883972167969, -514.0322265625, -538.2850341796875]\n",
      "\n",
      "Instance 1571 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 17: [-0.1370951533317566, 2.782883405685425, 0.298944890499115, -0.799278974533081, -2.522371768951416]\n",
      "Grand sum of 1176 tensor sets is: [455.96954345703125, 1905.915283203125, -351.5894470214844, -514.8314819335938, -540.8074340820312]\n",
      "\n",
      "Instance 1572 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1573 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 21: [1.5392062664031982, 2.886685609817505, -0.2398948073387146, -0.11052403599023819, 0.46389132738113403]\n",
      "Grand sum of 1177 tensor sets is: [457.5087585449219, 1908.802001953125, -351.829345703125, -514.9420166015625, -540.3435668945312]\n",
      "\n",
      "Instance 1574 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 4: [-0.7935940027236938, 2.984351873397827, 0.9678618311882019, 1.545642375946045, 2.3159232139587402]\n",
      "Grand sum of 1178 tensor sets is: [456.7151794433594, 1911.786376953125, -350.8614807128906, -513.3963623046875, -538.0276489257812]\n",
      "\n",
      "Instance 1575 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1576 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 7: [0.5267570614814758, 0.3237723708152771, -0.5787134170532227, 0.611681342124939, 3.6575207710266113]\n",
      "Grand sum of 1179 tensor sets is: [457.241943359375, 1912.110107421875, -351.440185546875, -512.78466796875, -534.3701171875]\n",
      "\n",
      "Instance 1577 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 12: [0.9733579158782959, 1.8668631315231323, -1.0226261615753174, -0.5717093348503113, 1.9198763370513916]\n",
      "Grand sum of 1180 tensor sets is: [458.2153015136719, 1913.9769287109375, -352.4627990722656, -513.3563842773438, -532.4502563476562]\n",
      "\n",
      "Instance 1578 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 2: [0.5089498162269592, 1.2250118255615234, -0.4348948001861572, 0.2436465471982956, 1.9816761016845703]\n",
      "Grand sum of 1181 tensor sets is: [458.7242431640625, 1915.201904296875, -352.897705078125, -513.1127319335938, -530.4685668945312]\n",
      "\n",
      "Instance 1579 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1580 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [153]\n",
      "Size of token embeddings is torch.Size([260, 13, 768])\n",
      "Shape of summed layers is: 260 x 768\n",
      "car at index 153: [0.3154677450656891, 0.5212739706039429, -0.8707873225212097, 0.6595086455345154, 2.88153076171875]\n",
      "Grand sum of 1182 tensor sets is: [459.0397033691406, 1915.72314453125, -353.76849365234375, -512.4532470703125, -527.5870361328125]\n",
      "\n",
      "Instance 1581 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 20, 33]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 7: [1.684279203414917, -0.13653863966464996, 0.787824273109436, -0.05228663980960846, 0.37207210063934326]\n",
      "car at index 20: [1.529049038887024, 0.6126744747161865, 1.412924885749817, -0.2870640456676483, 1.4518251419067383]\n",
      "car at index 33: [1.0943424701690674, 0.06699226796627045, 1.2793599367141724, -0.6836405396461487, 0.8954669237136841]\n",
      "Grand sum of 1183 tensor sets is: [460.4755859375, 1915.9041748046875, -352.60845947265625, -512.7942504882812, -526.6806030273438]\n",
      "\n",
      "Instance 1582 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1583 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 26: [-0.22358329594135284, 4.1088337898254395, -0.025407062843441963, 0.7382439970970154, -2.1445415019989014]\n",
      "Grand sum of 1184 tensor sets is: [460.25201416015625, 1920.0130615234375, -352.6338806152344, -512.0560302734375, -528.8251342773438]\n",
      "\n",
      "Instance 1584 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 9: [-0.6239804029464722, 0.9217734336853027, -0.3589511513710022, -1.071016550064087, -0.9401765465736389]\n",
      "Grand sum of 1185 tensor sets is: [459.6280212402344, 1920.934814453125, -352.9928283691406, -513.1270751953125, -529.7653198242188]\n",
      "\n",
      "Instance 1585 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 3: [0.47316551208496094, 1.2746825218200684, -0.6332388520240784, -0.03694228455424309, -0.05793733894824982]\n",
      "Grand sum of 1186 tensor sets is: [460.1011962890625, 1922.20947265625, -353.6260681152344, -513.1640014648438, -529.8232421875]\n",
      "\n",
      "Instance 1586 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 13: [0.411440372467041, 2.3337562084198, -0.36634209752082825, -1.5666674375534058, 3.0067787170410156]\n",
      "Grand sum of 1187 tensor sets is: [460.51263427734375, 1924.543212890625, -353.9924011230469, -514.7306518554688, -526.8164672851562]\n",
      "\n",
      "Instance 1587 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1588 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 10: [0.4448973536491394, 2.836730480194092, 0.5980661511421204, -1.0228602886199951, -2.4308385848999023]\n",
      "Grand sum of 1188 tensor sets is: [460.95751953125, 1927.3798828125, -353.39434814453125, -515.7535400390625, -529.247314453125]\n",
      "\n",
      "Instance 1589 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1590 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 8: [1.0165553092956543, 1.8760285377502441, -0.4810715913772583, -0.060473229736089706, -0.7841489315032959]\n",
      "Grand sum of 1189 tensor sets is: [461.97406005859375, 1929.255859375, -353.87542724609375, -515.8140258789062, -530.0314331054688]\n",
      "\n",
      "Instance 1591 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 10: [1.086884617805481, -1.4231560230255127, -0.5264614820480347, 1.5128443241119385, 4.126638412475586]\n",
      "Grand sum of 1190 tensor sets is: [463.0609436035156, 1927.832763671875, -354.4018859863281, -514.3012084960938, -525.90478515625]\n",
      "\n",
      "Instance 1592 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 20: [1.2219443321228027, 2.223160982131958, -0.32202908396720886, -0.5065451860427856, -1.7357414960861206]\n",
      "Grand sum of 1191 tensor sets is: [464.28289794921875, 1930.055908203125, -354.7239074707031, -514.8077392578125, -527.6405029296875]\n",
      "\n",
      "Instance 1593 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 16: [0.39789262413978577, 1.3782942295074463, 1.0893981456756592, 0.193081334233284, -1.923492431640625]\n",
      "Grand sum of 1192 tensor sets is: [464.6807861328125, 1931.4342041015625, -353.634521484375, -514.6146850585938, -529.56396484375]\n",
      "\n",
      "Instance 1594 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 21: [-0.07006845623254776, 0.15221863985061646, 0.7203547358512878, 1.991243600845337, -1.4879703521728516]\n",
      "Grand sum of 1193 tensor sets is: [464.6107177734375, 1931.58642578125, -352.9141540527344, -512.6234130859375, -531.0519409179688]\n",
      "\n",
      "Instance 1595 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 5: [0.14605900645256042, 1.0280698537826538, 0.172708660364151, -0.3667756915092468, -0.5867078900337219]\n",
      "Grand sum of 1194 tensor sets is: [464.75677490234375, 1932.614501953125, -352.741455078125, -512.9901733398438, -531.638671875]\n",
      "\n",
      "Instance 1596 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1597 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 9: [-0.3498260974884033, 2.2369723320007324, -0.18284839391708374, -2.393188953399658, 0.7360492944717407]\n",
      "Grand sum of 1195 tensor sets is: [464.4069519042969, 1934.8514404296875, -352.92431640625, -515.3833618164062, -530.9026489257812]\n",
      "\n",
      "Instance 1598 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 27: [-0.0465942919254303, 1.2003780603408813, 0.8753229975700378, 2.3133468627929688, -2.2303223609924316]\n",
      "Grand sum of 1196 tensor sets is: [464.3603515625, 1936.0517578125, -352.0489807128906, -513.0700073242188, -533.1329956054688]\n",
      "\n",
      "Instance 1599 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3, 13]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 3: [1.6390790939331055, -0.07601349800825119, -0.10798197984695435, -0.1274685561656952, -2.379060745239258]\n",
      "car at index 13: [1.3216774463653564, 0.01965067908167839, 0.342734158039093, 0.01995941624045372, -2.9353067874908447]\n",
      "Grand sum of 1197 tensor sets is: [465.8407287597656, 1936.0235595703125, -351.9316101074219, -513.123779296875, -535.7901611328125]\n",
      "\n",
      "Instance 1600 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 11: [0.8678771257400513, 2.725325345993042, -0.30008581280708313, -0.5114850401878357, -0.21646924316883087]\n",
      "Grand sum of 1198 tensor sets is: [466.7086181640625, 1938.7489013671875, -352.231689453125, -513.63525390625, -536.0066528320312]\n",
      "\n",
      "Instance 1601 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1602 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 21: [1.3807393312454224, -0.36863166093826294, -0.2778560519218445, -0.9082018136978149, -0.5178453922271729]\n",
      "Grand sum of 1199 tensor sets is: [468.08935546875, 1938.3802490234375, -352.5095520019531, -514.54345703125, -536.5244750976562]\n",
      "\n",
      "Instance 1603 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [118]\n",
      "Size of token embeddings is torch.Size([150, 13, 768])\n",
      "Shape of summed layers is: 150 x 768\n",
      "car at index 118: [0.9818136096000671, -1.2912129163742065, -0.20893123745918274, 1.1144907474517822, 2.207206964492798]\n",
      "Grand sum of 1200 tensor sets is: [469.0711669921875, 1937.0889892578125, -352.7184753417969, -513.428955078125, -534.3172607421875]\n",
      "\n",
      "Instance 1604 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 9: [-0.3710952699184418, 1.4496008157730103, -0.8677626848220825, -1.2565956115722656, -0.7829316258430481]\n",
      "Grand sum of 1201 tensor sets is: [468.7000732421875, 1938.53857421875, -353.58624267578125, -514.685546875, -535.1002197265625]\n",
      "\n",
      "Instance 1605 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [142]\n",
      "Size of token embeddings is torch.Size([158, 13, 768])\n",
      "Shape of summed layers is: 158 x 768\n",
      "car at index 142: [0.11948449909687042, 1.789970874786377, 0.39920830726623535, -0.7098368406295776, -0.19586099684238434]\n",
      "Grand sum of 1202 tensor sets is: [468.8195495605469, 1940.3284912109375, -353.1870422363281, -515.3953857421875, -535.2960815429688]\n",
      "\n",
      "Instance 1606 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1607 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [64]\n",
      "Size of token embeddings is torch.Size([80, 13, 768])\n",
      "Shape of summed layers is: 80 x 768\n",
      "car at index 64: [0.5011236667633057, -0.5404707193374634, -0.3565501570701599, 2.1022276878356934, 1.9737175703048706]\n",
      "Grand sum of 1203 tensor sets is: [469.3206787109375, 1939.7879638671875, -353.5435791015625, -513.2931518554688, -533.3223876953125]\n",
      "\n",
      "Instance 1608 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 7: [0.303362101316452, 2.122786521911621, -0.3776763081550598, -3.0616021156311035, -2.9835963249206543]\n",
      "Grand sum of 1204 tensor sets is: [469.6240539550781, 1941.9107666015625, -353.9212646484375, -516.354736328125, -536.3059692382812]\n",
      "\n",
      "Instance 1609 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 13: [0.5828547477722168, 1.4597395658493042, -0.025440193712711334, 1.9343581199645996, -0.05954868346452713]\n",
      "Grand sum of 1205 tensor sets is: [470.2069091796875, 1943.3704833984375, -353.94671630859375, -514.4203491210938, -536.3655395507812]\n",
      "\n",
      "Instance 1610 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 23: [-0.23935098946094513, 1.7404935359954834, -0.24629297852516174, 1.7991554737091064, -1.1730831861495972]\n",
      "Grand sum of 1206 tensor sets is: [469.9675598144531, 1945.1109619140625, -354.1930236816406, -512.6212158203125, -537.5386352539062]\n",
      "\n",
      "Instance 1611 of car.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1612 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 37: [0.8388009071350098, 2.612922191619873, -1.2474427223205566, -2.1574041843414307, -0.49036920070648193]\n",
      "Grand sum of 1207 tensor sets is: [470.8063659667969, 1947.723876953125, -355.4404602050781, -514.7786254882812, -538.0289916992188]\n",
      "\n",
      "Instance 1613 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 22: [-0.1643073707818985, 0.6610088348388672, -0.4196251928806305, -1.4899158477783203, 0.352272629737854]\n",
      "Grand sum of 1208 tensor sets is: [470.6420593261719, 1948.3848876953125, -355.8600769042969, -516.2685546875, -537.6766967773438]\n",
      "\n",
      "Instance 1614 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1615 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [349]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 349: [0.7516657710075378, 1.616869330406189, 0.46168041229248047, -0.5724605917930603, -2.186948537826538]\n",
      "Grand sum of 1209 tensor sets is: [471.39373779296875, 1950.001708984375, -355.3984069824219, -516.8410034179688, -539.8636474609375]\n",
      "\n",
      "Instance 1616 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 22: [-0.08817429840564728, 2.005045175552368, 0.584335207939148, -1.325696349143982, 1.9519238471984863]\n",
      "Grand sum of 1210 tensor sets is: [471.3055725097656, 1952.0067138671875, -354.8140869140625, -518.1666870117188, -537.9117431640625]\n",
      "\n",
      "Instance 1617 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 6: [-0.01721731573343277, 2.0752146244049072, -0.6032305955886841, -1.7270081043243408, -0.8680769801139832]\n",
      "Grand sum of 1211 tensor sets is: [471.2883605957031, 1954.0819091796875, -355.4173278808594, -519.8936767578125, -538.7798461914062]\n",
      "\n",
      "Instance 1618 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 30: [-0.42332345247268677, 2.4611356258392334, 0.7036333680152893, -1.8006000518798828, -2.19525146484375]\n",
      "Grand sum of 1212 tensor sets is: [470.86505126953125, 1956.5430908203125, -354.71368408203125, -521.6942749023438, -540.97509765625]\n",
      "\n",
      "Instance 1619 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 20: [0.4476260244846344, 1.2102327346801758, -0.3696351945400238, -2.6801681518554688, -0.5193340182304382]\n",
      "Grand sum of 1213 tensor sets is: [471.31268310546875, 1957.7532958984375, -355.08331298828125, -524.3744506835938, -541.4944458007812]\n",
      "\n",
      "Instance 1620 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [-0.06521861255168915, 2.049959421157837, 0.6513343453407288, 0.5792451500892639, -1.1102032661437988]\n",
      "Grand sum of 1214 tensor sets is: [471.2474670410156, 1959.80322265625, -354.4319763183594, -523.7952270507812, -542.6046752929688]\n",
      "\n",
      "Instance 1621 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 4: [-0.019979357719421387, 1.1311523914337158, -0.9227957725524902, -2.111135721206665, 0.006429582834243774]\n",
      "Grand sum of 1215 tensor sets is: [471.22747802734375, 1960.934326171875, -355.3547668457031, -525.9063720703125, -542.5982666015625]\n",
      "\n",
      "Instance 1622 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 15: [1.2875640392303467, 0.8913868069648743, -1.0529791116714478, -0.14192113280296326, -0.8215354681015015]\n",
      "Grand sum of 1216 tensor sets is: [472.5150451660156, 1961.82568359375, -356.4077453613281, -526.0482788085938, -543.4197998046875]\n",
      "\n",
      "Instance 1623 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 2: [1.2304753065109253, 2.2443735599517822, -1.0733730792999268, -0.7778468132019043, -1.0170470476150513]\n",
      "Grand sum of 1217 tensor sets is: [473.7455139160156, 1964.070068359375, -357.4811096191406, -526.8261108398438, -544.4368286132812]\n",
      "\n",
      "Instance 1624 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 20: [0.9024960398674011, 1.4616758823394775, -0.537548303604126, 1.6394835710525513, -1.380225419998169]\n",
      "Grand sum of 1218 tensor sets is: [474.64801025390625, 1965.53173828125, -358.0186462402344, -525.1866455078125, -545.8170776367188]\n",
      "\n",
      "Instance 1625 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 21: [0.3019579350948334, 2.5594890117645264, -1.2348380088806152, 0.708839476108551, -0.9255393147468567]\n",
      "Grand sum of 1219 tensor sets is: [474.9499816894531, 1968.0911865234375, -359.25347900390625, -524.477783203125, -546.7426147460938]\n",
      "\n",
      "Instance 1626 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1627 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1628 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 25: [0.8358173966407776, 1.6755506992340088, -0.33174216747283936, -0.00579848513007164, 1.066593050956726]\n",
      "Grand sum of 1220 tensor sets is: [475.7857971191406, 1969.7667236328125, -359.5852355957031, -524.4835815429688, -545.676025390625]\n",
      "\n",
      "Instance 1629 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1630 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1631 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1632 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 6: [-0.3050228953361511, 0.0004931651055812836, -0.24562671780586243, -0.3624975383281708, 1.427851676940918]\n",
      "Grand sum of 1221 tensor sets is: [475.48077392578125, 1969.7672119140625, -359.83087158203125, -524.8460693359375, -544.2481689453125]\n",
      "\n",
      "Instance 1633 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 32: [-0.20310887694358826, 0.8468647599220276, 0.19407978653907776, -2.2137703895568848, 1.080853819847107]\n",
      "Grand sum of 1222 tensor sets is: [475.2776794433594, 1970.6141357421875, -359.63677978515625, -527.059814453125, -543.1672973632812]\n",
      "\n",
      "Instance 1634 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 5: [0.8032663464546204, 2.005141258239746, -0.6409927010536194, -0.15011253952980042, 0.5436933040618896]\n",
      "Grand sum of 1223 tensor sets is: [476.0809326171875, 1972.6192626953125, -360.27777099609375, -527.2098999023438, -542.6235961914062]\n",
      "\n",
      "Instance 1635 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 8: [-0.3653201162815094, 1.6115875244140625, 0.039571698755025864, -2.6609275341033936, 0.25016605854034424]\n",
      "Grand sum of 1224 tensor sets is: [475.7156066894531, 1974.2308349609375, -360.2381896972656, -529.870849609375, -542.3734130859375]\n",
      "\n",
      "Instance 1636 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1637 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([141, 13, 768])\n",
      "Shape of summed layers is: 141 x 768\n",
      "car at index 48: [1.0218454599380493, -0.5804813504219055, -0.6249481439590454, 1.6348211765289307, 1.8374247550964355]\n",
      "Grand sum of 1225 tensor sets is: [476.7374572753906, 1973.650390625, -360.8631286621094, -528.2360229492188, -540.5360107421875]\n",
      "\n",
      "Instance 1638 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 14: [0.5077294707298279, -0.7011361122131348, -0.3534137010574341, 2.9087536334991455, 3.4050307273864746]\n",
      "Grand sum of 1226 tensor sets is: [477.24517822265625, 1972.94921875, -361.216552734375, -525.3272705078125, -537.1309814453125]\n",
      "\n",
      "Instance 1639 of car.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1640 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1641 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 19: [0.2880852222442627, 3.666043519973755, 0.49473467469215393, -0.35583359003067017, 0.29037296772003174]\n",
      "Grand sum of 1227 tensor sets is: [477.53326416015625, 1976.615234375, -360.7218322753906, -525.68310546875, -536.8406372070312]\n",
      "\n",
      "Instance 1642 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1643 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 33: [0.3325096368789673, 1.660369873046875, 0.8558371067047119, -0.8347465991973877, 1.527486801147461]\n",
      "Grand sum of 1228 tensor sets is: [477.86578369140625, 1978.275634765625, -359.8659973144531, -526.517822265625, -535.3131713867188]\n",
      "\n",
      "Instance 1644 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 3: [0.7500537633895874, -1.5276405811309814, -0.060372382402420044, 0.22695814073085785, 5.489576816558838]\n",
      "Grand sum of 1229 tensor sets is: [478.6158447265625, 1976.748046875, -359.9263610839844, -526.2908935546875, -529.8236083984375]\n",
      "\n",
      "Instance 1645 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 22: [0.4577976167201996, 1.672736406326294, -0.6018365621566772, -0.6490373611450195, -0.8340923190116882]\n",
      "Grand sum of 1230 tensor sets is: [479.0736389160156, 1978.4207763671875, -360.5281982421875, -526.93994140625, -530.65771484375]\n",
      "\n",
      "Instance 1646 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [76]\n",
      "Size of token embeddings is torch.Size([121, 13, 768])\n",
      "Shape of summed layers is: 121 x 768\n",
      "car at index 76: [-0.360916405916214, 1.1502865552902222, -0.34115728735923767, -0.23433244228363037, -0.5337107181549072]\n",
      "Grand sum of 1231 tensor sets is: [478.71270751953125, 1979.571044921875, -360.8693542480469, -527.1742553710938, -531.19140625]\n",
      "\n",
      "Instance 1647 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([87, 13, 768])\n",
      "Shape of summed layers is: 87 x 768\n",
      "car at index 46: [0.14516228437423706, 2.7589004039764404, -0.7174637913703918, -1.6049493551254272, -3.6031177043914795]\n",
      "Grand sum of 1232 tensor sets is: [478.8578796386719, 1982.3299560546875, -361.5868225097656, -528.7791748046875, -534.7944946289062]\n",
      "\n",
      "Instance 1648 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9, 54]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "car at index 9: [-0.1325506567955017, 1.7323992252349854, -0.6339234709739685, -1.7948658466339111, 0.10674092918634415]\n",
      "car at index 54: [0.22501643002033234, 2.052769660949707, -1.4225425720214844, -2.130875825881958, 1.9015395641326904]\n",
      "Grand sum of 1233 tensor sets is: [478.90411376953125, 1984.2225341796875, -362.61505126953125, -530.7420654296875, -533.7903442382812]\n",
      "\n",
      "Instance 1649 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28, 41]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "car at index 28: [1.6760480403900146, -1.4624916315078735, 0.20056159794330597, 0.24963155388832092, 1.1645056009292603]\n",
      "car at index 41: [0.3550589382648468, -1.5163791179656982, -0.2702077031135559, -0.9557319283485413, 1.5634464025497437]\n",
      "Grand sum of 1234 tensor sets is: [479.919677734375, 1982.733154296875, -362.6498718261719, -531.0950927734375, -532.4263916015625]\n",
      "\n",
      "Instance 1650 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "car at index 44: [0.834579586982727, 1.5563483238220215, -0.06502550840377808, -1.3163812160491943, -4.024265766143799]\n",
      "Grand sum of 1235 tensor sets is: [480.7542724609375, 1984.28955078125, -362.71490478515625, -532.4114990234375, -536.45068359375]\n",
      "\n",
      "Instance 1651 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15, 48]\n",
      "Size of token embeddings is torch.Size([75, 13, 768])\n",
      "Shape of summed layers is: 75 x 768\n",
      "car at index 15: [1.8785126209259033, 2.1689178943634033, 0.42973029613494873, 1.724460482597351, -0.2341110110282898]\n",
      "car at index 48: [1.4263339042663574, 2.5858559608459473, 0.04864058643579483, 1.711175560951233, 0.7265081405639648]\n",
      "Grand sum of 1236 tensor sets is: [482.4067077636719, 1986.6669921875, -362.4757080078125, -530.6936645507812, -536.2044677734375]\n",
      "\n",
      "Instance 1652 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1653 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1654 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 7: [0.24774087965488434, 2.1266214847564697, -1.1870800256729126, -1.009212851524353, -0.6404874324798584]\n",
      "Grand sum of 1237 tensor sets is: [482.6544494628906, 1988.7935791015625, -363.66278076171875, -531.702880859375, -536.844970703125]\n",
      "\n",
      "Instance 1655 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1656 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 7: [-0.37334346771240234, 0.5738538503646851, -0.47528940439224243, 1.9667625427246094, -2.5870895385742188]\n",
      "Grand sum of 1238 tensor sets is: [482.2810974121094, 1989.367431640625, -364.1380615234375, -529.7361450195312, -539.4320678710938]\n",
      "\n",
      "Instance 1657 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [54]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "car at index 54: [1.2041350603103638, -1.8827111721038818, -1.656875729560852, 2.4527688026428223, 1.2568215131759644]\n",
      "Grand sum of 1239 tensor sets is: [483.4852294921875, 1987.4847412109375, -365.7949523925781, -527.2833862304688, -538.1752319335938]\n",
      "\n",
      "Instance 1658 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 3: [0.9868065118789673, 2.5514566898345947, -1.2595247030258179, -0.48252373933792114, 0.10868822038173676]\n",
      "Grand sum of 1240 tensor sets is: [484.4720458984375, 1990.0362548828125, -367.0544738769531, -527.7659301757812, -538.0665283203125]\n",
      "\n",
      "Instance 1659 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 8: [-0.22918304800987244, 2.063965320587158, -0.8491874933242798, -0.20289328694343567, -1.333139181137085]\n",
      "Grand sum of 1241 tensor sets is: [484.24285888671875, 1992.1002197265625, -367.9036560058594, -527.9688110351562, -539.399658203125]\n",
      "\n",
      "Instance 1660 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 24: [0.6663258671760559, -1.0992763042449951, -0.19843560457229614, 1.1198673248291016, 2.1628801822662354]\n",
      "Grand sum of 1242 tensor sets is: [484.9091796875, 1991.0009765625, -368.1020812988281, -526.8489379882812, -537.2367553710938]\n",
      "\n",
      "Instance 1661 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 35: [0.5995727181434631, -0.4188743829727173, -0.5310269594192505, 1.5275895595550537, 2.011003017425537]\n",
      "Grand sum of 1243 tensor sets is: [485.5087585449219, 1990.5821533203125, -368.63311767578125, -525.3213500976562, -535.2257690429688]\n",
      "\n",
      "Instance 1662 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 8: [0.0785273015499115, 2.5421159267425537, -0.07858199626207352, -3.290231943130493, 0.6511632204055786]\n",
      "Grand sum of 1244 tensor sets is: [485.5872802734375, 1993.124267578125, -368.7117004394531, -528.611572265625, -534.5745849609375]\n",
      "\n",
      "Instance 1663 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 41]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 11: [1.4786709547042847, -1.7715098857879639, 0.36489778757095337, 0.44179508090019226, 3.1274962425231934]\n",
      "car at index 41: [0.7755841612815857, -1.5699564218521118, -0.48535817861557007, 0.9848301410675049, 4.729362964630127]\n",
      "Grand sum of 1245 tensor sets is: [486.71441650390625, 1991.4534912109375, -368.7719421386719, -527.8982543945312, -530.6461791992188]\n",
      "\n",
      "Instance 1664 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 17: [-0.23128221929073334, 2.869314432144165, -0.6767625212669373, 0.7734901905059814, -1.685655117034912]\n",
      "Grand sum of 1246 tensor sets is: [486.4831237792969, 1994.32275390625, -369.4486999511719, -527.124755859375, -532.3318481445312]\n",
      "\n",
      "Instance 1665 of car.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1666 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 13: [-0.14639347791671753, 1.2056900262832642, 0.7493655681610107, 1.3969345092773438, -2.2804718017578125]\n",
      "Grand sum of 1247 tensor sets is: [486.33673095703125, 1995.5284423828125, -368.6993408203125, -525.7278442382812, -534.6123046875]\n",
      "\n",
      "Instance 1667 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([105, 13, 768])\n",
      "Shape of summed layers is: 105 x 768\n",
      "car at index 28: [2.3687167167663574, -1.1898947954177856, 0.9271491169929504, 0.4055589735507965, 0.7192715406417847]\n",
      "Grand sum of 1248 tensor sets is: [488.7054443359375, 1994.3385009765625, -367.7721862792969, -525.322265625, -533.8930053710938]\n",
      "\n",
      "Instance 1668 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 15: [0.7086018919944763, 2.4650392532348633, -0.1470440775156021, -1.3268346786499023, 0.06004342436790466]\n",
      "Grand sum of 1249 tensor sets is: [489.4140319824219, 1996.8035888671875, -367.9192199707031, -526.6491088867188, -533.8329467773438]\n",
      "\n",
      "Instance 1669 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 37: [0.009369835257530212, 2.168177604675293, -0.8499977588653564, -3.988117218017578, 0.36098605394363403]\n",
      "Grand sum of 1250 tensor sets is: [489.42340087890625, 1998.9718017578125, -368.76922607421875, -530.63720703125, -533.4719848632812]\n",
      "\n",
      "Instance 1670 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1671 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 24: [0.5340868830680847, 0.6727510690689087, -0.3748301565647125, -1.150245189666748, 0.43664684891700745]\n",
      "Grand sum of 1251 tensor sets is: [489.9574890136719, 1999.64453125, -369.14404296875, -531.7874755859375, -533.0353393554688]\n",
      "\n",
      "Instance 1672 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 16: [-0.11379434168338776, 2.262739419937134, 0.036126479506492615, 2.21151065826416, -2.69635009765625]\n",
      "Grand sum of 1252 tensor sets is: [489.84368896484375, 2001.9072265625, -369.10791015625, -529.5759887695312, -535.731689453125]\n",
      "\n",
      "Instance 1673 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [52]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "car at index 52: [0.786101758480072, 1.8444159030914307, -0.20631644129753113, 0.31646639108657837, -2.008169412612915]\n",
      "Grand sum of 1253 tensor sets is: [490.6297912597656, 2003.7515869140625, -369.3142395019531, -529.259521484375, -537.7398681640625]\n",
      "\n",
      "Instance 1674 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 20: [0.5957136154174805, 1.0743480920791626, -0.6009268760681152, -0.005090951919555664, 1.1403330564498901]\n",
      "Grand sum of 1254 tensor sets is: [491.2254943847656, 2004.825927734375, -369.9151611328125, -529.2645874023438, -536.5995483398438]\n",
      "\n",
      "Instance 1675 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 11: [0.9902135133743286, 2.366619825363159, -0.25523626804351807, -0.31367459893226624, -2.413771867752075]\n",
      "Grand sum of 1255 tensor sets is: [492.2156982421875, 2007.1925048828125, -370.17041015625, -529.5782470703125, -539.0133056640625]\n",
      "\n",
      "Instance 1676 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1677 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9, 27]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 9: [0.45582225918769836, 0.9486573934555054, -0.3676043152809143, -0.5630059242248535, -1.9879875183105469]\n",
      "car at index 27: [0.09798231720924377, 1.005472183227539, 0.09229254722595215, -0.5117113590240479, -2.5952396392822266]\n",
      "Grand sum of 1256 tensor sets is: [492.49261474609375, 2008.1695556640625, -370.3080749511719, -530.1156005859375, -541.304931640625]\n",
      "\n",
      "Instance 1678 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([78, 13, 768])\n",
      "Shape of summed layers is: 78 x 768\n",
      "car at index 47: [-0.32204437255859375, 2.2664377689361572, -0.9469659328460693, -1.3629629611968994, -2.306093454360962]\n",
      "Grand sum of 1257 tensor sets is: [492.1705627441406, 2010.43603515625, -371.2550354003906, -531.4785766601562, -543.6110229492188]\n",
      "\n",
      "Instance 1679 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 28: [-0.7081716060638428, 1.5160777568817139, -0.0562857985496521, 3.041388988494873, -1.3655892610549927]\n",
      "Grand sum of 1258 tensor sets is: [491.46240234375, 2011.9521484375, -371.3113098144531, -528.4371948242188, -544.9766235351562]\n",
      "\n",
      "Instance 1680 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 26: [0.8652987480163574, 0.9929817914962769, -0.8719568252563477, -2.021397113800049, -0.5613244771957397]\n",
      "Grand sum of 1259 tensor sets is: [492.32769775390625, 2012.9451904296875, -372.1832580566406, -530.4586181640625, -545.5379638671875]\n",
      "\n",
      "Instance 1681 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "car at index 53: [0.802128791809082, 0.42168551683425903, -0.5085760951042175, -0.4670476019382477, -1.8748486042022705]\n",
      "Grand sum of 1260 tensor sets is: [493.12982177734375, 2013.3668212890625, -372.69183349609375, -530.9256591796875, -547.412841796875]\n",
      "\n",
      "Instance 1682 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 27: [1.0585591793060303, 2.0579118728637695, -0.5080869197845459, 1.738524079322815, -1.8689407110214233]\n",
      "Grand sum of 1261 tensor sets is: [494.1883850097656, 2015.4246826171875, -373.1999206542969, -529.1871337890625, -549.2817993164062]\n",
      "\n",
      "Instance 1683 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1684 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 1262 tensor sets is: [496.3060302734375, 2017.10107421875, -374.4342041015625, -530.8526611328125, -549.4263305664062]\n",
      "\n",
      "Instance 1685 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "car at index 33: [0.7688978910446167, 2.6723196506500244, 0.17211946845054626, 1.5325649976730347, -0.16561688482761383]\n",
      "Grand sum of 1263 tensor sets is: [497.0749206542969, 2019.7734375, -374.2620849609375, -529.320068359375, -549.5919189453125]\n",
      "\n",
      "Instance 1686 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 12: [0.7895424962043762, 1.5086956024169922, -0.896685779094696, -0.38341495394706726, 1.4558045864105225]\n",
      "Grand sum of 1264 tensor sets is: [497.8644714355469, 2021.2821044921875, -375.1587829589844, -529.7034912109375, -548.1361083984375]\n",
      "\n",
      "Instance 1687 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 7: [-0.8075791597366333, 2.4841463565826416, -1.2342493534088135, -1.4008601903915405, -1.8458881378173828]\n",
      "Grand sum of 1265 tensor sets is: [497.056884765625, 2023.7662353515625, -376.3930358886719, -531.1043701171875, -549.9819946289062]\n",
      "\n",
      "Instance 1688 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "car at index 10: [0.6206629276275635, 1.5356405973434448, -0.012764845043420792, -1.7647953033447266, -1.1937345266342163]\n",
      "Grand sum of 1266 tensor sets is: [497.67755126953125, 2025.3018798828125, -376.4057922363281, -532.869140625, -551.1757202148438]\n",
      "\n",
      "Instance 1689 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 6: [-0.3386300504207611, 2.8244903087615967, 0.13431505858898163, 0.03412340581417084, -1.3696280717849731]\n",
      "Grand sum of 1267 tensor sets is: [497.33892822265625, 2028.1263427734375, -376.271484375, -532.8350219726562, -552.5453491210938]\n",
      "\n",
      "Instance 1690 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 1268 tensor sets is: [499.4565734863281, 2029.802734375, -377.5057678222656, -534.5005493164062, -552.6898803710938]\n",
      "\n",
      "Instance 1691 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 13: [-0.12304986268281937, 2.0597634315490723, -0.9223794937133789, -2.3129639625549316, -2.1711673736572266]\n",
      "Grand sum of 1269 tensor sets is: [499.3335266113281, 2031.862548828125, -378.42816162109375, -536.8135375976562, -554.8610229492188]\n",
      "\n",
      "Instance 1692 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 8: [0.7617937326431274, 2.1016178131103516, -1.0061143636703491, -0.02404293417930603, -1.3070075511932373]\n",
      "Grand sum of 1270 tensor sets is: [500.0953063964844, 2033.964111328125, -379.43426513671875, -536.8375854492188, -556.1680297851562]\n",
      "\n",
      "Instance 1693 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 7: [1.1021946668624878, 0.6990246772766113, 0.7789089679718018, -0.47896289825439453, -0.592618465423584]\n",
      "Grand sum of 1271 tensor sets is: [501.197509765625, 2034.6630859375, -378.6553649902344, -537.3165283203125, -556.7606201171875]\n",
      "\n",
      "Instance 1694 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "car at index 2: [-0.4883171319961548, 2.274179458618164, -0.023546963930130005, -1.2426812648773193, 0.08348720520734787]\n",
      "Grand sum of 1272 tensor sets is: [500.7091979980469, 2036.937255859375, -378.6789245605469, -538.5592041015625, -556.6771240234375]\n",
      "\n",
      "Instance 1695 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1696 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 21: [-0.0013185292482376099, 2.0330934524536133, -0.4909957945346832, -0.34847119450569153, -1.6023377180099487]\n",
      "Grand sum of 1273 tensor sets is: [500.7078857421875, 2038.9703369140625, -379.169921875, -538.9076538085938, -558.2794799804688]\n",
      "\n",
      "Instance 1697 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 15: [-0.053556591272354126, 1.2887492179870605, 0.2826696038246155, 0.479511022567749, 1.1469907760620117]\n",
      "Grand sum of 1274 tensor sets is: [500.6543273925781, 2040.259033203125, -378.8872375488281, -538.4281616210938, -557.1325073242188]\n",
      "\n",
      "Instance 1698 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 18: [0.7554068565368652, 1.9804019927978516, 0.01121659204363823, -1.460788607597351, -1.6094287633895874]\n",
      "Grand sum of 1275 tensor sets is: [501.40972900390625, 2042.2393798828125, -378.8760070800781, -539.8889770507812, -558.741943359375]\n",
      "\n",
      "Instance 1699 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1700 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 22: [0.010601520538330078, 2.7209863662719727, 0.3613215386867523, -0.8290690183639526, -1.3043397665023804]\n",
      "Grand sum of 1276 tensor sets is: [501.4203186035156, 2044.9603271484375, -378.5146789550781, -540.718017578125, -560.0462646484375]\n",
      "\n",
      "Instance 1701 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 16: [1.1637518405914307, 2.5196409225463867, -0.8441024422645569, 0.6121204495429993, -2.1567325592041016]\n",
      "Grand sum of 1277 tensor sets is: [502.5840759277344, 2047.47998046875, -379.3587951660156, -540.1058959960938, -562.2030029296875]\n",
      "\n",
      "Instance 1702 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([167, 13, 768])\n",
      "Shape of summed layers is: 167 x 768\n",
      "car at index 36: [1.3584723472595215, 0.7657484412193298, -0.06704919040203094, 0.21383538842201233, -1.4879825115203857]\n",
      "Grand sum of 1278 tensor sets is: [503.9425354003906, 2048.245849609375, -379.42584228515625, -539.89208984375, -563.6909790039062]\n",
      "\n",
      "Instance 1703 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 2: [0.14080165326595306, 1.868193507194519, -0.5354375243186951, -0.9036597609519958, 2.5169196128845215]\n",
      "Grand sum of 1279 tensor sets is: [504.0833435058594, 2050.114013671875, -379.9612731933594, -540.7957763671875, -561.174072265625]\n",
      "\n",
      "Instance 1704 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 16: [1.3031209707260132, 2.628411293029785, -0.16636037826538086, -0.3909784257411957, -1.2319415807724]\n",
      "Grand sum of 1280 tensor sets is: [505.386474609375, 2052.742431640625, -380.12762451171875, -541.186767578125, -562.406005859375]\n",
      "\n",
      "Instance 1705 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12, 21]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 12: [1.260138988494873, 1.7144767045974731, 0.04905498027801514, 0.83460533618927, 2.2707817554473877]\n",
      "car at index 21: [0.3589089810848236, 2.29339861869812, 0.10093289613723755, 2.8886845111846924, -2.1092169284820557]\n",
      "Grand sum of 1281 tensor sets is: [506.19598388671875, 2054.746337890625, -380.0526428222656, -539.3251342773438, -562.3251953125]\n",
      "\n",
      "Instance 1706 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 4: [0.7065838575363159, -0.42131900787353516, 0.11634969711303711, 0.015451550483703613, 3.7114462852478027]\n",
      "Grand sum of 1282 tensor sets is: [506.9025573730469, 2054.324951171875, -379.936279296875, -539.3096923828125, -558.61376953125]\n",
      "\n",
      "Instance 1707 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([121, 13, 768])\n",
      "Shape of summed layers is: 121 x 768\n",
      "car at index 27: [0.41757792234420776, 3.626535415649414, -0.1039411649107933, 0.3127431571483612, -2.2491700649261475]\n",
      "Grand sum of 1283 tensor sets is: [507.32012939453125, 2057.951416015625, -380.04022216796875, -538.9969482421875, -560.8629150390625]\n",
      "\n",
      "Instance 1708 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 18: [-0.9051724672317505, 2.268866777420044, -0.049289003014564514, 0.36821556091308594, -1.4544893503189087]\n",
      "Grand sum of 1284 tensor sets is: [506.4149475097656, 2060.22021484375, -380.0895080566406, -538.6287231445312, -562.3173828125]\n",
      "\n",
      "Instance 1709 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 7: [-0.19001194834709167, 1.8546092510223389, -0.126310333609581, -1.2111152410507202, -1.3919751644134521]\n",
      "Grand sum of 1285 tensor sets is: [506.2249450683594, 2062.07470703125, -380.2158203125, -539.83984375, -563.7093505859375]\n",
      "\n",
      "Instance 1710 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 8: [0.10200188308954239, 1.841710090637207, 0.7030806541442871, -0.42346715927124023, -1.1145870685577393]\n",
      "Grand sum of 1286 tensor sets is: [506.3269348144531, 2063.91650390625, -379.5127258300781, -540.2633056640625, -564.8239135742188]\n",
      "\n",
      "Instance 1711 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1712 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 32: [0.6634145379066467, 0.7446624636650085, -0.477547824382782, -0.07870423048734665, -0.8047734498977661]\n",
      "Grand sum of 1287 tensor sets is: [506.9903564453125, 2064.6611328125, -379.9902648925781, -540.3419799804688, -565.628662109375]\n",
      "\n",
      "Instance 1713 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1714 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 15: [0.7037893533706665, 1.8074610233306885, 0.4203755855560303, -1.2315614223480225, -2.3415191173553467]\n",
      "Grand sum of 1288 tensor sets is: [507.69415283203125, 2066.468505859375, -379.56988525390625, -541.5735473632812, -567.9701538085938]\n",
      "\n",
      "Instance 1715 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 20: [0.32560107111930847, 1.2581892013549805, -1.3619238138198853, -0.21730324625968933, -1.1547791957855225]\n",
      "Grand sum of 1289 tensor sets is: [508.0197448730469, 2067.726806640625, -380.93182373046875, -541.7908325195312, -569.1249389648438]\n",
      "\n",
      "Instance 1716 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 13: [-0.14639347791671753, 1.2056900262832642, 0.7493655681610107, 1.3969345092773438, -2.2804718017578125]\n",
      "Grand sum of 1290 tensor sets is: [507.87335205078125, 2068.9326171875, -380.1824645996094, -540.3939208984375, -571.4053955078125]\n",
      "\n",
      "Instance 1717 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1718 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 15: [0.29058870673179626, 1.6797538995742798, -0.9314714670181274, -1.3881220817565918, -2.5149619579315186]\n",
      "Grand sum of 1291 tensor sets is: [508.1639404296875, 2070.6123046875, -381.1139221191406, -541.7820434570312, -573.9203491210938]\n",
      "\n",
      "Instance 1719 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1720 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 15: [1.174025535583496, 2.3916239738464355, -1.295694351196289, -0.41517704725265503, -0.0850021243095398]\n",
      "Grand sum of 1292 tensor sets is: [509.33795166015625, 2073.00390625, -382.40960693359375, -542.1972045898438, -574.00537109375]\n",
      "\n",
      "Instance 1721 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1722 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 5: [0.5447202324867249, 2.405729055404663, -0.10543100535869598, -1.3549671173095703, -2.858088731765747]\n",
      "Grand sum of 1293 tensor sets is: [509.8826599121094, 2075.40966796875, -382.5150451660156, -543.5521850585938, -576.8634643554688]\n",
      "\n",
      "Instance 1723 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 2: [0.24597324430942535, 1.5221103429794312, -0.012935720384120941, 0.3642660677433014, -1.719627022743225]\n",
      "Grand sum of 1294 tensor sets is: [510.1286315917969, 2076.931884765625, -382.5279846191406, -543.1879272460938, -578.5830688476562]\n",
      "\n",
      "Instance 1724 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1725 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 20: [-0.30542653799057007, 4.357576370239258, 0.02846353128552437, -0.5145940780639648, -0.5168198943138123]\n",
      "Grand sum of 1295 tensor sets is: [509.8232116699219, 2081.28955078125, -382.49951171875, -543.7025146484375, -579.0999145507812]\n",
      "\n",
      "Instance 1726 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 9: [-0.23976118862628937, 1.9163811206817627, -0.8665618300437927, -1.5010960102081299, -2.5892271995544434]\n",
      "Grand sum of 1296 tensor sets is: [509.5834655761719, 2083.205810546875, -383.3660583496094, -545.20361328125, -581.6891479492188]\n",
      "\n",
      "Instance 1727 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1728 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 28: [0.6937361359596252, 3.039062023162842, -0.6037263870239258, 0.30757129192352295, -0.3888857364654541]\n",
      "Grand sum of 1297 tensor sets is: [510.2771911621094, 2086.244873046875, -383.96978759765625, -544.8960571289062, -582.0780639648438]\n",
      "\n",
      "Instance 1729 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 6: [0.10370876640081406, 1.849672794342041, -0.04165814816951752, -1.2207082509994507, -0.6878551840782166]\n",
      "Grand sum of 1298 tensor sets is: [510.3808898925781, 2088.094482421875, -384.0114440917969, -546.1167602539062, -582.7659301757812]\n",
      "\n",
      "Instance 1730 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 4: [-0.3255205750465393, 3.9517321586608887, 0.16692429780960083, 2.483560085296631, 1.5004726648330688]\n",
      "Grand sum of 1299 tensor sets is: [510.05535888671875, 2092.046142578125, -383.8445129394531, -543.6331787109375, -581.2654418945312]\n",
      "\n",
      "Instance 1731 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 8: [-0.09368337690830231, 2.414226770401001, 0.23032313585281372, -1.759530782699585, -2.5241706371307373]\n",
      "Grand sum of 1300 tensor sets is: [509.961669921875, 2094.46044921875, -383.61419677734375, -545.3927001953125, -583.7896118164062]\n",
      "\n",
      "Instance 1732 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 16: [-0.595373272895813, 0.1552019715309143, 0.44008827209472656, -0.1059807762503624, 1.3263769149780273]\n",
      "Grand sum of 1301 tensor sets is: [509.3663024902344, 2094.61572265625, -383.1741027832031, -545.4986572265625, -582.4632568359375]\n",
      "\n",
      "Instance 1733 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27, 43]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 27: [0.35997700691223145, 0.6733922362327576, -0.1509675532579422, -0.289272278547287, -0.2578226923942566]\n",
      "car at index 43: [-0.14659500122070312, 1.165525197982788, 1.105067491531372, -1.637567162513733, -0.8474071621894836]\n",
      "Grand sum of 1302 tensor sets is: [509.4729919433594, 2095.53515625, -382.6970520019531, -546.4620971679688, -583.015869140625]\n",
      "\n",
      "Instance 1734 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([75, 13, 768])\n",
      "Shape of summed layers is: 75 x 768\n",
      "car at index 44: [1.3014131784439087, 0.5870497226715088, 0.18689844012260437, 0.6903224587440491, 4.322251319885254]\n",
      "Grand sum of 1303 tensor sets is: [510.7744140625, 2096.122314453125, -382.5101623535156, -545.7717895507812, -578.693603515625]\n",
      "\n",
      "Instance 1735 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27, 98]\n",
      "Size of token embeddings is torch.Size([120, 13, 768])\n",
      "Shape of summed layers is: 120 x 768\n",
      "car at index 27: [-0.7191031575202942, -0.3807680010795593, -0.9675107002258301, -1.2469782829284668, -2.048553466796875]\n",
      "car at index 98: [-0.3322997987270355, 2.7957379817962646, 0.1709137260913849, -3.551800489425659, -0.460052490234375]\n",
      "Grand sum of 1304 tensor sets is: [510.24871826171875, 2097.329833984375, -382.908447265625, -548.1712036132812, -579.9478759765625]\n",
      "\n",
      "Instance 1736 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 9: [-0.165449857711792, 1.2120935916900635, -0.6719849109649658, -2.734927177429199, -2.218092203140259]\n",
      "Grand sum of 1305 tensor sets is: [510.0832824707031, 2098.5419921875, -383.5804443359375, -550.9061279296875, -582.1659545898438]\n",
      "\n",
      "Instance 1737 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1738 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 2: [0.03864194452762604, 0.596358597278595, -1.3175687789916992, -0.31818363070487976, 1.1102135181427002]\n",
      "Grand sum of 1306 tensor sets is: [510.1219177246094, 2099.138427734375, -384.89801025390625, -551.2243041992188, -581.0557250976562]\n",
      "\n",
      "Instance 1739 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8, 14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 8: [0.7193893790245056, 3.339106559753418, -0.41662999987602234, -1.5099234580993652, -1.5677615404129028]\n",
      "car at index 14: [0.8644391298294067, 1.7849681377410889, -0.6053488254547119, -0.5757601261138916, -0.3247172236442566]\n",
      "Grand sum of 1307 tensor sets is: [510.913818359375, 2101.700439453125, -385.40899658203125, -552.2671508789062, -582.001953125]\n",
      "\n",
      "Instance 1740 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 7: [0.5359987020492554, 3.73435378074646, -0.8664228320121765, 1.3211289644241333, -3.7753124237060547]\n",
      "Grand sum of 1308 tensor sets is: [511.4498291015625, 2105.434814453125, -386.2754211425781, -550.946044921875, -585.7772827148438]\n",
      "\n",
      "Instance 1741 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2, 27]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 2: [0.6776370406150818, 1.879455327987671, -0.11148273199796677, -0.523036003112793, 0.6491993069648743]\n",
      "car at index 27: [0.401017963886261, 2.9647717475891113, -0.03414115682244301, -3.1111714839935303, -1.078198790550232]\n",
      "Grand sum of 1309 tensor sets is: [511.9891662597656, 2107.85693359375, -386.3482360839844, -552.7631225585938, -585.9917602539062]\n",
      "\n",
      "Instance 1742 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1743 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 8: [1.8332031965255737, 2.0746994018554688, 0.6138944625854492, -0.05033285170793533, -1.6064720153808594]\n",
      "Grand sum of 1310 tensor sets is: [513.8223876953125, 2109.931640625, -385.7343444824219, -552.8134765625, -587.5982055664062]\n",
      "\n",
      "Instance 1744 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 1311 tensor sets is: [515.9400634765625, 2111.60791015625, -386.9686279296875, -554.47900390625, -587.7427368164062]\n",
      "\n",
      "Instance 1745 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1746 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 8: [0.7308618426322937, -0.3624713122844696, -0.28694820404052734, 1.5653374195098877, -2.3946406841278076]\n",
      "Grand sum of 1312 tensor sets is: [516.6708984375, 2111.245361328125, -387.2555847167969, -552.9136962890625, -590.1373901367188]\n",
      "\n",
      "Instance 1747 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 22: [0.10775935649871826, 3.6142899990081787, -0.6440696120262146, -0.8090090751647949, 0.3666098117828369]\n",
      "Grand sum of 1313 tensor sets is: [516.7786865234375, 2114.859619140625, -387.899658203125, -553.7227172851562, -589.770751953125]\n",
      "\n",
      "Instance 1748 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 3: [-0.36164748668670654, 1.2718875408172607, 0.2184043675661087, -0.2922457158565521, 0.40338170528411865]\n",
      "Grand sum of 1314 tensor sets is: [516.4170532226562, 2116.131591796875, -387.6812438964844, -554.0149536132812, -589.3673706054688]\n",
      "\n",
      "Instance 1749 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 13: [1.0259844064712524, 1.9626728296279907, -0.2450217306613922, -0.9669564962387085, 0.9460951089859009]\n",
      "Grand sum of 1315 tensor sets is: [517.4430541992188, 2118.09423828125, -387.92626953125, -554.98193359375, -588.4212646484375]\n",
      "\n",
      "Instance 1750 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 1316 tensor sets is: [519.5607299804688, 2119.7705078125, -389.1605529785156, -556.6474609375, -588.5657958984375]\n",
      "\n",
      "Instance 1751 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1752 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 9: [0.40144112706184387, -0.5721169710159302, 0.7542988061904907, 1.9011942148208618, -1.373953104019165]\n",
      "Grand sum of 1317 tensor sets is: [519.962158203125, 2119.198486328125, -388.40625, -554.7462768554688, -589.9397583007812]\n",
      "\n",
      "Instance 1753 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1754 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1755 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 6: [0.054508358240127563, 0.7847051024436951, -0.15310676395893097, -1.2029439210891724, -2.169196605682373]\n",
      "Grand sum of 1318 tensor sets is: [520.0166625976562, 2119.983154296875, -388.5593566894531, -555.94921875, -592.1089477539062]\n",
      "\n",
      "Instance 1756 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 2: [-0.41834962368011475, 4.218685150146484, -0.2518047094345093, 1.7304373979568481, 2.0073695182800293]\n",
      "Grand sum of 1319 tensor sets is: [519.5983276367188, 2124.201904296875, -388.8111572265625, -554.2188110351562, -590.1015625]\n",
      "\n",
      "Instance 1757 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1758 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40, 58]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "car at index 40: [0.44921356439590454, 1.9718945026397705, 0.32442787289619446, 1.6000518798828125, -2.002253532409668]\n",
      "car at index 58: [0.6604846715927124, 2.2815134525299072, -0.7152231931686401, 0.29305893182754517, -1.7873823642730713]\n",
      "Grand sum of 1320 tensor sets is: [520.1531982421875, 2126.32861328125, -389.0065612792969, -553.2722778320312, -591.9963989257812]\n",
      "\n",
      "Instance 1759 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 12: [0.598872184753418, 2.831716537475586, -0.20405378937721252, -1.8283624649047852, 0.5547298192977905]\n",
      "Grand sum of 1321 tensor sets is: [520.7520751953125, 2129.160400390625, -389.2106018066406, -555.1006469726562, -591.441650390625]\n",
      "\n",
      "Instance 1760 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1761 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 27: [0.24165375530719757, 1.6832695007324219, 1.3353285789489746, 0.5842447876930237, -3.437239646911621]\n",
      "Grand sum of 1322 tensor sets is: [520.9937133789062, 2130.84375, -387.8752746582031, -554.5164184570312, -594.87890625]\n",
      "\n",
      "Instance 1762 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 4: [0.23183061182498932, 2.3007514476776123, -0.49600157141685486, -2.852689504623413, -1.5852768421173096]\n",
      "Grand sum of 1323 tensor sets is: [521.2255249023438, 2133.14453125, -388.37127685546875, -557.3690795898438, -596.4641723632812]\n",
      "\n",
      "Instance 1763 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 1324 tensor sets is: [523.3432006835938, 2134.82080078125, -389.6055603027344, -559.0346069335938, -596.6087036132812]\n",
      "\n",
      "Instance 1764 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([106, 13, 768])\n",
      "Shape of summed layers is: 106 x 768\n",
      "car at index 38: [0.6699236631393433, -1.999105453491211, -0.8952964544296265, 1.5495216846466064, 1.535900354385376]\n",
      "Grand sum of 1325 tensor sets is: [524.0131225585938, 2132.82177734375, -390.5008544921875, -557.485107421875, -595.0728149414062]\n",
      "\n",
      "Instance 1765 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 24: [0.10977965593338013, 1.6668328046798706, -0.0812736451625824, -3.1196885108947754, 0.16241580247879028]\n",
      "Grand sum of 1326 tensor sets is: [524.1229248046875, 2134.488525390625, -390.5821228027344, -560.6047973632812, -594.910400390625]\n",
      "\n",
      "Instance 1766 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4, 14, 24]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 4: [0.8423440456390381, 2.047046184539795, -1.1071274280548096, 0.027145735919475555, 2.0917205810546875]\n",
      "car at index 14: [0.5090777277946472, 2.400977611541748, -0.5479916930198669, 0.11892499029636383, 1.5063433647155762]\n",
      "car at index 24: [0.3453114628791809, 2.031360626220703, -0.3911576271057129, -0.5771510004997253, 0.9516577124595642]\n",
      "Grand sum of 1327 tensor sets is: [524.6884765625, 2136.6484375, -391.26422119140625, -560.7484741210938, -593.393798828125]\n",
      "\n",
      "Instance 1767 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "car at index 5: [0.34705686569213867, 0.8237831592559814, -0.9739100337028503, -1.201111078262329, 0.5348206758499146]\n",
      "Grand sum of 1328 tensor sets is: [525.0355224609375, 2137.47216796875, -392.2381286621094, -561.9495849609375, -592.8589477539062]\n",
      "\n",
      "Instance 1768 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2, 8]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 2: [0.8706681728363037, 3.271397829055786, -0.5447748899459839, -1.063900351524353, -0.34072622656822205]\n",
      "car at index 8: [0.21532118320465088, 3.343280076980591, 1.0236990451812744, -0.9845895767211914, -3.775111198425293]\n",
      "Grand sum of 1329 tensor sets is: [525.5784912109375, 2140.779541015625, -391.9986572265625, -562.9738159179688, -594.9168701171875]\n",
      "\n",
      "Instance 1769 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 21: [0.7086817026138306, 3.5725278854370117, -0.10870611667633057, -2.073153495788574, -1.2724684476852417]\n",
      "Grand sum of 1330 tensor sets is: [526.2871704101562, 2144.35205078125, -392.10736083984375, -565.0469970703125, -596.1893310546875]\n",
      "\n",
      "Instance 1770 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([88, 13, 768])\n",
      "Shape of summed layers is: 88 x 768\n",
      "car at index 10: [0.6644056439399719, 1.7004693746566772, -0.4302651882171631, -0.31938621401786804, -0.25702616572380066]\n",
      "Grand sum of 1331 tensor sets is: [526.9515991210938, 2146.052490234375, -392.5376281738281, -565.3663940429688, -596.4463500976562]\n",
      "\n",
      "Instance 1771 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 3: [0.3272326588630676, 1.3120300769805908, 0.06452726572751999, -0.9707447290420532, -0.6135157942771912]\n",
      "Grand sum of 1332 tensor sets is: [527.27880859375, 2147.364501953125, -392.4731140136719, -566.337158203125, -597.0598754882812]\n",
      "\n",
      "Instance 1772 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1773 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 12: [-0.36977067589759827, 1.5851086378097534, 0.006048358976840973, -1.0702568292617798, 1.1037429571151733]\n",
      "Grand sum of 1333 tensor sets is: [526.9090576171875, 2148.94970703125, -392.4670715332031, -567.4074096679688, -595.9561157226562]\n",
      "\n",
      "Instance 1774 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1775 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 28: [0.4822162687778473, 0.8791693449020386, -1.3122413158416748, 1.5013580322265625, 2.4923095703125]\n",
      "Grand sum of 1334 tensor sets is: [527.3912963867188, 2149.828857421875, -393.7793273925781, -565.9060668945312, -593.4638061523438]\n",
      "\n",
      "Instance 1776 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1777 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1778 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 16: [0.37598711252212524, 1.1679861545562744, 0.33041447401046753, 2.8587260246276855, -1.6749223470687866]\n",
      "Grand sum of 1335 tensor sets is: [527.7672729492188, 2150.996826171875, -393.44891357421875, -563.04736328125, -595.1387329101562]\n",
      "\n",
      "Instance 1779 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 9: [0.9402937889099121, 2.1503448486328125, -0.48040369153022766, -0.29129117727279663, -0.35244694352149963]\n",
      "Grand sum of 1336 tensor sets is: [528.7075805664062, 2153.147216796875, -393.9293212890625, -563.3386840820312, -595.4911499023438]\n",
      "\n",
      "Instance 1780 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 18: [0.1252642273902893, 3.1237127780914307, -0.4384554326534271, -1.1605703830718994, -1.7281215190887451]\n",
      "Grand sum of 1337 tensor sets is: [528.8328247070312, 2156.27099609375, -394.3677673339844, -564.499267578125, -597.2192993164062]\n",
      "\n",
      "Instance 1781 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 11: [0.6252005100250244, 2.040456771850586, -0.36055704951286316, -0.9461166262626648, 1.1429252624511719]\n",
      "Grand sum of 1338 tensor sets is: [529.4580078125, 2158.3115234375, -394.72833251953125, -565.4453735351562, -596.0763549804688]\n",
      "\n",
      "Instance 1782 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 11: [0.09082884341478348, 2.340622663497925, 0.025653697550296783, 1.798965334892273, 0.24025413393974304]\n",
      "Grand sum of 1339 tensor sets is: [529.548828125, 2160.652099609375, -394.7026672363281, -563.6464233398438, -595.8361206054688]\n",
      "\n",
      "Instance 1783 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [1]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 1: [-0.014136655256152153, 2.753897190093994, -0.7668264508247375, -1.510549783706665, 0.753516674041748]\n",
      "Grand sum of 1340 tensor sets is: [529.53466796875, 2163.406005859375, -395.469482421875, -565.156982421875, -595.0825805664062]\n",
      "\n",
      "Instance 1784 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1785 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 11: [-0.06783133745193481, 3.3409383296966553, 0.17909297347068787, 0.037884414196014404, -0.30128559470176697]\n",
      "Grand sum of 1341 tensor sets is: [529.4668579101562, 2166.746826171875, -395.2903747558594, -565.1190795898438, -595.3838500976562]\n",
      "\n",
      "Instance 1786 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 28: [-0.19318173825740814, 3.008094549179077, 0.13816121220588684, 0.3950977325439453, 1.50560462474823]\n",
      "Grand sum of 1342 tensor sets is: [529.273681640625, 2169.7548828125, -395.1522216796875, -564.7239990234375, -593.8782348632812]\n",
      "\n",
      "Instance 1787 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 12: [-0.41388797760009766, 2.123253345489502, 0.4770509600639343, 1.7456334829330444, -1.7348692417144775]\n",
      "Grand sum of 1343 tensor sets is: [528.8598022460938, 2171.878173828125, -394.6751708984375, -562.9783935546875, -595.6130981445312]\n",
      "\n",
      "Instance 1788 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 8: [0.7862568497657776, 3.5488691329956055, -0.46217572689056396, -1.1326704025268555, -1.6816856861114502]\n",
      "Grand sum of 1344 tensor sets is: [529.6460571289062, 2175.427001953125, -395.1373596191406, -564.111083984375, -597.2947998046875]\n",
      "\n",
      "Instance 1789 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 9: [0.7466775178909302, 2.338505983352661, 0.28651368618011475, 0.11160814762115479, -2.1933324337005615]\n",
      "Grand sum of 1345 tensor sets is: [530.3927612304688, 2177.765625, -394.8508605957031, -563.9994506835938, -599.4881591796875]\n",
      "\n",
      "Instance 1790 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([131, 13, 768])\n",
      "Shape of summed layers is: 131 x 768\n",
      "car at index 34: [0.9161348342895508, 1.954730749130249, 0.23594124615192413, -0.13925231993198395, -0.651687502861023]\n",
      "Grand sum of 1346 tensor sets is: [531.3088989257812, 2179.720458984375, -394.61492919921875, -564.1387329101562, -600.1398315429688]\n",
      "\n",
      "Instance 1791 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 9: [-0.34123390913009644, 3.9740285873413086, 0.19876930117607117, -1.3471193313598633, 0.006233841180801392]\n",
      "Grand sum of 1347 tensor sets is: [530.9676513671875, 2183.694580078125, -394.4161682128906, -565.48583984375, -600.1336059570312]\n",
      "\n",
      "Instance 1792 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5, 19]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 5: [0.7016205787658691, 2.443122625350952, -0.37260884046554565, -1.3537280559539795, 1.3805642127990723]\n",
      "car at index 19: [1.0045208930969238, 3.3970954418182373, -0.04848919063806534, -1.0764591693878174, 0.7059623599052429]\n",
      "Grand sum of 1348 tensor sets is: [531.8207397460938, 2186.61474609375, -394.626708984375, -566.700927734375, -599.09033203125]\n",
      "\n",
      "Instance 1793 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 26: [1.0415836572647095, 2.015671968460083, -0.06174086779356003, -1.1496520042419434, -2.042900323867798]\n",
      "Grand sum of 1349 tensor sets is: [532.8623046875, 2188.63037109375, -394.6884460449219, -567.8505859375, -601.1332397460938]\n",
      "\n",
      "Instance 1794 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 10: [0.005714005790650845, 2.459869861602783, -0.7626628875732422, -1.5531013011932373, 2.296844482421875]\n",
      "Grand sum of 1350 tensor sets is: [532.8680419921875, 2191.09033203125, -395.45111083984375, -569.4036865234375, -598.83642578125]\n",
      "\n",
      "Instance 1795 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 11: [-0.809104859828949, 1.8444573879241943, 0.41723382472991943, -1.222859263420105, -2.0571975708007812]\n",
      "Grand sum of 1351 tensor sets is: [532.0589599609375, 2192.934814453125, -395.03387451171875, -570.6265258789062, -600.8936157226562]\n",
      "\n",
      "Instance 1796 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 25: [-0.1327390968799591, 2.993306875228882, -1.0225059986114502, -1.7805171012878418, -1.8600678443908691]\n",
      "Grand sum of 1352 tensor sets is: [531.9262084960938, 2195.92822265625, -396.0563659667969, -572.4070434570312, -602.753662109375]\n",
      "\n",
      "Instance 1797 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 12: [0.02483285963535309, 1.6449388265609741, -0.1956508755683899, -1.9531424045562744, 0.08822421729564667]\n",
      "Grand sum of 1353 tensor sets is: [531.9510498046875, 2197.5732421875, -396.25201416015625, -574.3601684570312, -602.6654663085938]\n",
      "\n",
      "Instance 1798 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1799 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "car at index 10: [-0.5017676949501038, 0.341582715511322, -0.7091020345687866, 0.7423580884933472, 2.250765800476074]\n",
      "Grand sum of 1354 tensor sets is: [531.4492797851562, 2197.914794921875, -396.96112060546875, -573.6177978515625, -600.4146728515625]\n",
      "\n",
      "Instance 1800 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5, 11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 5: [0.284564346075058, 1.4225667715072632, 0.05942120403051376, 1.1011641025543213, -0.92658931016922]\n",
      "car at index 11: [0.23586253821849823, 1.6971592903137207, 0.14670437574386597, -0.7708382606506348, -0.9426771402359009]\n",
      "Grand sum of 1355 tensor sets is: [531.70947265625, 2199.474609375, -396.8580627441406, -573.45263671875, -601.3493041992188]\n",
      "\n",
      "Instance 1801 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 12: [0.7756279706954956, 2.3091976642608643, -0.3345298171043396, -1.2533429861068726, -1.164483666419983]\n",
      "Grand sum of 1356 tensor sets is: [532.485107421875, 2201.78369140625, -397.1925964355469, -574.7059936523438, -602.5137939453125]\n",
      "\n",
      "Instance 1802 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 26: [-0.12661480903625488, 5.4467315673828125, 0.7378829717636108, 1.321735143661499, -0.7286618947982788]\n",
      "Grand sum of 1357 tensor sets is: [532.3585205078125, 2207.23046875, -396.4547119140625, -573.38427734375, -603.242431640625]\n",
      "\n",
      "Instance 1803 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 5: [0.46898022294044495, 1.8955529928207397, 0.24403749406337738, -1.9863793849945068, -2.4404892921447754]\n",
      "Grand sum of 1358 tensor sets is: [532.8275146484375, 2209.1259765625, -396.2106628417969, -575.3706665039062, -605.6829223632812]\n",
      "\n",
      "Instance 1804 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 16: [0.904191255569458, 1.2312450408935547, -0.6142644286155701, -0.9423782825469971, 0.17366153001785278]\n",
      "Grand sum of 1359 tensor sets is: [533.731689453125, 2210.357177734375, -396.8249206542969, -576.3130493164062, -605.50927734375]\n",
      "\n",
      "Instance 1805 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16, 33]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 16: [-0.9735534191131592, 1.4280320405960083, -0.12734930217266083, -0.6918582320213318, -2.8806700706481934]\n",
      "car at index 33: [0.9747537970542908, 0.6798098683357239, -0.5051027536392212, -0.2813475728034973, -1.259454607963562]\n",
      "Grand sum of 1360 tensor sets is: [533.7322998046875, 2211.4111328125, -397.1411437988281, -576.7996826171875, -607.579345703125]\n",
      "\n",
      "Instance 1806 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 9: [-0.07287457585334778, 1.1291248798370361, 0.7727987766265869, -3.480067729949951, 0.6293919682502747]\n",
      "Grand sum of 1361 tensor sets is: [533.659423828125, 2212.540283203125, -396.36834716796875, -580.2797241210938, -606.949951171875]\n",
      "\n",
      "Instance 1807 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 4: [1.0151842832565308, 2.85809326171875, -0.5400334000587463, -0.4629516303539276, 0.6221609711647034]\n",
      "Grand sum of 1362 tensor sets is: [534.6746215820312, 2215.3984375, -396.90838623046875, -580.74267578125, -606.3278198242188]\n",
      "\n",
      "Instance 1808 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6, 26, 40]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "car at index 6: [1.4113179445266724, 1.969515085220337, -0.8686373233795166, -1.270142674446106, 0.8745416402816772]\n",
      "car at index 26: [0.9393206834793091, 2.883286952972412, -0.23656754195690155, -3.1481246948242188, -0.8773607015609741]\n",
      "car at index 40: [0.7632534503936768, 1.782193899154663, -0.7917232513427734, -1.6499271392822266, -0.2645432949066162]\n",
      "Grand sum of 1363 tensor sets is: [535.7125854492188, 2217.610107421875, -397.54071044921875, -582.765380859375, -606.4169311523438]\n",
      "\n",
      "Instance 1809 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([96, 13, 768])\n",
      "Shape of summed layers is: 96 x 768\n",
      "car at index 9: [0.8625897169113159, -0.967861533164978, -0.27002936601638794, 1.1863915920257568, 4.50601863861084]\n",
      "Grand sum of 1364 tensor sets is: [536.5751953125, 2216.642333984375, -397.81072998046875, -581.5789794921875, -601.910888671875]\n",
      "\n",
      "Instance 1810 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1811 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1812 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "car at index 34: [1.1340982913970947, -0.9922190308570862, -0.8673510551452637, 2.0625600814819336, 1.311204195022583]\n",
      "Grand sum of 1365 tensor sets is: [537.7092895507812, 2215.650146484375, -398.6780700683594, -579.5164184570312, -600.5996704101562]\n",
      "\n",
      "Instance 1813 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1814 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1815 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 4: [-0.23509670794010162, 1.5920113325119019, 0.07533038407564163, 0.294240266084671, -1.6124284267425537]\n",
      "Grand sum of 1366 tensor sets is: [537.4741821289062, 2217.2421875, -398.6027526855469, -579.22216796875, -602.2120971679688]\n",
      "\n",
      "Instance 1816 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1817 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 9: [0.3591257631778717, 2.2141671180725098, -1.2028802633285522, 0.41385728120803833, -2.310786247253418]\n",
      "Grand sum of 1367 tensor sets is: [537.8333129882812, 2219.456298828125, -399.8056335449219, -578.8082885742188, -604.5228881835938]\n",
      "\n",
      "Instance 1818 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1819 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1820 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 20: [0.29008227586746216, 1.9970579147338867, 0.16272954642772675, 0.9235777854919434, 1.501127004623413]\n",
      "Grand sum of 1368 tensor sets is: [538.1234130859375, 2221.453369140625, -399.6429138183594, -577.8847045898438, -603.0217895507812]\n",
      "\n",
      "Instance 1821 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 13: [0.913756251335144, 1.4556212425231934, -0.25893542170524597, -0.8000431656837463, -0.1768912374973297]\n",
      "Grand sum of 1369 tensor sets is: [539.0371704101562, 2222.908935546875, -399.90185546875, -578.6847534179688, -603.1986694335938]\n",
      "\n",
      "Instance 1822 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1823 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 9: [-0.5279299020767212, 1.0342023372650146, -1.1200662851333618, -0.40823596715927124, -1.856662631034851]\n",
      "Grand sum of 1370 tensor sets is: [538.5092163085938, 2223.943115234375, -401.02191162109375, -579.093017578125, -605.0553588867188]\n",
      "\n",
      "Instance 1824 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1825 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 5: [-0.4118591248989105, 2.400219678878784, -0.5814874768257141, 1.9865303039550781, -1.659644365310669]\n",
      "Grand sum of 1371 tensor sets is: [538.0973510742188, 2226.34326171875, -401.6033935546875, -577.1065063476562, -606.7150268554688]\n",
      "\n",
      "Instance 1826 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 20: [0.5674139261245728, 1.8944213390350342, 0.9131309390068054, 1.6611177921295166, -2.3993706703186035]\n",
      "Grand sum of 1372 tensor sets is: [538.664794921875, 2228.23779296875, -400.6902770996094, -575.4453735351562, -609.1143798828125]\n",
      "\n",
      "Instance 1827 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 19: [1.1494932174682617, 2.2128262519836426, -0.3751460313796997, -1.4242526292800903, 0.03963138163089752]\n",
      "Grand sum of 1373 tensor sets is: [539.8142700195312, 2230.45068359375, -401.0654296875, -576.86962890625, -609.0747680664062]\n",
      "\n",
      "Instance 1828 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1829 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [185, 195, 206, 210]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 185: [0.9227235317230225, 1.8280186653137207, -1.6366472244262695, -0.3030359745025635, -0.5601290464401245]\n",
      "car at index 195: [1.2540801763534546, 0.021760210394859314, -1.6516214609146118, 0.18948596715927124, -0.9766244888305664]\n",
      "car at index 206: [0.2883068919181824, 1.7267096042633057, -0.15957346558570862, 0.6103811264038086, 1.0673035383224487]\n",
      "car at index 210: [0.642581045627594, 2.2169041633605957, -0.23664340376853943, -0.4146449565887451, 1.555628776550293]\n",
      "Grand sum of 1374 tensor sets is: [540.5911865234375, 2231.89892578125, -401.9865417480469, -576.8490600585938, -608.80322265625]\n",
      "\n",
      "Instance 1830 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 5: [-0.2036377191543579, 1.5842797756195068, -0.04344296455383301, -1.8169089555740356, -1.305199384689331]\n",
      "Grand sum of 1375 tensor sets is: [540.3875732421875, 2233.483154296875, -402.0299987792969, -578.6659545898438, -610.1083984375]\n",
      "\n",
      "Instance 1831 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "car at index 28: [-0.18839383125305176, 1.5602364540100098, 0.5779149532318115, 1.4218270778656006, -2.3871054649353027]\n",
      "Grand sum of 1376 tensor sets is: [540.1991577148438, 2235.04345703125, -401.45208740234375, -577.244140625, -612.4954833984375]\n",
      "\n",
      "Instance 1832 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1833 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 9: [1.539341688156128, 1.721354365348816, -1.1944353580474854, -1.0224164724349976, -0.4783373475074768]\n",
      "Grand sum of 1377 tensor sets is: [541.738525390625, 2236.764892578125, -402.6465148925781, -578.2665405273438, -612.9738159179688]\n",
      "\n",
      "Instance 1834 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 6: [-0.5910548567771912, 1.7382786273956299, 0.5025734901428223, 0.13859567046165466, -2.7289364337921143]\n",
      "Grand sum of 1378 tensor sets is: [541.1474609375, 2238.503173828125, -402.1439514160156, -578.1279296875, -615.7027587890625]\n",
      "\n",
      "Instance 1835 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1836 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 16: [1.056018352508545, 2.1070468425750732, 0.1476946473121643, 0.5097435712814331, -2.2069458961486816]\n",
      "Grand sum of 1379 tensor sets is: [542.2034912109375, 2240.610107421875, -401.9962463378906, -577.6181640625, -617.9097290039062]\n",
      "\n",
      "Instance 1837 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [1, 12]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 1: [1.064674735069275, 2.1486616134643555, -1.0427411794662476, 0.6697355508804321, 1.1621750593185425]\n",
      "car at index 12: [1.0685534477233887, 2.7401113510131836, 0.44979459047317505, 0.3333149254322052, 0.6959309577941895]\n",
      "Grand sum of 1380 tensor sets is: [543.2700805664062, 2243.054443359375, -402.292724609375, -577.1166381835938, -616.9806518554688]\n",
      "\n",
      "Instance 1838 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 9: [1.946509838104248, -1.1320960521697998, -0.09476436674594879, 1.413802981376648, 0.6881838440895081]\n",
      "Grand sum of 1381 tensor sets is: [545.2166137695312, 2241.92236328125, -402.3874816894531, -575.7028198242188, -616.29248046875]\n",
      "\n",
      "Instance 1839 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5, 21]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 5: [0.243363156914711, 1.1209220886230469, -1.6388510465621948, -2.7079529762268066, -1.5217938423156738]\n",
      "car at index 21: [-0.9592444896697998, 1.6982632875442505, -0.46189194917678833, -2.2162604331970215, -2.083627462387085]\n",
      "Grand sum of 1382 tensor sets is: [544.858642578125, 2243.33203125, -403.4378662109375, -578.1649169921875, -618.09521484375]\n",
      "\n",
      "Instance 1840 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 47]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 7: [0.7560781240463257, 1.592909812927246, -0.572348415851593, 0.1756933629512787, -1.9016339778900146]\n",
      "car at index 47: [-0.2461206018924713, 1.4118090867996216, -0.8985605239868164, -0.30819398164749146, -1.9890527725219727]\n",
      "Grand sum of 1383 tensor sets is: [545.1136474609375, 2244.83447265625, -404.1733093261719, -578.2311401367188, -620.0405883789062]\n",
      "\n",
      "Instance 1841 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1842 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 7: [-0.6489450335502625, 2.012122631072998, -0.46711841225624084, 1.8982361555099487, -3.6389060020446777]\n",
      "Grand sum of 1384 tensor sets is: [544.4647216796875, 2246.8466796875, -404.64044189453125, -576.3328857421875, -623.6795043945312]\n",
      "\n",
      "Instance 1843 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1844 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "car at index 3: [-0.5259571671485901, 2.4536192417144775, -0.26023274660110474, -0.9719247817993164, -2.155341625213623]\n",
      "Grand sum of 1385 tensor sets is: [543.9387817382812, 2249.30029296875, -404.9006652832031, -577.3048095703125, -625.8348388671875]\n",
      "\n",
      "Instance 1845 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1846 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 3: [0.24113133549690247, 2.018177032470703, -0.8072721362113953, -2.9362258911132812, -0.2572895586490631]\n",
      "Grand sum of 1386 tensor sets is: [544.179931640625, 2251.318359375, -405.70794677734375, -580.2410278320312, -626.0921020507812]\n",
      "\n",
      "Instance 1847 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "car at index 48: [0.19696711003780365, -0.4367152452468872, -1.6965967416763306, 1.2814249992370605, 2.090348482131958]\n",
      "Grand sum of 1387 tensor sets is: [544.3768920898438, 2250.881591796875, -407.404541015625, -578.9595947265625, -624.0017700195312]\n",
      "\n",
      "Instance 1848 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "car at index 19: [0.5624754428863525, 2.6260647773742676, -1.1239607334136963, -1.774344801902771, -0.16545741260051727]\n",
      "Grand sum of 1388 tensor sets is: [544.9393920898438, 2253.507568359375, -408.52850341796875, -580.7339477539062, -624.167236328125]\n",
      "\n",
      "Instance 1849 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 14: [0.3401907682418823, 2.171011447906494, -0.7774637937545776, -0.20795583724975586, -0.2848491072654724]\n",
      "Grand sum of 1389 tensor sets is: [545.2796020507812, 2255.678466796875, -409.30596923828125, -580.94189453125, -624.4520874023438]\n",
      "\n",
      "Instance 1850 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 11: [0.49129390716552734, 0.9473126530647278, -0.49489519000053406, 0.34504538774490356, 0.18612682819366455]\n",
      "Grand sum of 1390 tensor sets is: [545.7708740234375, 2256.625732421875, -409.8008728027344, -580.5968627929688, -624.2659301757812]\n",
      "\n",
      "Instance 1851 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1852 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1853 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 4: [1.1220200061798096, 0.7904162406921387, -0.169931098818779, -2.2036406993865967, -1.384897232055664]\n",
      "Grand sum of 1391 tensor sets is: [546.8928833007812, 2257.416259765625, -409.9707946777344, -582.8004760742188, -625.6508178710938]\n",
      "\n",
      "Instance 1854 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1855 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 18: [0.21181240677833557, 3.0086722373962402, 0.16079404950141907, -2.150794744491577, -1.1827400922775269]\n",
      "Grand sum of 1392 tensor sets is: [547.1046752929688, 2260.425048828125, -409.80999755859375, -584.9512939453125, -626.8335571289062]\n",
      "\n",
      "Instance 1856 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [1, 6]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 1: [0.4326351284980774, 0.3784176707267761, -0.7516354918479919, 0.1312677562236786, 0.7534320950508118]\n",
      "car at index 6: [0.36976274847984314, 0.6803067922592163, -0.13706250488758087, 0.10943814367055893, -1.6319395303726196]\n",
      "Grand sum of 1393 tensor sets is: [547.505859375, 2260.954345703125, -410.25433349609375, -584.8309326171875, -627.2728271484375]\n",
      "\n",
      "Instance 1857 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1858 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([99, 13, 768])\n",
      "Shape of summed layers is: 99 x 768\n",
      "car at index 17: [-0.44690099358558655, -0.15620644390583038, -1.4030228853225708, 1.739141583442688, -2.3342432975769043]\n",
      "Grand sum of 1394 tensor sets is: [547.0589599609375, 2260.798095703125, -411.6573486328125, -583.091796875, -629.6070556640625]\n",
      "\n",
      "Instance 1859 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 12: [0.7883492112159729, 3.6696624755859375, 1.408756971359253, 1.4900881052017212, -0.5421361327171326]\n",
      "Grand sum of 1395 tensor sets is: [547.8472900390625, 2264.4677734375, -410.24859619140625, -581.6016845703125, -630.149169921875]\n",
      "\n",
      "Instance 1860 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1861 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 22: [-0.6705458164215088, 2.90203595161438, 1.2911897897720337, -0.31309884786605835, -0.9601607322692871]\n",
      "Grand sum of 1396 tensor sets is: [547.1767578125, 2267.369873046875, -408.9573974609375, -581.914794921875, -631.1093139648438]\n",
      "\n",
      "Instance 1862 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 10: [0.12760165333747864, 2.5598461627960205, -0.2126004993915558, 1.431509017944336, -1.0839099884033203]\n",
      "Grand sum of 1397 tensor sets is: [547.3043823242188, 2269.9296875, -409.16998291015625, -580.4832763671875, -632.1932373046875]\n",
      "\n",
      "Instance 1863 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 15: [0.5456152558326721, 1.2586685419082642, 0.07187454402446747, -1.443554162979126, -0.8645212650299072]\n",
      "Grand sum of 1398 tensor sets is: [547.8499755859375, 2271.1884765625, -409.0981140136719, -581.9268188476562, -633.0577392578125]\n",
      "\n",
      "Instance 1864 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 43: [0.4122127294540405, 2.0258233547210693, -0.5648598670959473, -0.7292879223823547, -0.10718367248773575]\n",
      "Grand sum of 1399 tensor sets is: [548.26220703125, 2273.21435546875, -409.6629638671875, -582.6561279296875, -633.1649169921875]\n",
      "\n",
      "Instance 1865 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 12: [0.3086026906967163, 1.7394258975982666, 0.5259462594985962, -0.984571635723114, 0.2542429566383362]\n",
      "Grand sum of 1400 tensor sets is: [548.57080078125, 2274.953857421875, -409.13702392578125, -583.6406860351562, -632.91064453125]\n",
      "\n",
      "Instance 1866 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 44: [0.8798028230667114, 1.887597918510437, 0.03294253721833229, -0.13129986822605133, -1.4473648071289062]\n",
      "Grand sum of 1401 tensor sets is: [549.4506225585938, 2276.841552734375, -409.1040954589844, -583.77197265625, -634.3580322265625]\n",
      "\n",
      "Instance 1867 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 15: [0.06904026865959167, 2.0699527263641357, -0.19329294562339783, 0.3255554139614105, -0.9660124182701111]\n",
      "Grand sum of 1402 tensor sets is: [549.5196533203125, 2278.91162109375, -409.2973937988281, -583.4464111328125, -635.3240356445312]\n",
      "\n",
      "Instance 1868 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 21: [-0.0013185292482376099, 2.0330934524536133, -0.4909957945346832, -0.34847119450569153, -1.6023377180099487]\n",
      "Grand sum of 1403 tensor sets is: [549.518310546875, 2280.94482421875, -409.78839111328125, -583.7948608398438, -636.9263916015625]\n",
      "\n",
      "Instance 1869 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9, 17]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 9: [0.9221291542053223, 1.1975334882736206, -0.34467247128486633, -0.5027514100074768, 0.5603002309799194]\n",
      "car at index 17: [0.8791962265968323, 1.6371896266937256, -0.8232624530792236, -0.6578643321990967, -1.2705055475234985]\n",
      "Grand sum of 1404 tensor sets is: [550.4189453125, 2282.3623046875, -410.3723449707031, -584.3751831054688, -637.281494140625]\n",
      "\n",
      "Instance 1870 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1871 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 11: [-0.41219764947891235, 1.7459396123886108, -1.172688603401184, -2.184128761291504, -0.1697944849729538]\n",
      "Grand sum of 1405 tensor sets is: [550.0067749023438, 2284.108154296875, -411.5450439453125, -586.559326171875, -637.4512939453125]\n",
      "\n",
      "Instance 1872 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 18: [0.5027228593826294, 0.03307221829891205, -0.3682069182395935, 0.6355028748512268, -0.9022837281227112]\n",
      "Grand sum of 1406 tensor sets is: [550.509521484375, 2284.14111328125, -411.9132385253906, -585.923828125, -638.3535766601562]\n",
      "\n",
      "Instance 1873 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 6: [1.109812617301941, 1.000185251235962, -1.1382689476013184, -0.4711992144584656, -1.019654631614685]\n",
      "Grand sum of 1407 tensor sets is: [551.6193237304688, 2285.141357421875, -413.051513671875, -586.39501953125, -639.3732299804688]\n",
      "\n",
      "Instance 1874 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 8: [0.6272561550140381, 2.4196128845214844, -0.9196394085884094, 0.3255048096179962, -0.5400972962379456]\n",
      "Grand sum of 1408 tensor sets is: [552.24658203125, 2287.56103515625, -413.9711608886719, -586.0695190429688, -639.913330078125]\n",
      "\n",
      "Instance 1875 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 17: [-0.16176417469978333, 0.7896863222122192, 1.1143985986709595, 2.3862202167510986, -2.8734891414642334]\n",
      "Grand sum of 1409 tensor sets is: [552.0848388671875, 2288.350830078125, -412.85675048828125, -583.6832885742188, -642.7868041992188]\n",
      "\n",
      "Instance 1876 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 12: [-0.1577366590499878, 0.16665729880332947, 0.3058450520038605, 1.8946843147277832, -0.16075792908668518]\n",
      "Grand sum of 1410 tensor sets is: [551.9271240234375, 2288.517578125, -412.5509033203125, -581.78857421875, -642.9475708007812]\n",
      "\n",
      "Instance 1877 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [53, 83]\n",
      "Size of token embeddings is torch.Size([115, 13, 768])\n",
      "Shape of summed layers is: 115 x 768\n",
      "car at index 53: [0.7019076347351074, 1.772056221961975, -0.41616562008857727, -1.190219759941101, -0.0462932288646698]\n",
      "car at index 83: [0.0472736656665802, 2.0120599269866943, -0.07355264574289322, -2.1506927013397217, 0.22362810373306274]\n",
      "Grand sum of 1411 tensor sets is: [552.3016967773438, 2290.40966796875, -412.7957763671875, -583.4590454101562, -642.85888671875]\n",
      "\n",
      "Instance 1878 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 19: [0.6093558669090271, -0.4280335307121277, -0.16956952214241028, 3.075209856033325, 0.1536668837070465]\n",
      "Grand sum of 1412 tensor sets is: [552.9110717773438, 2289.981689453125, -412.96533203125, -580.3838500976562, -642.7052001953125]\n",
      "\n",
      "Instance 1879 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 15: [0.5634946823120117, 1.364711046218872, 0.5257006883621216, -0.5709870457649231, 0.07859078049659729]\n",
      "Grand sum of 1413 tensor sets is: [553.4745483398438, 2291.346435546875, -412.43963623046875, -580.954833984375, -642.6265869140625]\n",
      "\n",
      "Instance 1880 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 30: [0.44149813055992126, 1.1880285739898682, -0.0417892262339592, 1.5337860584259033, -0.9103814959526062]\n",
      "Grand sum of 1414 tensor sets is: [553.9160766601562, 2292.534423828125, -412.4814147949219, -579.4210205078125, -643.5369873046875]\n",
      "\n",
      "Instance 1881 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 19: [-0.9088457822799683, 2.6159520149230957, 0.48501327633857727, -0.8610239028930664, -3.8284430503845215]\n",
      "Grand sum of 1415 tensor sets is: [553.0072021484375, 2295.150390625, -411.99639892578125, -580.2820434570312, -647.3654174804688]\n",
      "\n",
      "Instance 1882 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 10: [0.6696166396141052, 2.0975141525268555, 0.4765411615371704, 0.9991117119789124, 0.3741573393344879]\n",
      "Grand sum of 1416 tensor sets is: [553.6768188476562, 2297.247802734375, -411.5198669433594, -579.282958984375, -646.9912719726562]\n",
      "\n",
      "Instance 1883 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15, 25]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 15: [0.5940617918968201, 1.37178635597229, -1.1198819875717163, -2.0236551761627197, 2.6393890380859375]\n",
      "car at index 25: [0.6024391055107117, 0.7575421333312988, -0.6202216744422913, 0.17938530445098877, 1.6100609302520752]\n",
      "Grand sum of 1417 tensor sets is: [554.2750854492188, 2298.3125, -412.3899230957031, -580.205078125, -644.8665771484375]\n",
      "\n",
      "Instance 1884 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1885 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 7: [-0.3146740794181824, 1.0947415828704834, 0.13089877367019653, 2.5722670555114746, -3.0643668174743652]\n",
      "Grand sum of 1418 tensor sets is: [553.9603881835938, 2299.4072265625, -412.259033203125, -577.6328125, -647.9309692382812]\n",
      "\n",
      "Instance 1886 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 12: [-0.1598987579345703, 0.44982337951660156, -0.5816481709480286, 4.181264877319336, -2.6428074836730957]\n",
      "Grand sum of 1419 tensor sets is: [553.8004760742188, 2299.85693359375, -412.8406677246094, -573.4515380859375, -650.5737915039062]\n",
      "\n",
      "Instance 1887 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 9: [-0.3771006762981415, 1.5236486196517944, 0.15272274613380432, -0.812812864780426, -1.602191686630249]\n",
      "Grand sum of 1420 tensor sets is: [553.4234008789062, 2301.380615234375, -412.6879577636719, -574.2643432617188, -652.1759643554688]\n",
      "\n",
      "Instance 1888 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 23: [-0.21488499641418457, 1.1846129894256592, 0.07764963060617447, -1.314852237701416, -0.28687721490859985]\n",
      "Grand sum of 1421 tensor sets is: [553.20849609375, 2302.565185546875, -412.6103210449219, -575.5792236328125, -652.4628295898438]\n",
      "\n",
      "Instance 1889 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 10: [0.6687584519386292, 1.366288423538208, -0.9644168019294739, 1.6214748620986938, -1.1171107292175293]\n",
      "Grand sum of 1422 tensor sets is: [553.8772583007812, 2303.931396484375, -413.5747375488281, -573.957763671875, -653.5799560546875]\n",
      "\n",
      "Instance 1890 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1891 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 38: [0.5802810192108154, 2.3883445262908936, 0.32199668884277344, -3.57307505607605, -2.5109927654266357]\n",
      "Grand sum of 1423 tensor sets is: [554.45751953125, 2306.31982421875, -413.25274658203125, -577.5308227539062, -656.0909423828125]\n",
      "\n",
      "Instance 1892 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12, 20]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 12: [0.8803752660751343, 4.2109856605529785, -0.5396491885185242, 0.23331913352012634, -0.052095115184783936]\n",
      "car at index 20: [0.515160322189331, 3.4122462272644043, 0.323536217212677, -0.8589909076690674, 1.8385201692581177]\n",
      "Grand sum of 1424 tensor sets is: [555.1552734375, 2310.13134765625, -413.3608093261719, -577.8436889648438, -655.19775390625]\n",
      "\n",
      "Instance 1893 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6, 35]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 6: [0.3995026648044586, 1.3802016973495483, 1.0589641332626343, -2.426919937133789, 1.0027458667755127]\n",
      "car at index 35: [-0.4052315354347229, 3.1192538738250732, 0.7501972317695618, -1.5157090425491333, -3.3281490802764893]\n",
      "Grand sum of 1425 tensor sets is: [555.1524047851562, 2312.381103515625, -412.45623779296875, -579.8150024414062, -656.3604736328125]\n",
      "\n",
      "Instance 1894 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 11: [0.09446392208337784, 1.897304654121399, 0.5544774532318115, 0.061816632747650146, -1.0248671770095825]\n",
      "Grand sum of 1426 tensor sets is: [555.2468872070312, 2314.2783203125, -411.9017639160156, -579.753173828125, -657.3853149414062]\n",
      "\n",
      "Instance 1895 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 31: [0.6407320499420166, 2.6260948181152344, -0.5516409873962402, -1.6144014596939087, -0.6937705874443054]\n",
      "Grand sum of 1427 tensor sets is: [555.8876342773438, 2316.904296875, -412.4533996582031, -581.3675537109375, -658.0791015625]\n",
      "\n",
      "Instance 1896 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1897 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 9: [0.8688203692436218, 1.986860752105713, 0.3887818455696106, -1.001659870147705, -1.34639310836792]\n",
      "Grand sum of 1428 tensor sets is: [556.7564697265625, 2318.89111328125, -412.0646057128906, -582.3692016601562, -659.4254760742188]\n",
      "\n",
      "Instance 1898 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 8: [-0.5132115483283997, 1.7653850317001343, -0.11025083065032959, 0.4957565367221832, -1.7473671436309814]\n",
      "Grand sum of 1429 tensor sets is: [556.2432861328125, 2320.656494140625, -412.17486572265625, -581.8734741210938, -661.1728515625]\n",
      "\n",
      "Instance 1899 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17, 20, 25, 30]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "car at index 17: [0.7242787480354309, 1.222398042678833, -0.19793209433555603, -1.2858877182006836, -2.2380378246307373]\n",
      "car at index 20: [0.4404619336128235, 1.8278440237045288, 0.2716705799102783, -0.8880179524421692, -1.6394338607788086]\n",
      "car at index 25: [0.02067974954843521, 1.470453143119812, 0.07614052295684814, -1.6293517351150513, -2.8649094104766846]\n",
      "car at index 30: [0.08886465430259705, 1.290534257888794, 0.08438226580619812, -1.7489922046661377, -2.898124933242798]\n",
      "Grand sum of 1430 tensor sets is: [556.5618286132812, 2322.109375, -412.1163024902344, -583.2615356445312, -663.5830078125]\n",
      "\n",
      "Instance 1900 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 27: [-0.19790001213550568, 2.201857566833496, -0.29018956422805786, -2.448321580886841, -2.106405258178711]\n",
      "Grand sum of 1431 tensor sets is: [556.3639526367188, 2324.311279296875, -412.406494140625, -585.7098388671875, -665.6893920898438]\n",
      "\n",
      "Instance 1901 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1902 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 7: [-0.22557520866394043, 2.3461480140686035, 0.0037744268774986267, 1.4975039958953857, -1.8456634283065796]\n",
      "Grand sum of 1432 tensor sets is: [556.1383666992188, 2326.657470703125, -412.4027099609375, -584.2123413085938, -667.5350341796875]\n",
      "\n",
      "Instance 1903 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 30: [-0.05753639340400696, 3.4107091426849365, -0.39869824051856995, 0.24184955656528473, 0.5705156326293945]\n",
      "Grand sum of 1433 tensor sets is: [556.080810546875, 2330.068115234375, -412.8014221191406, -583.9705200195312, -666.9645385742188]\n",
      "\n",
      "Instance 1904 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 28: [-0.13585138320922852, 2.890146255493164, 1.0066652297973633, -2.2601850032806396, -2.993946075439453]\n",
      "Grand sum of 1434 tensor sets is: [555.9449462890625, 2332.958251953125, -411.7947692871094, -586.230712890625, -669.95849609375]\n",
      "\n",
      "Instance 1905 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 26: [0.6554191708564758, 1.8300747871398926, -0.3280097246170044, -0.7583394646644592, -1.664613962173462]\n",
      "Grand sum of 1435 tensor sets is: [556.600341796875, 2334.788330078125, -412.1227722167969, -586.9890747070312, -671.6231079101562]\n",
      "\n",
      "Instance 1906 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [73]\n",
      "Size of token embeddings is torch.Size([93, 13, 768])\n",
      "Shape of summed layers is: 93 x 768\n",
      "car at index 73: [1.0797432661056519, 0.424882173538208, -1.6533706188201904, -0.10800756514072418, 2.9250648021698]\n",
      "Grand sum of 1436 tensor sets is: [557.6801147460938, 2335.213134765625, -413.7761535644531, -587.0971069335938, -668.6980590820312]\n",
      "\n",
      "Instance 1907 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 25: [0.6294340491294861, 1.7899545431137085, -1.3467665910720825, -0.6557190418243408, -1.372619390487671]\n",
      "Grand sum of 1437 tensor sets is: [558.3095703125, 2337.003173828125, -415.1229248046875, -587.7528076171875, -670.0706787109375]\n",
      "\n",
      "Instance 1908 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1909 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 10: [-0.42974260449409485, 4.330865859985352, -0.35465753078460693, 0.7335762977600098, 2.174827814102173]\n",
      "Grand sum of 1438 tensor sets is: [557.8798217773438, 2341.333984375, -415.4775695800781, -587.0192260742188, -667.8958740234375]\n",
      "\n",
      "Instance 1910 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "car at index 12: [0.5139888525009155, 1.6607545614242554, 0.5883565545082092, -1.5519758462905884, 0.2044614851474762]\n",
      "Grand sum of 1439 tensor sets is: [558.393798828125, 2342.99462890625, -414.88922119140625, -588.5712280273438, -667.69140625]\n",
      "\n",
      "Instance 1911 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 14: [-0.020325085148215294, 3.491342544555664, 0.29828640818595886, 2.0962986946105957, -0.8973249793052673]\n",
      "Grand sum of 1440 tensor sets is: [558.3734741210938, 2346.486083984375, -414.5909423828125, -586.4749145507812, -668.5887451171875]\n",
      "\n",
      "Instance 1912 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 20: [-0.42689529061317444, 1.5952192544937134, -0.8818086385726929, -2.027200698852539, -1.482189416885376]\n",
      "Grand sum of 1441 tensor sets is: [557.9465942382812, 2348.081298828125, -415.4727478027344, -588.5021362304688, -670.0709228515625]\n",
      "\n",
      "Instance 1913 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "car at index 53: [-0.3486129641532898, 2.5142605304718018, -0.1323358416557312, -0.7286252975463867, -1.8380179405212402]\n",
      "Grand sum of 1442 tensor sets is: [557.5979614257812, 2350.595458984375, -415.6050720214844, -589.2307739257812, -671.908935546875]\n",
      "\n",
      "Instance 1914 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 11: [0.3119999170303345, 1.213815689086914, -1.3051083087921143, -1.669263482093811, -1.7248103618621826]\n",
      "Grand sum of 1443 tensor sets is: [557.9099731445312, 2351.809326171875, -416.9101867675781, -590.9000244140625, -673.6337280273438]\n",
      "\n",
      "Instance 1915 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 22: [-0.20547863841056824, 1.7968932390213013, -0.13202331960201263, -1.461184024810791, -1.5468738079071045]\n",
      "Grand sum of 1444 tensor sets is: [557.7044677734375, 2353.606201171875, -417.0422058105469, -592.3612060546875, -675.1806030273438]\n",
      "\n",
      "Instance 1916 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "car at index 6: [0.10361917316913605, 1.5090020895004272, 1.5756556987762451, -1.0590981245040894, -2.2084619998931885]\n",
      "Grand sum of 1445 tensor sets is: [557.80810546875, 2355.115234375, -415.466552734375, -593.4202880859375, -677.3890380859375]\n",
      "\n",
      "Instance 1917 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1918 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 13: [0.476380318403244, 2.3028595447540283, -1.4719359874725342, 1.0514239072799683, -0.29737067222595215]\n",
      "Grand sum of 1446 tensor sets is: [558.2844848632812, 2357.418212890625, -416.9384765625, -592.3688354492188, -677.6864013671875]\n",
      "\n",
      "Instance 1919 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 7: [-0.5389019846916199, 2.5290355682373047, -0.5890585780143738, -1.8549009561538696, -1.5738767385482788]\n",
      "Grand sum of 1447 tensor sets is: [557.74560546875, 2359.947265625, -417.52752685546875, -594.2237548828125, -679.26025390625]\n",
      "\n",
      "Instance 1920 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 20: [0.2316894829273224, 1.1776000261306763, -0.45897218585014343, 1.791783094406128, -0.31438973546028137]\n",
      "Grand sum of 1448 tensor sets is: [557.977294921875, 2361.124755859375, -417.98651123046875, -592.4319458007812, -679.5746459960938]\n",
      "\n",
      "Instance 1921 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 23: [0.308824360370636, 1.186735987663269, 0.041418664157390594, 0.2788686454296112, 0.09456801414489746]\n",
      "Grand sum of 1449 tensor sets is: [558.2861328125, 2362.3115234375, -417.9450988769531, -592.153076171875, -679.4801025390625]\n",
      "\n",
      "Instance 1922 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3, 33]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 3: [0.3782065510749817, 2.2572765350341797, -0.1683565229177475, 0.3791579306125641, -0.8561955094337463]\n",
      "car at index 33: [0.6562938690185547, 2.531358242034912, -0.32797104120254517, -0.6847097873687744, -0.2724643051624298]\n",
      "Grand sum of 1450 tensor sets is: [558.8034057617188, 2364.705810546875, -418.1932678222656, -592.3058471679688, -680.04443359375]\n",
      "\n",
      "Instance 1923 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "car at index 32: [1.6618432998657227, 0.003438785672187805, 0.5489473342895508, -0.06823503971099854, 0.6107518672943115]\n",
      "Grand sum of 1451 tensor sets is: [560.4652709960938, 2364.709228515625, -417.6443176269531, -592.3740844726562, -679.4336547851562]\n",
      "\n",
      "Instance 1924 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 7: [0.3159121572971344, 1.6457401514053345, -1.2761976718902588, -1.236846923828125, 0.9159621596336365]\n",
      "Grand sum of 1452 tensor sets is: [560.7811889648438, 2366.35498046875, -418.9205017089844, -593.6109619140625, -678.5177001953125]\n",
      "\n",
      "Instance 1925 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 9: [0.03735259175300598, 2.13474178314209, -1.0726186037063599, -2.503157377243042, -0.10842017829418182]\n",
      "Grand sum of 1453 tensor sets is: [560.8185424804688, 2368.48974609375, -419.9931335449219, -596.1141357421875, -678.6260986328125]\n",
      "\n",
      "Instance 1926 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 39: [0.445712149143219, 1.9804226160049438, -1.0730551481246948, -1.4025017023086548, 0.03554758429527283]\n",
      "Grand sum of 1454 tensor sets is: [561.2642822265625, 2370.47021484375, -421.0661926269531, -597.5166625976562, -678.590576171875]\n",
      "\n",
      "Instance 1927 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([91, 13, 768])\n",
      "Shape of summed layers is: 91 x 768\n",
      "car at index 21: [0.735815703868866, 0.279043972492218, -0.04168049991130829, 1.5145506858825684, 2.3769683837890625]\n",
      "Grand sum of 1455 tensor sets is: [562.0001220703125, 2370.749267578125, -421.1078796386719, -596.0021362304688, -676.213623046875]\n",
      "\n",
      "Instance 1928 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [76]\n",
      "Size of token embeddings is torch.Size([110, 13, 768])\n",
      "Shape of summed layers is: 110 x 768\n",
      "car at index 76: [0.4666948914527893, -1.6687570810317993, -2.9313294887542725, 1.7969353199005127, 1.7903704643249512]\n",
      "Grand sum of 1456 tensor sets is: [562.466796875, 2369.08056640625, -424.0392150878906, -594.2052001953125, -674.4232788085938]\n",
      "\n",
      "Instance 1929 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 20: [0.5831174850463867, 1.6549336910247803, -0.47607356309890747, -2.57320237159729, -0.8793084025382996]\n",
      "Grand sum of 1457 tensor sets is: [563.0499267578125, 2370.735595703125, -424.5152893066406, -596.7783813476562, -675.3026123046875]\n",
      "\n",
      "Instance 1930 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([80, 13, 768])\n",
      "Shape of summed layers is: 80 x 768\n",
      "car at index 34: [1.209854006767273, -0.9392646551132202, -0.01956304907798767, 1.1107429265975952, 2.074105739593506]\n",
      "Grand sum of 1458 tensor sets is: [564.259765625, 2369.79638671875, -424.53485107421875, -595.6676635742188, -673.228515625]\n",
      "\n",
      "Instance 1931 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1932 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 6: [0.6212210655212402, 0.4260699450969696, -0.5699312686920166, -0.6496745944023132, -1.3133212327957153]\n",
      "Grand sum of 1459 tensor sets is: [564.8809814453125, 2370.222412109375, -425.10479736328125, -596.3173217773438, -674.5418090820312]\n",
      "\n",
      "Instance 1933 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1934 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 12: [0.806852400302887, -0.6062864065170288, 0.673358678817749, -0.052927665412425995, 1.0403194427490234]\n",
      "Grand sum of 1460 tensor sets is: [565.6878051757812, 2369.6162109375, -424.4314270019531, -596.3702392578125, -673.50146484375]\n",
      "\n",
      "Instance 1935 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1936 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 29: [-0.45825862884521484, 1.113759994506836, -0.21503639221191406, 0.5627538561820984, -1.0144402980804443]\n",
      "Grand sum of 1461 tensor sets is: [565.2295532226562, 2370.72998046875, -424.6464538574219, -595.8074951171875, -674.5159301757812]\n",
      "\n",
      "Instance 1937 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 5: [1.435086965560913, 2.7406394481658936, 0.021044058725237846, -2.55058217048645, -1.6758791208267212]\n",
      "Grand sum of 1462 tensor sets is: [566.6646118164062, 2373.470703125, -424.6253967285156, -598.3580932617188, -676.1918334960938]\n",
      "\n",
      "Instance 1938 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 43: [0.1064453125, 1.5248533487319946, -1.0151702165603638, -0.3312273621559143, -1.6340969800949097]\n",
      "Grand sum of 1463 tensor sets is: [566.7710571289062, 2374.99560546875, -425.64056396484375, -598.6893310546875, -677.825927734375]\n",
      "\n",
      "Instance 1939 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 9: [-0.40097546577453613, 1.5499154329299927, 0.622665524482727, -1.7215616703033447, -2.367891788482666]\n",
      "Grand sum of 1464 tensor sets is: [566.3700561523438, 2376.54541015625, -425.01788330078125, -600.410888671875, -680.19384765625]\n",
      "\n",
      "Instance 1940 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 20: [1.3408958911895752, 2.3753583431243896, -0.9426171779632568, 0.2189926952123642, -1.0826765298843384]\n",
      "Grand sum of 1465 tensor sets is: [567.7109375, 2378.920654296875, -425.96051025390625, -600.19189453125, -681.2765502929688]\n",
      "\n",
      "Instance 1941 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 2: [-0.3971606492996216, 1.737775206565857, -0.4159707725048065, -1.2791831493377686, -0.7002418041229248]\n",
      "Grand sum of 1466 tensor sets is: [567.3137817382812, 2380.658447265625, -426.3764953613281, -601.4710693359375, -681.976806640625]\n",
      "\n",
      "Instance 1942 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 6: [0.2545458972454071, 1.6761244535446167, -0.22883978486061096, -3.1321358680725098, -1.3244199752807617]\n",
      "Grand sum of 1467 tensor sets is: [567.5682983398438, 2382.33447265625, -426.6053466796875, -604.6032104492188, -683.3012084960938]\n",
      "\n",
      "Instance 1943 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 7: [-0.14732149243354797, 0.673999547958374, -0.8501595854759216, 3.1382863521575928, -2.300889492034912]\n",
      "Grand sum of 1468 tensor sets is: [567.4209594726562, 2383.008544921875, -427.45550537109375, -601.4649047851562, -685.6021118164062]\n",
      "\n",
      "Instance 1944 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 8: [0.5226955413818359, 2.177264451980591, 0.3205935060977936, -1.1147310733795166, -0.6414032578468323]\n",
      "Grand sum of 1469 tensor sets is: [567.9436645507812, 2385.185791015625, -427.1349182128906, -602.5796508789062, -686.2435302734375]\n",
      "\n",
      "Instance 1945 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1946 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 38: [0.8617132902145386, 2.225079298019409, 0.24254131317138672, -2.8120789527893066, -2.301851272583008]\n",
      "Grand sum of 1470 tensor sets is: [568.8053588867188, 2387.410888671875, -426.8923645019531, -605.3917236328125, -688.54541015625]\n",
      "\n",
      "Instance 1947 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [83]\n",
      "Size of token embeddings is torch.Size([146, 13, 768])\n",
      "Shape of summed layers is: 146 x 768\n",
      "car at index 83: [0.7842099666595459, -0.8667717576026917, 1.303114891052246, 0.48844218254089355, 1.3442045450210571]\n",
      "Grand sum of 1471 tensor sets is: [569.5895385742188, 2386.544189453125, -425.5892639160156, -604.9032592773438, -687.2012329101562]\n",
      "\n",
      "Instance 1948 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 16: [0.7989422082901001, 0.8761277198791504, -0.7977009415626526, -0.8891752362251282, -0.2021888792514801]\n",
      "Grand sum of 1472 tensor sets is: [570.3884887695312, 2387.42041015625, -426.386962890625, -605.7924194335938, -687.4034423828125]\n",
      "\n",
      "Instance 1949 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 33: [0.23374450206756592, -1.4648292064666748, -1.0515992641448975, 1.2309162616729736, 3.6863110065460205]\n",
      "Grand sum of 1473 tensor sets is: [570.6222534179688, 2385.95556640625, -427.4385681152344, -604.5615234375, -683.7171020507812]\n",
      "\n",
      "Instance 1950 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1951 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13, 26]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 13: [1.0963213443756104, 2.315387725830078, -0.5476093888282776, -0.3352837860584259, 3.1437695026397705]\n",
      "car at index 26: [0.9000679850578308, 2.8225221633911133, -0.7319295406341553, -0.633129358291626, 1.4038788080215454]\n",
      "Grand sum of 1474 tensor sets is: [571.6204223632812, 2388.5244140625, -428.0783386230469, -605.0457153320312, -681.4432983398438]\n",
      "\n",
      "Instance 1952 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 6: [0.7252352237701416, 1.6100423336029053, -0.6555039286613464, 0.30935394763946533, 2.544055938720703]\n",
      "Grand sum of 1475 tensor sets is: [572.3456420898438, 2390.134521484375, -428.7338562011719, -604.7363891601562, -678.8992309570312]\n",
      "\n",
      "Instance 1953 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 6: [-0.24653804302215576, 1.1841967105865479, -1.2691742181777954, -1.6606247425079346, -0.45903849601745605]\n",
      "Grand sum of 1476 tensor sets is: [572.09912109375, 2391.318603515625, -430.0030212402344, -606.3970336914062, -679.3582763671875]\n",
      "\n",
      "Instance 1954 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1955 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1956 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [342, 382]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 342: [-0.5212914347648621, -0.20281556248664856, -0.3051457405090332, 0.9402506351470947, 3.7351601123809814]\n",
      "car at index 382: [1.8545074462890625, -1.2855037450790405, 0.7320283055305481, -0.18330489099025726, 1.6900068521499634]\n",
      "Grand sum of 1477 tensor sets is: [572.7657470703125, 2390.574462890625, -429.7895812988281, -606.0185546875, -676.6456909179688]\n",
      "\n",
      "Instance 1957 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 42: [0.8565922379493713, 1.3795408010482788, -1.0206246376037598, -0.5512416958808899, 0.2177269458770752]\n",
      "Grand sum of 1478 tensor sets is: [573.622314453125, 2391.9541015625, -430.8102111816406, -606.56982421875, -676.427978515625]\n",
      "\n",
      "Instance 1958 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 7: [0.10591985285282135, 1.8949254751205444, -0.8443973064422607, -3.0330333709716797, -1.0206880569458008]\n",
      "Grand sum of 1479 tensor sets is: [573.7282104492188, 2393.84912109375, -431.65460205078125, -609.6028442382812, -677.4486694335938]\n",
      "\n",
      "Instance 1959 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 14: [-0.17941434681415558, 2.247936964035034, 0.29622548818588257, 0.06564315408468246, -2.033881664276123]\n",
      "Grand sum of 1480 tensor sets is: [573.5487670898438, 2396.09716796875, -431.3583679199219, -609.5372314453125, -679.4825439453125]\n",
      "\n",
      "Instance 1960 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1961 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 11: [-0.6908421516418457, 2.5382702350616455, -0.0669204518198967, -0.6345595121383667, -1.1718604564666748]\n",
      "Grand sum of 1481 tensor sets is: [572.85791015625, 2398.635498046875, -431.42529296875, -610.1718139648438, -680.6544189453125]\n",
      "\n",
      "Instance 1962 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8, 15]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 8: [0.38815128803253174, 1.6251329183578491, -1.1867636442184448, -1.215703010559082, -0.30937471985816956]\n",
      "car at index 15: [-0.5568827390670776, 2.143869161605835, 0.7395299077033997, -2.385037422180176, -2.0749871730804443]\n",
      "Grand sum of 1482 tensor sets is: [572.7735595703125, 2400.52001953125, -431.6488952636719, -611.97216796875, -681.8466186523438]\n",
      "\n",
      "Instance 1963 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 15: [1.2602789402008057, 3.7317421436309814, 0.2610366642475128, -1.5479406118392944, -0.7584348917007446]\n",
      "Grand sum of 1483 tensor sets is: [574.0338134765625, 2404.251708984375, -431.3878479003906, -613.5200805664062, -682.6050415039062]\n",
      "\n",
      "Instance 1964 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 25: [0.1986391246318817, 1.5502169132232666, 0.8467753529548645, -3.4123518466949463, -0.3156570494174957]\n",
      "Grand sum of 1484 tensor sets is: [574.2324829101562, 2405.802001953125, -430.54107666015625, -616.9324340820312, -682.9207153320312]\n",
      "\n",
      "Instance 1965 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 3: [-0.18694721162319183, 1.843418002128601, -1.070474624633789, -2.2402212619781494, 0.04379785805940628]\n",
      "Grand sum of 1485 tensor sets is: [574.0455322265625, 2407.6455078125, -431.6115417480469, -619.1726684570312, -682.8768920898438]\n",
      "\n",
      "Instance 1966 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 2: [0.037117719650268555, 1.7520254850387573, 0.13139593601226807, -2.368345260620117, -1.2353260517120361]\n",
      "Grand sum of 1486 tensor sets is: [574.0826416015625, 2409.3974609375, -431.4801330566406, -621.541015625, -684.1122436523438]\n",
      "\n",
      "Instance 1967 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 14: [0.4580685496330261, -0.30002304911613464, -0.061063364148139954, -1.7866780757904053, -1.8043744564056396]\n",
      "Grand sum of 1487 tensor sets is: [574.5407104492188, 2409.097412109375, -431.54119873046875, -623.3276977539062, -685.9166259765625]\n",
      "\n",
      "Instance 1968 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 3: [0.9428887963294983, 1.588451623916626, -0.47040265798568726, -1.6566985845565796, 1.877027988433838]\n",
      "Grand sum of 1488 tensor sets is: [575.4835815429688, 2410.685791015625, -432.0115966796875, -624.984375, -684.0396118164062]\n",
      "\n",
      "Instance 1969 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19, 70]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([80, 13, 768])\n",
      "Shape of summed layers is: 80 x 768\n",
      "car at index 19: [0.3360101580619812, 0.9218423962593079, -1.7845200300216675, 0.030263729393482208, 2.8870041370391846]\n",
      "car at index 70: [0.28961220383644104, 1.3288538455963135, -1.5914201736450195, 0.47924354672431946, 2.2458488941192627]\n",
      "Grand sum of 1489 tensor sets is: [575.79638671875, 2411.81103515625, -433.6995544433594, -624.7296142578125, -681.4732055664062]\n",
      "\n",
      "Instance 1970 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 7: [-9.971298277378082e-05, 1.6470420360565186, -1.1461912393569946, -1.835148811340332, -1.8937243223190308]\n",
      "Grand sum of 1490 tensor sets is: [575.7962646484375, 2413.4580078125, -434.8457336425781, -626.5647583007812, -683.366943359375]\n",
      "\n",
      "Instance 1971 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 18: [-0.2097357213497162, 4.898232936859131, -0.13572272658348083, 1.5930191278457642, -0.525748074054718]\n",
      "Grand sum of 1491 tensor sets is: [575.5865478515625, 2418.356201171875, -434.9814453125, -624.9717407226562, -683.8927001953125]\n",
      "\n",
      "Instance 1972 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2, 19]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 2: [-0.18918783962726593, 1.641301155090332, 0.36500874161720276, -0.805494487285614, 1.9870014190673828]\n",
      "car at index 19: [-0.7412868738174438, 1.5541530847549438, 0.6449187397956848, -1.73744535446167, 1.2739557027816772]\n",
      "Grand sum of 1492 tensor sets is: [575.121337890625, 2419.953857421875, -434.4764709472656, -626.2432250976562, -682.26220703125]\n",
      "\n",
      "Instance 1973 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 7: [-0.08713485300540924, 1.8419396877288818, -0.2370632439851761, -0.8711645007133484, -2.398261785507202]\n",
      "Grand sum of 1493 tensor sets is: [575.0341796875, 2421.7958984375, -434.7135314941406, -627.1143798828125, -684.6604614257812]\n",
      "\n",
      "Instance 1974 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1975 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1976 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 15: [1.0678056478500366, 2.6408681869506836, 0.02053309977054596, -2.4981045722961426, -2.3694326877593994]\n",
      "Grand sum of 1494 tensor sets is: [576.1019897460938, 2424.436767578125, -434.6929931640625, -629.6124877929688, -687.0299072265625]\n",
      "\n",
      "Instance 1977 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 5: [-0.11858981847763062, 1.2957648038864136, -0.4573444128036499, -1.6460551023483276, -1.171617031097412]\n",
      "Grand sum of 1495 tensor sets is: [575.9833984375, 2425.732421875, -435.15032958984375, -631.258544921875, -688.2015380859375]\n",
      "\n",
      "Instance 1978 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 9: [-0.17446252703666687, 2.0638442039489746, -0.3080635666847229, -2.431476354598999, -1.0085312128067017]\n",
      "Grand sum of 1496 tensor sets is: [575.8089599609375, 2427.79638671875, -435.4584045410156, -633.6900024414062, -689.2100830078125]\n",
      "\n",
      "Instance 1979 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 25: [0.3776107132434845, 1.7635588645935059, -0.04001668095588684, -0.5654371976852417, -1.619475245475769]\n",
      "Grand sum of 1497 tensor sets is: [576.1865844726562, 2429.56005859375, -435.4984130859375, -634.2554321289062, -690.8295288085938]\n",
      "\n",
      "Instance 1980 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 36: [-0.28840506076812744, 1.7285454273223877, -0.6444436311721802, -2.1835978031158447, -1.1361500024795532]\n",
      "Grand sum of 1498 tensor sets is: [575.898193359375, 2431.28857421875, -436.1428527832031, -636.4390258789062, -691.9656982421875]\n",
      "\n",
      "Instance 1981 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 47: [1.0849909782409668, 0.4255654215812683, -0.37214094400405884, -2.364149808883667, -0.49169084429740906]\n",
      "Grand sum of 1499 tensor sets is: [576.983154296875, 2431.714111328125, -436.5149841308594, -638.8031616210938, -692.4573974609375]\n",
      "\n",
      "Instance 1982 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 7: [-0.6376726627349854, 2.081512689590454, -0.23070666193962097, -1.0194319486618042, -1.1953269243240356]\n",
      "Grand sum of 1500 tensor sets is: [576.345458984375, 2433.795654296875, -436.7456970214844, -639.8225708007812, -693.6527099609375]\n",
      "\n",
      "Instance 1983 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 18: [-0.22777360677719116, 1.2612720727920532, 0.8710771203041077, -0.8292205333709717, -1.1881382465362549]\n",
      "Grand sum of 1501 tensor sets is: [576.11767578125, 2435.056884765625, -435.8746337890625, -640.6517944335938, -694.8408203125]\n",
      "\n",
      "Instance 1984 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 18: [-0.2373177707195282, 3.9465599060058594, -0.3002338409423828, -0.6752881407737732, -1.738073468208313]\n",
      "Grand sum of 1502 tensor sets is: [575.88037109375, 2439.00341796875, -436.17486572265625, -641.3270874023438, -696.5789184570312]\n",
      "\n",
      "Instance 1985 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 7: [-0.12610453367233276, 1.4618415832519531, -1.6328580379486084, -0.7342445254325867, -0.7768790125846863]\n",
      "Grand sum of 1503 tensor sets is: [575.7542724609375, 2440.46533203125, -437.8077087402344, -642.0613403320312, -697.3557739257812]\n",
      "\n",
      "Instance 1986 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 18: [-0.7166361212730408, 3.0447452068328857, 0.3379702568054199, -0.8346721529960632, -0.63483065366745]\n",
      "Grand sum of 1504 tensor sets is: [575.0376586914062, 2443.510009765625, -437.4697265625, -642.89599609375, -697.9906005859375]\n",
      "\n",
      "Instance 1987 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 3: [0.6631022095680237, 2.94295072555542, 0.08771422505378723, -2.5476300716400146, -1.2544610500335693]\n",
      "Grand sum of 1505 tensor sets is: [575.7007446289062, 2446.452880859375, -437.38201904296875, -645.443603515625, -699.2450561523438]\n",
      "\n",
      "Instance 1988 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 14: [0.751043438911438, 2.095263719558716, 0.36308279633522034, -2.3636908531188965, -0.5461295247077942]\n",
      "Grand sum of 1506 tensor sets is: [576.4517822265625, 2448.548095703125, -437.0189514160156, -647.8073120117188, -699.7911987304688]\n",
      "\n",
      "Instance 1989 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 8: [1.1622730493545532, 1.9454137086868286, -0.006686869077384472, 0.0301794596016407, -1.4978289604187012]\n",
      "Grand sum of 1507 tensor sets is: [577.6140747070312, 2450.493408203125, -437.025634765625, -647.7771606445312, -701.2890014648438]\n",
      "\n",
      "Instance 1990 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "car at index 16: [2.3556389808654785, -2.0216097831726074, -0.26914262771606445, 0.12859520316123962, 0.5023811459541321]\n",
      "Grand sum of 1508 tensor sets is: [579.9697265625, 2448.4716796875, -437.2947692871094, -647.6485595703125, -700.78662109375]\n",
      "\n",
      "Instance 1991 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 16: [-0.1800464689731598, 1.6833314895629883, -0.16039414703845978, -0.5098570585250854, -2.1579723358154297]\n",
      "Grand sum of 1509 tensor sets is: [579.7896728515625, 2450.155029296875, -437.4551696777344, -648.1583862304688, -702.944580078125]\n",
      "\n",
      "Instance 1992 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1993 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 26: [-0.1272372305393219, 0.8191285133361816, 0.41995006799697876, -0.022760337218642235, -2.4232256412506104]\n",
      "Grand sum of 1510 tensor sets is: [579.6624145507812, 2450.97412109375, -437.03521728515625, -648.18115234375, -705.3677978515625]\n",
      "\n",
      "Instance 1994 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 7: [0.302795946598053, 1.6260290145874023, -0.9463202357292175, 0.011254459619522095, 1.834433913230896]\n",
      "car at index 19: [-0.18377476930618286, 1.6436315774917603, -1.091164469718933, 0.4272432327270508, -1.9473673105239868]\n",
      "Grand sum of 1511 tensor sets is: [579.721923828125, 2452.60888671875, -438.053955078125, -647.9619140625, -705.4242553710938]\n",
      "\n",
      "Instance 1995 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 13: [0.16179008781909943, 1.5807340145111084, -0.6210787296295166, -0.9942111968994141, -1.464756965637207]\n",
      "Grand sum of 1512 tensor sets is: [579.8837280273438, 2454.189697265625, -438.675048828125, -648.9561157226562, -706.8890380859375]\n",
      "\n",
      "Instance 1996 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [132]\n",
      "Size of token embeddings is torch.Size([209, 13, 768])\n",
      "Shape of summed layers is: 209 x 768\n",
      "car at index 132: [0.48151302337646484, 0.32335931062698364, -0.23476611077785492, 0.7542795538902283, 2.5637974739074707]\n",
      "Grand sum of 1513 tensor sets is: [580.365234375, 2454.512939453125, -438.9098205566406, -648.2018432617188, -704.3252563476562]\n",
      "\n",
      "Instance 1997 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([129, 13, 768])\n",
      "Shape of summed layers is: 129 x 768\n",
      "car at index 65: [0.8504551649093628, -1.8472323417663574, -1.481078028678894, 2.083134651184082, 2.7053658962249756]\n",
      "Grand sum of 1514 tensor sets is: [581.2156982421875, 2452.665771484375, -440.3908996582031, -646.1187133789062, -701.619873046875]\n",
      "\n",
      "Instance 1998 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 1999 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2000 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2001 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [296]\n",
      "Size of token embeddings is torch.Size([489, 13, 768])\n",
      "Shape of summed layers is: 489 x 768\n",
      "car at index 296: [1.4125421047210693, 2.5514111518859863, -0.4282034635543823, -2.0862691402435303, 2.1149308681488037]\n",
      "Grand sum of 1515 tensor sets is: [582.6282348632812, 2455.21728515625, -440.819091796875, -648.2049560546875, -699.5049438476562]\n",
      "\n",
      "Instance 2002 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [67]\n",
      "Size of token embeddings is torch.Size([113, 13, 768])\n",
      "Shape of summed layers is: 113 x 768\n",
      "car at index 67: [1.4454762935638428, -1.2955102920532227, -2.0542476177215576, 1.6506614685058594, 1.625829815864563]\n",
      "Grand sum of 1516 tensor sets is: [584.07373046875, 2453.921875, -442.87335205078125, -646.5543212890625, -697.8790893554688]\n",
      "\n",
      "Instance 2003 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 13: [-0.2829970717430115, 1.2260138988494873, -0.32158994674682617, -1.649915099143982, -0.13339708745479584]\n",
      "Grand sum of 1517 tensor sets is: [583.7907104492188, 2455.14794921875, -443.1949462890625, -648.2042236328125, -698.0125122070312]\n",
      "\n",
      "Instance 2004 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 16: [0.2036914825439453, 3.103715419769287, -0.39562949538230896, 0.7333928942680359, -2.992218017578125]\n",
      "Grand sum of 1518 tensor sets is: [583.994384765625, 2458.251708984375, -443.590576171875, -647.4708251953125, -701.0047607421875]\n",
      "\n",
      "Instance 2005 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 18: [0.33464300632476807, 2.5214076042175293, 0.9751924872398376, 0.43164175748825073, -3.1687562465667725]\n",
      "Grand sum of 1519 tensor sets is: [584.3290405273438, 2460.773193359375, -442.6153869628906, -647.0391845703125, -704.1735229492188]\n",
      "\n",
      "Instance 2006 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 18: [1.3955971002578735, -0.3414853513240814, -0.6145102381706238, 0.5386614203453064, -1.2269799709320068]\n",
      "Grand sum of 1520 tensor sets is: [585.724609375, 2460.431640625, -443.2298889160156, -646.5005493164062, -705.4005126953125]\n",
      "\n",
      "Instance 2007 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 6: [0.765644907951355, 2.661524772644043, -0.8906533718109131, -0.20878680050373077, 0.6733366250991821]\n",
      "Grand sum of 1521 tensor sets is: [586.490234375, 2463.09326171875, -444.12054443359375, -646.7093505859375, -704.7271728515625]\n",
      "\n",
      "Instance 2008 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2009 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 12: [0.1704617738723755, 1.7147879600524902, 0.04182755574584007, -0.3196446895599365, -1.1033575534820557]\n",
      "Grand sum of 1522 tensor sets is: [586.6607055664062, 2464.80810546875, -444.0787048339844, -647.0289916992188, -705.8305053710938]\n",
      "\n",
      "Instance 2010 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5, 40]\n",
      "Size of token embeddings is torch.Size([66, 13, 768])\n",
      "Shape of summed layers is: 66 x 768\n",
      "car at index 5: [0.9231826663017273, -0.05817572772502899, -0.11323757469654083, 0.17828698456287384, 1.0317901372909546]\n",
      "car at index 40: [0.16680127382278442, 1.364798665046692, -1.2456214427947998, -0.7482562065124512, -0.08898237347602844]\n",
      "Grand sum of 1523 tensor sets is: [587.2056884765625, 2465.46142578125, -444.7581481933594, -647.31396484375, -705.359130859375]\n",
      "\n",
      "Instance 2011 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 15: [0.687456488609314, 1.4563297033309937, -0.9253143668174744, -0.06625490635633469, -2.1734490394592285]\n",
      "Grand sum of 1524 tensor sets is: [587.8931274414062, 2466.917724609375, -445.6834716796875, -647.3802490234375, -707.5325927734375]\n",
      "\n",
      "Instance 2012 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2013 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 24: [0.2799616754055023, 0.28243201971054077, -2.195120334625244, -0.36685919761657715, 1.945844292640686]\n",
      "Grand sum of 1525 tensor sets is: [588.173095703125, 2467.2001953125, -447.87860107421875, -647.7471313476562, -705.5867309570312]\n",
      "\n",
      "Instance 2014 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [69]\n",
      "Size of token embeddings is torch.Size([90, 13, 768])\n",
      "Shape of summed layers is: 90 x 768\n",
      "car at index 69: [0.06810688227415085, -0.9255341291427612, -0.7322084903717041, 3.246033191680908, 4.406833171844482]\n",
      "Grand sum of 1526 tensor sets is: [588.2412109375, 2466.274658203125, -448.6108093261719, -644.5010986328125, -701.1798706054688]\n",
      "\n",
      "Instance 2015 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 18: [-1.092283844947815, 2.3548619747161865, -0.7358078956604004, -1.196833610534668, -1.237843632698059]\n",
      "Grand sum of 1527 tensor sets is: [587.14892578125, 2468.629638671875, -449.34661865234375, -645.6979370117188, -702.417724609375]\n",
      "\n",
      "Instance 2016 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 6: [0.4120479226112366, 1.8406003713607788, -0.5476147532463074, -0.4907354712486267, -2.249210834503174]\n",
      "Grand sum of 1528 tensor sets is: [587.5609741210938, 2470.47021484375, -449.89422607421875, -646.1886596679688, -704.6669311523438]\n",
      "\n",
      "Instance 2017 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2018 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 19: [0.770069420337677, 1.62618088722229, -0.5564650893211365, 0.22857549786567688, -0.554885983467102]\n",
      "Grand sum of 1529 tensor sets is: [588.3310546875, 2472.096435546875, -450.45068359375, -645.9600830078125, -705.2218017578125]\n",
      "\n",
      "Instance 2019 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [47, 97, 189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([227, 13, 768])\n",
      "Shape of summed layers is: 227 x 768\n",
      "car at index 47: [1.106095552444458, 0.669709324836731, -0.019730476662516594, 3.7895376682281494, -1.760806918144226]\n",
      "car at index 97: [-0.03928358107805252, -0.2148306667804718, -0.27802759408950806, 1.0847949981689453, 3.2923262119293213]\n",
      "car at index 189: [-0.17931994795799255, -1.0953190326690674, -0.29395997524261475, -0.2584114074707031, 4.407750129699707]\n",
      "Grand sum of 1530 tensor sets is: [588.6268920898438, 2471.883056640625, -450.6479187011719, -644.4214477539062, -703.2420654296875]\n",
      "\n",
      "Instance 2020 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2021 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2022 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 1531 tensor sets is: [590.7445678710938, 2473.559326171875, -451.8822021484375, -646.0869750976562, -703.3865966796875]\n",
      "\n",
      "Instance 2023 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 10: [-0.26031216979026794, 2.0877203941345215, 0.10181550681591034, 0.5098637342453003, -1.2123243808746338]\n",
      "Grand sum of 1532 tensor sets is: [590.4842529296875, 2475.64697265625, -451.7803955078125, -645.5770874023438, -704.5989379882812]\n",
      "\n",
      "Instance 2024 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2025 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 13: [0.5611079335212708, 3.6025936603546143, -0.11120118200778961, -0.7905427813529968, -0.14342589676380157]\n",
      "Grand sum of 1533 tensor sets is: [591.0453491210938, 2479.24951171875, -451.8916015625, -646.3676147460938, -704.7423706054688]\n",
      "\n",
      "Instance 2026 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 8: [0.3993886709213257, 1.7166049480438232, -0.5974134802818298, -1.5994831323623657, -1.4501241445541382]\n",
      "Grand sum of 1534 tensor sets is: [591.4447631835938, 2480.966064453125, -452.489013671875, -647.9671020507812, -706.1925048828125]\n",
      "\n",
      "Instance 2027 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 14: [0.13824591040611267, 2.543966054916382, -0.3914397358894348, -1.0960465669631958, -1.3377418518066406]\n",
      "Grand sum of 1535 tensor sets is: [591.5830078125, 2483.510009765625, -452.8804626464844, -649.0631713867188, -707.5302734375]\n",
      "\n",
      "Instance 2028 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 15: [0.3374633193016052, 0.818449079990387, -0.2051876336336136, -3.539285659790039, 0.6372012495994568]\n",
      "Grand sum of 1536 tensor sets is: [591.9204711914062, 2484.328369140625, -453.0856628417969, -652.6024780273438, -706.89306640625]\n",
      "\n",
      "Instance 2029 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 4: [-0.12834325432777405, 1.7011500597000122, -0.5971000790596008, -1.925870418548584, -0.9797890186309814]\n",
      "Grand sum of 1537 tensor sets is: [591.7921142578125, 2486.029541015625, -453.6827697753906, -654.5283203125, -707.8728637695312]\n",
      "\n",
      "Instance 2030 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2031 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5, 22]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 5: [-0.37398555874824524, 1.4593571424484253, -1.2272932529449463, -1.1749190092086792, -1.5774785280227661]\n",
      "car at index 22: [-0.4812825620174408, 2.3682608604431152, -0.9852398037910461, -2.4802403450012207, -1.0271294116973877]\n",
      "Grand sum of 1538 tensor sets is: [591.364501953125, 2487.943359375, -454.7890319824219, -656.3558959960938, -709.1751708984375]\n",
      "\n",
      "Instance 2032 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 43: [1.1770873069763184, 2.1193370819091797, 0.017606312409043312, -1.9351699352264404, -1.3044190406799316]\n",
      "Grand sum of 1539 tensor sets is: [592.5415649414062, 2490.062744140625, -454.77142333984375, -658.2910766601562, -710.4796142578125]\n",
      "\n",
      "Instance 2033 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2034 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2035 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 5: [-0.03900539502501488, 4.555899620056152, 0.05383121222257614, 1.0453912019729614, 0.4666404128074646]\n",
      "Grand sum of 1540 tensor sets is: [592.5025634765625, 2494.61865234375, -454.71759033203125, -657.2456665039062, -710.0130004882812]\n",
      "\n",
      "Instance 2036 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2037 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 5: [0.9067238569259644, 0.31598392128944397, 0.08989644050598145, -0.9059982299804688, -1.8633008003234863]\n",
      "Grand sum of 1541 tensor sets is: [593.4093017578125, 2494.9345703125, -454.627685546875, -658.1516723632812, -711.8762817382812]\n",
      "\n",
      "Instance 2038 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 7: [0.6582855582237244, 0.4851142466068268, 1.1953601837158203, 2.657644271850586, -2.491546392440796]\n",
      "Grand sum of 1542 tensor sets is: [594.0675659179688, 2495.419677734375, -453.43231201171875, -655.4940185546875, -714.3677978515625]\n",
      "\n",
      "Instance 2039 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2040 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2041 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2042 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2043 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2044 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2045 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 2: [0.8969845175743103, 3.3993372917175293, -0.5783730745315552, -3.2716028690338135, 3.25579833984375]\n",
      "Grand sum of 1543 tensor sets is: [594.9645385742188, 2498.819091796875, -454.01068115234375, -658.765625, -711.1119995117188]\n",
      "\n",
      "Instance 2046 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 21: [0.27511197328567505, 0.65665203332901, 0.11517604440450668, 3.8575706481933594, -1.5007350444793701]\n",
      "Grand sum of 1544 tensor sets is: [595.2396240234375, 2499.475830078125, -453.8955078125, -654.9080810546875, -712.6127319335938]\n",
      "\n",
      "Instance 2047 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 2: [0.6678402423858643, 1.1743050813674927, -0.12545233964920044, -2.14847993850708, -0.04385007917881012]\n",
      "Grand sum of 1545 tensor sets is: [595.907470703125, 2500.650146484375, -454.0209655761719, -657.0565795898438, -712.6565551757812]\n",
      "\n",
      "Instance 2048 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 7: [0.5716264247894287, 1.8150228261947632, -0.7954275012016296, -2.1603410243988037, -0.9205493330955505]\n",
      "Grand sum of 1546 tensor sets is: [596.4791259765625, 2502.465087890625, -454.81640625, -659.2169189453125, -713.5770874023438]\n",
      "\n",
      "Instance 2049 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2050 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13, 16]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 13: [0.01597725600004196, 0.9864614605903625, 0.9393746852874756, 0.6069194674491882, -1.9776313304901123]\n",
      "car at index 16: [0.3590920567512512, 1.0626106262207031, 0.9778695106506348, -1.7278800010681152, -1.6541969776153564]\n",
      "Grand sum of 1547 tensor sets is: [596.6666870117188, 2503.489501953125, -453.8577880859375, -659.7774047851562, -715.3930053710938]\n",
      "\n",
      "Instance 2051 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2052 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2053 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([75, 13, 768])\n",
      "Shape of summed layers is: 75 x 768\n",
      "car at index 46: [0.1323995441198349, 0.744743824005127, -0.6942113637924194, -0.8366369009017944, 0.7050557136535645]\n",
      "Grand sum of 1548 tensor sets is: [596.799072265625, 2504.234130859375, -454.552001953125, -660.614013671875, -714.6879272460938]\n",
      "\n",
      "Instance 2054 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "car at index 46: [-0.6695523262023926, 0.6001327037811279, -1.043607234954834, 2.63687801361084, 2.626648426055908]\n",
      "Grand sum of 1549 tensor sets is: [596.1295166015625, 2504.834228515625, -455.5956115722656, -657.9771118164062, -712.061279296875]\n",
      "\n",
      "Instance 2055 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2056 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2057 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 10: [0.1873636692762375, 1.5020989179611206, -0.805534839630127, -2.439040184020996, -0.2628944516181946]\n",
      "Grand sum of 1550 tensor sets is: [596.31689453125, 2506.33642578125, -456.4011535644531, -660.4161376953125, -712.3241577148438]\n",
      "\n",
      "Instance 2058 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 13: [-0.8465152382850647, 1.7746424674987793, 0.7394267916679382, -0.19069626927375793, -1.368623971939087]\n",
      "Grand sum of 1551 tensor sets is: [595.4703979492188, 2508.111083984375, -455.6617126464844, -660.6068115234375, -713.6928100585938]\n",
      "\n",
      "Instance 2059 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 19: [-1.310920238494873, 1.0156681537628174, 0.32509782910346985, -2.64947247505188, -1.4051817655563354]\n",
      "Grand sum of 1552 tensor sets is: [594.1594848632812, 2509.126708984375, -455.33660888671875, -663.2562866210938, -715.0979614257812]\n",
      "\n",
      "Instance 2060 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([75, 13, 768])\n",
      "Shape of summed layers is: 75 x 768\n",
      "car at index 37: [0.3436959683895111, 1.1878535747528076, -0.4119797945022583, -1.0204075574874878, 0.5351415872573853]\n",
      "Grand sum of 1553 tensor sets is: [594.503173828125, 2510.314453125, -455.74859619140625, -664.2766723632812, -714.5628051757812]\n",
      "\n",
      "Instance 2061 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 12: [0.9733579158782959, 1.8668631315231323, -1.0226261615753174, -0.5717093348503113, 1.9198763370513916]\n",
      "Grand sum of 1554 tensor sets is: [595.4765014648438, 2512.181396484375, -456.7712097167969, -664.848388671875, -712.6429443359375]\n",
      "\n",
      "Instance 2062 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 6: [0.6277763247489929, 1.634118676185608, -1.6621489524841309, -1.3926461935043335, 0.9800939559936523]\n",
      "Grand sum of 1555 tensor sets is: [596.104248046875, 2513.8154296875, -458.433349609375, -666.2410278320312, -711.662841796875]\n",
      "\n",
      "Instance 2063 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 7: [-0.21334446966648102, 3.3123772144317627, -0.22519280016422272, -1.8531737327575684, -2.109778881072998]\n",
      "Grand sum of 1556 tensor sets is: [595.8909301757812, 2517.127685546875, -458.6585388183594, -668.0941772460938, -713.7726440429688]\n",
      "\n",
      "Instance 2064 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2065 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([70, 13, 768])\n",
      "Shape of summed layers is: 70 x 768\n",
      "car at index 18: [-0.22929461300373077, 1.659271478652954, 0.10279689729213715, 2.980006456375122, -2.3430700302124023]\n",
      "Grand sum of 1557 tensor sets is: [595.66162109375, 2518.786865234375, -458.5557556152344, -665.1141967773438, -716.11572265625]\n",
      "\n",
      "Instance 2066 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 8: [0.4005311131477356, 2.1813087463378906, -0.490021675825119, 0.03761535882949829, -0.7165870666503906]\n",
      "Grand sum of 1558 tensor sets is: [596.0621337890625, 2520.96826171875, -459.0457763671875, -665.0765991210938, -716.8323364257812]\n",
      "\n",
      "Instance 2067 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 7: [0.632367730140686, 1.198809266090393, -0.5778796672821045, 1.1278314590454102, -2.2843973636627197]\n",
      "Grand sum of 1559 tensor sets is: [596.6945190429688, 2522.1669921875, -459.6236572265625, -663.9487915039062, -719.1167602539062]\n",
      "\n",
      "Instance 2068 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2069 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2070 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 20: [-0.193840891122818, 3.838073492050171, 0.788254976272583, 0.1575419008731842, -1.6067399978637695]\n",
      "Grand sum of 1560 tensor sets is: [596.5006713867188, 2526.005126953125, -458.83538818359375, -663.791259765625, -720.7235107421875]\n",
      "\n",
      "Instance 2071 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2072 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 6: [-0.0313151478767395, 2.4892373085021973, -0.9916964769363403, -2.3294525146484375, -0.6068727374076843]\n",
      "Grand sum of 1561 tensor sets is: [596.4693603515625, 2528.494384765625, -459.82708740234375, -666.1207275390625, -721.3303833007812]\n",
      "\n",
      "Instance 2073 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 16: [0.8522335886955261, 2.534795045852661, 0.08145607262849808, -0.7127499580383301, -2.8872528076171875]\n",
      "Grand sum of 1562 tensor sets is: [597.3215942382812, 2531.029296875, -459.7456359863281, -666.83349609375, -724.2176513671875]\n",
      "\n",
      "Instance 2074 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 4: [1.0028833150863647, 1.9453589916229248, -0.3559931814670563, -2.185822010040283, -1.2470450401306152]\n",
      "Grand sum of 1563 tensor sets is: [598.324462890625, 2532.974609375, -460.10162353515625, -669.0193481445312, -725.4647216796875]\n",
      "\n",
      "Instance 2075 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 12: [0.2551746368408203, 2.2352635860443115, -0.7588742971420288, 1.7764467000961304, -0.3613584041595459]\n",
      "Grand sum of 1564 tensor sets is: [598.5796508789062, 2535.2099609375, -460.8605041503906, -667.242919921875, -725.8260498046875]\n",
      "\n",
      "Instance 2076 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 8: [-0.46223777532577515, 0.8090658783912659, 1.3844155073165894, 2.1072726249694824, -3.3804821968078613]\n",
      "Grand sum of 1565 tensor sets is: [598.117431640625, 2536.01904296875, -459.47607421875, -665.1356201171875, -729.20654296875]\n",
      "\n",
      "Instance 2077 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8, 23]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 8: [1.082198143005371, 3.2967112064361572, -0.8162350654602051, -0.08062340319156647, -1.7449601888656616]\n",
      "car at index 23: [0.66469407081604, 3.1168603897094727, 0.3867229223251343, -0.17594441771507263, -1.7224745750427246]\n",
      "Grand sum of 1566 tensor sets is: [598.9909057617188, 2539.225830078125, -459.6908264160156, -665.263916015625, -730.9402465820312]\n",
      "\n",
      "Instance 2078 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2079 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([402, 13, 768])\n",
      "Shape of summed layers is: 402 x 768\n",
      "car at index 383: [-0.11610588431358337, 0.19914761185646057, -1.1755744218826294, 0.8213372826576233, 1.4826987981796265]\n",
      "Grand sum of 1567 tensor sets is: [598.8748168945312, 2539.425048828125, -460.86639404296875, -664.4425659179688, -729.45751953125]\n",
      "\n",
      "Instance 2080 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 25: [0.27503639459609985, -0.07150432467460632, -0.17235976457595825, -1.7617324590682983, -1.057305932044983]\n",
      "Grand sum of 1568 tensor sets is: [599.1498413085938, 2539.353515625, -461.03875732421875, -666.2042846679688, -730.5148315429688]\n",
      "\n",
      "Instance 2081 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2082 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "car at index 36: [1.0430495738983154, 2.8419127464294434, 0.47083625197410583, 0.6948601603507996, -4.187784671783447]\n",
      "Grand sum of 1569 tensor sets is: [600.19287109375, 2542.1953125, -460.56793212890625, -665.5093994140625, -734.70263671875]\n",
      "\n",
      "Instance 2083 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 9: [0.06401076912879944, 2.455517053604126, -0.7003766894340515, 0.02884950116276741, -2.0700037479400635]\n",
      "Grand sum of 1570 tensor sets is: [600.2568969726562, 2544.65087890625, -461.268310546875, -665.4805297851562, -736.7726440429688]\n",
      "\n",
      "Instance 2084 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 12: [1.1750636100769043, 2.160944700241089, -0.8323274850845337, -2.356637954711914, 1.1940240859985352]\n",
      "Grand sum of 1571 tensor sets is: [601.4319458007812, 2546.811767578125, -462.10064697265625, -667.837158203125, -735.57861328125]\n",
      "\n",
      "Instance 2085 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 18: [-0.03693004325032234, 0.68632972240448, -1.0132914781570435, -0.6566720604896545, -2.1010630130767822]\n",
      "Grand sum of 1572 tensor sets is: [601.39501953125, 2547.498046875, -463.11395263671875, -668.4938354492188, -737.6796875]\n",
      "\n",
      "Instance 2086 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [70]\n",
      "Size of token embeddings is torch.Size([122, 13, 768])\n",
      "Shape of summed layers is: 122 x 768\n",
      "car at index 70: [0.9851038455963135, 0.9116265773773193, 0.7458715438842773, -1.6831353902816772, -0.911991536617279]\n",
      "Grand sum of 1573 tensor sets is: [602.380126953125, 2548.40966796875, -462.3680725097656, -670.1769409179688, -738.5916748046875]\n",
      "\n",
      "Instance 2087 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "car at index 50: [0.6773045063018799, 3.1993274688720703, 0.6700952649116516, 1.6682403087615967, -1.0122146606445312]\n",
      "Grand sum of 1574 tensor sets is: [603.0574340820312, 2551.60888671875, -461.6979675292969, -668.5087280273438, -739.6038818359375]\n",
      "\n",
      "Instance 2088 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 11: [0.6096721887588501, 1.4829822778701782, -0.7708828449249268, 0.26843926310539246, -0.48549139499664307]\n",
      "Grand sum of 1575 tensor sets is: [603.6671142578125, 2553.091796875, -462.4688415527344, -668.2402954101562, -740.08935546875]\n",
      "\n",
      "Instance 2089 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2090 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 3: [1.1095154285430908, 1.08852219581604, -1.4842722415924072, 1.8711438179016113, 0.5091173648834229]\n",
      "Grand sum of 1576 tensor sets is: [604.776611328125, 2554.180419921875, -463.953125, -666.369140625, -739.5802612304688]\n",
      "\n",
      "Instance 2091 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 15: [0.4508517384529114, 2.063094139099121, -0.9418642520904541, -1.326501488685608, -1.6434143781661987]\n",
      "Grand sum of 1577 tensor sets is: [605.2274780273438, 2556.243408203125, -464.8949890136719, -667.6956176757812, -741.2236938476562]\n",
      "\n",
      "Instance 2092 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 7: [-0.09538452327251434, 0.9821842312812805, -0.09022872895002365, 1.4017150402069092, -3.191425323486328]\n",
      "Grand sum of 1578 tensor sets is: [605.132080078125, 2557.2255859375, -464.9852294921875, -666.2938842773438, -744.4151000976562]\n",
      "\n",
      "Instance 2093 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2094 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 27: [0.9217132329940796, 0.22706879675388336, -0.36258116364479065, 2.4033589363098145, 3.1799418926239014]\n",
      "Grand sum of 1579 tensor sets is: [606.0537719726562, 2557.45263671875, -465.3478088378906, -663.8905029296875, -741.2351684570312]\n",
      "\n",
      "Instance 2095 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2096 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [58, 64, 71]\n",
      "Size of token embeddings is torch.Size([80, 13, 768])\n",
      "Shape of summed layers is: 80 x 768\n",
      "car at index 58: [1.37691068649292, 2.4115912914276123, -0.4429846704006195, -0.6823780536651611, -0.47033050656318665]\n",
      "car at index 64: [0.8975704312324524, 2.7577874660491943, -0.4373767375946045, -0.33240044116973877, -1.0163122415542603]\n",
      "car at index 71: [0.33840423822402954, 2.3281984329223633, -0.7690393924713135, -1.2662005424499512, -2.2694075107574463]\n",
      "Grand sum of 1580 tensor sets is: [606.9247436523438, 2559.951904296875, -465.8976135253906, -664.6508178710938, -742.4871826171875]\n",
      "\n",
      "Instance 2097 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 6: [-0.09737776219844818, 1.3706564903259277, -0.971081018447876, -1.9289839267730713, -2.2654430866241455]\n",
      "Grand sum of 1581 tensor sets is: [606.827392578125, 2561.322509765625, -466.8686828613281, -666.5797729492188, -744.7526245117188]\n",
      "\n",
      "Instance 2098 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 8: [0.5058164596557617, 3.2374820709228516, -0.6388489007949829, -1.3989052772521973, -0.42957934737205505]\n",
      "Grand sum of 1582 tensor sets is: [607.3331909179688, 2564.56005859375, -467.5075378417969, -667.9786987304688, -745.1821899414062]\n",
      "\n",
      "Instance 2099 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2100 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 6: [-0.037051014602184296, 3.015939474105835, -1.157409906387329, -0.698021650314331, -3.13206148147583]\n",
      "Grand sum of 1583 tensor sets is: [607.296142578125, 2567.575927734375, -468.6649475097656, -668.6766967773438, -748.3142700195312]\n",
      "\n",
      "Instance 2101 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 21: [0.026048943400382996, 2.0947790145874023, 0.12075503170490265, -2.334592342376709, -1.5641191005706787]\n",
      "Grand sum of 1584 tensor sets is: [607.3222045898438, 2569.670654296875, -468.544189453125, -671.0112915039062, -749.87841796875]\n",
      "\n",
      "Instance 2102 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 5: [-0.6913316249847412, 2.1664233207702637, -0.5889244079589844, -0.46398141980171204, -1.227203607559204]\n",
      "Grand sum of 1585 tensor sets is: [606.630859375, 2571.837158203125, -469.13311767578125, -671.4752807617188, -751.1056518554688]\n",
      "\n",
      "Instance 2103 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 21: [-0.08849446475505829, 1.0694198608398438, -0.4670460820198059, -1.2476474046707153, -1.6272852420806885]\n",
      "Grand sum of 1586 tensor sets is: [606.5423583984375, 2572.906494140625, -469.60015869140625, -672.722900390625, -752.73291015625]\n",
      "\n",
      "Instance 2104 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 9: [0.913994550704956, 2.2500112056732178, -0.5951656103134155, -1.0253777503967285, 0.4955972731113434]\n",
      "Grand sum of 1587 tensor sets is: [607.4563598632812, 2575.156494140625, -470.1953125, -673.748291015625, -752.2373046875]\n",
      "\n",
      "Instance 2105 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 7: [-0.6419509649276733, 2.2223453521728516, 0.08452729880809784, 2.1967878341674805, -3.039231777191162]\n",
      "Grand sum of 1588 tensor sets is: [606.8143920898438, 2577.37890625, -470.11077880859375, -671.551513671875, -755.2765502929688]\n",
      "\n",
      "Instance 2106 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 17: [0.8383500576019287, 2.348914384841919, -0.746776819229126, -0.8713098168373108, -2.1721103191375732]\n",
      "Grand sum of 1589 tensor sets is: [607.6527709960938, 2579.727783203125, -470.8575439453125, -672.4228515625, -757.4486694335938]\n",
      "\n",
      "Instance 2107 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2108 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 13: [1.574028730392456, -1.5262089967727661, -0.193751722574234, 0.5365080833435059, 3.976482391357422]\n",
      "Grand sum of 1590 tensor sets is: [609.226806640625, 2578.20166015625, -471.0513000488281, -671.8863525390625, -753.47216796875]\n",
      "\n",
      "Instance 2109 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 21: [0.08339343965053558, 1.7182600498199463, -1.1538203954696655, -0.8072729706764221, 1.6862421035766602]\n",
      "Grand sum of 1591 tensor sets is: [609.3101806640625, 2579.919921875, -472.2051086425781, -672.693603515625, -751.7859497070312]\n",
      "\n",
      "Instance 2110 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 32: [0.3506052792072296, 1.3283487558364868, 0.5625134706497192, -1.296820878982544, -0.8440924882888794]\n",
      "Grand sum of 1592 tensor sets is: [609.6607666015625, 2581.248291015625, -471.6426086425781, -673.9904174804688, -752.6300659179688]\n",
      "\n",
      "Instance 2111 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2112 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "car at index 37: [0.34517619013786316, -0.0028645824640989304, -0.8804641962051392, 1.4240431785583496, 2.505789279937744]\n",
      "Grand sum of 1593 tensor sets is: [610.0059204101562, 2581.245361328125, -472.5230712890625, -672.5663452148438, -750.124267578125]\n",
      "\n",
      "Instance 2113 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2114 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 21: [1.1395889520645142, 1.8807551860809326, -0.6382241249084473, -1.8312952518463135, 2.5693957805633545]\n",
      "Grand sum of 1594 tensor sets is: [611.1455078125, 2583.126220703125, -473.1612854003906, -674.3976440429688, -747.5548706054688]\n",
      "\n",
      "Instance 2115 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [243]\n",
      "Size of token embeddings is torch.Size([312, 13, 768])\n",
      "Shape of summed layers is: 312 x 768\n",
      "car at index 243: [1.1742727756500244, 1.5154693126678467, -0.2354646474123001, -0.11148384213447571, 3.3132266998291016]\n",
      "Grand sum of 1595 tensor sets is: [612.3197631835938, 2584.6416015625, -473.3967590332031, -674.5091552734375, -744.2416381835938]\n",
      "\n",
      "Instance 2116 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 7: [0.3880169689655304, 2.266207218170166, -1.1323200464248657, -0.44938331842422485, -1.7942827939987183]\n",
      "Grand sum of 1596 tensor sets is: [612.707763671875, 2586.90771484375, -474.5290832519531, -674.9585571289062, -746.0359497070312]\n",
      "\n",
      "Instance 2117 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2118 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2119 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2120 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 7: [0.9757659435272217, 2.546248197555542, -0.5873447060585022, -1.1409556865692139, 0.9633769989013672]\n",
      "Grand sum of 1597 tensor sets is: [613.6835327148438, 2589.453857421875, -475.1164245605469, -676.0994873046875, -745.0725708007812]\n",
      "\n",
      "Instance 2121 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20, 26]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 20: [1.643261432647705, -0.35253870487213135, 0.1837649643421173, -1.0350160598754883, -0.7842836380004883]\n",
      "car at index 26: [0.4135439991950989, 0.7081380486488342, 0.3188924193382263, -2.670210361480713, -1.9280154705047607]\n",
      "Grand sum of 1598 tensor sets is: [614.7119140625, 2589.631591796875, -474.8650817871094, -677.9520874023438, -746.4287109375]\n",
      "\n",
      "Instance 2122 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 8: [0.26498350501060486, 1.1592628955841064, -0.7719418406486511, -1.0750739574432373, 0.99178147315979]\n",
      "Grand sum of 1599 tensor sets is: [614.9768676757812, 2590.790771484375, -475.63702392578125, -679.0271606445312, -745.4369506835938]\n",
      "\n",
      "Instance 2123 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 17: [1.266392707824707, 1.161285161972046, 0.35659584403038025, 0.627074658870697, 0.5464916229248047]\n",
      "Grand sum of 1600 tensor sets is: [616.2432861328125, 2591.9521484375, -475.2804260253906, -678.4000854492188, -744.8904418945312]\n",
      "\n",
      "Instance 2124 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 32: [-0.2662789523601532, 1.2623909711837769, -0.08252839744091034, -1.6386313438415527, -1.2824665307998657]\n",
      "Grand sum of 1601 tensor sets is: [615.9769897460938, 2593.214599609375, -475.3629455566406, -680.0386962890625, -746.1729125976562]\n",
      "\n",
      "Instance 2125 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 26: [-0.5203645825386047, 1.6342476606369019, 0.5245992541313171, -0.9027838110923767, -0.2446112334728241]\n",
      "Grand sum of 1602 tensor sets is: [615.4566040039062, 2594.848876953125, -474.8383483886719, -680.9414672851562, -746.4175415039062]\n",
      "\n",
      "Instance 2126 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 5: [1.1232669353485107, 1.9963297843933105, -1.3737539052963257, -0.629304051399231, 0.8637549877166748]\n",
      "Grand sum of 1603 tensor sets is: [616.5798950195312, 2596.84521484375, -476.21209716796875, -681.57080078125, -745.5537719726562]\n",
      "\n",
      "Instance 2127 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16, 42]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 16: [0.4960101842880249, 1.55015230178833, -0.0928947925567627, -1.0633958578109741, -0.5994744896888733]\n",
      "car at index 42: [0.6671879291534424, 2.044496536254883, 0.4623877704143524, -2.380276918411255, -0.22000610828399658]\n",
      "Grand sum of 1604 tensor sets is: [617.1614990234375, 2598.642578125, -476.02734375, -683.2926635742188, -745.9635009765625]\n",
      "\n",
      "Instance 2128 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2129 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 14: [0.8923118710517883, 2.512564182281494, -0.8883541822433472, 0.18568705022335052, -0.4278263449668884]\n",
      "Grand sum of 1605 tensor sets is: [618.0538330078125, 2601.155029296875, -476.91571044921875, -683.1069946289062, -746.391357421875]\n",
      "\n",
      "Instance 2130 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2131 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 16: [0.09871140122413635, 2.1802165508270264, 0.5384813547134399, -0.019171766936779022, 0.35582074522972107]\n",
      "Grand sum of 1606 tensor sets is: [618.1525268554688, 2603.335205078125, -476.3772277832031, -683.1261596679688, -746.0355224609375]\n",
      "\n",
      "Instance 2132 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 12: [0.5588098764419556, 2.768159866333008, 0.13813400268554688, 0.44885215163230896, 0.2501557469367981]\n",
      "Grand sum of 1607 tensor sets is: [618.7113647460938, 2606.103271484375, -476.2391052246094, -682.6773071289062, -745.7853393554688]\n",
      "\n",
      "Instance 2133 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 17: [0.4958459138870239, 1.9606860876083374, -0.5798646211624146, 0.3308042883872986, -2.0391180515289307]\n",
      "Grand sum of 1608 tensor sets is: [619.2072143554688, 2608.06396484375, -476.8189697265625, -682.3464965820312, -747.824462890625]\n",
      "\n",
      "Instance 2134 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2135 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 6: [0.20635370910167694, 1.7547296285629272, -0.10738080739974976, -2.4530720710754395, -0.5429653525352478]\n",
      "Grand sum of 1609 tensor sets is: [619.41357421875, 2609.818603515625, -476.9263610839844, -684.799560546875, -748.367431640625]\n",
      "\n",
      "Instance 2136 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2137 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "car at index 19: [1.4806126356124878, -1.853369116783142, -0.43570244312286377, 1.9100768566131592, 0.8183552622795105]\n",
      "Grand sum of 1610 tensor sets is: [620.8941650390625, 2607.96533203125, -477.362060546875, -682.8894653320312, -747.549072265625]\n",
      "\n",
      "Instance 2138 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 4: [0.6308749914169312, 2.468881845474243, -1.50184166431427, -0.7444412708282471, -0.8814088106155396]\n",
      "Grand sum of 1611 tensor sets is: [621.5250244140625, 2610.434326171875, -478.8638916015625, -683.6339111328125, -748.4304809570312]\n",
      "\n",
      "Instance 2139 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6, 31]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 6: [1.068668007850647, 1.9048404693603516, -0.6580805778503418, 0.7842934131622314, 0.275797039270401]\n",
      "car at index 31: [0.10866622626781464, 3.177669048309326, -1.1479228734970093, 0.03134007751941681, -0.469774067401886]\n",
      "Grand sum of 1612 tensor sets is: [622.1137084960938, 2612.9755859375, -479.76690673828125, -683.22607421875, -748.5274658203125]\n",
      "\n",
      "Instance 2140 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6, 18, 30]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 6: [0.8909354209899902, 0.5958536267280579, -0.95221346616745, 0.5759319067001343, -0.6192348003387451]\n",
      "car at index 18: [0.7143850922584534, 0.7059105038642883, 0.02073216438293457, 0.27192366123199463, -1.1673624515533447]\n",
      "car at index 30: [0.45767444372177124, 0.3784726560115814, -0.10658830404281616, 0.6607139110565186, -0.23145583271980286]\n",
      "Grand sum of 1613 tensor sets is: [622.8013916015625, 2613.53564453125, -480.1129150390625, -682.7232055664062, -749.2001342773438]\n",
      "\n",
      "Instance 2141 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 9: [0.7085163593292236, 2.984988212585449, -0.09876228868961334, -0.7348918914794922, -1.6381317377090454]\n",
      "Grand sum of 1614 tensor sets is: [623.5098876953125, 2616.520751953125, -480.211669921875, -683.4580688476562, -750.8382568359375]\n",
      "\n",
      "Instance 2142 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2143 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2144 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21, 55]\n",
      "Size of token embeddings is torch.Size([95, 13, 768])\n",
      "Shape of summed layers is: 95 x 768\n",
      "car at index 21: [0.2540951371192932, 2.306042194366455, -1.1430367231369019, -1.200313687324524, 1.5041676759719849]\n",
      "car at index 55: [1.049847960472107, 1.8604388236999512, -0.592330813407898, -2.6252329349517822, -0.1736222207546234]\n",
      "Grand sum of 1615 tensor sets is: [624.161865234375, 2618.60400390625, -481.079345703125, -685.370849609375, -750.1729736328125]\n",
      "\n",
      "Instance 2145 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 12: [0.5375745892524719, 3.3960132598876953, -0.4697292447090149, -0.19553479552268982, -0.6782290935516357]\n",
      "Grand sum of 1616 tensor sets is: [624.699462890625, 2622.0, -481.549072265625, -685.56640625, -750.8511962890625]\n",
      "\n",
      "Instance 2146 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 9: [-0.3281145691871643, 2.7413744926452637, -0.3013084828853607, -1.7940905094146729, 0.6985341906547546]\n",
      "Grand sum of 1617 tensor sets is: [624.371337890625, 2624.741455078125, -481.8503723144531, -687.3604736328125, -750.1526489257812]\n",
      "\n",
      "Instance 2147 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 25: [-0.6456826329231262, 1.8550838232040405, 0.4361847937107086, -0.7807508111000061, -1.698796272277832]\n",
      "Grand sum of 1618 tensor sets is: [623.7256469726562, 2626.596435546875, -481.4141845703125, -688.1412353515625, -751.8514404296875]\n",
      "\n",
      "Instance 2148 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 22: [-0.08817429840564728, 2.005045175552368, 0.584335207939148, -1.325696349143982, 1.9519238471984863]\n",
      "Grand sum of 1619 tensor sets is: [623.637451171875, 2628.6015625, -480.8298645019531, -689.4669189453125, -749.8995361328125]\n",
      "\n",
      "Instance 2149 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5, 8]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 5: [-0.3086538016796112, 2.0899972915649414, -0.7110586166381836, 0.5921619534492493, -2.9670825004577637]\n",
      "car at index 8: [0.28877830505371094, 1.133146047592163, -0.2423456311225891, 0.5984951853752136, -3.4337472915649414]\n",
      "Grand sum of 1620 tensor sets is: [623.6275024414062, 2630.213134765625, -481.30657958984375, -688.87158203125, -753.0999755859375]\n",
      "\n",
      "Instance 2150 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2151 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2152 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 15: [0.4508517384529114, 2.063094139099121, -0.9418642520904541, -1.326501488685608, -1.6434143781661987]\n",
      "Grand sum of 1621 tensor sets is: [624.078369140625, 2632.276123046875, -482.2484436035156, -690.1980590820312, -754.743408203125]\n",
      "\n",
      "Instance 2153 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [92]\n",
      "Size of token embeddings is torch.Size([162, 13, 768])\n",
      "Shape of summed layers is: 162 x 768\n",
      "car at index 92: [0.18616370856761932, -1.2844476699829102, -0.6478930711746216, 0.3862268030643463, 3.7838070392608643]\n",
      "Grand sum of 1622 tensor sets is: [624.2645263671875, 2630.99169921875, -482.8963317871094, -689.8118286132812, -750.9595947265625]\n",
      "\n",
      "Instance 2154 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 14: [0.0952506959438324, 3.2151927947998047, -0.4584595859050751, 0.47711434960365295, -0.3636910915374756]\n",
      "Grand sum of 1623 tensor sets is: [624.3598022460938, 2634.206787109375, -483.35479736328125, -689.334716796875, -751.3233032226562]\n",
      "\n",
      "Instance 2155 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2156 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 40: [-0.15944071114063263, 1.1225242614746094, -1.2974960803985596, -1.4201703071594238, -0.7030712962150574]\n",
      "Grand sum of 1624 tensor sets is: [624.2003784179688, 2635.329345703125, -484.65228271484375, -690.7548828125, -752.0263671875]\n",
      "\n",
      "Instance 2157 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 11: [-0.08909805119037628, 3.063128709793091, 0.616165816783905, -1.6761860847473145, -1.3691496849060059]\n",
      "Grand sum of 1625 tensor sets is: [624.1112670898438, 2638.392578125, -484.0361022949219, -692.4310913085938, -753.3955078125]\n",
      "\n",
      "Instance 2158 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 13: [0.3151713013648987, 1.9749592542648315, 0.33679187297821045, 1.741910696029663, -1.161426067352295]\n",
      "Grand sum of 1626 tensor sets is: [624.4264526367188, 2640.367431640625, -483.6993103027344, -690.689208984375, -754.5569458007812]\n",
      "\n",
      "Instance 2159 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([98, 13, 768])\n",
      "Shape of summed layers is: 98 x 768\n",
      "car at index 48: [0.9441468715667725, 3.0408527851104736, 0.5510377287864685, -1.3876147270202637, -1.2934991121292114]\n",
      "Grand sum of 1627 tensor sets is: [625.37060546875, 2643.408203125, -483.1482849121094, -692.0768432617188, -755.8504638671875]\n",
      "\n",
      "Instance 2160 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "car at index 41: [0.3871464431285858, -0.24971599876880646, -0.7425899505615234, 1.7806942462921143, 1.1490001678466797]\n",
      "Grand sum of 1628 tensor sets is: [625.7577514648438, 2643.158447265625, -483.890869140625, -690.296142578125, -754.7014770507812]\n",
      "\n",
      "Instance 2161 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2162 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([83, 13, 768])\n",
      "Shape of summed layers is: 83 x 768\n",
      "car at index 34: [1.1501377820968628, 0.831885039806366, -0.09860166907310486, -1.252285122871399, -3.0660674571990967]\n",
      "Grand sum of 1629 tensor sets is: [626.9078979492188, 2643.990234375, -483.9894714355469, -691.5484008789062, -757.7675170898438]\n",
      "\n",
      "Instance 2163 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [269, 272]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 269: [-0.061310913413763046, -3.0823965072631836, -0.3681918978691101, -1.1109614372253418, 2.403843402862549]\n",
      "car at index 272: [-0.45665404200553894, -1.6344871520996094, 0.2088729292154312, -2.058497190475464, -0.387115478515625]\n",
      "Grand sum of 1630 tensor sets is: [626.64892578125, 2641.6318359375, -484.0691223144531, -693.1331176757812, -756.7591552734375]\n",
      "\n",
      "Instance 2164 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2165 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [145, 402]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 145: [0.16032499074935913, -1.1530282497406006, 0.32888948917388916, 2.4306039810180664, 4.348527908325195]\n",
      "car at index 402: [1.4276176691055298, -2.1110503673553467, -0.2885342240333557, 1.8837257623672485, 2.293120861053467]\n",
      "Grand sum of 1631 tensor sets is: [627.44287109375, 2639.999755859375, -484.0489501953125, -690.9759521484375, -753.4383544921875]\n",
      "\n",
      "Instance 2166 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 25: [0.5994590520858765, 1.982478380203247, 0.4235615134239197, -1.1071810722351074, -0.4576132595539093]\n",
      "Grand sum of 1632 tensor sets is: [628.0423583984375, 2641.982177734375, -483.6253967285156, -692.0831298828125, -753.89599609375]\n",
      "\n",
      "Instance 2167 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 9: [0.32807642221450806, 1.8288531303405762, -0.12900128960609436, -1.108622670173645, -1.0466196537017822]\n",
      "Grand sum of 1633 tensor sets is: [628.3704223632812, 2643.81103515625, -483.75439453125, -693.1917724609375, -754.942626953125]\n",
      "\n",
      "Instance 2168 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2169 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 17: [-0.17071035504341125, 0.8075131177902222, -0.08179222047328949, 0.14151784777641296, -3.7939023971557617]\n",
      "Grand sum of 1634 tensor sets is: [628.19970703125, 2644.61865234375, -483.836181640625, -693.0502319335938, -758.7365112304688]\n",
      "\n",
      "Instance 2170 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16, 26, 44]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 16: [0.8347800374031067, 1.3689544200897217, -0.5289684534072876, -1.1024330854415894, 1.0761899948120117]\n",
      "car at index 26: [0.9049997925758362, 1.1656527519226074, -0.5234478712081909, -1.4250104427337646, 0.10248955339193344]\n",
      "car at index 44: [0.7113847136497498, 2.1076982021331787, -1.1052453517913818, -1.6625964641571045, 0.8629416823387146]\n",
      "Grand sum of 1635 tensor sets is: [629.0167846679688, 2646.166015625, -484.5553894042969, -694.4468994140625, -758.0559692382812]\n",
      "\n",
      "Instance 2171 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 6: [-1.1029255390167236, 1.7696201801300049, -0.25602394342422485, 1.1490559577941895, 4.322484970092773]\n",
      "Grand sum of 1636 tensor sets is: [627.9138793945312, 2647.935546875, -484.8114013671875, -693.2978515625, -753.7334594726562]\n",
      "\n",
      "Instance 2172 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2173 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 16: [0.2036914825439453, 3.103715419769287, -0.39562949538230896, 0.7333928942680359, -2.992218017578125]\n",
      "Grand sum of 1637 tensor sets is: [628.1175537109375, 2651.039306640625, -485.20703125, -692.564453125, -756.7257080078125]\n",
      "\n",
      "Instance 2174 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "car at index 34: [0.30760544538497925, 1.7580547332763672, 0.23716698586940765, -1.2573295831680298, -1.7697871923446655]\n",
      "Grand sum of 1638 tensor sets is: [628.4251708984375, 2652.79736328125, -484.9698791503906, -693.82177734375, -758.4954833984375]\n",
      "\n",
      "Instance 2175 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 20: [0.34019726514816284, 0.0820092260837555, -0.8536250591278076, 2.342373847961426, 3.638563394546509]\n",
      "Grand sum of 1639 tensor sets is: [628.765380859375, 2652.87939453125, -485.8235168457031, -691.4794311523438, -754.85693359375]\n",
      "\n",
      "Instance 2176 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 2: [1.1578842401504517, 2.735502243041992, -0.5291255712509155, -1.6772552728652954, -1.721725344657898]\n",
      "Grand sum of 1640 tensor sets is: [629.9232788085938, 2655.614990234375, -486.3526306152344, -693.1566772460938, -756.5786743164062]\n",
      "\n",
      "Instance 2177 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 15: [0.4056874215602875, 2.5079078674316406, -0.5127514600753784, 0.2932012379169464, -1.7652287483215332]\n",
      "Grand sum of 1641 tensor sets is: [630.3289794921875, 2658.122802734375, -486.8653869628906, -692.8634643554688, -758.3439331054688]\n",
      "\n",
      "Instance 2178 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 22: [0.4030115604400635, 1.7873451709747314, -0.8446499705314636, -0.5899503231048584, 0.05425058305263519]\n",
      "Grand sum of 1642 tensor sets is: [630.7319946289062, 2659.91015625, -487.71002197265625, -693.4534301757812, -758.2896728515625]\n",
      "\n",
      "Instance 2179 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 1643 tensor sets is: [632.8496704101562, 2661.58642578125, -488.9443054199219, -695.1189575195312, -758.4342041015625]\n",
      "\n",
      "Instance 2180 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2181 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 7: [0.23711004853248596, 1.8795576095581055, -0.8416624665260315, 0.8748337030410767, -2.712080478668213]\n",
      "Grand sum of 1644 tensor sets is: [633.0867919921875, 2663.466064453125, -489.7859802246094, -694.244140625, -761.1463012695312]\n",
      "\n",
      "Instance 2182 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "car at index 62: [0.17647884786128998, 1.0212446451187134, -0.5728802680969238, -1.4559520483016968, -1.5542563199996948]\n",
      "Grand sum of 1645 tensor sets is: [633.2632446289062, 2664.4873046875, -490.3588562011719, -695.7000732421875, -762.7005615234375]\n",
      "\n",
      "Instance 2183 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 23: [1.6555002927780151, 1.308961272239685, -0.44714581966400146, -1.469138264656067, -2.3813159465789795]\n",
      "Grand sum of 1646 tensor sets is: [634.9187622070312, 2665.79638671875, -490.8059997558594, -697.169189453125, -765.0818481445312]\n",
      "\n",
      "Instance 2184 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 13: [-0.26934152841567993, 2.1536502838134766, 0.40068888664245605, 1.0521131753921509, -1.9069435596466064]\n",
      "Grand sum of 1647 tensor sets is: [634.6494140625, 2667.949951171875, -490.4053039550781, -696.1170654296875, -766.98876953125]\n",
      "\n",
      "Instance 2185 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [59]\n",
      "Size of token embeddings is torch.Size([97, 13, 768])\n",
      "Shape of summed layers is: 97 x 768\n",
      "car at index 59: [1.1937289237976074, -1.8787771463394165, -0.39297962188720703, -0.3768547773361206, 3.093798875808716]\n",
      "Grand sum of 1648 tensor sets is: [635.8431396484375, 2666.0712890625, -490.79827880859375, -696.493896484375, -763.8949584960938]\n",
      "\n",
      "Instance 2186 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 10: [0.45695018768310547, 2.5637357234954834, -0.7372825741767883, -0.39883923530578613, -0.5752279758453369]\n",
      "Grand sum of 1649 tensor sets is: [636.3001098632812, 2668.635009765625, -491.5355529785156, -696.8927612304688, -764.47021484375]\n",
      "\n",
      "Instance 2187 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2188 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 16: [0.9031951427459717, 1.6993427276611328, -0.2463090717792511, 0.5027730464935303, -2.6615447998046875]\n",
      "Grand sum of 1650 tensor sets is: [637.2033081054688, 2670.33447265625, -491.7818603515625, -696.3900146484375, -767.1317749023438]\n",
      "\n",
      "Instance 2189 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2190 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 4: [0.8918139338493347, 2.321303367614746, -0.5704606771469116, -3.070422649383545, -1.063446283340454]\n",
      "Grand sum of 1651 tensor sets is: [638.0950927734375, 2672.65576171875, -492.3523254394531, -699.46044921875, -768.1952514648438]\n",
      "\n",
      "Instance 2191 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 44: [0.8798028230667114, 1.887597918510437, 0.03294253721833229, -0.13129986822605133, -1.4473648071289062]\n",
      "Grand sum of 1652 tensor sets is: [638.9749145507812, 2674.54345703125, -492.31939697265625, -699.5917358398438, -769.6426391601562]\n",
      "\n",
      "Instance 2192 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 18: [-0.25205549597740173, 2.4210987091064453, -0.5810098052024841, -1.6192543506622314, -1.7166638374328613]\n",
      "Grand sum of 1653 tensor sets is: [638.7228393554688, 2676.964599609375, -492.9004211425781, -701.2109985351562, -771.3593139648438]\n",
      "\n",
      "Instance 2193 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 15: [1.5166270732879639, -1.4294095039367676, -0.28970739245414734, 0.6923461556434631, 1.015568494796753]\n",
      "Grand sum of 1654 tensor sets is: [640.2394409179688, 2675.53515625, -493.19012451171875, -700.5186767578125, -770.34375]\n",
      "\n",
      "Instance 2194 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [52]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "car at index 52: [0.23244713246822357, -0.569398045539856, -1.296372413635254, -0.4683775305747986, 2.8246335983276367]\n",
      "Grand sum of 1655 tensor sets is: [640.4718627929688, 2674.9658203125, -494.48651123046875, -700.987060546875, -767.5191040039062]\n",
      "\n",
      "Instance 2195 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([99, 13, 768])\n",
      "Shape of summed layers is: 99 x 768\n",
      "car at index 36: [0.7293326258659363, 2.0604515075683594, 0.2002931833267212, -0.8630681037902832, 0.01178179681301117]\n",
      "Grand sum of 1656 tensor sets is: [641.201171875, 2677.0263671875, -494.2862243652344, -701.8501586914062, -767.50732421875]\n",
      "\n",
      "Instance 2196 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [124]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 124: [0.08009412884712219, 1.557975172996521, -1.342355728149414, -0.015960007905960083, 0.668647289276123]\n",
      "Grand sum of 1657 tensor sets is: [641.28125, 2678.584228515625, -495.6285705566406, -701.8660888671875, -766.8386840820312]\n",
      "\n",
      "Instance 2197 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 7: [-0.6028317213058472, 0.7278409600257874, 0.9085682034492493, -3.315871477127075, -1.8438870906829834]\n",
      "Grand sum of 1658 tensor sets is: [640.6784057617188, 2679.31201171875, -494.7200012207031, -705.1819458007812, -768.6825561523438]\n",
      "\n",
      "Instance 2198 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 24: [-0.05131794884800911, 1.8682447671890259, -0.11892533302307129, -2.120095729827881, -2.1705269813537598]\n",
      "Grand sum of 1659 tensor sets is: [640.6270751953125, 2681.18017578125, -494.83892822265625, -707.3020629882812, -770.8530883789062]\n",
      "\n",
      "Instance 2199 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "car at index 13: [-0.5130207538604736, 1.8529640436172485, 0.2747284471988678, -0.17519859969615936, -0.4568762183189392]\n",
      "Grand sum of 1660 tensor sets is: [640.1140747070312, 2683.033203125, -494.564208984375, -707.4772338867188, -771.3099365234375]\n",
      "\n",
      "Instance 2200 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 2: [0.6651780009269714, 3.157759428024292, -1.2398369312286377, 1.3860459327697754, -2.7417244911193848]\n",
      "Grand sum of 1661 tensor sets is: [640.7792358398438, 2686.19091796875, -495.8040466308594, -706.0911865234375, -774.0516357421875]\n",
      "\n",
      "Instance 2201 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21, 31]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 21: [0.1377064436674118, 0.5614062547683716, 1.0737082958221436, -0.08526220917701721, -0.9039781093597412]\n",
      "car at index 31: [0.5023081302642822, 1.5471770763397217, 0.3038709759712219, -0.8412141799926758, -0.9515076875686646]\n",
      "Grand sum of 1662 tensor sets is: [641.0992431640625, 2687.2451171875, -495.1152648925781, -706.554443359375, -774.9793701171875]\n",
      "\n",
      "Instance 2202 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 5: [1.1169708967208862, 0.13814221322536469, 0.09664681553840637, -0.3237771987915039, -0.5920218229293823]\n",
      "Grand sum of 1663 tensor sets is: [642.2161865234375, 2687.38330078125, -495.01861572265625, -706.8782348632812, -775.5714111328125]\n",
      "\n",
      "Instance 2203 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2204 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8, 18]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 8: [1.1325693130493164, 1.1469886302947998, -1.2758700847625732, -2.775524854660034, -0.6133987903594971]\n",
      "car at index 18: [0.5067879557609558, 1.6959080696105957, -1.128984808921814, -1.2300364971160889, 0.5498050451278687]\n",
      "Grand sum of 1664 tensor sets is: [643.035888671875, 2688.8046875, -496.2210388183594, -708.8810424804688, -775.6032104492188]\n",
      "\n",
      "Instance 2205 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2206 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 12: [0.5436354875564575, 2.484530448913574, 0.14408721029758453, 0.531075119972229, 0.6357536315917969]\n",
      "Grand sum of 1665 tensor sets is: [643.5795288085938, 2691.289306640625, -496.07696533203125, -708.3499755859375, -774.9674682617188]\n",
      "\n",
      "Instance 2207 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 38: [0.8696686029434204, 1.7524439096450806, -0.9689851403236389, -0.007860809564590454, 1.7109417915344238]\n",
      "Grand sum of 1666 tensor sets is: [644.44921875, 2693.041748046875, -497.04595947265625, -708.3578491210938, -773.2565307617188]\n",
      "\n",
      "Instance 2208 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 10: [0.26686328649520874, 0.3383328318595886, 0.21936215460300446, -2.7534689903259277, -2.2833616733551025]\n",
      "Grand sum of 1667 tensor sets is: [644.716064453125, 2693.380126953125, -496.82659912109375, -711.111328125, -775.5399169921875]\n",
      "\n",
      "Instance 2209 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 15: [1.0755960941314697, 0.9523071646690369, -0.9625322222709656, 0.31697696447372437, 0.19832871854305267]\n",
      "Grand sum of 1668 tensor sets is: [645.7916870117188, 2694.33251953125, -497.78912353515625, -710.7943725585938, -775.3416137695312]\n",
      "\n",
      "Instance 2210 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 17: [-0.5808597207069397, 2.164292335510254, 0.8589649200439453, -0.9802142381668091, -3.309187412261963]\n",
      "Grand sum of 1669 tensor sets is: [645.2108154296875, 2696.496826171875, -496.9301452636719, -711.7745971679688, -778.6508178710938]\n",
      "\n",
      "Instance 2211 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 15: [0.568497896194458, 1.525889277458191, -0.3660084307193756, 1.6627318859100342, -2.39951753616333]\n",
      "Grand sum of 1670 tensor sets is: [645.779296875, 2698.022705078125, -497.296142578125, -710.1118774414062, -781.0503540039062]\n",
      "\n",
      "Instance 2212 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 4: [0.34017854928970337, -0.007666990160942078, -0.04547996073961258, -2.1401007175445557, -0.9594212770462036]\n",
      "Grand sum of 1671 tensor sets is: [646.1194458007812, 2698.01513671875, -497.34161376953125, -712.251953125, -782.009765625]\n",
      "\n",
      "Instance 2213 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 6: [0.473992258310318, 3.0022120475769043, -0.2909393012523651, -0.15667058527469635, -0.8515393733978271]\n",
      "Grand sum of 1672 tensor sets is: [646.5934448242188, 2701.017333984375, -497.6325378417969, -712.4086303710938, -782.861328125]\n",
      "\n",
      "Instance 2214 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8, 16, 34]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 8: [0.6578599214553833, 0.23214854300022125, -0.04652530699968338, 1.3002517223358154, -1.1249041557312012]\n",
      "car at index 16: [0.649973452091217, 0.9049568772315979, 0.7385531067848206, 0.554416298866272, -2.756427526473999]\n",
      "car at index 34: [0.6507639288902283, 0.5559854507446289, 0.7007315158843994, -0.09524785727262497, 0.8911346197128296]\n",
      "Grand sum of 1673 tensor sets is: [647.246337890625, 2701.581787109375, -497.16827392578125, -711.8221435546875, -783.8580322265625]\n",
      "\n",
      "Instance 2215 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 9: [0.4259093999862671, 1.833824872970581, -0.3147381544113159, -0.2553282380104065, -1.7501912117004395]\n",
      "Grand sum of 1674 tensor sets is: [647.6722412109375, 2703.41552734375, -497.4830017089844, -712.0774536132812, -785.6082153320312]\n",
      "\n",
      "Instance 2216 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2217 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 14: [-0.8533906936645508, 1.710620403289795, -0.27855756878852844, -0.9363524913787842, -1.9527021646499634]\n",
      "Grand sum of 1675 tensor sets is: [646.81884765625, 2705.126220703125, -497.7615661621094, -713.0137939453125, -787.5609130859375]\n",
      "\n",
      "Instance 2218 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 16: [0.6040158271789551, 3.1271262168884277, -1.1764802932739258, -0.0941743403673172, -2.152061700820923]\n",
      "Grand sum of 1676 tensor sets is: [647.4228515625, 2708.25341796875, -498.93804931640625, -713.1079711914062, -789.7129516601562]\n",
      "\n",
      "Instance 2219 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [57]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "car at index 57: [0.8908291459083557, 2.2220535278320312, -0.650149941444397, -0.1844196766614914, -0.5182154178619385]\n",
      "Grand sum of 1677 tensor sets is: [648.3136596679688, 2710.4755859375, -499.58819580078125, -713.2924194335938, -790.2311401367188]\n",
      "\n",
      "Instance 2220 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [269, 272]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 269: [-0.061310913413763046, -3.0823965072631836, -0.3681918978691101, -1.1109614372253418, 2.403843402862549]\n",
      "car at index 272: [-0.45665404200553894, -1.6344871520996094, 0.2088729292154312, -2.058497190475464, -0.387115478515625]\n",
      "Grand sum of 1678 tensor sets is: [648.0546875, 2708.1171875, -499.6678466796875, -714.8771362304688, -789.2227783203125]\n",
      "\n",
      "Instance 2221 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5, 27]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 5: [0.3024851381778717, 0.6642332673072815, -0.4854402542114258, -2.9871959686279297, 1.8407317399978638]\n",
      "car at index 27: [0.0997336208820343, 1.8038674592971802, -0.19586844742298126, -3.255007028579712, -1.0203585624694824]\n",
      "Grand sum of 1679 tensor sets is: [648.2557983398438, 2709.351318359375, -500.0085144042969, -717.9982299804688, -788.8126220703125]\n",
      "\n",
      "Instance 2222 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2223 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 4: [0.6252532601356506, 0.8702688217163086, -1.5811488628387451, -0.2045753449201584, 0.6489587426185608]\n",
      "Grand sum of 1680 tensor sets is: [648.8810424804688, 2710.2216796875, -501.58966064453125, -718.2028198242188, -788.1636352539062]\n",
      "\n",
      "Instance 2224 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 8: [0.4571407437324524, 1.1794567108154297, -1.2549453973770142, -2.161794424057007, -0.252620667219162]\n",
      "Grand sum of 1681 tensor sets is: [649.3381958007812, 2711.401123046875, -502.8446044921875, -720.3646240234375, -788.416259765625]\n",
      "\n",
      "Instance 2225 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2226 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 6: [1.1917929649353027, 1.2276941537857056, -0.2119193971157074, 0.24879616498947144, -0.44137799739837646]\n",
      "Grand sum of 1682 tensor sets is: [650.5299682617188, 2712.62890625, -503.0565185546875, -720.1158447265625, -788.857666015625]\n",
      "\n",
      "Instance 2227 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 21: [0.7978061437606812, 2.330535411834717, -0.3187488317489624, 0.14723370969295502, -2.1046464443206787]\n",
      "Grand sum of 1683 tensor sets is: [651.3277587890625, 2714.95947265625, -503.3752746582031, -719.9686279296875, -790.9623413085938]\n",
      "\n",
      "Instance 2228 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 2: [-0.20906737446784973, 2.06009578704834, -0.5570106506347656, -0.9089236259460449, -0.5896868109703064]\n",
      "Grand sum of 1684 tensor sets is: [651.1187133789062, 2717.01953125, -503.9322814941406, -720.8775634765625, -791.552001953125]\n",
      "\n",
      "Instance 2229 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 6: [0.10501120984554291, 1.9564746618270874, -0.15615032613277435, -0.6783731579780579, -2.2708888053894043]\n",
      "Grand sum of 1685 tensor sets is: [651.2237548828125, 2718.97607421875, -504.08843994140625, -721.555908203125, -793.8228759765625]\n",
      "\n",
      "Instance 2230 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 19: [-0.348611980676651, 1.1770132780075073, 0.8827509880065918, 3.7965266704559326, -3.282640218734741]\n",
      "Grand sum of 1686 tensor sets is: [650.8751220703125, 2720.153076171875, -503.2056884765625, -717.7593994140625, -797.1055297851562]\n",
      "\n",
      "Instance 2231 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 4: [0.09293654561042786, 1.1274241209030151, -0.4998161196708679, -2.7867672443389893, -0.388904869556427]\n",
      "Grand sum of 1687 tensor sets is: [650.9680786132812, 2721.280517578125, -503.70550537109375, -720.546142578125, -797.4944458007812]\n",
      "\n",
      "Instance 2232 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2233 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 33: [0.17145350575447083, 2.0411617755889893, 0.20168745517730713, -0.8432794213294983, -0.27395129203796387]\n",
      "Grand sum of 1688 tensor sets is: [651.1395263671875, 2723.32177734375, -503.5038146972656, -721.389404296875, -797.7683715820312]\n",
      "\n",
      "Instance 2234 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 16: [0.9310050010681152, 1.3955669403076172, -0.9154138565063477, -0.5203990340232849, -2.7229251861572266]\n",
      "Grand sum of 1689 tensor sets is: [652.070556640625, 2724.71728515625, -504.4192199707031, -721.9097900390625, -800.4912719726562]\n",
      "\n",
      "Instance 2235 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 9: [0.41737204790115356, 2.135063886642456, -0.7478811144828796, 2.882047176361084, -0.773865818977356]\n",
      "Grand sum of 1690 tensor sets is: [652.4879150390625, 2726.852294921875, -505.1671142578125, -719.0277709960938, -801.26513671875]\n",
      "\n",
      "Instance 2236 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 5: [0.3770807981491089, 2.4440438747406006, -0.5733095407485962, -0.4899091124534607, -1.9908353090286255]\n",
      "Grand sum of 1691 tensor sets is: [652.864990234375, 2729.29638671875, -505.74041748046875, -719.5177001953125, -803.2559814453125]\n",
      "\n",
      "Instance 2237 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 19: [0.8921530246734619, 2.717916250228882, 0.1841278374195099, -0.6446965336799622, -1.0275709629058838]\n",
      "Grand sum of 1692 tensor sets is: [653.7571411132812, 2732.014404296875, -505.5562744140625, -720.1624145507812, -804.2835693359375]\n",
      "\n",
      "Instance 2238 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 3: [0.719024121761322, 2.0381999015808105, -0.9288591742515564, -0.5013315677642822, 0.09547103196382523]\n",
      "Grand sum of 1693 tensor sets is: [654.4761352539062, 2734.052490234375, -506.4851379394531, -720.6637573242188, -804.1881103515625]\n",
      "\n",
      "Instance 2239 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "car at index 13: [0.7974098324775696, 1.4291526079177856, -1.1902090311050415, -2.130491256713867, 0.5666926503181458]\n",
      "Grand sum of 1694 tensor sets is: [655.2735595703125, 2735.481689453125, -507.67535400390625, -722.7942504882812, -803.6213989257812]\n",
      "\n",
      "Instance 2240 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 29: [0.06654025614261627, 1.6598175764083862, -0.3801761567592621, -3.299539566040039, 0.25376516580581665]\n",
      "Grand sum of 1695 tensor sets is: [655.340087890625, 2737.1416015625, -508.0555419921875, -726.0938110351562, -803.3676147460938]\n",
      "\n",
      "Instance 2241 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 14: [0.4369942545890808, 2.5858874320983887, 0.2984820604324341, -0.1401289403438568, -1.199324369430542]\n",
      "Grand sum of 1696 tensor sets is: [655.777099609375, 2739.7275390625, -507.7570495605469, -726.2339477539062, -804.5669555664062]\n",
      "\n",
      "Instance 2242 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 8: [0.17384527623653412, 3.442399024963379, 0.4975733160972595, -2.284821033477783, -1.772430419921875]\n",
      "Grand sum of 1697 tensor sets is: [655.950927734375, 2743.169921875, -507.2594909667969, -728.518798828125, -806.33935546875]\n",
      "\n",
      "Instance 2243 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8, 20]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 8: [0.1695524901151657, 1.8470985889434814, -1.0772918462753296, -0.554667592048645, 1.508984088897705]\n",
      "car at index 20: [-0.15423332154750824, 1.438602089881897, -0.9742311239242554, 0.19581769406795502, -1.4085843563079834]\n",
      "Grand sum of 1698 tensor sets is: [655.9585571289062, 2744.812744140625, -508.2852478027344, -728.6982421875, -806.2891845703125]\n",
      "\n",
      "Instance 2244 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 15: [0.8497419357299805, 2.4823968410491943, -0.5111265182495117, -1.7589977979660034, -0.5140311121940613]\n",
      "Grand sum of 1699 tensor sets is: [656.8082885742188, 2747.295166015625, -508.79638671875, -730.4572143554688, -806.80322265625]\n",
      "\n",
      "Instance 2245 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 3: [0.03299473226070404, 1.2165554761886597, 0.06168266758322716, -2.0475385189056396, -0.9000144600868225]\n",
      "Grand sum of 1700 tensor sets is: [656.84130859375, 2748.51171875, -508.7347106933594, -732.5047607421875, -807.7032470703125]\n",
      "\n",
      "Instance 2246 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 10: [-0.4740353226661682, 0.35666027665138245, -0.5481013059616089, 3.272855281829834, -2.1127758026123047]\n",
      "Grand sum of 1701 tensor sets is: [656.3672485351562, 2748.868408203125, -509.2828063964844, -729.23193359375, -809.8160400390625]\n",
      "\n",
      "Instance 2247 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 12: [0.69696044921875, 1.969333291053772, -0.5012431144714355, -0.2988499104976654, -0.14151856303215027]\n",
      "Grand sum of 1702 tensor sets is: [657.064208984375, 2750.837646484375, -509.7840576171875, -729.53076171875, -809.9575805664062]\n",
      "\n",
      "Instance 2248 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 7: [-0.5740009546279907, 2.6980347633361816, 0.01768086478114128, 2.4406824111938477, -3.272984027862549]\n",
      "Grand sum of 1703 tensor sets is: [656.490234375, 2753.53564453125, -509.7663879394531, -727.090087890625, -813.2305908203125]\n",
      "\n",
      "Instance 2249 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 23: [1.244552493095398, 2.431034564971924, -0.7951350808143616, -2.0651159286499023, -1.2474937438964844]\n",
      "Grand sum of 1704 tensor sets is: [657.7348022460938, 2755.966796875, -510.5615234375, -729.1552124023438, -814.4780883789062]\n",
      "\n",
      "Instance 2250 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2251 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 22: [-0.5031893849372864, 5.439108848571777, 0.27138885855674744, 0.6764214038848877, 2.813166856765747]\n",
      "Grand sum of 1705 tensor sets is: [657.2316284179688, 2761.406005859375, -510.2901306152344, -728.4788208007812, -811.6649169921875]\n",
      "\n",
      "Instance 2252 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2253 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2254 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 22: [-0.6955478191375732, 2.2029542922973633, -0.09382865577936172, -2.380610704421997, -1.1098426580429077]\n",
      "Grand sum of 1706 tensor sets is: [656.5360717773438, 2763.60888671875, -510.38397216796875, -730.8594360351562, -812.7747802734375]\n",
      "\n",
      "Instance 2255 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 6: [0.599175751209259, 1.834089756011963, 0.16280394792556763, -0.4584175944328308, -2.642580032348633]\n",
      "Grand sum of 1707 tensor sets is: [657.13525390625, 2765.44287109375, -510.2211608886719, -731.31787109375, -815.4173583984375]\n",
      "\n",
      "Instance 2256 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 5: [0.519068717956543, 1.6557345390319824, -1.1002228260040283, -1.6637369394302368, -2.1679205894470215]\n",
      "Grand sum of 1708 tensor sets is: [657.654296875, 2767.0986328125, -511.3213806152344, -732.9816284179688, -817.5852661132812]\n",
      "\n",
      "Instance 2257 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2258 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2259 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2260 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 8: [-0.2839396595954895, 2.329324960708618, -1.7658090591430664, -1.9591736793518066, -1.2794009447097778]\n",
      "Grand sum of 1709 tensor sets is: [657.370361328125, 2769.427978515625, -513.0872192382812, -734.9407958984375, -818.8646850585938]\n",
      "\n",
      "Instance 2261 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2262 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 9: [0.4713587760925293, 3.216620445251465, -0.29324737191200256, -0.7245972156524658, -1.4077215194702148]\n",
      "Grand sum of 1710 tensor sets is: [657.8417358398438, 2772.64453125, -513.3804931640625, -735.6654052734375, -820.2723999023438]\n",
      "\n",
      "Instance 2263 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 14: [0.7625828981399536, 2.3282148838043213, -0.4452468156814575, 0.7607694268226624, 0.1564893275499344]\n",
      "Grand sum of 1711 tensor sets is: [658.6043090820312, 2774.97265625, -513.8257446289062, -734.9046630859375, -820.1159057617188]\n",
      "\n",
      "Instance 2264 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 3: [0.3970242142677307, 1.9583622217178345, -0.4041938781738281, -2.217477321624756, -1.092505931854248]\n",
      "Grand sum of 1712 tensor sets is: [659.0013427734375, 2776.930908203125, -514.2299194335938, -737.1221313476562, -821.2084350585938]\n",
      "\n",
      "Instance 2265 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5, 15]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 5: [0.25003668665885925, 0.9547230005264282, -1.5317474603652954, -2.1208438873291016, -1.75582754611969]\n",
      "car at index 15: [0.16216450929641724, 1.5819478034973145, -1.0332154035568237, -3.274819850921631, -1.4879491329193115]\n",
      "Grand sum of 1713 tensor sets is: [659.2074584960938, 2778.19921875, -515.5123901367188, -739.8199462890625, -822.830322265625]\n",
      "\n",
      "Instance 2266 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 7: [0.17211152613162994, 1.2498211860656738, -1.5417052507400513, -0.5355934500694275, 0.7693555951118469]\n",
      "Grand sum of 1714 tensor sets is: [659.3795776367188, 2779.448974609375, -517.0540771484375, -740.3555297851562, -822.0609741210938]\n",
      "\n",
      "Instance 2267 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 20]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 7: [-0.03151465952396393, 2.1776552200317383, -0.8070635795593262, -2.0857291221618652, -0.5895825624465942]\n",
      "car at index 20: [0.13737718760967255, 1.601938009262085, 0.4143539071083069, -1.4610481262207031, 0.030356310307979584]\n",
      "Grand sum of 1715 tensor sets is: [659.4324951171875, 2781.3388671875, -517.2504272460938, -742.12890625, -822.340576171875]\n",
      "\n",
      "Instance 2268 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2269 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2270 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "car at index 4: [-0.7291566133499146, 1.049365758895874, -0.47173354029655457, -1.940508246421814, 0.10369589924812317]\n",
      "Grand sum of 1716 tensor sets is: [658.7033081054688, 2782.38818359375, -517.72216796875, -744.0693969726562, -822.2368774414062]\n",
      "\n",
      "Instance 2271 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 8: [0.3977265954017639, 0.4729987382888794, -0.8435055613517761, -0.21571139991283417, -2.66347599029541]\n",
      "Grand sum of 1717 tensor sets is: [659.1010131835938, 2782.861083984375, -518.565673828125, -744.2850952148438, -824.9003295898438]\n",
      "\n",
      "Instance 2272 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2273 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 4: [1.2074073553085327, 1.8911097049713135, -0.5552731156349182, -1.8485051393508911, 1.252228856086731]\n",
      "Grand sum of 1718 tensor sets is: [660.3084106445312, 2784.752197265625, -519.1209716796875, -746.1336059570312, -823.6480712890625]\n",
      "\n",
      "Instance 2274 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2275 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2276 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 13: [0.3063840866088867, 2.7783737182617188, 0.4696349501609802, -1.9541406631469727, -1.1194137334823608]\n",
      "Grand sum of 1719 tensor sets is: [660.6148071289062, 2787.530517578125, -518.6513671875, -748.0877685546875, -824.7674560546875]\n",
      "\n",
      "Instance 2277 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 6: [0.33130693435668945, 1.979200005531311, 0.6304455995559692, 0.19170011579990387, -1.5242547988891602]\n",
      "Grand sum of 1720 tensor sets is: [660.9461059570312, 2789.509765625, -518.0209350585938, -747.8960571289062, -826.2916870117188]\n",
      "\n",
      "Instance 2278 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "car at index 32: [0.15981075167655945, 2.041151285171509, 0.28935298323631287, -2.4281702041625977, -0.2656225562095642]\n",
      "Grand sum of 1721 tensor sets is: [661.1058959960938, 2791.551025390625, -517.7315673828125, -750.32421875, -826.5573120117188]\n",
      "\n",
      "Instance 2279 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 31: [1.4536173343658447, 1.5297516584396362, -0.23515640199184418, -1.5745800733566284, -2.648557662963867]\n",
      "Grand sum of 1722 tensor sets is: [662.5595092773438, 2793.080810546875, -517.9667358398438, -751.8988037109375, -829.2058715820312]\n",
      "\n",
      "Instance 2280 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 6: [-0.13306114077568054, 1.9967228174209595, 1.131788730621338, -1.4437386989593506, -2.1929774284362793]\n",
      "Grand sum of 1723 tensor sets is: [662.4264526367188, 2795.07763671875, -516.8349609375, -753.342529296875, -831.3988647460938]\n",
      "\n",
      "Instance 2281 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [198]\n",
      "Size of token embeddings is torch.Size([337, 13, 768])\n",
      "Shape of summed layers is: 337 x 768\n",
      "car at index 198: [-0.157190203666687, 1.9466458559036255, -0.3513749837875366, -1.130003809928894, -0.6120810508728027]\n",
      "Grand sum of 1724 tensor sets is: [662.269287109375, 2797.024169921875, -517.1863403320312, -754.4725341796875, -832.0109252929688]\n",
      "\n",
      "Instance 2282 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 7: [-1.030181646347046, 2.8242528438568115, 0.1517374962568283, 0.19154876470565796, -2.4455277919769287]\n",
      "Grand sum of 1725 tensor sets is: [661.2391357421875, 2799.848388671875, -517.0346069335938, -754.281005859375, -834.4564819335938]\n",
      "\n",
      "Instance 2283 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2284 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 9: [0.1350184828042984, -0.06328842043876648, -0.1899605691432953, -2.265789270401001, -0.9096949100494385]\n",
      "Grand sum of 1726 tensor sets is: [661.3741455078125, 2799.78515625, -517.2245483398438, -756.5468139648438, -835.3661499023438]\n",
      "\n",
      "Instance 2285 of car.\n",
      "Looking for vocab token: car\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 2286 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [111]\n",
      "Size of token embeddings is torch.Size([261, 13, 768])\n",
      "Shape of summed layers is: 261 x 768\n",
      "car at index 111: [0.601767897605896, -1.229949712753296, -1.0881457328796387, -0.1500876545906067, 4.48917293548584]\n",
      "Grand sum of 1727 tensor sets is: [661.9758911132812, 2798.55517578125, -518.3126831054688, -756.6968994140625, -830.876953125]\n",
      "\n",
      "Instance 2287 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 13: [0.9340450763702393, 0.16214580833911896, -1.0036752223968506, -1.2336138486862183, 1.0784233808517456]\n",
      "Grand sum of 1728 tensor sets is: [662.909912109375, 2798.71728515625, -519.3163452148438, -757.9305419921875, -829.7985229492188]\n",
      "\n",
      "Instance 2288 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 16: [0.5029954314231873, 1.548677682876587, -1.2421400547027588, -1.755326509475708, -0.447019100189209]\n",
      "Grand sum of 1729 tensor sets is: [663.4129028320312, 2800.265869140625, -520.5584716796875, -759.6858520507812, -830.2455444335938]\n",
      "\n",
      "Instance 2289 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2290 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 23: [0.4358096122741699, 1.7040328979492188, -0.604753851890564, -0.5240879654884338, -0.052922479808330536]\n",
      "Grand sum of 1730 tensor sets is: [663.8486938476562, 2801.969970703125, -521.1632080078125, -760.2099609375, -830.2984619140625]\n",
      "\n",
      "Instance 2291 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [68]\n",
      "Size of token embeddings is torch.Size([96, 13, 768])\n",
      "Shape of summed layers is: 96 x 768\n",
      "car at index 68: [0.546779215335846, -0.7625817060470581, -1.4038984775543213, 1.0709359645843506, 3.1506989002227783]\n",
      "Grand sum of 1731 tensor sets is: [664.3954467773438, 2801.207275390625, -522.5670776367188, -759.1390380859375, -827.1477661132812]\n",
      "\n",
      "Instance 2292 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 10: [0.6825562715530396, 2.7915213108062744, 0.14533448219299316, -1.8198858499526978, 1.3615717887878418]\n",
      "Grand sum of 1732 tensor sets is: [665.0780029296875, 2803.998779296875, -522.4217529296875, -760.9589233398438, -825.7861938476562]\n",
      "\n",
      "Instance 2293 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [55]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "car at index 55: [0.4758654236793518, 1.4995763301849365, -0.3540995121002197, -0.8950551748275757, -1.0992326736450195]\n",
      "Grand sum of 1733 tensor sets is: [665.5538940429688, 2805.498291015625, -522.77587890625, -761.85400390625, -826.8854370117188]\n",
      "\n",
      "Instance 2294 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 19: [0.32148081064224243, -0.007356852293014526, -0.609519362449646, 1.3734230995178223, 1.1826006174087524]\n",
      "Grand sum of 1734 tensor sets is: [665.8753662109375, 2805.490966796875, -523.3853759765625, -760.4805908203125, -825.7028198242188]\n",
      "\n",
      "Instance 2295 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2296 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2297 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 28: [0.4723649024963379, 1.8664754629135132, -0.4090213477611542, -1.7806004285812378, -1.6475590467453003]\n",
      "Grand sum of 1735 tensor sets is: [666.3477172851562, 2807.357421875, -523.7943725585938, -762.2611694335938, -827.3504028320312]\n",
      "\n",
      "Instance 2298 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([389, 13, 768])\n",
      "Shape of summed layers is: 389 x 768\n",
      "car at index 74: [0.5909023284912109, -0.9459004402160645, 0.7420952320098877, -0.7513425350189209, -0.5070445537567139]\n",
      "Grand sum of 1736 tensor sets is: [666.9385986328125, 2806.41162109375, -523.0523071289062, -763.0125122070312, -827.857421875]\n",
      "\n",
      "Instance 2299 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "car at index 17: [0.20199570059776306, 1.8038018941879272, -0.9561833143234253, 0.6106546521186829, -2.5931599140167236]\n",
      "Grand sum of 1737 tensor sets is: [667.1405639648438, 2808.21533203125, -524.0084838867188, -762.40185546875, -830.4505615234375]\n",
      "\n",
      "Instance 2300 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [107]\n",
      "Size of token embeddings is torch.Size([130, 13, 768])\n",
      "Shape of summed layers is: 130 x 768\n",
      "car at index 107: [-0.23564717173576355, 3.114441394805908, -0.023222187533974648, -0.3753979802131653, -1.5687971115112305]\n",
      "Grand sum of 1738 tensor sets is: [666.9049072265625, 2811.329833984375, -524.0316772460938, -762.7772827148438, -832.0193481445312]\n",
      "\n",
      "Instance 2301 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2302 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 4: [1.2614601850509644, 2.4560694694519043, 0.008085696026682854, -2.7913639545440674, -1.149603247642517]\n",
      "Grand sum of 1739 tensor sets is: [668.1663818359375, 2813.785888671875, -524.0236206054688, -765.5686645507812, -833.1689453125]\n",
      "\n",
      "Instance 2303 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 2: [-0.8739386796951294, 2.4665539264678955, 0.47578251361846924, 1.0076889991760254, -0.8111791610717773]\n",
      "Grand sum of 1740 tensor sets is: [667.2924194335938, 2816.25244140625, -523.5478515625, -764.5609741210938, -833.9801025390625]\n",
      "\n",
      "Instance 2304 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 32: [0.13610714673995972, 2.58182954788208, -0.8675680160522461, 0.7743229269981384, 0.6474024653434753]\n",
      "Grand sum of 1741 tensor sets is: [667.4285278320312, 2818.834228515625, -524.4154052734375, -763.78662109375, -833.3327026367188]\n",
      "\n",
      "Instance 2305 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "car at index 45: [0.019346237182617188, 1.5241037607192993, 0.4114370048046112, 1.62386953830719, -1.9195950031280518]\n",
      "Grand sum of 1742 tensor sets is: [667.4478759765625, 2820.3583984375, -524.0039672851562, -762.1627807617188, -835.2523193359375]\n",
      "\n",
      "Instance 2306 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 18: [-0.07638529688119888, 3.3187100887298584, -0.28807777166366577, 0.5155877470970154, -1.224151849746704]\n",
      "Grand sum of 1743 tensor sets is: [667.3715209960938, 2823.677001953125, -524.2920532226562, -761.647216796875, -836.4765014648438]\n",
      "\n",
      "Instance 2307 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2308 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 8: [1.080733299255371, 2.1625988483428955, -0.7366951704025269, -1.4042423963546753, -0.9346609115600586]\n",
      "Grand sum of 1744 tensor sets is: [668.4522705078125, 2825.839599609375, -525.0287475585938, -763.0514526367188, -837.4111328125]\n",
      "\n",
      "Instance 2309 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 4: [0.03859663009643555, 1.8863636255264282, -0.5991345047950745, -2.2907495498657227, -1.2780046463012695]\n",
      "Grand sum of 1745 tensor sets is: [668.4908447265625, 2827.72607421875, -525.6278686523438, -765.3422241210938, -838.6891479492188]\n",
      "\n",
      "Instance 2310 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14, 29]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 14: [0.2973620891571045, 2.093568801879883, 0.1046992614865303, -0.7227076888084412, -0.7852758169174194]\n",
      "car at index 29: [0.41788777709007263, 2.3169057369232178, 0.327240914106369, -0.2892645597457886, 1.2557168006896973]\n",
      "Grand sum of 1746 tensor sets is: [668.8484497070312, 2829.931396484375, -525.4119262695312, -765.8482055664062, -838.4539184570312]\n",
      "\n",
      "Instance 2311 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 43]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 11: [0.6347211599349976, 1.3548164367675781, -1.0143011808395386, -0.32032084465026855, 0.3730972707271576]\n",
      "car at index 43: [0.7167781591415405, 2.1960701942443848, -1.204422116279602, -0.6696293950080872, -0.01512831449508667]\n",
      "Grand sum of 1747 tensor sets is: [669.524169921875, 2831.706787109375, -526.5213012695312, -766.3432006835938, -838.2749633789062]\n",
      "\n",
      "Instance 2312 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22, 25, 49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "car at index 22: [-0.3507293462753296, 2.8257460594177246, -0.07131098955869675, -3.650763988494873, 0.06006672978401184]\n",
      "car at index 25: [-0.04204617068171501, 0.982154905796051, 0.7381594777107239, -3.040466785430908, -0.1220388412475586]\n",
      "car at index 49: [0.29441937804222107, 2.391993999481201, -0.022932350635528564, -4.277101039886475, -0.23348462581634521]\n",
      "Grand sum of 1748 tensor sets is: [669.4913940429688, 2833.7734375, -526.306640625, -769.9993286132812, -838.3734741210938]\n",
      "\n",
      "Instance 2313 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 6: [0.768073558807373, -0.16524666547775269, -0.4214218854904175, -0.3235166668891907, -1.813096284866333]\n",
      "Grand sum of 1749 tensor sets is: [670.2594604492188, 2833.608154296875, -526.7280883789062, -770.3228149414062, -840.1865844726562]\n",
      "\n",
      "Instance 2314 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2315 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12, 31]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 12: [0.5119976997375488, 1.7822544574737549, -0.7717248797416687, -1.829799771308899, -0.6296060681343079]\n",
      "car at index 31: [-0.1215638816356659, 2.8919517993927, -0.18976666033267975, -3.0269734859466553, -1.755347490310669]\n",
      "Grand sum of 1750 tensor sets is: [670.4546508789062, 2835.9453125, -527.2088623046875, -772.751220703125, -841.3790893554688]\n",
      "\n",
      "Instance 2316 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 14: [-0.08102937042713165, 1.7369792461395264, -0.5473873019218445, -1.6257885694503784, -0.996042013168335]\n",
      "Grand sum of 1751 tensor sets is: [670.3735961914062, 2837.682373046875, -527.7562255859375, -774.3770141601562, -842.3751220703125]\n",
      "\n",
      "Instance 2317 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [80]\n",
      "Size of token embeddings is torch.Size([89, 13, 768])\n",
      "Shape of summed layers is: 89 x 768\n",
      "car at index 80: [0.47847074270248413, 3.1186070442199707, -0.6240663528442383, 1.2850040197372437, 0.9493836760520935]\n",
      "Grand sum of 1752 tensor sets is: [670.85205078125, 2840.801025390625, -528.3803100585938, -773.0919799804688, -841.4257202148438]\n",
      "\n",
      "Instance 2318 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 11: [0.39257562160491943, 1.5281834602355957, -0.6189863681793213, -1.2172225713729858, -0.05670745670795441]\n",
      "Grand sum of 1753 tensor sets is: [671.24462890625, 2842.3291015625, -528.999267578125, -774.3092041015625, -841.482421875]\n",
      "\n",
      "Instance 2319 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "car at index 39: [1.1724382638931274, 2.1528940200805664, 0.5170647501945496, 0.30672723054885864, -2.105952262878418]\n",
      "Grand sum of 1754 tensor sets is: [672.4170532226562, 2844.48193359375, -528.482177734375, -774.0025024414062, -843.58837890625]\n",
      "\n",
      "Instance 2320 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 8: [-0.20556491613388062, 3.0451807975769043, -0.9259757995605469, -1.7641782760620117, -0.23907148838043213]\n",
      "Grand sum of 1755 tensor sets is: [672.2114868164062, 2847.527099609375, -529.4081420898438, -775.7666625976562, -843.8274536132812]\n",
      "\n",
      "Instance 2321 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 16: [-0.0032809823751449585, 1.7164312601089478, 0.22348466515541077, 1.183032512664795, -4.267752647399902]\n",
      "Grand sum of 1756 tensor sets is: [672.2081909179688, 2849.24365234375, -529.1846313476562, -774.5836181640625, -848.09521484375]\n",
      "\n",
      "Instance 2322 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 14: [1.071324110031128, 0.08424340188503265, -1.147665023803711, -0.738924503326416, 3.630126953125]\n",
      "Grand sum of 1757 tensor sets is: [673.279541015625, 2849.327880859375, -530.332275390625, -775.3225708007812, -844.465087890625]\n",
      "\n",
      "Instance 2323 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2324 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2325 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6, 9, 11]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 6: [0.03809408098459244, -0.6427074670791626, -0.274373322725296, 0.6854625940322876, 1.7834374904632568]\n",
      "car at index 9: [0.13445894420146942, -0.2227344661951065, 1.107896327972412, 0.035222165286540985, 0.969712495803833]\n",
      "car at index 11: [0.22483167052268982, -0.1387397050857544, 1.1827460527420044, 0.4939495325088501, 1.144399642944336]\n",
      "Grand sum of 1758 tensor sets is: [673.4119873046875, 2848.9931640625, -529.66015625, -774.9176635742188, -843.1658935546875]\n",
      "\n",
      "Instance 2326 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 18: [0.3614485263824463, 0.9923585057258606, -0.2431427240371704, -1.5406911373138428, 0.4028869867324829]\n",
      "Grand sum of 1759 tensor sets is: [673.7734375, 2849.985595703125, -529.9033203125, -776.4583740234375, -842.7630004882812]\n",
      "\n",
      "Instance 2327 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 28: [-0.17137615382671356, 0.8964769244194031, -1.5164847373962402, -2.5652267932891846, -1.3840241432189941]\n",
      "Grand sum of 1760 tensor sets is: [673.60205078125, 2850.882080078125, -531.4197998046875, -779.0236206054688, -844.1470336914062]\n",
      "\n",
      "Instance 2328 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 15: [0.5359340906143188, 1.16709566116333, -0.03431133180856705, 0.47080564498901367, -1.6606862545013428]\n",
      "Grand sum of 1761 tensor sets is: [674.1380004882812, 2852.049072265625, -531.4541015625, -778.5527954101562, -845.8077392578125]\n",
      "\n",
      "Instance 2329 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 17: [0.5321305990219116, 1.0588483810424805, -0.5799562335014343, 0.5242674350738525, 0.458476722240448]\n",
      "Grand sum of 1762 tensor sets is: [674.6701049804688, 2853.10791015625, -532.0340576171875, -778.0285034179688, -845.3492431640625]\n",
      "\n",
      "Instance 2330 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 8: [-0.15884537994861603, 1.8042348623275757, -0.3339434564113617, 0.7764499187469482, -0.2170831263065338]\n",
      "Grand sum of 1763 tensor sets is: [674.51123046875, 2854.912109375, -532.3679809570312, -777.2520751953125, -845.5663452148438]\n",
      "\n",
      "Instance 2331 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 7: [-0.65531325340271, 2.1402883529663086, 0.2102224975824356, -1.4775710105895996, -2.698532819747925]\n",
      "Grand sum of 1764 tensor sets is: [673.8558959960938, 2857.052490234375, -532.1577758789062, -778.7296752929688, -848.264892578125]\n",
      "\n",
      "Instance 2332 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2333 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2334 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 5: [0.53702712059021, 1.348286509513855, -1.4326978921890259, -2.6456055641174316, -0.8084617853164673]\n",
      "Grand sum of 1765 tensor sets is: [674.3929443359375, 2858.40087890625, -533.5904541015625, -781.3753051757812, -849.0733642578125]\n",
      "\n",
      "Instance 2335 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 13: [0.8614962697029114, 1.6708143949508667, -0.6954786777496338, -2.2230536937713623, 0.4190799295902252]\n",
      "Grand sum of 1766 tensor sets is: [675.2544555664062, 2860.07177734375, -534.2859497070312, -783.598388671875, -848.654296875]\n",
      "\n",
      "Instance 2336 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 6: [1.0670583248138428, 0.75867760181427, 0.04355586692690849, -1.7809141874313354, 1.0877233743667603]\n",
      "Grand sum of 1767 tensor sets is: [676.321533203125, 2860.83056640625, -534.2423706054688, -785.3792724609375, -847.5665893554688]\n",
      "\n",
      "Instance 2337 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 14: [0.9490557909011841, 2.7152912616729736, -1.8337366580963135, -0.6312738060951233, 0.5180123448371887]\n",
      "Grand sum of 1768 tensor sets is: [677.2705688476562, 2863.5458984375, -536.0761108398438, -786.0105590820312, -847.048583984375]\n",
      "\n",
      "Instance 2338 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26, 33]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 26: [0.2183692455291748, 1.1107432842254639, -0.39179864525794983, 0.6571943759918213, -0.1758168339729309]\n",
      "car at index 33: [0.6617956161499023, 2.422436237335205, -1.1620780229568481, -0.9159223437309265, 0.14771956205368042]\n",
      "Grand sum of 1769 tensor sets is: [677.7106323242188, 2865.3125, -536.85302734375, -786.139892578125, -847.0626220703125]\n",
      "\n",
      "Instance 2339 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 21: [0.3733465373516083, 1.2652208805084229, 0.0963323786854744, 3.827943801879883, -1.2589411735534668]\n",
      "Grand sum of 1770 tensor sets is: [678.083984375, 2866.57763671875, -536.7567138671875, -782.3119506835938, -848.321533203125]\n",
      "\n",
      "Instance 2340 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2341 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 5: [0.7253754734992981, 1.939155101776123, -0.4750145375728607, 0.7447187900543213, -0.2895548343658447]\n",
      "Grand sum of 1771 tensor sets is: [678.8093872070312, 2868.516845703125, -537.2317504882812, -781.5672607421875, -848.611083984375]\n",
      "\n",
      "Instance 2342 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 180]\n",
      "Size of token embeddings is torch.Size([348, 13, 768])\n",
      "Shape of summed layers is: 348 x 768\n",
      "car at index 11: [1.0416138172149658, 1.7930841445922852, 0.07253870368003845, -0.618110179901123, 2.3780133724212646]\n",
      "car at index 180: [0.6048741340637207, 2.349008083343506, -0.6580943465232849, -0.6255120038986206, 1.6197737455368042]\n",
      "Grand sum of 1772 tensor sets is: [679.6326293945312, 2870.587890625, -537.5245361328125, -782.1890869140625, -846.6121826171875]\n",
      "\n",
      "Instance 2343 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "car at index 29: [1.340667486190796, -0.4167885184288025, -0.06813015043735504, 1.4364984035491943, 3.1967172622680664]\n",
      "Grand sum of 1773 tensor sets is: [680.9732666015625, 2870.171142578125, -537.5926513671875, -780.7525634765625, -843.4154663085938]\n",
      "\n",
      "Instance 2344 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 14: [0.542732834815979, 2.579357385635376, -0.014579486101865768, -0.8083070516586304, -2.248953104019165]\n",
      "Grand sum of 1774 tensor sets is: [681.5159912109375, 2872.75048828125, -537.6072387695312, -781.5608520507812, -845.6644287109375]\n",
      "\n",
      "Instance 2345 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "car at index 4: [0.27938705682754517, 0.6059145927429199, -1.0140637159347534, 0.4153786301612854, -0.8163982629776001]\n",
      "Grand sum of 1775 tensor sets is: [681.7953491210938, 2873.3564453125, -538.6212768554688, -781.1454467773438, -846.4808349609375]\n",
      "\n",
      "Instance 2346 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 33: [0.8033102750778198, 0.1295710802078247, -0.4658340811729431, 0.4405220150947571, 2.2592201232910156]\n",
      "Grand sum of 1776 tensor sets is: [682.5986328125, 2873.486083984375, -539.0870971679688, -780.7048950195312, -844.2216186523438]\n",
      "\n",
      "Instance 2347 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 26: [0.2846032381057739, 3.348803758621216, 0.30607202649116516, 0.12912990152835846, -2.3157799243927]\n",
      "Grand sum of 1777 tensor sets is: [682.8832397460938, 2876.8349609375, -538.781005859375, -780.5757446289062, -846.5374145507812]\n",
      "\n",
      "Instance 2348 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "car at index 2: [-0.2953271269798279, 3.418206214904785, 0.08968909084796906, 0.17959578335285187, 1.0090285539627075]\n",
      "Grand sum of 1778 tensor sets is: [682.587890625, 2880.253173828125, -538.6913452148438, -780.3961791992188, -845.5283813476562]\n",
      "\n",
      "Instance 2349 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20, 37, 93]\n",
      "Size of token embeddings is torch.Size([117, 13, 768])\n",
      "Shape of summed layers is: 117 x 768\n",
      "car at index 20: [0.14060673117637634, 2.2915401458740234, -0.3839700520038605, -2.5728812217712402, -0.2955898940563202]\n",
      "car at index 37: [0.975908100605011, 2.594332218170166, -1.3965468406677246, -1.8908569812774658, -2.3230597972869873]\n",
      "car at index 93: [0.3278610408306122, 1.8480807542800903, -1.2258201837539673, -1.9417893886566162, -0.26908397674560547]\n",
      "Grand sum of 1779 tensor sets is: [683.0693359375, 2882.497802734375, -539.6934814453125, -782.5313720703125, -846.490966796875]\n",
      "\n",
      "Instance 2350 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 20: [1.3880903720855713, 2.337523937225342, 0.8811126947402954, -1.0911922454833984, 0.2971317768096924]\n",
      "Grand sum of 1780 tensor sets is: [684.4573974609375, 2884.835205078125, -538.8123779296875, -783.62255859375, -846.19384765625]\n",
      "\n",
      "Instance 2351 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2352 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 14: [0.10254748165607452, -0.8546440601348877, -0.2081942856311798, 0.19035077095031738, 0.7183389663696289]\n",
      "Grand sum of 1781 tensor sets is: [684.5599365234375, 2883.98046875, -539.0205688476562, -783.4321899414062, -845.4755249023438]\n",
      "\n",
      "Instance 2353 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 6: [1.0156848430633545, 2.4270291328430176, 0.16698992252349854, -1.8815233707427979, -1.0138896703720093]\n",
      "Grand sum of 1782 tensor sets is: [685.5756225585938, 2886.407470703125, -538.8535766601562, -785.313720703125, -846.4894409179688]\n",
      "\n",
      "Instance 2354 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 7: [0.663607656955719, 2.7261314392089844, 0.49572309851646423, -0.9089083075523376, -2.702573776245117]\n",
      "Grand sum of 1783 tensor sets is: [686.2392578125, 2889.133544921875, -538.3578491210938, -786.22265625, -849.1920166015625]\n",
      "\n",
      "Instance 2355 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2356 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2357 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2358 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 10: [0.3532294034957886, 2.222059726715088, -2.3248772621154785, -0.03505465015769005, -0.7822215557098389]\n",
      "Grand sum of 1784 tensor sets is: [686.5924682617188, 2891.355712890625, -540.6827392578125, -786.2576904296875, -849.9742431640625]\n",
      "\n",
      "Instance 2359 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([102, 13, 768])\n",
      "Shape of summed layers is: 102 x 768\n",
      "car at index 94: [2.0562686920166016, -1.6993740797042847, -1.9171077013015747, 0.3336646854877472, 1.3676304817199707]\n",
      "Grand sum of 1785 tensor sets is: [688.6487426757812, 2889.65625, -542.599853515625, -785.9240112304688, -848.6066284179688]\n",
      "\n",
      "Instance 2360 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 14: [-0.6843618154525757, 2.5247974395751953, 0.021325580775737762, -1.1987266540527344, -2.4011619091033936]\n",
      "Grand sum of 1786 tensor sets is: [687.96435546875, 2892.18115234375, -542.5785522460938, -787.1227416992188, -851.0078125]\n",
      "\n",
      "Instance 2361 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2362 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14, 21]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 14: [0.13290444016456604, 2.099790573120117, -0.5858502388000488, 0.5107999444007874, -0.7311915159225464]\n",
      "car at index 21: [0.8275496959686279, 2.6744282245635986, -0.014028076082468033, -0.4044950604438782, -1.5292319059371948]\n",
      "Grand sum of 1787 tensor sets is: [688.444580078125, 2894.568359375, -542.8784790039062, -787.069580078125, -852.1380004882812]\n",
      "\n",
      "Instance 2363 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 8: [0.8916155099868774, 2.0862231254577637, -0.2866131067276001, -0.03440617024898529, -3.8145127296447754]\n",
      "Grand sum of 1788 tensor sets is: [689.336181640625, 2896.654541015625, -543.1651000976562, -787.10400390625, -855.9525146484375]\n",
      "\n",
      "Instance 2364 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2365 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 2: [0.04080325365066528, 1.5673229694366455, -0.803573489189148, -2.120260715484619, 0.14749230444431305]\n",
      "Grand sum of 1789 tensor sets is: [689.3770141601562, 2898.221923828125, -543.9686889648438, -789.2242431640625, -855.8049926757812]\n",
      "\n",
      "Instance 2366 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 3: [-0.12528136372566223, 2.391396999359131, -0.15264122188091278, -2.4754040241241455, 0.9026199579238892]\n",
      "Grand sum of 1790 tensor sets is: [689.251708984375, 2900.61328125, -544.121337890625, -791.6996459960938, -854.90234375]\n",
      "\n",
      "Instance 2367 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 20: [-0.43287739157676697, 2.5296471118927, 0.24150928854942322, -0.5376105308532715, -1.545466661453247]\n",
      "Grand sum of 1791 tensor sets is: [688.81884765625, 2903.142822265625, -543.8798217773438, -792.2372436523438, -856.4478149414062]\n",
      "\n",
      "Instance 2368 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 10: [0.6345304846763611, 1.749498724937439, -0.10216442495584488, -0.3448891043663025, -0.5732999444007874]\n",
      "Grand sum of 1792 tensor sets is: [689.453369140625, 2904.892333984375, -543.9819946289062, -792.5821533203125, -857.0211181640625]\n",
      "\n",
      "Instance 2369 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 24: [0.2618755102157593, 2.5364418029785156, 0.45335063338279724, -1.8635568618774414, -1.976609706878662]\n",
      "Grand sum of 1793 tensor sets is: [689.7152709960938, 2907.4287109375, -543.5286254882812, -794.4457397460938, -858.9977416992188]\n",
      "\n",
      "Instance 2370 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 3: [0.5761487483978271, 0.08668997138738632, -0.7867816090583801, 0.8377299308776855, -0.35132044553756714]\n",
      "Grand sum of 1794 tensor sets is: [690.2914428710938, 2907.515380859375, -544.3154296875, -793.6080322265625, -859.3490600585938]\n",
      "\n",
      "Instance 2371 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 29: [0.39213013648986816, 2.4860689640045166, 0.8051410913467407, 1.4276981353759766, -1.1215271949768066]\n",
      "Grand sum of 1795 tensor sets is: [690.68359375, 2910.00146484375, -543.5103149414062, -792.1803588867188, -860.4705810546875]\n",
      "\n",
      "Instance 2372 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [401]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 401: [0.9898282289505005, 2.6501264572143555, -0.8075618743896484, -1.8985214233398438, -0.018684130162000656]\n",
      "Grand sum of 1796 tensor sets is: [691.6734008789062, 2912.651611328125, -544.31787109375, -794.078857421875, -860.4892578125]\n",
      "\n",
      "Instance 2373 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 10: [0.6522090435028076, 1.9977830648422241, -1.1860358715057373, -1.3901128768920898, 2.304389476776123]\n",
      "Grand sum of 1797 tensor sets is: [692.3256225585938, 2914.6494140625, -545.50390625, -795.468994140625, -858.1848754882812]\n",
      "\n",
      "Instance 2374 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40, 89, 133]\n",
      "Size of token embeddings is torch.Size([139, 13, 768])\n",
      "Shape of summed layers is: 139 x 768\n",
      "car at index 40: [0.39288240671157837, 3.3459410667419434, 0.3604832589626312, -1.5358253717422485, -1.9502266645431519]\n",
      "car at index 89: [0.454825758934021, 0.9184327721595764, -1.356785774230957, -1.381296992301941, -0.910920262336731]\n",
      "car at index 133: [0.36142098903656006, 1.3634058237075806, 0.23260626196861267, -1.3111019134521484, 0.3789973258972168]\n",
      "Grand sum of 1798 tensor sets is: [692.7286376953125, 2916.525390625, -545.7584838867188, -796.87841796875, -859.0122680664062]\n",
      "\n",
      "Instance 2375 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 2: [0.14469532668590546, 3.139512777328491, -0.4075801372528076, -1.0830049514770508, -1.573627233505249]\n",
      "Grand sum of 1799 tensor sets is: [692.8733520507812, 2919.664794921875, -546.1660766601562, -797.96142578125, -860.5858764648438]\n",
      "\n",
      "Instance 2376 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2377 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 13: [0.3068583011627197, 2.250215530395508, -1.1017513275146484, -1.7271010875701904, -1.3939335346221924]\n",
      "Grand sum of 1800 tensor sets is: [693.1802368164062, 2921.9150390625, -547.267822265625, -799.6885375976562, -861.9797973632812]\n",
      "\n",
      "Instance 2378 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 4: [0.7449319958686829, 1.9769976139068604, 0.25843048095703125, 0.16321006417274475, 0.5867841243743896]\n",
      "Grand sum of 1801 tensor sets is: [693.9251708984375, 2923.89208984375, -547.0093994140625, -799.5253295898438, -861.3930053710938]\n",
      "\n",
      "Instance 2379 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 6: [0.36702701449394226, 1.2627354860305786, -0.3327518105506897, -2.2922608852386475, -1.3004988431930542]\n",
      "Grand sum of 1802 tensor sets is: [694.2921752929688, 2925.15478515625, -547.3421630859375, -801.8175659179688, -862.6934814453125]\n",
      "\n",
      "Instance 2380 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 10: [1.0214931964874268, 2.102893352508545, -0.15599985420703888, -0.8075895309448242, 0.17029674351215363]\n",
      "Grand sum of 1803 tensor sets is: [695.3136596679688, 2927.257568359375, -547.4981689453125, -802.6251831054688, -862.523193359375]\n",
      "\n",
      "Instance 2381 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 5: [0.5756725072860718, 1.3532520532608032, -1.3438135385513306, -2.431385040283203, -0.6614287495613098]\n",
      "Grand sum of 1804 tensor sets is: [695.8893432617188, 2928.61083984375, -548.8419799804688, -805.0565795898438, -863.1846313476562]\n",
      "\n",
      "Instance 2382 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 11: [0.965718150138855, 2.5734035968780518, -0.10324250161647797, -1.1514395475387573, -2.621535301208496]\n",
      "Grand sum of 1805 tensor sets is: [696.8550415039062, 2931.184326171875, -548.9452514648438, -806.2080078125, -865.80615234375]\n",
      "\n",
      "Instance 2383 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 13: [0.5414742231369019, 1.4919064044952393, -1.347314715385437, -2.1204919815063477, -0.1366165727376938]\n",
      "Grand sum of 1806 tensor sets is: [697.3965454101562, 2932.67626953125, -550.2925415039062, -808.3284912109375, -865.9427490234375]\n",
      "\n",
      "Instance 2384 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 18: [0.620170533657074, 2.6803483963012695, -0.40266451239585876, 0.8120765686035156, -1.6038306951522827]\n",
      "Grand sum of 1807 tensor sets is: [698.0167236328125, 2935.356689453125, -550.6951904296875, -807.5164184570312, -867.5465698242188]\n",
      "\n",
      "Instance 2385 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 19: [-0.05440773814916611, 1.9317984580993652, -0.46795254945755005, -2.101807117462158, 1.5030438899993896]\n",
      "Grand sum of 1808 tensor sets is: [697.9623413085938, 2937.28857421875, -551.1631469726562, -809.6182250976562, -866.0435180664062]\n",
      "\n",
      "Instance 2386 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 16: [-0.05644698813557625, 2.2385315895080566, -1.2225732803344727, -1.6056238412857056, -1.8838622570037842]\n",
      "Grand sum of 1809 tensor sets is: [697.9058837890625, 2939.527099609375, -552.3857421875, -811.223876953125, -867.9273681640625]\n",
      "\n",
      "Instance 2387 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 31: [0.8941063284873962, 1.5937288999557495, -1.3680455684661865, -1.287471055984497, 0.4352916479110718]\n",
      "Grand sum of 1810 tensor sets is: [698.7999877929688, 2941.120849609375, -553.7537841796875, -812.5113525390625, -867.4920654296875]\n",
      "\n",
      "Instance 2388 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [126]\n",
      "Size of token embeddings is torch.Size([312, 13, 768])\n",
      "Shape of summed layers is: 312 x 768\n",
      "car at index 126: [0.39226460456848145, 0.2810796797275543, -0.06570366024971008, -0.5152440667152405, 1.244384527206421]\n",
      "Grand sum of 1811 tensor sets is: [699.1922607421875, 2941.40185546875, -553.8194580078125, -813.026611328125, -866.2476806640625]\n",
      "\n",
      "Instance 2389 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 8: [0.32735568284988403, 2.3062150478363037, -1.3482587337493896, -1.886533260345459, -0.3244250416755676]\n",
      "Grand sum of 1812 tensor sets is: [699.5195922851562, 2943.7080078125, -555.167724609375, -814.9131469726562, -866.5720825195312]\n",
      "\n",
      "Instance 2390 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 3: [0.7164012789726257, 2.6421782970428467, -0.09180101007223129, -2.9331140518188477, -0.7986611127853394]\n",
      "Grand sum of 1813 tensor sets is: [700.2360229492188, 2946.35009765625, -555.259521484375, -817.8462524414062, -867.3707275390625]\n",
      "\n",
      "Instance 2391 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 19: [-0.47915714979171753, 1.8395678997039795, -0.5126100182533264, -1.5071687698364258, -1.7757576704025269]\n",
      "Grand sum of 1814 tensor sets is: [699.7568359375, 2948.189697265625, -555.7721557617188, -819.3533935546875, -869.146484375]\n",
      "\n",
      "Instance 2392 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 10: [1.178847312927246, 1.8197287321090698, -0.0015132874250411987, 1.4623823165893555, -1.7730356454849243]\n",
      "Grand sum of 1815 tensor sets is: [700.9356689453125, 2950.009521484375, -555.773681640625, -817.8909912109375, -870.9194946289062]\n",
      "\n",
      "Instance 2393 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2394 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 7: [0.8560937643051147, 2.3591864109039307, -0.39728090167045593, -0.02009742334485054, 1.0944594144821167]\n",
      "Grand sum of 1816 tensor sets is: [701.791748046875, 2952.36865234375, -556.1709594726562, -817.9110717773438, -869.8250122070312]\n",
      "\n",
      "Instance 2395 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 7: [0.8710851073265076, 3.6613821983337402, -0.18537364900112152, -2.0012457370758057, -1.7633209228515625]\n",
      "Grand sum of 1817 tensor sets is: [702.662841796875, 2956.030029296875, -556.3563232421875, -819.9122924804688, -871.5883178710938]\n",
      "\n",
      "Instance 2396 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 10: [1.4131762981414795, 1.88522207736969, 0.5882644057273865, -2.423396110534668, -1.320235252380371]\n",
      "Grand sum of 1818 tensor sets is: [704.0759887695312, 2957.915283203125, -555.76806640625, -822.335693359375, -872.9085693359375]\n",
      "\n",
      "Instance 2397 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 2: [1.1928958892822266, 2.318847179412842, -1.0351489782333374, -1.3794808387756348, -0.5760138034820557]\n",
      "Grand sum of 1819 tensor sets is: [705.2688598632812, 2960.234130859375, -556.80322265625, -823.7151489257812, -873.4845581054688]\n",
      "\n",
      "Instance 2398 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2399 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 13: [0.5813921093940735, 1.3238978385925293, -0.8620299696922302, 0.5631764531135559, -0.1829262226819992]\n",
      "Grand sum of 1820 tensor sets is: [705.8502807617188, 2961.55810546875, -557.6652221679688, -823.1519775390625, -873.66748046875]\n",
      "\n",
      "Instance 2400 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 9: [-0.1379563957452774, 2.5816760063171387, -0.16419440507888794, -2.0406200885772705, -1.2003254890441895]\n",
      "Grand sum of 1821 tensor sets is: [705.7123413085938, 2964.139892578125, -557.8294067382812, -825.192626953125, -874.8677978515625]\n",
      "\n",
      "Instance 2401 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2402 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 25: [0.26509642601013184, 3.2530672550201416, -0.6646245718002319, -1.4852306842803955, -0.7770057320594788]\n",
      "Grand sum of 1822 tensor sets is: [705.9774169921875, 2967.39306640625, -558.4940185546875, -826.6778564453125, -875.644775390625]\n",
      "\n",
      "Instance 2403 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "car at index 49: [0.6259723901748657, -0.9957343339920044, -1.0153285264968872, 0.2449081838130951, 3.0523412227630615]\n",
      "Grand sum of 1823 tensor sets is: [706.6033935546875, 2966.397216796875, -559.5093383789062, -826.4329223632812, -872.5924072265625]\n",
      "\n",
      "Instance 2404 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "car at index 14: [0.8284928798675537, 2.113314628601074, -0.1419190764427185, -0.00046078115701675415, -2.0489940643310547]\n",
      "Grand sum of 1824 tensor sets is: [707.431884765625, 2968.510498046875, -559.6512451171875, -826.4334106445312, -874.6414184570312]\n",
      "\n",
      "Instance 2405 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 3: [0.43303483724594116, 2.4837450981140137, -0.3014421761035919, -3.262566089630127, -1.3925317525863647]\n",
      "Grand sum of 1825 tensor sets is: [707.8649291992188, 2970.994140625, -559.9526977539062, -829.6959838867188, -876.033935546875]\n",
      "\n",
      "Instance 2406 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 31: [-0.5433061122894287, 1.644169807434082, 0.4670715034008026, -0.6613966822624207, -1.2630610466003418]\n",
      "Grand sum of 1826 tensor sets is: [707.3215942382812, 2972.638427734375, -559.4856567382812, -830.3573608398438, -877.2969970703125]\n",
      "\n",
      "Instance 2407 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "car at index 25: [1.8526489734649658, -1.3631353378295898, -0.16820955276489258, 1.7766793966293335, 1.1539593935012817]\n",
      "Grand sum of 1827 tensor sets is: [709.1742553710938, 2971.275390625, -559.6538696289062, -828.5806884765625, -876.14306640625]\n",
      "\n",
      "Instance 2408 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 2: [-0.3365727961063385, 2.3667664527893066, -0.625179648399353, -0.7956634759902954, -1.1283671855926514]\n",
      "Grand sum of 1828 tensor sets is: [708.8377075195312, 2973.64208984375, -560.279052734375, -829.3763427734375, -877.2714233398438]\n",
      "\n",
      "Instance 2409 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 16: [0.7732008695602417, 2.703334331512451, -0.3984220623970032, -0.7829582095146179, -0.07986347377300262]\n",
      "Grand sum of 1829 tensor sets is: [709.6109008789062, 2976.345458984375, -560.677490234375, -830.1593017578125, -877.3512573242188]\n",
      "\n",
      "Instance 2410 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 11: [0.6270328164100647, 2.3406119346618652, 0.5006227493286133, 3.5146937370300293, -2.7405965328216553]\n",
      "Grand sum of 1830 tensor sets is: [710.2379150390625, 2978.68603515625, -560.1768798828125, -826.6445922851562, -880.0918579101562]\n",
      "\n",
      "Instance 2411 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 17: [1.996400237083435, -2.1735637187957764, -0.025077059864997864, 1.7508779764175415, 0.5411455035209656]\n",
      "Grand sum of 1831 tensor sets is: [712.2343139648438, 2976.512451171875, -560.2019653320312, -824.8937377929688, -879.5507202148438]\n",
      "\n",
      "Instance 2412 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 8: [0.612941563129425, 3.3904051780700684, 0.45599278807640076, -1.426620364189148, -1.669670820236206]\n",
      "Grand sum of 1832 tensor sets is: [712.8472290039062, 2979.90283203125, -559.7459716796875, -826.3203735351562, -881.2203979492188]\n",
      "\n",
      "Instance 2413 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 26: [0.764913022518158, 0.9205777645111084, -0.8892605304718018, -1.0814692974090576, -1.7824480533599854]\n",
      "Grand sum of 1833 tensor sets is: [713.6121215820312, 2980.823486328125, -560.63525390625, -827.40185546875, -883.0028686523438]\n",
      "\n",
      "Instance 2414 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 24: [0.42428287863731384, -0.8639863133430481, -0.6704849004745483, 2.3916995525360107, 2.932338237762451]\n",
      "Grand sum of 1834 tensor sets is: [714.036376953125, 2979.95947265625, -561.3057250976562, -825.0101318359375, -880.070556640625]\n",
      "\n",
      "Instance 2415 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "car at index 13: [0.12075857818126678, 1.0641199350357056, 0.29697558283805847, -1.057706356048584, -0.3110201358795166]\n",
      "Grand sum of 1835 tensor sets is: [714.1571655273438, 2981.023681640625, -561.0087280273438, -826.0678100585938, -880.381591796875]\n",
      "\n",
      "Instance 2416 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 24: [0.036568284034729004, 2.287203073501587, -0.6893144845962524, -2.160316228866577, -3.5812735557556152]\n",
      "Grand sum of 1836 tensor sets is: [714.1937255859375, 2983.310791015625, -561.6980590820312, -828.2281494140625, -883.962890625]\n",
      "\n",
      "Instance 2417 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 9: [0.40144112706184387, -0.5721169710159302, 0.7542988061904907, 1.9011942148208618, -1.373953104019165]\n",
      "Grand sum of 1837 tensor sets is: [714.5951538085938, 2982.73876953125, -560.9437866210938, -826.3269653320312, -885.3368530273438]\n",
      "\n",
      "Instance 2418 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([82, 13, 768])\n",
      "Shape of summed layers is: 82 x 768\n",
      "car at index 20: [0.31987959146499634, 1.011634349822998, -0.05158761143684387, 2.1890275478363037, 4.614805698394775]\n",
      "Grand sum of 1838 tensor sets is: [714.9150390625, 2983.75048828125, -560.995361328125, -824.137939453125, -880.7220458984375]\n",
      "\n",
      "Instance 2419 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [240]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 240: [-0.18234819173812866, 2.1261186599731445, 0.7883957624435425, 0.06624143570661545, -1.1618021726608276]\n",
      "Grand sum of 1839 tensor sets is: [714.732666015625, 2985.876708984375, -560.2069702148438, -824.0717163085938, -881.8838500976562]\n",
      "\n",
      "Instance 2420 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 6: [-0.6402226090431213, 2.2384045124053955, 0.594333827495575, -0.6046203374862671, -2.995469093322754]\n",
      "Grand sum of 1840 tensor sets is: [714.0924682617188, 2988.115234375, -559.6126098632812, -824.6763305664062, -884.8793334960938]\n",
      "\n",
      "Instance 2421 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "car at index 15: [-0.20335985720157623, 1.977944016456604, -0.3498154580593109, 1.0655114650726318, -0.9676290154457092]\n",
      "Grand sum of 1841 tensor sets is: [713.8890991210938, 2990.09326171875, -559.96240234375, -823.61083984375, -885.8469848632812]\n",
      "\n",
      "Instance 2422 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10, 15]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 10: [0.615373432636261, 2.142672300338745, -0.13029704988002777, 0.6861779689788818, 2.717528820037842]\n",
      "car at index 15: [0.1834639012813568, 2.4854907989501953, 0.3523561358451843, 0.2071431577205658, -0.1740652322769165]\n",
      "Grand sum of 1842 tensor sets is: [714.2885131835938, 2992.4072265625, -559.8513793945312, -823.1641845703125, -884.5752563476562]\n",
      "\n",
      "Instance 2423 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [34, 43]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "car at index 34: [-0.4122750759124756, 1.940110206604004, -1.2832002639770508, -2.0969765186309814, -0.9647737741470337]\n",
      "car at index 43: [0.15115578472614288, 0.6773935556411743, -0.3788818120956421, -2.828160047531128, -0.9242313504219055]\n",
      "Grand sum of 1843 tensor sets is: [714.157958984375, 2993.716064453125, -560.6824340820312, -825.6267700195312, -885.519775390625]\n",
      "\n",
      "Instance 2424 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 4: [-0.09359483420848846, 2.1450788974761963, -0.10227885097265244, -1.1869537830352783, -1.8264122009277344]\n",
      "Grand sum of 1844 tensor sets is: [714.0643920898438, 2995.861083984375, -560.7847290039062, -826.813720703125, -887.34619140625]\n",
      "\n",
      "Instance 2425 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2426 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2427 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 2: [0.8118786215782166, 2.34069561958313, -0.49028995633125305, -1.4921760559082031, -0.4224174916744232]\n",
      "Grand sum of 1845 tensor sets is: [714.8762817382812, 2998.20166015625, -561.2750244140625, -828.305908203125, -887.7686157226562]\n",
      "\n",
      "Instance 2428 of car.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2429 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "car at index 19: [0.16362281143665314, 1.254760980606079, 1.0245654582977295, 2.5590603351593018, -2.516730308532715]\n",
      "Grand sum of 1846 tensor sets is: [715.0399169921875, 2999.45654296875, -560.25048828125, -825.746826171875, -890.2853393554688]\n",
      "\n",
      "Instance 2430 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 28: [0.6445776224136353, 1.1245591640472412, -0.7465119957923889, -0.5999858975410461, 0.09959502518177032]\n",
      "Grand sum of 1847 tensor sets is: [715.6845092773438, 3000.5810546875, -560.9970092773438, -826.3468017578125, -890.1857299804688]\n",
      "\n",
      "Instance 2431 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 11: [0.2875162363052368, 1.084078073501587, 0.10417258739471436, 3.760402202606201, -1.2221513986587524]\n",
      "Grand sum of 1848 tensor sets is: [715.9720458984375, 3001.6650390625, -560.892822265625, -822.58642578125, -891.4078979492188]\n",
      "\n",
      "Instance 2432 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 35: [1.4206442832946777, 2.707764148712158, 0.151507169008255, 0.24623097479343414, -1.2923636436462402]\n",
      "Grand sum of 1849 tensor sets is: [717.3927001953125, 3004.372802734375, -560.7413330078125, -822.3402099609375, -892.7002563476562]\n",
      "\n",
      "Instance 2433 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 24]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 11: [0.9831068515777588, 2.1229209899902344, -0.12655922770500183, -1.7210257053375244, -0.16199570894241333]\n",
      "car at index 24: [0.4484776258468628, 1.8601899147033691, -0.2612365782260895, -2.501539468765259, -1.0258965492248535]\n",
      "Grand sum of 1850 tensor sets is: [718.1085205078125, 3006.3642578125, -560.9352416992188, -824.4514770507812, -893.294189453125]\n",
      "\n",
      "Instance 2434 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2435 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2436 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "car at index 2: [0.5976176261901855, 1.2365199327468872, -1.1965011358261108, -0.047534335404634476, -2.352142095565796]\n",
      "Grand sum of 1851 tensor sets is: [718.7061157226562, 3007.600830078125, -562.1317138671875, -824.4990234375, -895.6463012695312]\n",
      "\n",
      "Instance 2437 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2438 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2439 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 7: [-0.2250100076198578, 2.0379960536956787, 1.4436697959899902, -1.1912087202072144, -1.0549073219299316]\n",
      "Grand sum of 1852 tensor sets is: [718.4810791015625, 3009.638916015625, -560.6880493164062, -825.6902465820312, -896.7012329101562]\n",
      "\n",
      "Instance 2440 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 10: [0.14267317950725555, 0.14906857907772064, 1.0594943761825562, 0.3084428012371063, -0.5595228672027588]\n",
      "Grand sum of 1853 tensor sets is: [718.623779296875, 3009.7880859375, -559.6285400390625, -825.3817749023438, -897.2607421875]\n",
      "\n",
      "Instance 2441 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2442 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2443 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 7: [0.4027699828147888, 1.7257671356201172, -0.20478227734565735, -2.113694429397583, -1.3312857151031494]\n",
      "Grand sum of 1854 tensor sets is: [719.0265502929688, 3011.513916015625, -559.8333129882812, -827.4954833984375, -898.592041015625]\n",
      "\n",
      "Instance 2444 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 8: [0.18184779584407806, 2.798438549041748, -0.033011943101882935, -1.9482213258743286, -1.8120537996292114]\n",
      "Grand sum of 1855 tensor sets is: [719.2083740234375, 3014.312255859375, -559.8663330078125, -829.4437255859375, -900.4041137695312]\n",
      "\n",
      "Instance 2445 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 48: [0.16444426774978638, 1.1873949766159058, -0.44895368814468384, 0.29476943612098694, -1.306168556213379]\n",
      "Grand sum of 1856 tensor sets is: [719.372802734375, 3015.499755859375, -560.3153076171875, -829.14892578125, -901.7102661132812]\n",
      "\n",
      "Instance 2446 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 21: [1.3167939186096191, 2.3306386470794678, 0.005043063312768936, -1.4154274463653564, -1.9337667226791382]\n",
      "Grand sum of 1857 tensor sets is: [720.6895751953125, 3017.830322265625, -560.3102416992188, -830.5643310546875, -903.64404296875]\n",
      "\n",
      "Instance 2447 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 16: [0.0805913433432579, -0.1326293647289276, -1.1913970708847046, -1.366640567779541, -0.44639042019844055]\n",
      "Grand sum of 1858 tensor sets is: [720.7701416015625, 3017.69775390625, -561.5016479492188, -831.9309692382812, -904.0904541015625]\n",
      "\n",
      "Instance 2448 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2449 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [51]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "car at index 51: [0.7755938768386841, -0.6266902685165405, -1.5038468837738037, 1.313691258430481, 3.050722599029541]\n",
      "Grand sum of 1859 tensor sets is: [721.5457153320312, 3017.071044921875, -563.0054931640625, -830.6172485351562, -901.0397338867188]\n",
      "\n",
      "Instance 2450 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 11: [1.2114702463150024, 1.5984861850738525, 0.12574005126953125, 0.9668138027191162, -2.507925510406494]\n",
      "Grand sum of 1860 tensor sets is: [722.7572021484375, 3018.66943359375, -562.8797607421875, -829.6504516601562, -903.5476684570312]\n",
      "\n",
      "Instance 2451 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 17: [0.614067018032074, 1.9808006286621094, 0.619557797908783, -2.228302478790283, -0.9649387001991272]\n",
      "Grand sum of 1861 tensor sets is: [723.3712768554688, 3020.650146484375, -562.2601928710938, -831.8787841796875, -904.5126342773438]\n",
      "\n",
      "Instance 2452 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 15: [0.24503222107887268, 1.673292875289917, -0.29797422885894775, -1.5155220031738281, -1.5799143314361572]\n",
      "Grand sum of 1862 tensor sets is: [723.6163330078125, 3022.323486328125, -562.5581665039062, -833.394287109375, -906.092529296875]\n",
      "\n",
      "Instance 2453 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2454 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2455 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2456 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "car at index 7: [0.6156613230705261, 1.9543644189834595, 0.509894847869873, -1.0834985971450806, -3.454981803894043]\n",
      "Grand sum of 1863 tensor sets is: [724.2319946289062, 3024.27783203125, -562.0482788085938, -834.477783203125, -909.5474853515625]\n",
      "\n",
      "Instance 2457 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2458 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [1]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 1: [0.8671425580978394, -1.1646419763565063, -0.7251536250114441, 2.386249303817749, 3.065497875213623]\n",
      "Grand sum of 1864 tensor sets is: [725.09912109375, 3023.11328125, -562.7734375, -832.091552734375, -906.4819946289062]\n",
      "\n",
      "Instance 2459 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2460 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 11: [-0.5712490081787109, 1.7486186027526855, -1.3120245933532715, -2.099252700805664, -0.9436187744140625]\n",
      "car at index 15: [-1.1396976709365845, 2.8101768493652344, 0.44345545768737793, -2.2842137813568115, -3.984623908996582]\n",
      "Grand sum of 1865 tensor sets is: [724.24365234375, 3025.392578125, -563.2077026367188, -834.2832641601562, -908.9461059570312]\n",
      "\n",
      "Instance 2461 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2462 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 29: [1.490838646888733, 2.0305562019348145, -0.9401968121528625, 0.6321548223495483, -1.7064261436462402]\n",
      "Grand sum of 1866 tensor sets is: [725.7344970703125, 3027.423095703125, -564.1478881835938, -833.651123046875, -910.6525268554688]\n",
      "\n",
      "Instance 2463 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 23: [0.5897663235664368, 3.740399122238159, -0.2993948757648468, -1.4972736835479736, 1.0816001892089844]\n",
      "Grand sum of 1867 tensor sets is: [726.3242797851562, 3031.16357421875, -564.447265625, -835.1483764648438, -909.5709228515625]\n",
      "\n",
      "Instance 2464 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "car at index 28: [0.5832555294036865, 1.071787714958191, -0.9669443368911743, 1.5202045440673828, 3.885256290435791]\n",
      "Grand sum of 1868 tensor sets is: [726.9075317382812, 3032.2353515625, -565.4141845703125, -833.628173828125, -905.6856689453125]\n",
      "\n",
      "Instance 2465 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([83, 13, 768])\n",
      "Shape of summed layers is: 83 x 768\n",
      "car at index 49: [1.7711825370788574, -2.545077085494995, -0.7973734140396118, 0.7165083289146423, 0.7218150496482849]\n",
      "Grand sum of 1869 tensor sets is: [728.6787109375, 3029.690185546875, -566.2115478515625, -832.9116821289062, -904.9638671875]\n",
      "\n",
      "Instance 2466 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 20: [0.7717598676681519, 0.5936710834503174, 0.27053552865982056, -0.6518146395683289, -0.4464893341064453]\n",
      "Grand sum of 1870 tensor sets is: [729.4505004882812, 3030.283935546875, -565.9410400390625, -833.5634765625, -905.4103393554688]\n",
      "\n",
      "Instance 2467 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "car at index 3: [0.2293650358915329, 1.4140052795410156, -1.2204581499099731, -0.2403094619512558, 0.10940234363079071]\n",
      "Grand sum of 1871 tensor sets is: [729.6798706054688, 3031.697998046875, -567.1614990234375, -833.8037719726562, -905.3009643554688]\n",
      "\n",
      "Instance 2468 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 5: [-0.2759811282157898, 1.3001857995986938, 0.30629974603652954, -0.4316667914390564, -2.2945499420166016]\n",
      "Grand sum of 1872 tensor sets is: [729.4038696289062, 3032.998291015625, -566.855224609375, -834.2354125976562, -907.5955200195312]\n",
      "\n",
      "Instance 2469 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2470 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [11, 15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 11: [-0.5712490081787109, 1.7486186027526855, -1.3120245933532715, -2.099252700805664, -0.9436187744140625]\n",
      "car at index 15: [-1.1396976709365845, 2.8101768493652344, 0.44345545768737793, -2.2842137813568115, -3.984623908996582]\n",
      "Grand sum of 1873 tensor sets is: [728.5484008789062, 3035.277587890625, -567.2894897460938, -836.4271240234375, -910.0596313476562]\n",
      "\n",
      "Instance 2471 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 3: [0.9158575534820557, 1.563395380973816, -0.6403849720954895, -1.5526654720306396, -2.5001420974731445]\n",
      "Grand sum of 1874 tensor sets is: [729.4642333984375, 3036.841064453125, -567.9298706054688, -837.9797973632812, -912.5597534179688]\n",
      "\n",
      "Instance 2472 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 12: [0.7015044093132019, 2.0086798667907715, -0.5281325578689575, -0.1497894674539566, 0.7107287645339966]\n",
      "Grand sum of 1875 tensor sets is: [730.1657104492188, 3038.849853515625, -568.4580078125, -838.1295776367188, -911.8489990234375]\n",
      "\n",
      "Instance 2473 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([102, 13, 768])\n",
      "Shape of summed layers is: 102 x 768\n",
      "car at index 42: [0.9500076770782471, -0.8878223896026611, 0.23968060314655304, -0.5379461646080017, 1.2866489887237549]\n",
      "Grand sum of 1876 tensor sets is: [731.11572265625, 3037.9619140625, -568.2183227539062, -838.6675415039062, -910.5623779296875]\n",
      "\n",
      "Instance 2474 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "car at index 22: [1.2756723165512085, 1.9942306280136108, -1.553816795349121, 1.372470498085022, -1.8245465755462646]\n",
      "Grand sum of 1877 tensor sets is: [732.3914184570312, 3039.9560546875, -569.7721557617188, -837.2950439453125, -912.3869018554688]\n",
      "\n",
      "Instance 2475 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "car at index 6: [0.7520670890808105, 2.220250129699707, -0.008669231086969376, -2.861156463623047, -1.5199791193008423]\n",
      "Grand sum of 1878 tensor sets is: [733.1434936523438, 3042.17626953125, -569.7808227539062, -840.1561889648438, -913.9068603515625]\n",
      "\n",
      "Instance 2476 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2477 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "car at index 10: [1.0976214408874512, 1.5420140027999878, -0.8735131621360779, -0.37828975915908813, 3.3673408031463623]\n",
      "Grand sum of 1879 tensor sets is: [734.2410888671875, 3043.71826171875, -570.6543579101562, -840.5344848632812, -910.5394897460938]\n",
      "\n",
      "Instance 2478 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 2: [-0.33742931485176086, 2.927654504776001, 0.7681776881217957, -1.644540786743164, -1.1348381042480469]\n",
      "Grand sum of 1880 tensor sets is: [733.9036865234375, 3046.64599609375, -569.8861694335938, -842.1790161132812, -911.67431640625]\n",
      "\n",
      "Instance 2479 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2480 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2481 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "car at index 5: [-0.576494038105011, 1.9307491779327393, -0.8206588625907898, 1.7863829135894775, -2.755143880844116]\n",
      "Grand sum of 1881 tensor sets is: [733.3272094726562, 3048.57666015625, -570.7068481445312, -840.3926391601562, -914.429443359375]\n",
      "\n",
      "Instance 2482 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2483 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 2: [-0.221831813454628, 1.510014295578003, -0.0016994364559650421, -1.4527496099472046, -0.7926939725875854]\n",
      "Grand sum of 1882 tensor sets is: [733.1054077148438, 3050.086669921875, -570.7085571289062, -841.8453979492188, -915.2221069335938]\n",
      "\n",
      "Instance 2484 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2485 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 18: [1.150897741317749, 2.565431833267212, -0.6291803121566772, -0.5159868001937866, -2.6767072677612305]\n",
      "Grand sum of 1883 tensor sets is: [734.2562866210938, 3052.652099609375, -571.3377075195312, -842.3613891601562, -917.8988037109375]\n",
      "\n",
      "Instance 2486 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 4: [1.2128419876098633, 1.0713248252868652, 1.1952840089797974, -1.7912298440933228, 0.07648617029190063]\n",
      "Grand sum of 1884 tensor sets is: [735.4691162109375, 3053.723388671875, -570.1423950195312, -844.1526489257812, -917.8223266601562]\n",
      "\n",
      "Instance 2487 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([82, 13, 768])\n",
      "Shape of summed layers is: 82 x 768\n",
      "car at index 48: [1.8160284757614136, -2.4207260608673096, -0.6987699270248413, 0.8241002559661865, 0.5926706194877625]\n",
      "Grand sum of 1885 tensor sets is: [737.28515625, 3051.302734375, -570.8411865234375, -843.3285522460938, -917.2296752929688]\n",
      "\n",
      "Instance 2488 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2489 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2490 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2491 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 10: [-0.08801424503326416, 1.8941677808761597, -0.5958227515220642, -0.20391859114170074, -2.74419903755188]\n",
      "Grand sum of 1886 tensor sets is: [737.1971435546875, 3053.197021484375, -571.43701171875, -843.532470703125, -919.973876953125]\n",
      "\n",
      "Instance 2492 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "car at index 47: [-0.011188775300979614, 1.9506587982177734, 0.3646344840526581, 3.126593828201294, -2.9270224571228027]\n",
      "Grand sum of 1887 tensor sets is: [737.1859741210938, 3055.147705078125, -571.0723876953125, -840.4058837890625, -922.90087890625]\n",
      "\n",
      "Instance 2493 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [71]\n",
      "Size of token embeddings is torch.Size([90, 13, 768])\n",
      "Shape of summed layers is: 90 x 768\n",
      "car at index 71: [0.10996809601783752, 1.3426786661148071, -1.1011806726455688, -0.28668421506881714, -1.699690580368042]\n",
      "Grand sum of 1888 tensor sets is: [737.2959594726562, 3056.490478515625, -572.173583984375, -840.6925659179688, -924.6005859375]\n",
      "\n",
      "Instance 2494 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 2: [0.5054959654808044, 1.3924981355667114, 0.44301319122314453, -1.3699108362197876, 0.6656923890113831]\n",
      "Grand sum of 1889 tensor sets is: [737.8014526367188, 3057.883056640625, -571.7305908203125, -842.0625, -923.9348754882812]\n",
      "\n",
      "Instance 2495 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2496 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2497 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 16: [0.7952682971954346, 2.6705873012542725, -1.1045573949813843, -0.3418191969394684, 1.0224634408950806]\n",
      "Grand sum of 1890 tensor sets is: [738.5967407226562, 3060.5537109375, -572.8351440429688, -842.404296875, -922.9124145507812]\n",
      "\n",
      "Instance 2498 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 2: [0.7427071332931519, 2.3434245586395264, -1.2725919485092163, -1.2318024635314941, 0.7777884006500244]\n",
      "Grand sum of 1891 tensor sets is: [739.3394775390625, 3062.897216796875, -574.1077270507812, -843.6361083984375, -922.1346435546875]\n",
      "\n",
      "Instance 2499 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 2: [-0.05896657705307007, 0.8682588338851929, 0.41742178797721863, -3.5074093341827393, 1.6618828773498535]\n",
      "Grand sum of 1892 tensor sets is: [739.280517578125, 3063.765380859375, -573.6903076171875, -847.1434936523438, -920.4727783203125]\n",
      "\n",
      "Instance 2500 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 21: [0.974717378616333, 1.559771180152893, -0.6800407767295837, -0.7756120562553406, 0.9119793176651001]\n",
      "Grand sum of 1893 tensor sets is: [740.2552490234375, 3065.3251953125, -574.370361328125, -847.9191284179688, -919.560791015625]\n",
      "\n",
      "Instance 2501 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 18: [0.23674903810024261, 1.0561928749084473, 0.5552283525466919, -2.2397842407226562, -2.3355679512023926]\n",
      "Grand sum of 1894 tensor sets is: [740.4920043945312, 3066.38134765625, -573.8151245117188, -850.158935546875, -921.8963623046875]\n",
      "\n",
      "Instance 2502 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 3: [0.6959238052368164, 0.5751667618751526, -0.6663695573806763, 1.727015495300293, -1.8766919374465942]\n",
      "Grand sum of 1895 tensor sets is: [741.1879272460938, 3066.95654296875, -574.4815063476562, -848.4319458007812, -923.7730712890625]\n",
      "\n",
      "Instance 2503 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 8: [-0.5225724577903748, 2.6695492267608643, -1.230393886566162, -0.205577552318573, -2.162355422973633]\n",
      "Grand sum of 1896 tensor sets is: [740.6653442382812, 3069.6259765625, -575.7119140625, -848.6375122070312, -925.9354248046875]\n",
      "\n",
      "Instance 2504 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 23]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 7: [0.010037526488304138, 1.6735206842422485, -0.5474114418029785, -1.8629647493362427, -1.183915615081787]\n",
      "car at index 23: [-0.20006895065307617, 2.092007637023926, -0.16295716166496277, -3.2147724628448486, -0.8882225155830383]\n",
      "Grand sum of 1897 tensor sets is: [740.5703125, 3071.5087890625, -576.0670776367188, -851.1763916015625, -926.9714965820312]\n",
      "\n",
      "Instance 2505 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [59]\n",
      "Size of token embeddings is torch.Size([374, 13, 768])\n",
      "Shape of summed layers is: 374 x 768\n",
      "car at index 59: [0.10188916325569153, 0.41500893235206604, 0.27052152156829834, 0.32830119132995605, 2.343092203140259]\n",
      "Grand sum of 1898 tensor sets is: [740.6721801757812, 3071.923828125, -575.7965698242188, -850.8480834960938, -924.62841796875]\n",
      "\n",
      "Instance 2506 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 6: [-0.2579590082168579, 3.894864320755005, -0.11252948641777039, 1.0728750228881836, -1.3012070655822754]\n",
      "Grand sum of 1899 tensor sets is: [740.4142456054688, 3075.818603515625, -575.9091186523438, -849.7752075195312, -925.9296264648438]\n",
      "\n",
      "Instance 2507 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17, 30]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 17: [-0.4829269349575043, 1.7503641843795776, -0.7693643569946289, 0.4232932925224304, -0.9344679117202759]\n",
      "car at index 30: [0.28556886315345764, -0.7575011253356934, -0.2903818190097809, 1.429121732711792, -1.5968503952026367]\n",
      "Grand sum of 1900 tensor sets is: [740.3155517578125, 3076.31494140625, -576.43896484375, -848.8489990234375, -927.1953125]\n",
      "\n",
      "Instance 2508 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 18: [0.7010448575019836, 2.0193464756011963, 0.19218945503234863, -1.043518304824829, 0.018986642360687256]\n",
      "Grand sum of 1901 tensor sets is: [741.0166015625, 3078.334228515625, -576.2467651367188, -849.8925170898438, -927.1763305664062]\n",
      "\n",
      "Instance 2509 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2510 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 7: [0.25750651955604553, 2.464240789413452, -0.3803221881389618, -2.11808443069458, -0.8835235238075256]\n",
      "Grand sum of 1902 tensor sets is: [741.2741088867188, 3080.798583984375, -576.6270751953125, -852.0106201171875, -928.0598754882812]\n",
      "\n",
      "Instance 2511 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17, 53]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "car at index 17: [1.3643133640289307, 0.8318836092948914, -0.7413824796676636, -1.4116852283477783, 0.36352893710136414]\n",
      "car at index 53: [0.9167885780334473, 2.347566604614258, -0.9665735363960266, -2.74894380569458, -0.13537660241127014]\n",
      "Grand sum of 1903 tensor sets is: [742.4146728515625, 3082.388427734375, -577.4810791015625, -854.0909423828125, -927.94580078125]\n",
      "\n",
      "Instance 2512 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2513 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2, 10, 23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 2: [0.5495949983596802, 1.7639418840408325, -0.8952964544296265, -0.8613556623458862, 1.958221673965454]\n",
      "car at index 10: [0.19300565123558044, 2.4209089279174805, -0.11946100741624832, -0.3386797606945038, -0.036009281873703]\n",
      "car at index 23: [0.4885624051094055, 2.804844379425049, -0.1133693978190422, -1.2443581819534302, -0.10606468468904495]\n",
      "Grand sum of 1904 tensor sets is: [742.8250732421875, 3084.71826171875, -577.8571166992188, -854.90576171875, -927.3403930664062]\n",
      "\n",
      "Instance 2514 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "car at index 3: [0.5166381597518921, 1.4211171865463257, -0.6619726419448853, -2.6284594535827637, 0.062359198927879333]\n",
      "Grand sum of 1905 tensor sets is: [743.3417358398438, 3086.139404296875, -578.5191040039062, -857.5342407226562, -927.2780151367188]\n",
      "\n",
      "Instance 2515 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([70, 13, 768])\n",
      "Shape of summed layers is: 70 x 768\n",
      "car at index 45: [0.8762848973274231, 0.31650257110595703, 0.14932170510292053, -1.661755919456482, 1.775117039680481]\n",
      "Grand sum of 1906 tensor sets is: [744.218017578125, 3086.455810546875, -578.3698120117188, -859.1959838867188, -925.5028686523438]\n",
      "\n",
      "Instance 2516 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 1907 tensor sets is: [746.335693359375, 3088.132080078125, -579.6040649414062, -860.8615112304688, -925.6473999023438]\n",
      "\n",
      "Instance 2517 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2518 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2519 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [35, 71]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "car at index 35: [0.758116602897644, 1.17562997341156, 0.6782047152519226, 0.5341227650642395, -2.5815422534942627]\n",
      "car at index 71: [0.00396757572889328, 1.866884708404541, -0.46190696954727173, 0.5921401977539062, -1.2595059871673584]\n",
      "Grand sum of 1908 tensor sets is: [746.7167358398438, 3089.6533203125, -579.4959106445312, -860.2984008789062, -927.5679321289062]\n",
      "\n",
      "Instance 2520 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 22: [0.4052867293357849, 0.9926459193229675, 0.1336062252521515, -0.692996621131897, 1.3051536083221436]\n",
      "Grand sum of 1909 tensor sets is: [747.1220092773438, 3090.64599609375, -579.3623046875, -860.9913940429688, -926.2627563476562]\n",
      "\n",
      "Instance 2521 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 29: [0.13024167716503143, 0.8577843308448792, -1.2101656198501587, -2.7103006839752197, 0.9277130365371704]\n",
      "Grand sum of 1910 tensor sets is: [747.2522583007812, 3091.503662109375, -580.5724487304688, -863.7017211914062, -925.3350219726562]\n",
      "\n",
      "Instance 2522 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 8: [0.4028330445289612, 0.31564339995384216, 0.0978115126490593, -0.7080340385437012, -2.2479190826416016]\n",
      "Grand sum of 1911 tensor sets is: [747.6550903320312, 3091.8193359375, -580.474609375, -864.4097290039062, -927.5829467773438]\n",
      "\n",
      "Instance 2523 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 9: [-0.9671124219894409, 3.988773822784424, 1.0595040321350098, 0.2713136672973633, -3.582958698272705]\n",
      "Grand sum of 1912 tensor sets is: [746.68798828125, 3095.80810546875, -579.4151000976562, -864.138427734375, -931.1658935546875]\n",
      "\n",
      "Instance 2524 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 12: [0.5596217513084412, 1.5451860427856445, -0.48296940326690674, -1.7190231084823608, -0.8779329061508179]\n",
      "Grand sum of 1913 tensor sets is: [747.2476196289062, 3097.353271484375, -579.8980712890625, -865.857421875, -932.0438232421875]\n",
      "\n",
      "Instance 2525 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 8: [-0.28810206055641174, 2.3759982585906982, -1.22421133518219, -0.7101373076438904, -2.972888946533203]\n",
      "Grand sum of 1914 tensor sets is: [746.9595336914062, 3099.729248046875, -581.1222534179688, -866.5675659179688, -935.0167236328125]\n",
      "\n",
      "Instance 2526 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2527 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9, 25]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 9: [0.5356975197792053, 3.2432773113250732, -1.9983196258544922, -1.717585563659668, 2.062586784362793]\n",
      "car at index 25: [0.05240872502326965, 1.9680622816085815, 0.1500556319952011, -2.295441150665283, 0.07674393057823181]\n",
      "Grand sum of 1915 tensor sets is: [747.2536010742188, 3102.3349609375, -582.04638671875, -868.5740966796875, -933.9470825195312]\n",
      "\n",
      "Instance 2528 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "car at index 31: [-0.27489256858825684, 2.3668675422668457, 0.01656915247440338, -1.4739222526550293, -0.32217761874198914]\n",
      "Grand sum of 1916 tensor sets is: [746.9786987304688, 3104.701904296875, -582.0298461914062, -870.0480346679688, -934.269287109375]\n",
      "\n",
      "Instance 2529 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 18]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 7: [1.283988356590271, 2.030327081680298, -0.09305950254201889, -1.6225478649139404, 0.4312518835067749]\n",
      "car at index 18: [1.7231111526489258, 1.7130517959594727, 0.6781378984451294, -1.6248445510864258, -0.6493427157402039]\n",
      "Grand sum of 1917 tensor sets is: [748.4822387695312, 3106.573486328125, -581.7373046875, -871.6717529296875, -934.3783569335938]\n",
      "\n",
      "Instance 2530 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 6: [0.708273708820343, -0.4766209125518799, 0.485393226146698, 1.5908256769180298, -2.6490068435668945]\n",
      "Grand sum of 1918 tensor sets is: [749.1904907226562, 3106.096923828125, -581.2518920898438, -870.0809326171875, -937.02734375]\n",
      "\n",
      "Instance 2531 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [44, 47]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "car at index 44: [0.03516785055398941, 0.5852158665657043, 0.7428256869316101, -1.9857233762741089, -1.9286967515945435]\n",
      "car at index 47: [0.2629294991493225, 3.15366792678833, 0.129023015499115, -0.7188623547554016, -1.720538854598999]\n",
      "Grand sum of 1919 tensor sets is: [749.3395385742188, 3107.96630859375, -580.8159790039062, -871.4332275390625, -938.8519897460938]\n",
      "\n",
      "Instance 2532 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "car at index 25: [1.2072277069091797, -0.6662072539329529, -0.5755239725112915, 2.5497853755950928, 3.355332851409912]\n",
      "Grand sum of 1920 tensor sets is: [750.5467529296875, 3107.300048828125, -581.3914794921875, -868.8834228515625, -935.4966430664062]\n",
      "\n",
      "Instance 2533 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2534 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 12: [0.11289110779762268, 4.112489700317383, 0.4076691269874573, 1.133124828338623, 1.2253369092941284]\n",
      "Grand sum of 1921 tensor sets is: [750.65966796875, 3111.41259765625, -580.9838256835938, -867.7503051757812, -934.2713012695312]\n",
      "\n",
      "Instance 2535 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2536 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 27: [0.4256659150123596, 1.485147476196289, -0.7679334878921509, -1.0181702375411987, -1.5422946214675903]\n",
      "Grand sum of 1922 tensor sets is: [751.0853271484375, 3112.897705078125, -581.7517700195312, -868.7684936523438, -935.8135986328125]\n",
      "\n",
      "Instance 2537 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "car at index 21: [0.05010836571455002, 1.5902729034423828, -0.8700929880142212, 0.9505292177200317, -1.568291187286377]\n",
      "Grand sum of 1923 tensor sets is: [751.1354370117188, 3114.488037109375, -582.6218872070312, -867.8179931640625, -937.3818969726562]\n",
      "\n",
      "Instance 2538 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13, 39]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "car at index 13: [-0.38895246386528015, 3.583082437515259, -1.2876977920532227, -1.197303295135498, -0.6692112684249878]\n",
      "car at index 39: [0.15252868831157684, 2.236306667327881, -0.9158656597137451, 0.17239636182785034, 0.6237689256668091]\n",
      "Grand sum of 1924 tensor sets is: [751.0172119140625, 3117.397705078125, -583.7236938476562, -868.3304443359375, -937.4046020507812]\n",
      "\n",
      "Instance 2539 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 3: [0.9088625907897949, 2.8025805950164795, -0.1444293111562729, -2.4687530994415283, -2.1376547813415527]\n",
      "Grand sum of 1925 tensor sets is: [751.9260864257812, 3120.2001953125, -583.8681030273438, -870.7991943359375, -939.542236328125]\n",
      "\n",
      "Instance 2540 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 12: [0.13728484511375427, 1.6247104406356812, -0.6020013689994812, -1.6625518798828125, -1.1927669048309326]\n",
      "Grand sum of 1926 tensor sets is: [752.0633544921875, 3121.824951171875, -584.4700927734375, -872.4617309570312, -940.7349853515625]\n",
      "\n",
      "Instance 2541 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "car at index 24: [1.058944821357727, 2.3061554431915283, -0.5619376301765442, 0.6721273064613342, -0.3850383758544922]\n",
      "Grand sum of 1927 tensor sets is: [753.122314453125, 3124.131103515625, -585.0320434570312, -871.7896118164062, -941.1199951171875]\n",
      "\n",
      "Instance 2542 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 7: [0.5988345146179199, 2.1621861457824707, -0.7107112407684326, -0.7548313736915588, 1.1531940698623657]\n",
      "Grand sum of 1928 tensor sets is: [753.7211303710938, 3126.293212890625, -585.7427368164062, -872.54443359375, -939.966796875]\n",
      "\n",
      "Instance 2543 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2544 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "car at index 13: [0.0844670832157135, 2.5117380619049072, -0.5272316932678223, -1.5621538162231445, 2.0656754970550537]\n",
      "Grand sum of 1929 tensor sets is: [753.8056030273438, 3128.804931640625, -586.2699584960938, -874.1065673828125, -937.901123046875]\n",
      "\n",
      "Instance 2545 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 10: [0.39172089099884033, 2.8564443588256836, -1.3298581838607788, 1.3430010080337524, -1.2871606349945068]\n",
      "Grand sum of 1930 tensor sets is: [754.1973266601562, 3131.661376953125, -587.5997924804688, -872.7635498046875, -939.1882934570312]\n",
      "\n",
      "Instance 2546 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 9: [1.2913340330123901, 2.4956448078155518, -1.0954426527023315, -2.4530441761016846, 0.007715575397014618]\n",
      "Grand sum of 1931 tensor sets is: [755.4886474609375, 3134.156982421875, -588.6952514648438, -875.2166137695312, -939.1806030273438]\n",
      "\n",
      "Instance 2547 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 33: [0.41272759437561035, 2.0845212936401367, -0.3522883951663971, -1.9013299942016602, 0.023163899779319763]\n",
      "Grand sum of 1932 tensor sets is: [755.9013671875, 3136.241455078125, -589.0475463867188, -877.117919921875, -939.1574096679688]\n",
      "\n",
      "Instance 2548 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 7: [0.9215257167816162, 1.9944432973861694, -0.043922990560531616, -0.02324312925338745, -0.4699312448501587]\n",
      "Grand sum of 1933 tensor sets is: [756.8228759765625, 3138.23583984375, -589.0914916992188, -877.1411743164062, -939.6273193359375]\n",
      "\n",
      "Instance 2549 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 15: [-0.15818758308887482, 1.5617079734802246, -0.8296588659286499, 1.7004311084747314, -3.384532928466797]\n",
      "Grand sum of 1934 tensor sets is: [756.6646728515625, 3139.797607421875, -589.921142578125, -875.4407348632812, -943.0118408203125]\n",
      "\n",
      "Instance 2550 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2551 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2552 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2553 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [211]\n",
      "Size of token embeddings is torch.Size([226, 13, 768])\n",
      "Shape of summed layers is: 226 x 768\n",
      "car at index 211: [0.04293772578239441, -0.35448622703552246, -1.261749267578125, 2.365834951400757, 3.1078808307647705]\n",
      "Grand sum of 1935 tensor sets is: [756.7075805664062, 3139.443115234375, -591.182861328125, -873.0748901367188, -939.9039306640625]\n",
      "\n",
      "Instance 2554 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 18: [0.8985767960548401, 2.6046876907348633, -0.7354323267936707, -0.8719192147254944, -0.5769267082214355]\n",
      "Grand sum of 1936 tensor sets is: [757.6061401367188, 3142.0478515625, -591.9182739257812, -873.9468383789062, -940.4808349609375]\n",
      "\n",
      "Instance 2555 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2556 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "car at index 33: [0.8492509126663208, -0.4098902940750122, -0.9216932058334351, -0.17904984951019287, 5.178677558898926]\n",
      "Grand sum of 1937 tensor sets is: [758.4553833007812, 3141.637939453125, -592.8399658203125, -874.1259155273438, -935.3021850585938]\n",
      "\n",
      "Instance 2557 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2558 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2559 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "car at index 5: [0.5552888512611389, 1.3011001348495483, -0.7137143015861511, -0.5664101243019104, -0.532131552696228]\n",
      "Grand sum of 1938 tensor sets is: [759.0106811523438, 3142.93896484375, -593.5536499023438, -874.6923217773438, -935.8342895507812]\n",
      "\n",
      "Instance 2560 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2561 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7, 29]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "car at index 7: [0.5299776196479797, 1.767735242843628, -0.32878074049949646, -0.4923645257949829, 0.3678557276725769]\n",
      "car at index 29: [0.12461511045694351, 2.44537615776062, -0.058159880340099335, -0.10635434091091156, -0.6030228137969971]\n",
      "Grand sum of 1939 tensor sets is: [759.3379516601562, 3145.04541015625, -593.7471313476562, -874.99169921875, -935.9518432617188]\n",
      "\n",
      "Instance 2562 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2563 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2564 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "car at index 28: [0.2441473752260208, 2.939188241958618, 0.87386155128479, -1.893798828125, -0.9130988121032715]\n",
      "Grand sum of 1940 tensor sets is: [759.5820922851562, 3147.984619140625, -592.873291015625, -876.885498046875, -936.8649291992188]\n",
      "\n",
      "Instance 2565 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 24: [0.16859036684036255, 0.9603707790374756, -0.3514403700828552, 1.3077361583709717, -1.5998502969741821]\n",
      "Grand sum of 1941 tensor sets is: [759.7506713867188, 3148.945068359375, -593.2247314453125, -875.5777587890625, -938.4647827148438]\n",
      "\n",
      "Instance 2566 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "car at index 27: [-0.0533038005232811, 2.3539788722991943, -0.9878630042076111, -1.743609070777893, -1.6318600177764893]\n",
      "Grand sum of 1942 tensor sets is: [759.6973876953125, 3151.299072265625, -594.2125854492188, -877.3213500976562, -940.0966186523438]\n",
      "\n",
      "Instance 2567 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2568 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [79, 88, 91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([107, 13, 768])\n",
      "Shape of summed layers is: 107 x 768\n",
      "car at index 79: [0.25172361731529236, 0.8743430376052856, -0.5363035798072815, 0.4715039134025574, 0.3054425120353699]\n",
      "car at index 88: [0.46087539196014404, 2.871586322784424, 0.7301235198974609, 0.9663454294204712, -2.277212619781494]\n",
      "car at index 91: [0.48874449729919434, 2.691603899002075, -0.3546059727668762, -1.4764149188995361, 1.7580100297927856]\n",
      "Grand sum of 1943 tensor sets is: [760.0978393554688, 3153.44482421875, -594.2661743164062, -877.334228515625, -940.1678466796875]\n",
      "\n",
      "Instance 2569 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2570 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 24: [-1.1964848041534424, 2.123687982559204, -0.24973464012145996, -1.9018795490264893, 0.6094409227371216]\n",
      "Grand sum of 1944 tensor sets is: [758.9013671875, 3155.568603515625, -594.5159301757812, -879.236083984375, -939.5584106445312]\n",
      "\n",
      "Instance 2571 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 7: [0.3998847007751465, 3.1280603408813477, -0.7933928370475769, -0.02286538854241371, -2.0155324935913086]\n",
      "Grand sum of 1945 tensor sets is: [759.30126953125, 3158.69677734375, -595.309326171875, -879.2589721679688, -941.5739135742188]\n",
      "\n",
      "Instance 2572 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9, 14]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 9: [0.15666133165359497, 2.0619661808013916, 0.11568841338157654, -1.5662529468536377, -3.250765085220337]\n",
      "car at index 14: [-0.10580965876579285, 3.5013062953948975, -0.3973357379436493, -1.4834436178207397, -3.1196508407592773]\n",
      "Grand sum of 1946 tensor sets is: [759.3267211914062, 3161.478515625, -595.4501342773438, -880.7838134765625, -944.7590942382812]\n",
      "\n",
      "Instance 2573 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "car at index 2: [-0.6311585307121277, 3.4559414386749268, 0.8263000845909119, -0.6167680025100708, -1.7089219093322754]\n",
      "Grand sum of 1947 tensor sets is: [758.695556640625, 3164.9345703125, -594.6238403320312, -881.4005737304688, -946.468017578125]\n",
      "\n",
      "Instance 2574 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "car at index 6: [-0.16930924355983734, -0.4531124234199524, -0.447690486907959, -0.036599233746528625, 0.07097725570201874]\n",
      "Grand sum of 1948 tensor sets is: [758.5262451171875, 3164.4814453125, -595.071533203125, -881.4371948242188, -946.3970336914062]\n",
      "\n",
      "Instance 2575 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2576 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2577 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 7: [-1.030181646347046, 2.8242528438568115, 0.1517374962568283, 0.19154876470565796, -2.4455277919769287]\n",
      "Grand sum of 1949 tensor sets is: [757.49609375, 3167.3056640625, -594.9197998046875, -881.2456665039062, -948.8425903320312]\n",
      "\n",
      "Instance 2578 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 20: [-0.25664836168289185, 3.5033133029937744, -0.31303948163986206, -1.405997395515442, -0.3038349151611328]\n",
      "Grand sum of 1950 tensor sets is: [757.2394409179688, 3170.80908203125, -595.2328491210938, -882.6516723632812, -949.1464233398438]\n",
      "\n",
      "Instance 2579 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "car at index 16: [1.0635535717010498, 1.8059293031692505, -1.2134904861450195, -2.7482848167419434, 1.0371257066726685]\n",
      "Grand sum of 1951 tensor sets is: [758.302978515625, 3172.614990234375, -596.4463500976562, -885.3999633789062, -948.1093139648438]\n",
      "\n",
      "Instance 2580 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "car at index 16: [1.0089651346206665, 3.5986382961273193, 0.11829978972673416, -1.9583001136779785, -1.1190165281295776]\n",
      "Grand sum of 1952 tensor sets is: [759.3119506835938, 3176.213623046875, -596.3280639648438, -887.3582763671875, -949.2283325195312]\n",
      "\n",
      "Instance 2581 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2582 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 7: [-0.916770339012146, 0.2849629819393158, 0.23310586810112, -0.821212887763977, 1.8891568183898926]\n",
      "Grand sum of 1953 tensor sets is: [758.3952026367188, 3176.49853515625, -596.094970703125, -888.1795043945312, -947.3391723632812]\n",
      "\n",
      "Instance 2583 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 4: [1.1239991188049316, 1.2967071533203125, -0.036586254835128784, 0.7231752276420593, -0.7445303201675415]\n",
      "Grand sum of 1954 tensor sets is: [759.5192260742188, 3177.795166015625, -596.1315307617188, -887.456298828125, -948.0836791992188]\n",
      "\n",
      "Instance 2584 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "car at index 22: [0.6613044142723083, 3.378126621246338, 0.9958884119987488, 0.7144119143486023, -0.6580960750579834]\n",
      "Grand sum of 1955 tensor sets is: [760.1805419921875, 3181.17333984375, -595.1356201171875, -886.7418823242188, -948.7417602539062]\n",
      "\n",
      "Instance 2585 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 21: [0.7081712484359741, 0.3920867443084717, -0.6987968683242798, -0.831909716129303, -0.37054258584976196]\n",
      "Grand sum of 1956 tensor sets is: [760.8887329101562, 3181.5654296875, -595.8344116210938, -887.5737915039062, -949.1123046875]\n",
      "\n",
      "Instance 2586 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 18: [0.050184622406959534, 1.758094072341919, -0.0267656072974205, -1.3235207796096802, -1.2342498302459717]\n",
      "Grand sum of 1957 tensor sets is: [760.9389038085938, 3183.323486328125, -595.8612060546875, -888.8973388671875, -950.3465576171875]\n",
      "\n",
      "Instance 2587 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 7: [0.5956078171730042, 0.7542744874954224, -1.354622721672058, 0.7373209595680237, -0.11270341277122498]\n",
      "Grand sum of 1958 tensor sets is: [761.5344848632812, 3184.077880859375, -597.2158203125, -888.1600341796875, -950.4592895507812]\n",
      "\n",
      "Instance 2588 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "car at index 13: [0.4360189139842987, -2.4370219707489014, -0.500978946685791, 2.339137315750122, 3.1836366653442383]\n",
      "Grand sum of 1959 tensor sets is: [761.9705200195312, 3181.640869140625, -597.716796875, -885.8209228515625, -947.275634765625]\n",
      "\n",
      "Instance 2589 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 3: [0.2128068506717682, 1.7444632053375244, -1.4143774509429932, -0.3836909532546997, -0.21605217456817627]\n",
      "Grand sum of 1960 tensor sets is: [762.183349609375, 3183.38525390625, -599.1311645507812, -886.20458984375, -947.49169921875]\n",
      "\n",
      "Instance 2590 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2591 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2592 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 10: [0.08920850604772568, 2.3768930435180664, 0.03820710629224777, 0.6150491833686829, -1.718193769454956]\n",
      "Grand sum of 1961 tensor sets is: [762.2725830078125, 3185.76220703125, -599.0929565429688, -885.5895385742188, -949.2098999023438]\n",
      "\n",
      "Instance 2593 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2594 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2595 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "car at index 12: [0.2379034459590912, 0.7329530715942383, 0.27124282717704773, -3.101332902908325, -0.5846772789955139]\n",
      "Grand sum of 1962 tensor sets is: [762.510498046875, 3186.4951171875, -598.8217163085938, -888.6908569335938, -949.7945556640625]\n",
      "\n",
      "Instance 2596 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([81, 13, 768])\n",
      "Shape of summed layers is: 81 x 768\n",
      "car at index 34: [1.118664026260376, -1.4973515272140503, -0.2987854480743408, -0.20829343795776367, 3.2802319526672363]\n",
      "Grand sum of 1963 tensor sets is: [763.629150390625, 3184.997802734375, -599.1204833984375, -888.899169921875, -946.5143432617188]\n",
      "\n",
      "Instance 2597 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2598 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [52]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "car at index 52: [0.33701446652412415, 0.38111773133277893, -0.7946928143501282, 0.9576861262321472, 0.5413501262664795]\n",
      "Grand sum of 1964 tensor sets is: [763.9661865234375, 3185.37890625, -599.9151611328125, -887.9414672851562, -945.9730224609375]\n",
      "\n",
      "Instance 2599 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2600 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "car at index 15: [-0.024514585733413696, 0.37653493881225586, 0.1768655925989151, -1.029903531074524, 0.4349572956562042]\n",
      "Grand sum of 1965 tensor sets is: [763.941650390625, 3185.75537109375, -599.73828125, -888.9713745117188, -945.5380859375]\n",
      "\n",
      "Instance 2601 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [64]\n",
      "Size of token embeddings is torch.Size([80, 13, 768])\n",
      "Shape of summed layers is: 80 x 768\n",
      "car at index 64: [0.5738987326622009, -0.022679604589939117, -0.5443331599235535, 2.3345553874969482, 1.5830817222595215]\n",
      "Grand sum of 1966 tensor sets is: [764.5155639648438, 3185.732666015625, -600.2825927734375, -886.6368408203125, -943.9550170898438]\n",
      "\n",
      "Instance 2602 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2603 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 9: [0.6037890315055847, 1.7199395895004272, 0.033674679696559906, -0.44129621982574463, -0.8517256379127502]\n",
      "Grand sum of 1967 tensor sets is: [765.1193237304688, 3187.45263671875, -600.2489013671875, -887.078125, -944.8067626953125]\n",
      "\n",
      "Instance 2604 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3, 33]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 3: [1.086727499961853, 2.0358777046203613, -0.041759930551052094, -0.2886967957019806, -0.8103432059288025]\n",
      "car at index 33: [0.24218207597732544, 2.9165735244750977, 0.5837653279304504, -0.33228570222854614, -2.8061487674713135]\n",
      "Grand sum of 1968 tensor sets is: [765.7837524414062, 3189.928955078125, -599.9779052734375, -887.3886108398438, -946.614990234375]\n",
      "\n",
      "Instance 2605 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "car at index 18: [-0.06268574297428131, 4.448274612426758, 0.6126548051834106, -1.3353883028030396, -3.8673532009124756]\n",
      "Grand sum of 1969 tensor sets is: [765.7210693359375, 3194.377197265625, -599.365234375, -888.7239990234375, -950.4823608398438]\n",
      "\n",
      "Instance 2606 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "car at index 12: [1.3477845191955566, 0.14921361207962036, 1.0067050457000732, 2.848522186279297, 3.2603042125701904]\n",
      "Grand sum of 1970 tensor sets is: [767.06884765625, 3194.5263671875, -598.3585205078125, -885.87548828125, -947.2220458984375]\n",
      "\n",
      "Instance 2607 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([167, 13, 768])\n",
      "Shape of summed layers is: 167 x 768\n",
      "car at index 36: [1.3584723472595215, 0.7657484412193298, -0.06704919040203094, 0.21383538842201233, -1.4879825115203857]\n",
      "Grand sum of 1971 tensor sets is: [768.4273071289062, 3195.292236328125, -598.4255981445312, -885.6616821289062, -948.7100219726562]\n",
      "\n",
      "Instance 2608 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 13: [-0.13671496510505676, 1.2133288383483887, -0.23261156678199768, -1.479243516921997, -0.29966482520103455]\n",
      "Grand sum of 1972 tensor sets is: [768.2905883789062, 3196.505615234375, -598.658203125, -887.1409301757812, -949.0097045898438]\n",
      "\n",
      "Instance 2609 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "car at index 40: [0.39238032698631287, 1.504565715789795, 1.2728970050811768, 1.0137525796890259, -0.33151471614837646]\n",
      "Grand sum of 1973 tensor sets is: [768.6829833984375, 3198.01025390625, -597.3853149414062, -886.127197265625, -949.3412475585938]\n",
      "\n",
      "Instance 2610 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "car at index 14: [-0.2291954755783081, 0.7872471213340759, -0.0429852157831192, -2.2776758670806885, 1.1108193397521973]\n",
      "Grand sum of 1974 tensor sets is: [768.4537963867188, 3198.797607421875, -597.4282836914062, -888.4048461914062, -948.2304077148438]\n",
      "\n",
      "Instance 2611 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2612 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 25: [0.4153217375278473, 3.247490167617798, -1.06563138961792, 1.3174649477005005, -0.2997792065143585]\n",
      "Grand sum of 1975 tensor sets is: [768.869140625, 3202.045166015625, -598.493896484375, -887.08740234375, -948.5302124023438]\n",
      "\n",
      "Instance 2613 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "car at index 10: [2.117654800415039, 1.6763826608657837, -1.2342777252197266, -1.6655229330062866, -0.1445026844739914]\n",
      "Grand sum of 1976 tensor sets is: [770.98681640625, 3203.721435546875, -599.7281494140625, -888.7529296875, -948.6747436523438]\n",
      "\n",
      "Instance 2614 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2615 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "car at index 31: [-0.02658950164914131, 0.7113311290740967, 0.45766717195510864, 1.9958993196487427, -2.0530288219451904]\n",
      "Grand sum of 1977 tensor sets is: [770.960205078125, 3204.432861328125, -599.2705078125, -886.7570190429688, -950.727783203125]\n",
      "\n",
      "Instance 2616 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "car at index 10: [0.5451425313949585, 0.6645281910896301, -1.3814301490783691, -1.029032588005066, 0.1471809446811676]\n",
      "Grand sum of 1978 tensor sets is: [771.50537109375, 3205.097412109375, -600.6519165039062, -887.7860717773438, -950.5806274414062]\n",
      "\n",
      "Instance 2617 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [29, 35]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "car at index 29: [0.8741143941879272, 2.910689115524292, -1.0035839080810547, -1.9696342945098877, 3.8240771293640137]\n",
      "car at index 35: [0.5248027443885803, 2.2408316135406494, -1.236173152923584, -0.48724254965782166, -0.3148491084575653]\n",
      "Grand sum of 1979 tensor sets is: [772.204833984375, 3207.673095703125, -601.7717895507812, -889.0145263671875, -948.8259887695312]\n",
      "\n",
      "Instance 2618 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "car at index 9: [-0.6600062847137451, 3.0037026405334473, 0.48783379793167114, -1.5184259414672852, -2.5111372470855713]\n",
      "Grand sum of 1980 tensor sets is: [771.5447998046875, 3210.6767578125, -601.283935546875, -890.532958984375, -951.3370971679688]\n",
      "\n",
      "Instance 2619 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "car at index 22: [-0.20223744213581085, 1.228338599205017, -0.4833741784095764, 0.7149882316589355, 2.8508100509643555]\n",
      "Grand sum of 1981 tensor sets is: [771.3425903320312, 3211.905029296875, -601.767333984375, -889.8179931640625, -948.4862670898438]\n",
      "\n",
      "Instance 2620 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2621 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [161, 166]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "car at index 161: [0.7976619005203247, -1.576216459274292, -0.7544938325881958, -0.4981543719768524, -1.3478929996490479]\n",
      "car at index 166: [0.50649493932724, -1.883012056350708, 0.2835734188556671, -2.009866952896118, -3.001314878463745]\n",
      "Grand sum of 1982 tensor sets is: [771.9946899414062, 3210.17529296875, -602.0028076171875, -891.072021484375, -950.660888671875]\n",
      "\n",
      "Instance 2622 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2623 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "car at index 9: [1.6875083446502686, 1.2987152338027954, -1.4667038917541504, 0.7133719325065613, 0.04262029007077217]\n",
      "Grand sum of 1983 tensor sets is: [773.6821899414062, 3211.47412109375, -603.469482421875, -890.358642578125, -950.6182861328125]\n",
      "\n",
      "Instance 2624 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "car at index 24: [0.7360079288482666, 2.3549184799194336, -1.151841640472412, -1.7490038871765137, 0.9391686916351318]\n",
      "Grand sum of 1984 tensor sets is: [774.418212890625, 3213.8291015625, -604.621337890625, -892.107666015625, -949.6791381835938]\n",
      "\n",
      "Instance 2625 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 15: [0.1436682641506195, 2.1759777069091797, -0.739022970199585, -0.09814438223838806, -0.448246568441391]\n",
      "Grand sum of 1985 tensor sets is: [774.5618896484375, 3216.005126953125, -605.3603515625, -892.205810546875, -950.1273803710938]\n",
      "\n",
      "Instance 2626 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2627 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "car at index 13: [0.7934927940368652, 1.2018039226531982, -1.620944619178772, 0.731507420539856, 0.4916122555732727]\n",
      "Grand sum of 1986 tensor sets is: [775.3554077148438, 3217.20703125, -606.9813232421875, -891.4743041992188, -949.6357421875]\n",
      "\n",
      "Instance 2628 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [61]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "car at index 61: [-0.035732075572013855, 2.002938747406006, 0.2694159746170044, -0.020946234464645386, 2.4975242614746094]\n",
      "Grand sum of 1987 tensor sets is: [775.3197021484375, 3219.2099609375, -606.7119140625, -891.4952392578125, -947.1382446289062]\n",
      "\n",
      "Instance 2629 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "car at index 3: [-0.18694721162319183, 1.843418002128601, -1.070474624633789, -2.2402212619781494, 0.04379785805940628]\n",
      "Grand sum of 1988 tensor sets is: [775.1327514648438, 3221.053466796875, -607.7824096679688, -893.7354736328125, -947.0944213867188]\n",
      "\n",
      "Instance 2630 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 7: [-0.3984798192977905, 2.3073437213897705, -0.8946385383605957, -1.6594903469085693, -2.1884944438934326]\n",
      "Grand sum of 1989 tensor sets is: [774.7342529296875, 3223.36083984375, -608.6770629882812, -895.3949584960938, -949.2828979492188]\n",
      "\n",
      "Instance 2631 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2632 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "car at index 7: [-0.2966998815536499, 2.0526249408721924, -0.12200019508600235, -2.183594226837158, -1.5661427974700928]\n",
      "Grand sum of 1990 tensor sets is: [774.4375610351562, 3225.41357421875, -608.799072265625, -897.5785522460938, -950.8490600585938]\n",
      "\n",
      "Instance 2633 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2634 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "car at index 25: [0.4070354700088501, 2.0356361865997314, 0.29902249574661255, -1.5575566291809082, -1.9854267835617065]\n",
      "Grand sum of 1991 tensor sets is: [774.8446044921875, 3227.44921875, -608.5000610351562, -899.1361083984375, -952.83447265625]\n",
      "\n",
      "Instance 2635 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "car at index 3: [0.028358638286590576, 2.614485502243042, 0.1499471664428711, -0.6146845817565918, -2.0658211708068848]\n",
      "Grand sum of 1992 tensor sets is: [774.8729858398438, 3230.063720703125, -608.35009765625, -899.7507934570312, -954.9002685546875]\n",
      "\n",
      "Instance 2636 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17, 21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "car at index 17: [0.7445952296257019, 1.1873983144760132, -0.1295362263917923, -1.8168747425079346, 0.5149712562561035]\n",
      "car at index 21: [1.0385807752609253, 2.2509236335754395, -0.7780609726905823, -2.0350003242492676, 1.8080995082855225]\n",
      "Grand sum of 1993 tensor sets is: [775.7645874023438, 3231.782958984375, -608.8038940429688, -901.6767578125, -953.7387084960938]\n",
      "\n",
      "Instance 2637 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [6, 15, 23, 27, 43, 75, 90]\n",
      "Size of token embeddings is torch.Size([92, 13, 768])\n",
      "Shape of summed layers is: 92 x 768\n",
      "car at index 6: [0.48903149366378784, 1.599402904510498, -1.265220046043396, -0.4056285619735718, 0.07185712456703186]\n",
      "car at index 15: [-0.48242712020874023, 2.9980156421661377, 0.7695455551147461, -0.5281850695610046, -2.6275882720947266]\n",
      "car at index 23: [-0.4488220512866974, 2.2339046001434326, -0.6080113053321838, 1.8715566396713257, -0.715402364730835]\n",
      "car at index 27: [0.3964235186576843, 3.7872793674468994, 0.20107710361480713, 1.1665022373199463, -3.0130228996276855]\n",
      "car at index 43: [-0.05327750742435455, 3.836209535598755, -0.39828363060951233, -1.4583253860473633, -1.907636284828186]\n",
      "car at index 75: [-0.7920160293579102, 2.3187012672424316, 0.3519899845123291, 0.07611344754695892, -2.2799181938171387]\n",
      "car at index 90: [0.805660605430603, 1.522743821144104, -0.10147836804389954, -1.7703648805618286, 1.002465844154358]\n",
      "Grand sum of 1994 tensor sets is: [775.7523803710938, 3234.396728515625, -608.9539184570312, -901.8265380859375, -955.0914306640625]\n",
      "\n",
      "Instance 2638 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2639 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "car at index 7: [0.22512468695640564, 1.1998157501220703, -0.5171031951904297, -2.824502468109131, -0.9211848974227905]\n",
      "Grand sum of 1995 tensor sets is: [775.9774780273438, 3235.596435546875, -609.4710083007812, -904.6510620117188, -956.0126342773438]\n",
      "\n",
      "Instance 2640 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "car at index 21: [0.7355591058731079, 2.2327046394348145, -1.029068112373352, -0.6583534479141235, 1.7776575088500977]\n",
      "Grand sum of 1996 tensor sets is: [776.7130126953125, 3237.8291015625, -610.5000610351562, -905.3093872070312, -954.2349853515625]\n",
      "\n",
      "Instance 2641 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [55]\n",
      "Size of token embeddings is torch.Size([94, 13, 768])\n",
      "Shape of summed layers is: 94 x 768\n",
      "car at index 55: [0.05150406435132027, 1.4021552801132202, -0.9032502174377441, -0.585829496383667, -0.6654158234596252]\n",
      "Grand sum of 1997 tensor sets is: [776.7645263671875, 3239.231201171875, -611.4033203125, -905.8952026367188, -954.900390625]\n",
      "\n",
      "Instance 2642 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2643 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "car at index 8: [-0.016945991665124893, 1.678212285041809, -0.8437232375144958, -2.2972867488861084, -1.653623342514038]\n",
      "Grand sum of 1998 tensor sets is: [776.74755859375, 3240.909423828125, -612.2470703125, -908.1925048828125, -956.5540161132812]\n",
      "\n",
      "Instance 2644 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "car at index 5: [0.19627313315868378, 2.716616630554199, 0.7560157179832458, -1.3019880056381226, -2.394765615463257]\n",
      "Grand sum of 1999 tensor sets is: [776.94384765625, 3243.6259765625, -611.4910278320312, -909.4945068359375, -958.9487915039062]\n",
      "\n",
      "Instance 2645 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2646 of car.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2647 of car.\n",
      "Looking for vocab token: car\n",
      "\n",
      "Instance 2648 of car.\n",
      "Looking for vocab token: car\n",
      "Indices are [17, 31]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "car at index 17: [0.9637019634246826, 4.030816078186035, -1.0064305067062378, -1.3718016147613525, -2.486605167388916]\n",
      "car at index 31: [0.22843937575817108, 3.2816288471221924, -0.4710523784160614, -1.9643092155456543, -0.9887767434120178]\n",
      "Grand sum of 2000 tensor sets is: [777.5399169921875, 3247.2822265625, -612.2297973632812, -911.1625366210938, -960.6864624023438]\n",
      "Mean of tensors is: tensor([ 0.3888,  1.6236, -0.3061, -0.4556, -0.4803]) (768 features in tensor)\n",
      "Saved the embedding for car.\n",
      "Saved the count of sentences used to create car embedding\n",
      "Run time for car was 254.24286235775799 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "fluent\n",
      "\n",
      "Instance 1 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "fluent at index 3: [0.1555183231830597, 0.4839668273925781, 1.5968825817108154, -0.8414345979690552, 0.06009417772293091]\n",
      "Grand sum of 1 tensor sets is: [0.1555183231830597, 0.4839668273925781, 1.5968825817108154, -0.8414345979690552, 0.06009417772293091]\n",
      "\n",
      "Instance 2 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 2 tensor sets is: [0.3230438530445099, 1.0756045579910278, 1.2517939805984497, -0.3675876259803772, 2.319284439086914]\n",
      "\n",
      "Instance 3 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "fluent at index 9: [-0.17014914751052856, 1.7774559259414673, 0.5592749118804932, 0.4729827046394348, -1.5170923471450806]\n",
      "Grand sum of 3 tensor sets is: [0.15289470553398132, 2.853060483932495, 1.8110688924789429, 0.10539507865905762, 0.8021920919418335]\n",
      "\n",
      "Instance 4 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "fluent at index 6: [-0.22996853291988373, 0.4803633391857147, 0.2960488200187683, -1.0893151760101318, -0.3646516799926758]\n",
      "Grand sum of 4 tensor sets is: [-0.0770738273859024, 3.3334238529205322, 2.1071176528930664, -0.9839200973510742, 0.4375404119491577]\n",
      "\n",
      "Instance 5 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [57]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "fluent at index 57: [-1.740936517715454, 1.2460150718688965, 0.7687811851501465, 0.29662826657295227, -0.8591666221618652]\n",
      "Grand sum of 5 tensor sets is: [-1.8180103302001953, 4.579439163208008, 2.875898838043213, -0.6872918605804443, -0.4216262102127075]\n",
      "\n",
      "Instance 6 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "fluent at index 12: [-1.6398897171020508, 2.6077816486358643, 1.4782661199569702, 0.3938499093055725, 1.1909942626953125]\n",
      "Grand sum of 6 tensor sets is: [-3.457900047302246, 7.187220573425293, 4.354165077209473, -0.2934419512748718, 0.769368052482605]\n",
      "\n",
      "Instance 7 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "fluent at index 3: [0.16663964092731476, 1.0309027433395386, 0.9795305132865906, 0.09952962398529053, -0.35216376185417175]\n",
      "Grand sum of 7 tensor sets is: [-3.2912604808807373, 8.218123435974121, 5.333695411682129, -0.1939123272895813, 0.4172042906284332]\n",
      "\n",
      "Instance 8 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "fluent at index 6: [1.0962066650390625, 2.560520887374878, 0.048209160566329956, 1.762920618057251, 3.747722864151001]\n",
      "Grand sum of 8 tensor sets is: [-2.195053815841675, 10.778644561767578, 5.381904602050781, 1.5690083503723145, 4.164927005767822]\n",
      "\n",
      "Instance 9 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 10 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "fluent at index 32: [-0.36448419094085693, 0.14185264706611633, 0.07580148428678513, 1.3165655136108398, 5.02119779586792]\n",
      "Grand sum of 9 tensor sets is: [-2.559537887573242, 10.920496940612793, 5.457705974578857, 2.8855738639831543, 9.186124801635742]\n",
      "\n",
      "Instance 11 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "fluent at index 7: [-1.7315969467163086, 2.3216094970703125, 1.38277006149292, 2.380769968032837, -1.6386232376098633]\n",
      "Grand sum of 10 tensor sets is: [-4.291134834289551, 13.242106437683105, 6.840476036071777, 5.26634407043457, 7.547501564025879]\n",
      "\n",
      "Instance 12 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 11 tensor sets is: [-5.015607833862305, 15.639780044555664, 6.716649532318115, 7.997761249542236, 9.553464889526367]\n",
      "\n",
      "Instance 13 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 14 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "fluent at index 17: [-0.3422560691833496, 2.072444200515747, 1.3911163806915283, 0.7293006777763367, -0.7658112049102783]\n",
      "Grand sum of 12 tensor sets is: [-5.357863903045654, 17.71222496032715, 8.107766151428223, 8.727062225341797, 8.787653923034668]\n",
      "\n",
      "Instance 15 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "fluent at index 26: [-0.5751979351043701, 2.5151801109313965, 1.2761948108673096, 0.0025113001465797424, 1.2401607036590576]\n",
      "Grand sum of 13 tensor sets is: [-5.933061599731445, 20.227405548095703, 9.383960723876953, 8.729573249816895, 10.027814865112305]\n",
      "\n",
      "Instance 16 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [115]\n",
      "Size of token embeddings is torch.Size([127, 13, 768])\n",
      "Shape of summed layers is: 127 x 768\n",
      "fluent at index 115: [-0.36943668127059937, 2.2915260791778564, 2.121124505996704, 1.0201537609100342, -1.6362661123275757]\n",
      "Grand sum of 14 tensor sets is: [-6.3024983406066895, 22.518932342529297, 11.505084991455078, 9.749727249145508, 8.391549110412598]\n",
      "\n",
      "Instance 17 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 18 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 19 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 15 tensor sets is: [-6.13497257232666, 23.110570907592773, 11.159996032714844, 10.2235746383667, 10.650739669799805]\n",
      "\n",
      "Instance 20 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "fluent at index 13: [-0.21219268441200256, 1.7272592782974243, 1.720029354095459, 1.7569211721420288, -0.14155519008636475]\n",
      "Grand sum of 16 tensor sets is: [-6.347165107727051, 24.83782958984375, 12.880025863647461, 11.98049545288086, 10.509184837341309]\n",
      "\n",
      "Instance 21 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 17 tensor sets is: [-6.17963981628418, 25.429468154907227, 12.534936904907227, 12.45434284210205, 12.768375396728516]\n",
      "\n",
      "Instance 22 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "fluent at index 23: [0.4502662718296051, 0.5234926342964172, 0.5924032330513, 0.3203531503677368, -2.1056530475616455]\n",
      "Grand sum of 18 tensor sets is: [-5.729373455047607, 25.952960968017578, 13.127340316772461, 12.774696350097656, 10.66272258758545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 23 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "fluent at index 8: [0.3229967951774597, -0.02279762178659439, 2.063123941421509, -0.15906713902950287, 2.2405009269714355]\n",
      "Grand sum of 19 tensor sets is: [-5.406376838684082, 25.93016242980957, 15.19046401977539, 12.615629196166992, 12.903223037719727]\n",
      "\n",
      "Instance 24 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "fluent at index 10: [-0.31170859932899475, 2.2781474590301514, 1.7930864095687866, 1.1040540933609009, 1.5407817363739014]\n",
      "Grand sum of 20 tensor sets is: [-5.718085289001465, 28.208309173583984, 16.983551025390625, 13.719683647155762, 14.444005012512207]\n",
      "\n",
      "Instance 25 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "fluent at index 32: [0.4990993142127991, -0.529347836971283, -0.37231141328811646, 2.075092315673828, 1.7007001638412476]\n",
      "Grand sum of 21 tensor sets is: [-5.2189860343933105, 27.6789608001709, 16.61124038696289, 15.79477596282959, 16.144704818725586]\n",
      "\n",
      "Instance 26 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 22 tensor sets is: [-5.051460266113281, 28.270599365234375, 16.266151428222656, 16.26862335205078, 18.403894424438477]\n",
      "\n",
      "Instance 27 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "fluent at index 19: [-0.8937036991119385, 1.4200170040130615, 1.3259397745132446, 0.26718395948410034, 2.293794631958008]\n",
      "Grand sum of 23 tensor sets is: [-5.945163726806641, 29.690616607666016, 17.592090606689453, 16.53580665588379, 20.697689056396484]\n",
      "\n",
      "Instance 28 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "fluent at index 14: [-0.7249525785446167, 1.9867570400238037, 0.8082952499389648, 1.3232768774032593, 0.6987618207931519]\n",
      "Grand sum of 24 tensor sets is: [-6.670116424560547, 31.6773738861084, 18.400386810302734, 17.85908317565918, 21.39645004272461]\n",
      "\n",
      "Instance 29 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "fluent at index 25: [0.2589185833930969, 1.7973474264144897, 0.3747175335884094, 0.8852308988571167, -0.6731592416763306]\n",
      "Grand sum of 25 tensor sets is: [-6.411197662353516, 33.4747200012207, 18.775104522705078, 18.744314193725586, 20.723291397094727]\n",
      "\n",
      "Instance 30 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "fluent at index 3: [-0.5816755294799805, 1.240372657775879, 1.2419594526290894, -0.9423662424087524, -0.6283304691314697]\n",
      "Grand sum of 26 tensor sets is: [-6.992873191833496, 34.715091705322266, 20.01706314086914, 17.80194854736328, 20.094961166381836]\n",
      "\n",
      "Instance 31 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "fluent at index 13: [-1.1277905702590942, 2.7761878967285156, 1.5253803730010986, 0.5504254698753357, 0.9581981897354126]\n",
      "Grand sum of 27 tensor sets is: [-8.1206636428833, 37.49127960205078, 21.542444229125977, 18.352373123168945, 21.053159713745117]\n",
      "\n",
      "Instance 32 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "fluent at index 2: [0.09728308022022247, 0.20413319766521454, 0.770634114742279, 2.743265151977539, -2.408751964569092]\n",
      "Grand sum of 28 tensor sets is: [-8.023380279541016, 37.695411682128906, 22.313077926635742, 21.095638275146484, 18.644407272338867]\n",
      "\n",
      "Instance 33 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 34 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 29 tensor sets is: [-8.74785327911377, 40.09308624267578, 22.189250946044922, 23.827054977416992, 20.65036964416504]\n",
      "\n",
      "Instance 35 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 36 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 37 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "fluent at index 17: [0.009116072207689285, 1.8598017692565918, 0.6909924745559692, -0.7542372941970825, -0.43501389026641846]\n",
      "Grand sum of 30 tensor sets is: [-8.738737106323242, 41.95288848876953, 22.8802433013916, 23.072816848754883, 20.215354919433594]\n",
      "\n",
      "Instance 38 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "fluent at index 35: [-0.18649828433990479, 2.2178122997283936, 1.3225173950195312, 1.7588287591934204, 0.9804231524467468]\n",
      "Grand sum of 31 tensor sets is: [-8.925235748291016, 44.17070007324219, 24.202760696411133, 24.831645965576172, 21.195777893066406]\n",
      "\n",
      "Instance 39 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "fluent at index 14: [-0.14726389944553375, 1.8849774599075317, 0.5129781365394592, -0.7258298397064209, 0.7000256776809692]\n",
      "Grand sum of 32 tensor sets is: [-9.07249927520752, 46.05567932128906, 24.71573829650879, 24.105815887451172, 21.895803451538086]\n",
      "\n",
      "Instance 40 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 41 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "fluent at index 20: [-1.2969688177108765, 2.2802071571350098, 0.5546587705612183, -0.6034397482872009, 1.9513132572174072]\n",
      "Grand sum of 33 tensor sets is: [-10.369467735290527, 48.33588790893555, 25.270397186279297, 23.502376556396484, 23.847116470336914]\n",
      "\n",
      "Instance 42 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "fluent at index 3: [0.26478537917137146, 1.6895110607147217, -0.808983564376831, 1.2878156900405884, 0.7861573696136475]\n",
      "Grand sum of 34 tensor sets is: [-10.104681968688965, 50.02539825439453, 24.461414337158203, 24.790191650390625, 24.63327407836914]\n",
      "\n",
      "Instance 43 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 35 tensor sets is: [-10.829154968261719, 52.423072814941406, 24.337587356567383, 27.521608352661133, 26.639236450195312]\n",
      "\n",
      "Instance 44 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 36 tensor sets is: [-10.661629676818848, 53.01470947265625, 23.99249839782715, 27.995454788208008, 28.898426055908203]\n",
      "\n",
      "Instance 45 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "fluent at index 10: [0.014412075281143188, 0.7097465991973877, 1.9586999416351318, 0.6050249338150024, 0.44562608003616333]\n",
      "Grand sum of 37 tensor sets is: [-10.647217750549316, 53.724456787109375, 25.95119857788086, 28.600479125976562, 29.344051361083984]\n",
      "\n",
      "Instance 46 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 38 tensor sets is: [-10.479692459106445, 54.31609344482422, 25.606109619140625, 29.074325561523438, 31.603240966796875]\n",
      "\n",
      "Instance 47 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 48 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "fluent at index 10: [0.062036290764808655, 1.8690439462661743, 0.8209178447723389, 0.5355299115180969, -0.2638755440711975]\n",
      "Grand sum of 39 tensor sets is: [-10.417655944824219, 56.18513870239258, 26.427026748657227, 29.60985565185547, 31.339365005493164]\n",
      "\n",
      "Instance 49 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "fluent at index 8: [-1.3767409324645996, 2.522202968597412, 2.0601377487182617, 1.024584412574768, 1.6459705829620361]\n",
      "Grand sum of 40 tensor sets is: [-11.794397354125977, 58.707340240478516, 28.487163543701172, 30.63443946838379, 32.98533630371094]\n",
      "\n",
      "Instance 50 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 51 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "fluent at index 4: [-1.4937039613723755, 2.7230491638183594, 1.9638550281524658, 0.566473126411438, -0.06980177760124207]\n",
      "Grand sum of 41 tensor sets is: [-13.288101196289062, 61.430389404296875, 30.451019287109375, 31.200912475585938, 32.91553497314453]\n",
      "\n",
      "Instance 52 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([100, 13, 768])\n",
      "Shape of summed layers is: 100 x 768\n",
      "fluent at index 26: [-0.45454633235931396, 0.5858739018440247, 1.8144707679748535, 1.816823959350586, -0.25287267565727234]\n",
      "Grand sum of 42 tensor sets is: [-13.742647171020508, 62.01626205444336, 32.2654914855957, 33.017738342285156, 32.662662506103516]\n",
      "\n",
      "Instance 53 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "fluent at index 3: [0.1555183231830597, 0.4839668273925781, 1.5968825817108154, -0.8414345979690552, 0.06009417772293091]\n",
      "Grand sum of 43 tensor sets is: [-13.587128639221191, 62.50022888183594, 33.86237335205078, 32.17630386352539, 32.722755432128906]\n",
      "\n",
      "Instance 54 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 44 tensor sets is: [-14.311601638793945, 64.89790344238281, 33.738548278808594, 34.90772247314453, 34.72871780395508]\n",
      "\n",
      "Instance 55 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "fluent at index 7: [-1.6700704097747803, 2.8388941287994385, 1.5016274452209473, 0.8590525984764099, 1.1821386814117432]\n",
      "Grand sum of 45 tensor sets is: [-15.981672286987305, 67.73680114746094, 35.240177154541016, 35.76677322387695, 35.910858154296875]\n",
      "\n",
      "Instance 56 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 57 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "fluent at index 2: [-0.6880232095718384, 0.5227637887001038, -0.7145121097564697, 0.6147733926773071, 1.4007220268249512]\n",
      "Grand sum of 46 tensor sets is: [-16.669694900512695, 68.25956726074219, 34.525665283203125, 36.38154602050781, 37.311580657958984]\n",
      "\n",
      "Instance 58 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "fluent at index 27: [-0.01896870881319046, 1.7663066387176514, 1.877779245376587, 0.7160832285881042, -0.2719344198703766]\n",
      "Grand sum of 47 tensor sets is: [-16.688663482666016, 70.02587127685547, 36.403446197509766, 37.09762954711914, 37.03964614868164]\n",
      "\n",
      "Instance 59 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "fluent at index 5: [0.8346508145332336, -0.7395029664039612, -1.503406286239624, 0.7761545181274414, 1.2521789073944092]\n",
      "Grand sum of 48 tensor sets is: [-15.854012489318848, 69.28636932373047, 34.90003967285156, 37.873783111572266, 38.29182434082031]\n",
      "\n",
      "Instance 60 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "fluent at index 22: [-1.1783860921859741, 2.5401134490966797, 1.199496865272522, 0.638207197189331, 1.0423592329025269]\n",
      "Grand sum of 49 tensor sets is: [-17.032398223876953, 71.82648468017578, 36.09953689575195, 38.51198959350586, 39.33418273925781]\n",
      "\n",
      "Instance 61 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 9: [-0.06320261210203171, 1.376335859298706, 0.8018566370010376, 0.2327817678451538, -1.0116550922393799]\n",
      "Grand sum of 50 tensor sets is: [-17.095600128173828, 73.20281982421875, 36.90139389038086, 38.74477005004883, 38.32252883911133]\n",
      "\n",
      "Instance 62 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 51 tensor sets is: [-17.8200740814209, 75.60049438476562, 36.77756881713867, 41.47618865966797, 40.3284912109375]\n",
      "\n",
      "Instance 63 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "fluent at index 12: [-0.03757374733686447, 2.1586616039276123, 1.340583086013794, -0.22591182589530945, -0.2705761194229126]\n",
      "Grand sum of 52 tensor sets is: [-17.857646942138672, 77.7591552734375, 38.1181526184082, 41.25027847290039, 40.05791473388672]\n",
      "\n",
      "Instance 64 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "fluent at index 5: [-0.33823511004447937, 0.7633867859840393, -0.3355520963668823, 3.119187831878662, 2.1406326293945312]\n",
      "Grand sum of 53 tensor sets is: [-18.19588279724121, 78.52254486083984, 37.78260040283203, 44.36946487426758, 42.19854736328125]\n",
      "\n",
      "Instance 65 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "fluent at index 25: [1.1110882759094238, 0.5325332283973694, -0.5961814522743225, 1.909279465675354, 0.3458813428878784]\n",
      "Grand sum of 54 tensor sets is: [-17.084794998168945, 79.0550765991211, 37.18642044067383, 46.278743743896484, 42.544429779052734]\n",
      "\n",
      "Instance 66 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 67 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "fluent at index 6: [-0.7246421575546265, 0.7935665845870972, 0.35508131980895996, 0.158686101436615, 0.3313712477684021]\n",
      "Grand sum of 55 tensor sets is: [-17.809436798095703, 79.84864044189453, 37.541500091552734, 46.43743133544922, 42.87580108642578]\n",
      "\n",
      "Instance 68 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 56 tensor sets is: [-17.641910552978516, 80.44027709960938, 37.1964111328125, 46.911277770996094, 45.13499069213867]\n",
      "\n",
      "Instance 69 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "fluent at index 4: [-0.11117058992385864, 1.9723860025405884, 1.6750177145004272, -0.6512816548347473, 0.8337399959564209]\n",
      "Grand sum of 57 tensor sets is: [-17.753080368041992, 82.41266632080078, 38.871429443359375, 46.25999450683594, 45.96873092651367]\n",
      "\n",
      "Instance 70 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "fluent at index 21: [-0.9031456708908081, 3.5205960273742676, 2.2717273235321045, 1.2135846614837646, 2.2761261463165283]\n",
      "Grand sum of 58 tensor sets is: [-18.656225204467773, 85.93326568603516, 41.143157958984375, 47.47357940673828, 48.24485778808594]\n",
      "\n",
      "Instance 71 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "fluent at index 11: [-0.33716198801994324, 2.1476266384124756, 1.252699613571167, 1.387398600578308, 2.5478012561798096]\n",
      "Grand sum of 59 tensor sets is: [-18.99338722229004, 88.08089447021484, 42.39585876464844, 48.86097717285156, 50.792659759521484]\n",
      "\n",
      "Instance 72 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "fluent at index 32: [-0.10650872439146042, 0.3474048972129822, 0.9177098274230957, -0.41199779510498047, -0.7909549474716187]\n",
      "Grand sum of 60 tensor sets is: [-19.099895477294922, 88.42829895019531, 43.313568115234375, 48.448978424072266, 50.001705169677734]\n",
      "\n",
      "Instance 73 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([160, 13, 768])\n",
      "Shape of summed layers is: 160 x 768\n",
      "fluent at index 30: [1.0511265993118286, -0.2579703629016876, -1.7042522430419922, 1.0054900646209717, 0.21358448266983032]\n",
      "Grand sum of 61 tensor sets is: [-18.048768997192383, 88.17032623291016, 41.60931396484375, 49.4544677734375, 50.21529006958008]\n",
      "\n",
      "Instance 74 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "fluent at index 17: [-0.49849435687065125, 1.3254234790802002, 0.3992040753364563, 1.4616966247558594, 0.7102113366127014]\n",
      "Grand sum of 62 tensor sets is: [-18.547264099121094, 89.4957504272461, 42.00851821899414, 50.91616439819336, 50.92550277709961]\n",
      "\n",
      "Instance 75 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "fluent at index 11: [-0.018231771886348724, 0.528271496295929, 0.8381611704826355, 0.9344722628593445, 0.08705061674118042]\n",
      "Grand sum of 63 tensor sets is: [-18.56549644470215, 90.0240249633789, 42.8466796875, 51.85063552856445, 51.01255416870117]\n",
      "\n",
      "Instance 76 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "fluent at index 11: [0.0888158455491066, 0.8230807781219482, 1.2984681129455566, -0.05385503172874451, 3.613828182220459]\n",
      "Grand sum of 64 tensor sets is: [-18.476680755615234, 90.84710693359375, 44.14514923095703, 51.79677963256836, 54.626380920410156]\n",
      "\n",
      "Instance 77 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "fluent at index 13: [-0.9218558669090271, 0.8294968605041504, -0.9870175123214722, 1.5333377122879028, 1.8627583980560303]\n",
      "Grand sum of 65 tensor sets is: [-19.398536682128906, 91.67660522460938, 43.15813064575195, 53.330116271972656, 56.489139556884766]\n",
      "\n",
      "Instance 78 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 79 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 66 tensor sets is: [-19.23101043701172, 92.26824188232422, 42.81304168701172, 53.80396270751953, 58.748329162597656]\n",
      "\n",
      "Instance 80 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "fluent at index 2: [-0.08982095122337341, 1.2601679563522339, 2.030825614929199, 0.9145314693450928, -0.21302223205566406]\n",
      "Grand sum of 67 tensor sets is: [-19.320831298828125, 93.52841186523438, 44.843868255615234, 54.7184944152832, 58.535308837890625]\n",
      "\n",
      "Instance 81 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "fluent at index 23: [0.06443251669406891, 1.9737420082092285, 0.8072207570075989, -0.0739365890622139, -0.14902865886688232]\n",
      "Grand sum of 68 tensor sets is: [-19.256399154663086, 95.50215148925781, 45.65108871459961, 54.64455795288086, 58.38628005981445]\n",
      "\n",
      "Instance 82 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "fluent at index 26: [-0.9485597610473633, 4.083241939544678, 0.6216907501220703, 0.9668054580688477, 1.2469741106033325]\n",
      "Grand sum of 69 tensor sets is: [-20.204959869384766, 99.58539581298828, 46.27278137207031, 55.61136245727539, 59.63325500488281]\n",
      "\n",
      "Instance 83 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 84 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "fluent at index 23: [-1.5647531747817993, 2.6026673316955566, 1.2846121788024902, 2.132920265197754, 2.3090157508850098]\n",
      "Grand sum of 70 tensor sets is: [-21.769712448120117, 102.18806457519531, 47.55739212036133, 57.74428176879883, 61.9422721862793]\n",
      "\n",
      "Instance 85 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "fluent at index 3: [0.1555183231830597, 0.4839668273925781, 1.5968825817108154, -0.8414345979690552, 0.06009417772293091]\n",
      "Grand sum of 71 tensor sets is: [-21.614194869995117, 102.67202758789062, 49.154273986816406, 56.90284729003906, 62.00236511230469]\n",
      "\n",
      "Instance 86 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 87 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "fluent at index 3: [-0.6327661871910095, 0.5867219567298889, -0.308234304189682, 2.215841054916382, 0.3885801434516907]\n",
      "Grand sum of 72 tensor sets is: [-22.24696159362793, 103.25875091552734, 48.846038818359375, 59.11868667602539, 62.39094543457031]\n",
      "\n",
      "Instance 88 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "fluent at index 8: [-0.5064111948013306, 1.6347198486328125, 0.958003044128418, -1.391402006149292, -1.7288978099822998]\n",
      "Grand sum of 73 tensor sets is: [-22.753372192382812, 104.89347076416016, 49.80404281616211, 57.7272834777832, 60.66204833984375]\n",
      "\n",
      "Instance 89 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "fluent at index 6: [-0.7095555663108826, 2.3785271644592285, 1.1232115030288696, -1.6027244329452515, 0.2935727834701538]\n",
      "Grand sum of 74 tensor sets is: [-23.462926864624023, 107.2719955444336, 50.92725372314453, 56.12455749511719, 60.95561981201172]\n",
      "\n",
      "Instance 90 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 75 tensor sets is: [-23.295400619506836, 107.86363220214844, 50.5821647644043, 56.59840393066406, 63.21480941772461]\n",
      "\n",
      "Instance 91 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 3: [0.35118263959884644, 0.7177648544311523, 1.5990772247314453, 0.04709240794181824, 0.35659945011138916]\n",
      "Grand sum of 76 tensor sets is: [-22.944217681884766, 108.5813980102539, 52.181243896484375, 56.6454963684082, 63.571407318115234]\n",
      "\n",
      "Instance 92 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 10: [-0.4775252640247345, 1.70479416847229, 1.9554650783538818, 0.3083556592464447, -1.2747392654418945]\n",
      "Grand sum of 77 tensor sets is: [-23.421743392944336, 110.28619384765625, 54.1367073059082, 56.953853607177734, 62.296669006347656]\n",
      "\n",
      "Instance 93 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "fluent at index 14: [-0.4963478744029999, 1.0539634227752686, 0.4980071783065796, 0.04173959419131279, -0.4332398474216461]\n",
      "Grand sum of 78 tensor sets is: [-23.9180908203125, 111.34015655517578, 54.63471603393555, 56.9955940246582, 61.86343002319336]\n",
      "\n",
      "Instance 94 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "fluent at index 9: [-1.405198574066162, 2.6640868186950684, 0.6089712381362915, 2.045961380004883, 0.3487541675567627]\n",
      "Grand sum of 79 tensor sets is: [-25.32328987121582, 114.00424194335938, 55.24368667602539, 59.04155731201172, 62.21218490600586]\n",
      "\n",
      "Instance 95 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "fluent at index 37: [-0.07620850205421448, 2.732729911804199, 1.8514399528503418, 2.53787899017334, 2.4092161655426025]\n",
      "Grand sum of 80 tensor sets is: [-25.399497985839844, 116.73696899414062, 57.09512710571289, 61.579437255859375, 64.62139892578125]\n",
      "\n",
      "Instance 96 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "fluent at index 3: [-0.9034007787704468, 3.028740644454956, 1.9618510007858276, 1.294079303741455, 2.6861164569854736]\n",
      "Grand sum of 81 tensor sets is: [-26.302898406982422, 119.76570892333984, 59.056976318359375, 62.87351608276367, 67.3075180053711]\n",
      "\n",
      "Instance 97 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "fluent at index 15: [-0.31671351194381714, 0.8407002687454224, 0.5120553970336914, 1.1048351526260376, 2.3495280742645264]\n",
      "Grand sum of 82 tensor sets is: [-26.619611740112305, 120.60640716552734, 59.56903076171875, 63.97835159301758, 69.65704345703125]\n",
      "\n",
      "Instance 98 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "fluent at index 23: [0.1691984236240387, 1.8962926864624023, 1.4088125228881836, 0.4641762375831604, -1.5855966806411743]\n",
      "Grand sum of 83 tensor sets is: [-26.45041275024414, 122.50270080566406, 60.97784423828125, 64.4425277709961, 68.07144927978516]\n",
      "\n",
      "Instance 99 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "fluent at index 15: [-0.09586840867996216, 1.6611465215682983, 2.2185323238372803, 0.2452593743801117, -1.6268994808197021]\n",
      "Grand sum of 84 tensor sets is: [-26.546281814575195, 124.16384887695312, 63.19637680053711, 64.68778991699219, 66.44454956054688]\n",
      "\n",
      "Instance 100 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "fluent at index 13: [-0.13804589211940765, 2.874521493911743, 2.0004189014434814, -0.24563530087471008, 1.2723448276519775]\n",
      "Grand sum of 85 tensor sets is: [-26.684328079223633, 127.03836822509766, 65.19679260253906, 64.44215393066406, 67.7168960571289]\n",
      "\n",
      "Instance 101 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "fluent at index 11: [0.2321082055568695, 1.358210563659668, -0.36157354712486267, 1.8967478275299072, 1.963590383529663]\n",
      "Grand sum of 86 tensor sets is: [-26.452219009399414, 128.39657592773438, 64.83522033691406, 66.33890533447266, 69.68048858642578]\n",
      "\n",
      "Instance 102 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "fluent at index 8: [-1.3456345796585083, 2.1577658653259277, 0.8611447811126709, 1.1173386573791504, 1.6828014850616455]\n",
      "Grand sum of 87 tensor sets is: [-27.797853469848633, 130.55433654785156, 65.69636535644531, 67.45624542236328, 71.36328887939453]\n",
      "\n",
      "Instance 103 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "fluent at index 9: [-0.9087888598442078, 3.1051859855651855, 1.6943943500518799, 2.0286426544189453, 2.0256943702697754]\n",
      "Grand sum of 88 tensor sets is: [-28.706642150878906, 133.65951538085938, 67.39076232910156, 69.4848861694336, 73.38898468017578]\n",
      "\n",
      "Instance 104 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "fluent at index 4: [-1.4937039613723755, 2.7230491638183594, 1.9638550281524658, 0.566473126411438, -0.06980177760124207]\n",
      "Grand sum of 89 tensor sets is: [-30.200345993041992, 136.382568359375, 69.3546142578125, 70.05136108398438, 73.31918334960938]\n",
      "\n",
      "Instance 105 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "fluent at index 6: [0.9835765957832336, -0.9611911177635193, -1.6135437488555908, 1.5261627435684204, 0.8361374139785767]\n",
      "Grand sum of 90 tensor sets is: [-29.21677017211914, 135.42137145996094, 67.74107360839844, 71.57752227783203, 74.15531921386719]\n",
      "\n",
      "Instance 106 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 107 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "fluent at index 16: [-0.1851160228252411, 2.66056489944458, 1.327040195465088, 0.611441969871521, 1.250622272491455]\n",
      "Grand sum of 91 tensor sets is: [-29.401885986328125, 138.08193969726562, 69.068115234375, 72.18896484375, 75.40594482421875]\n",
      "\n",
      "Instance 108 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 92 tensor sets is: [-29.234359741210938, 138.673583984375, 68.72303009033203, 72.66281127929688, 77.6651382446289]\n",
      "\n",
      "Instance 109 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 93 tensor sets is: [-29.958833694458008, 141.07125854492188, 68.59920501708984, 75.39422607421875, 79.67110443115234]\n",
      "\n",
      "Instance 110 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 22: [-0.6640641093254089, 2.291525363922119, 1.8893910646438599, 1.0164276361465454, 2.9339635372161865]\n",
      "Grand sum of 94 tensor sets is: [-30.62289810180664, 143.36277770996094, 70.48859405517578, 76.41065216064453, 82.60506439208984]\n",
      "\n",
      "Instance 111 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "fluent at index 16: [-0.5125834941864014, 2.458622694015503, 2.1225929260253906, 1.2053548097610474, -0.38213062286376953]\n",
      "Grand sum of 95 tensor sets is: [-31.135480880737305, 145.82139587402344, 72.61119079589844, 77.61600494384766, 82.22293090820312]\n",
      "\n",
      "Instance 112 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([81, 13, 768])\n",
      "Shape of summed layers is: 81 x 768\n",
      "fluent at index 20: [0.7924511432647705, 1.729933500289917, 0.8398063778877258, -1.520201325416565, 4.514045715332031]\n",
      "Grand sum of 96 tensor sets is: [-30.343029022216797, 147.55133056640625, 73.45099639892578, 76.0958023071289, 86.73697662353516]\n",
      "\n",
      "Instance 113 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 15: [-0.29310858249664307, 1.950860857963562, 1.5759657621383667, -0.20897984504699707, -2.9345648288726807]\n",
      "Grand sum of 97 tensor sets is: [-30.636137008666992, 149.502197265625, 75.02696228027344, 75.88682556152344, 83.80241394042969]\n",
      "\n",
      "Instance 114 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 98 tensor sets is: [-31.360610961914062, 151.89987182617188, 74.90313720703125, 78.61824035644531, 85.80838012695312]\n",
      "\n",
      "Instance 115 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "fluent at index 11: [-1.6058967113494873, 3.1375234127044678, 1.458402395248413, 1.633338212966919, 1.5393911600112915]\n",
      "Grand sum of 99 tensor sets is: [-32.96650695800781, 155.0373992919922, 76.36154174804688, 80.25157928466797, 87.34777069091797]\n",
      "\n",
      "Instance 116 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "fluent at index 14: [-1.0993092060089111, 2.8122732639312744, 2.175287961959839, 1.1573150157928467, 0.2980687916278839]\n",
      "Grand sum of 100 tensor sets is: [-34.06581497192383, 157.84967041015625, 78.53682708740234, 81.40889739990234, 87.64583587646484]\n",
      "\n",
      "Instance 117 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "fluent at index 22: [-0.8647012114524841, 2.117614269256592, 2.2457470893859863, 0.5453540086746216, 0.41801080107688904]\n",
      "Grand sum of 101 tensor sets is: [-34.93051528930664, 159.96728515625, 80.78257751464844, 81.95425415039062, 88.06385040283203]\n",
      "\n",
      "Instance 118 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "fluent at index 14: [-0.06278517842292786, 1.0252313613891602, -0.0830167829990387, -0.10219891369342804, -1.6470890045166016]\n",
      "Grand sum of 102 tensor sets is: [-34.99330139160156, 160.99252319335938, 80.6995620727539, 81.85205841064453, 86.41676330566406]\n",
      "\n",
      "Instance 119 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "fluent at index 19: [-0.5431798100471497, 2.0589652061462402, 0.8039119243621826, -0.07741200178861618, -0.7763499021530151]\n",
      "Grand sum of 103 tensor sets is: [-35.53647994995117, 163.05148315429688, 81.50347137451172, 81.77464294433594, 85.64041137695312]\n",
      "\n",
      "Instance 120 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 104 tensor sets is: [-35.368953704833984, 163.64312744140625, 81.15838623046875, 82.24848937988281, 87.89960479736328]\n",
      "\n",
      "Instance 121 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3, 10]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "fluent at index 3: [-0.6390718221664429, 0.344703733921051, 1.071596384048462, 0.9618670344352722, 0.854989767074585]\n",
      "fluent at index 10: [-0.8926980495452881, -0.9657371044158936, 0.5143038034439087, -2.413525342941284, 2.459209442138672]\n",
      "Grand sum of 105 tensor sets is: [-36.13483810424805, 163.33261108398438, 81.95133972167969, 81.52265930175781, 89.55670166015625]\n",
      "\n",
      "Instance 122 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 123 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "fluent at index 25: [-0.6763156652450562, 1.3041893243789673, 1.185732364654541, -0.2492862194776535, -1.8063145875930786]\n",
      "Grand sum of 106 tensor sets is: [-36.811153411865234, 164.6367950439453, 83.13706970214844, 81.27337646484375, 87.7503890991211]\n",
      "\n",
      "Instance 124 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "fluent at index 3: [-0.4228687286376953, 0.6854337453842163, 2.0413570404052734, 0.5372746586799622, 0.7996396422386169]\n",
      "Grand sum of 107 tensor sets is: [-37.23402404785156, 165.32223510742188, 85.17842864990234, 81.81065368652344, 88.5500259399414]\n",
      "\n",
      "Instance 125 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [78]\n",
      "Size of token embeddings is torch.Size([82, 13, 768])\n",
      "Shape of summed layers is: 82 x 768\n",
      "fluent at index 78: [-0.1809825897216797, 3.3441274166107178, 1.3963679075241089, 0.13129837810993195, -2.796231508255005]\n",
      "Grand sum of 108 tensor sets is: [-37.415008544921875, 168.66636657714844, 86.57479858398438, 81.94195556640625, 85.75379180908203]\n",
      "\n",
      "Instance 126 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 109 tensor sets is: [-37.24748229980469, 169.2580108642578, 86.2297134399414, 82.41580200195312, 88.01298522949219]\n",
      "\n",
      "Instance 127 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "fluent at index 9: [-0.4508342742919922, 1.6490414142608643, 0.7226027250289917, -0.3326590955257416, -1.1303879022598267]\n",
      "Grand sum of 110 tensor sets is: [-37.69831848144531, 170.9070587158203, 86.95231628417969, 82.08314514160156, 86.88259887695312]\n",
      "\n",
      "Instance 128 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 111 tensor sets is: [-37.530792236328125, 171.4987030029297, 86.60723114013672, 82.55699157714844, 89.14179229736328]\n",
      "\n",
      "Instance 129 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "fluent at index 7: [0.6732279062271118, -0.6399404406547546, 0.7670798301696777, 0.18160060048103333, -0.7638430595397949]\n",
      "Grand sum of 112 tensor sets is: [-36.85756301879883, 170.8587646484375, 87.37431335449219, 82.73859405517578, 88.3779525756836]\n",
      "\n",
      "Instance 130 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 113 tensor sets is: [-36.69003677368164, 171.45040893554688, 87.02922821044922, 83.21244049072266, 90.63714599609375]\n",
      "\n",
      "Instance 131 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "fluent at index 23: [-1.646203637123108, 2.3946313858032227, 2.5032758712768555, 1.8938928842544556, -0.7268512845039368]\n",
      "Grand sum of 114 tensor sets is: [-38.336238861083984, 173.8450469970703, 89.53250122070312, 85.10633087158203, 89.91029357910156]\n",
      "\n",
      "Instance 132 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 5: [0.32787469029426575, 0.4520457088947296, 0.17048346996307373, 0.5068720579147339, 0.0579964816570282]\n",
      "Grand sum of 115 tensor sets is: [-38.008365631103516, 174.29708862304688, 89.70298767089844, 85.61320495605469, 89.96829223632812]\n",
      "\n",
      "Instance 133 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "fluent at index 15: [-0.2189542055130005, 2.1441500186920166, 1.6429088115692139, 0.10328660905361176, -1.3075922727584839]\n",
      "Grand sum of 116 tensor sets is: [-38.22732162475586, 176.4412384033203, 91.34589385986328, 85.71649169921875, 88.66069793701172]\n",
      "\n",
      "Instance 134 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 117 tensor sets is: [-38.9517936706543, 178.8389129638672, 91.2220687866211, 88.44790649414062, 90.66666412353516]\n",
      "\n",
      "Instance 135 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "fluent at index 5: [-0.2095697969198227, 1.6003130674362183, 0.5689906477928162, 0.724713146686554, -1.1927614212036133]\n",
      "Grand sum of 118 tensor sets is: [-39.16136169433594, 180.43922424316406, 91.79106140136719, 89.17262268066406, 89.4738998413086]\n",
      "\n",
      "Instance 136 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "fluent at index 6: [-0.815117597579956, 1.0887919664382935, 0.9104069471359253, -0.33367472887039185, 0.6033973693847656]\n",
      "Grand sum of 119 tensor sets is: [-39.976478576660156, 181.52801513671875, 92.70146942138672, 88.83895111083984, 90.07730102539062]\n",
      "\n",
      "Instance 137 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "fluent at index 33: [-1.044844627380371, 2.447918653488159, 0.5147659778594971, 0.5324634313583374, 0.3112243711948395]\n",
      "Grand sum of 120 tensor sets is: [-41.021324157714844, 183.97593688964844, 93.21623229980469, 89.37141418457031, 90.3885269165039]\n",
      "\n",
      "Instance 138 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [176, 180, 184, 188, 192, 196, 202, 206, 210, 214, 218, 222, 226]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "fluent at index 176: [-0.2781304121017456, -0.9568163752555847, 0.815104067325592, -1.2265424728393555, -1.024113655090332]\n",
      "fluent at index 180: [-0.9592005014419556, -0.826110303401947, 0.7938107252120972, -0.16530364751815796, -2.622509717941284]\n",
      "fluent at index 184: [-0.6799860596656799, -0.8976505994796753, 1.1792060136795044, -0.9781264066696167, -2.4786016941070557]\n",
      "fluent at index 188: [-0.30206355452537537, -1.070599913597107, 0.9305887222290039, -1.0859272480010986, -2.9776434898376465]\n",
      "fluent at index 192: [-0.5685569047927856, -1.0030072927474976, 1.034042477607727, -1.1840379238128662, -2.5455644130706787]\n",
      "fluent at index 196: [-0.41967445611953735, -1.1575614213943481, 0.9939411282539368, -1.7493798732757568, -2.228937864303589]\n",
      "fluent at index 202: [-0.23844857513904572, -1.1215471029281616, 1.05048406124115, -2.076416015625, -2.19720721244812]\n",
      "fluent at index 206: [-0.5621075630187988, -0.6042888760566711, 0.8420152068138123, -2.109417676925659, -2.2066099643707275]\n",
      "fluent at index 210: [-0.6410033702850342, -0.6974570751190186, 0.9750257134437561, -2.4749772548675537, -2.42688250541687]\n",
      "fluent at index 214: [-0.6076511144638062, -0.8420413136482239, 0.9135690331459045, -2.4008193016052246, -3.1421656608581543]\n",
      "fluent at index 218: [-0.8630630970001221, -0.7759515047073364, 1.1067320108413696, -2.191837787628174, -2.036050319671631]\n",
      "fluent at index 222: [-0.8120923042297363, -0.7250651121139526, 1.2963390350341797, -2.686440944671631, -1.8748762607574463]\n",
      "fluent at index 226: [-0.9053555727005005, -1.36150062084198, 0.8893222212791443, -2.475541114807129, -2.6616320610046387]\n",
      "Grand sum of 121 tensor sets is: [-41.62419509887695, 183.04981994628906, 94.20240020751953, 87.61720275878906, 88.04830932617188]\n",
      "\n",
      "Instance 139 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [463]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "fluent at index 463: [-0.6438023447990417, 1.2844525575637817, 1.4079830646514893, 0.6087514162063599, -2.910813331604004]\n",
      "Grand sum of 122 tensor sets is: [-42.26799774169922, 184.3342742919922, 95.61038208007812, 88.2259521484375, 85.13749694824219]\n",
      "\n",
      "Instance 140 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "fluent at index 3: [-0.6910665035247803, 2.078314781188965, 2.0409302711486816, -0.5250962376594543, -0.07399791479110718]\n",
      "Grand sum of 123 tensor sets is: [-42.95906448364258, 186.41258239746094, 97.65131378173828, 87.70085906982422, 85.0634994506836]\n",
      "\n",
      "Instance 141 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([339, 13, 768])\n",
      "Shape of summed layers is: 339 x 768\n",
      "fluent at index 15: [-0.37391841411590576, 1.5917651653289795, 1.687656044960022, 0.8270617127418518, -0.5558464527130127]\n",
      "Grand sum of 124 tensor sets is: [-43.33298110961914, 188.0043487548828, 99.3389663696289, 88.52792358398438, 84.50765228271484]\n",
      "\n",
      "Instance 142 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "fluent at index 14: [0.35622042417526245, 1.5917737483978271, 0.7196696996688843, -1.9798436164855957, -0.18103301525115967]\n",
      "Grand sum of 125 tensor sets is: [-42.97676086425781, 189.5961151123047, 100.05863952636719, 86.54808044433594, 84.32662200927734]\n",
      "\n",
      "Instance 143 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 126 tensor sets is: [-43.70123291015625, 191.99378967285156, 99.934814453125, 89.27949523925781, 86.33258819580078]\n",
      "\n",
      "Instance 144 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "fluent at index 3: [-0.6910665035247803, 2.078314781188965, 2.0409302711486816, -0.5250962376594543, -0.07399791479110718]\n",
      "Grand sum of 127 tensor sets is: [-44.39229965209961, 194.0720977783203, 101.97574615478516, 88.75440216064453, 86.25859069824219]\n",
      "\n",
      "Instance 145 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 11: [-0.09141099452972412, -0.07150345295667648, 0.07617591321468353, 0.8562110662460327, -0.28080829977989197]\n",
      "Grand sum of 128 tensor sets is: [-44.48371124267578, 194.00059509277344, 102.05192565917969, 89.61061096191406, 85.977783203125]\n",
      "\n",
      "Instance 146 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "fluent at index 11: [-0.8104173541069031, 1.203636884689331, 1.4527394771575928, 1.0111770629882812, -0.33124202489852905]\n",
      "Grand sum of 129 tensor sets is: [-45.29412841796875, 195.20423889160156, 103.5046615600586, 90.62178802490234, 85.64653778076172]\n",
      "\n",
      "Instance 147 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 130 tensor sets is: [-46.01860046386719, 197.60191345214844, 103.3808364868164, 93.35320281982422, 87.65250396728516]\n",
      "\n",
      "Instance 148 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 149 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "fluent at index 6: [-0.7448487281799316, 1.1273616552352905, 1.4557952880859375, 0.04476897418498993, -1.0633001327514648]\n",
      "Grand sum of 131 tensor sets is: [-46.763450622558594, 198.72927856445312, 104.83663177490234, 93.3979721069336, 86.58920288085938]\n",
      "\n",
      "Instance 150 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "fluent at index 8: [-0.60541832447052, 2.164360761642456, 1.0725412368774414, -1.3174437284469604, -1.0485694408416748]\n",
      "Grand sum of 132 tensor sets is: [-47.36886978149414, 200.89364624023438, 105.90917205810547, 92.08052825927734, 85.54063415527344]\n",
      "\n",
      "Instance 151 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "fluent at index 3: [-0.2688046395778656, 1.6474624872207642, 2.3769662380218506, 0.6399873495101929, -0.9274413585662842]\n",
      "Grand sum of 133 tensor sets is: [-47.63767623901367, 202.54110717773438, 108.28614044189453, 92.72051239013672, 84.61318969726562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 152 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "fluent at index 28: [0.2846338748931885, 1.3612931966781616, 1.9480708837509155, -0.7731233835220337, -1.4045027494430542]\n",
      "Grand sum of 134 tensor sets is: [-47.35304260253906, 203.90240478515625, 110.23421478271484, 91.9473876953125, 83.20868682861328]\n",
      "\n",
      "Instance 153 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "fluent at index 27: [-0.9347012639045715, 2.114551305770874, 1.9283465147018433, -0.7281702756881714, -3.2033321857452393]\n",
      "Grand sum of 135 tensor sets is: [-48.287742614746094, 206.01695251464844, 112.16255950927734, 91.2192153930664, 80.00535583496094]\n",
      "\n",
      "Instance 154 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "fluent at index 11: [-1.6058967113494873, 3.1375234127044678, 1.458402395248413, 1.633338212966919, 1.5393911600112915]\n",
      "Grand sum of 136 tensor sets is: [-49.893638610839844, 209.15447998046875, 113.62096405029297, 92.85255432128906, 81.54474639892578]\n",
      "\n",
      "Instance 155 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 137 tensor sets is: [-49.726112365722656, 209.74612426757812, 113.27587890625, 93.32640075683594, 83.80393981933594]\n",
      "\n",
      "Instance 156 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "fluent at index 6: [-0.4796690344810486, -0.36462584137916565, -0.39829424023628235, 1.7128448486328125, -0.9795153737068176]\n",
      "Grand sum of 138 tensor sets is: [-50.205780029296875, 209.38150024414062, 112.8775863647461, 95.03924560546875, 82.82442474365234]\n",
      "\n",
      "Instance 157 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "fluent at index 3: [0.1566983014345169, 0.9957194328308105, 0.7128677368164062, 0.42032647132873535, 0.30084678530693054]\n",
      "Grand sum of 139 tensor sets is: [-50.04907989501953, 210.37721252441406, 113.5904541015625, 95.4595718383789, 83.12527465820312]\n",
      "\n",
      "Instance 158 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "fluent at index 3: [0.22030779719352722, 0.6899853348731995, 1.1574859619140625, 0.9094274640083313, -0.7685007452964783]\n",
      "Grand sum of 140 tensor sets is: [-49.828773498535156, 211.06719970703125, 114.74794006347656, 96.3689956665039, 82.35677337646484]\n",
      "\n",
      "Instance 159 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 141 tensor sets is: [-50.553245544433594, 213.46487426757812, 114.62411499023438, 99.10041046142578, 84.36273956298828]\n",
      "\n",
      "Instance 160 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "fluent at index 11: [-0.41912028193473816, 1.6920926570892334, 1.6984132528305054, 0.43373924493789673, -0.3359917998313904]\n",
      "Grand sum of 142 tensor sets is: [-50.97236633300781, 215.15696716308594, 116.32252502441406, 99.53414916992188, 84.02674865722656]\n",
      "\n",
      "Instance 161 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "fluent at index 7: [-1.6700704097747803, 2.8388941287994385, 1.5016274452209473, 0.8590525984764099, 1.1821386814117432]\n",
      "Grand sum of 143 tensor sets is: [-52.64243698120117, 217.99586486816406, 117.82415008544922, 100.39320373535156, 85.2088851928711]\n",
      "\n",
      "Instance 162 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "fluent at index 37: [-0.07620850205421448, 2.732729911804199, 1.8514399528503418, 2.53787899017334, 2.4092161655426025]\n",
      "Grand sum of 144 tensor sets is: [-52.71864700317383, 220.7285919189453, 119.67559051513672, 102.93108367919922, 87.61810302734375]\n",
      "\n",
      "Instance 163 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 145 tensor sets is: [-53.443119049072266, 223.1262664794922, 119.55176544189453, 105.6624984741211, 89.62406921386719]\n",
      "\n",
      "Instance 164 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 165 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 166 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "fluent at index 14: [-0.9155265092849731, 2.585352897644043, 0.7472262978553772, 1.4089672565460205, 2.0380802154541016]\n",
      "Grand sum of 146 tensor sets is: [-54.358646392822266, 225.7116241455078, 120.29898834228516, 107.07146453857422, 91.66214752197266]\n",
      "\n",
      "Instance 167 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "fluent at index 11: [0.2581942677497864, 1.839301586151123, 1.3929729461669922, 1.0513966083526611, 3.596914768218994]\n",
      "Grand sum of 147 tensor sets is: [-54.1004524230957, 227.55091857910156, 121.69196319580078, 108.12286376953125, 95.25906372070312]\n",
      "\n",
      "Instance 168 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "fluent at index 2: [-0.10938827693462372, 0.07569216191768646, 0.6434135437011719, 2.393195390701294, -2.6649491786956787]\n",
      "Grand sum of 148 tensor sets is: [-54.2098388671875, 227.62661743164062, 122.33537292480469, 110.51605987548828, 92.5941162109375]\n",
      "\n",
      "Instance 169 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "fluent at index 22: [-0.5090913772583008, 2.044909954071045, 1.4767467975616455, 1.0326560735702515, -1.4800705909729004]\n",
      "Grand sum of 149 tensor sets is: [-54.718929290771484, 229.67152404785156, 123.81211853027344, 111.54871368408203, 91.11404418945312]\n",
      "\n",
      "Instance 170 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "fluent at index 17: [-0.943183422088623, 1.6320773363113403, 2.499915838241577, -0.11541762948036194, -0.614593505859375]\n",
      "Grand sum of 150 tensor sets is: [-55.662113189697266, 231.30360412597656, 126.3120346069336, 111.43329620361328, 90.49945068359375]\n",
      "\n",
      "Instance 171 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 151 tensor sets is: [-56.3865852355957, 233.70127868652344, 126.1882095336914, 114.16471099853516, 92.50541687011719]\n",
      "\n",
      "Instance 172 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 152 tensor sets is: [-56.219058990478516, 234.2929229736328, 125.84312438964844, 114.63855743408203, 94.76461029052734]\n",
      "\n",
      "Instance 173 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([325, 13, 768])\n",
      "Shape of summed layers is: 325 x 768\n",
      "fluent at index 65: [-0.9779078364372253, 2.636883497238159, 0.8321911096572876, 2.340053081512451, 1.6902687549591064]\n",
      "Grand sum of 153 tensor sets is: [-57.19696807861328, 236.9298095703125, 126.6753158569336, 116.97860717773438, 96.45487976074219]\n",
      "\n",
      "Instance 174 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([239, 13, 768])\n",
      "Shape of summed layers is: 239 x 768\n",
      "fluent at index 135: [-0.5130208730697632, 0.8297559022903442, 1.6240720748901367, 0.039376333355903625, -2.2294552326202393]\n",
      "Grand sum of 154 tensor sets is: [-57.70998764038086, 237.7595672607422, 128.2993927001953, 117.01798248291016, 94.22542572021484]\n",
      "\n",
      "Instance 175 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "fluent at index 20: [-0.8725026845932007, 2.797415018081665, 0.2305719554424286, 0.9236805438995361, -0.1794048249721527]\n",
      "Grand sum of 155 tensor sets is: [-58.582489013671875, 240.55697631835938, 128.52996826171875, 117.94166564941406, 94.0460205078125]\n",
      "\n",
      "Instance 176 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "fluent at index 16: [-1.1254351139068604, 2.4702813625335693, 1.4200072288513184, 1.1440258026123047, 0.11307822167873383]\n",
      "Grand sum of 156 tensor sets is: [-59.707923889160156, 243.02725219726562, 129.94998168945312, 119.085693359375, 94.15909576416016]\n",
      "\n",
      "Instance 177 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 178 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "fluent at index 6: [-0.7012110948562622, 2.7823121547698975, 1.4135639667510986, 1.499273419380188, 0.5012944936752319]\n",
      "Grand sum of 157 tensor sets is: [-60.40913391113281, 245.8095703125, 131.36354064941406, 120.58496856689453, 94.66039276123047]\n",
      "\n",
      "Instance 179 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "fluent at index 3: [-1.0379666090011597, 0.46645069122314453, 0.19277431070804596, 1.3444404602050781, -0.4724445939064026]\n",
      "Grand sum of 158 tensor sets is: [-61.44710159301758, 246.27601623535156, 131.5563201904297, 121.92941284179688, 94.18795013427734]\n",
      "\n",
      "Instance 180 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "fluent at index 8: [-0.07690104097127914, 1.0646823644638062, 1.569579839706421, -0.763806164264679, -1.7878717184066772]\n",
      "Grand sum of 159 tensor sets is: [-61.52400207519531, 247.3406982421875, 133.1259002685547, 121.16560363769531, 92.40007781982422]\n",
      "\n",
      "Instance 181 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "fluent at index 23: [-0.22636666893959045, 1.9675476551055908, 0.4587710499763489, -0.10226105153560638, 0.3039170801639557]\n",
      "Grand sum of 160 tensor sets is: [-61.750370025634766, 249.30824279785156, 133.5846710205078, 121.06333923339844, 92.70399475097656]\n",
      "\n",
      "Instance 182 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "fluent at index 7: [0.19611281156539917, 0.7208666205406189, 0.1095815896987915, 0.2026892602443695, -0.1296912431716919]\n",
      "Grand sum of 161 tensor sets is: [-61.554256439208984, 250.02911376953125, 133.6942596435547, 121.26602935791016, 92.57430267333984]\n",
      "\n",
      "Instance 183 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 184 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "fluent at index 18: [-1.3565970659255981, 2.884974956512451, 1.2380176782608032, 2.3258023262023926, 0.5483405590057373]\n",
      "Grand sum of 162 tensor sets is: [-62.91085433959961, 252.91409301757812, 134.93228149414062, 123.59183502197266, 93.12264251708984]\n",
      "\n",
      "Instance 185 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "fluent at index 7: [-1.6554491519927979, 1.2199374437332153, 0.5602234601974487, 2.363924741744995, -1.0134308338165283]\n",
      "Grand sum of 163 tensor sets is: [-64.5663070678711, 254.134033203125, 135.4925079345703, 125.95575714111328, 92.10921478271484]\n",
      "\n",
      "Instance 186 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "fluent at index 11: [-0.5135263800621033, 3.4003586769104004, 1.2950488328933716, 0.6730057001113892, -0.08438310027122498]\n",
      "Grand sum of 164 tensor sets is: [-65.079833984375, 257.5343933105469, 136.7875518798828, 126.6287612915039, 92.02483367919922]\n",
      "\n",
      "Instance 187 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "fluent at index 4: [-1.4937039613723755, 2.7230491638183594, 1.9638550281524658, 0.566473126411438, -0.06980177760124207]\n",
      "Grand sum of 165 tensor sets is: [-66.57353973388672, 260.2574462890625, 138.75140380859375, 127.19523620605469, 91.95503234863281]\n",
      "\n",
      "Instance 188 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 166 tensor sets is: [-66.40601348876953, 260.8490905761719, 138.40631103515625, 127.66908264160156, 94.21422576904297]\n",
      "\n",
      "Instance 189 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "fluent at index 10: [0.062036290764808655, 1.8690439462661743, 0.8209178447723389, 0.5355299115180969, -0.2638755440711975]\n",
      "Grand sum of 167 tensor sets is: [-66.34397888183594, 262.7181396484375, 139.22723388671875, 128.20460510253906, 93.95034790039062]\n",
      "\n",
      "Instance 190 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "fluent at index 3: [0.1555183231830597, 0.4839668273925781, 1.5968825817108154, -0.8414345979690552, 0.06009417772293091]\n",
      "Grand sum of 168 tensor sets is: [-66.18846130371094, 263.2021179199219, 140.82411193847656, 127.36316680908203, 94.01044464111328]\n",
      "\n",
      "Instance 191 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "fluent at index 3: [-0.6910665035247803, 2.078314781188965, 2.0409302711486816, -0.5250962376594543, -0.07399791479110718]\n",
      "Grand sum of 169 tensor sets is: [-66.87952423095703, 265.2804260253906, 142.8650360107422, 126.83807373046875, 93.93644714355469]\n",
      "\n",
      "Instance 192 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 170 tensor sets is: [-67.60399627685547, 267.6781005859375, 142.7412109375, 129.56948852539062, 95.94241333007812]\n",
      "\n",
      "Instance 193 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 7: [-0.672227144241333, 1.744196891784668, 0.41649192571640015, -0.2808167338371277, -2.474720001220703]\n",
      "Grand sum of 171 tensor sets is: [-68.2762222290039, 269.42230224609375, 143.15769958496094, 129.28866577148438, 93.46769714355469]\n",
      "\n",
      "Instance 194 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 172 tensor sets is: [-69.00069427490234, 271.8199768066406, 143.03387451171875, 132.02008056640625, 95.47366333007812]\n",
      "\n",
      "Instance 195 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 196 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 11: [-1.1478819847106934, 2.7302086353302, 1.6893302202224731, 2.098822832107544, 1.2484140396118164]\n",
      "Grand sum of 173 tensor sets is: [-70.14857482910156, 274.5501708984375, 144.72320556640625, 134.118896484375, 96.72207641601562]\n",
      "\n",
      "Instance 197 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 20: [0.32567480206489563, 1.241713047027588, 1.0614731311798096, -0.32349085807800293, -0.7302277684211731]\n",
      "Grand sum of 174 tensor sets is: [-69.8228988647461, 275.7918701171875, 145.78468322753906, 133.79541015625, 95.99185180664062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 198 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 199 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "fluent at index 10: [0.062036290764808655, 1.8690439462661743, 0.8209178447723389, 0.5355299115180969, -0.2638755440711975]\n",
      "Grand sum of 175 tensor sets is: [-69.7608642578125, 277.6609191894531, 146.60560607910156, 134.3309326171875, 95.72797393798828]\n",
      "\n",
      "Instance 200 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "fluent at index 39: [0.04264812916517258, 2.202641248703003, 1.8462371826171875, -0.2981390953063965, -0.4149801731109619]\n",
      "Grand sum of 176 tensor sets is: [-69.71821594238281, 279.8635559082031, 148.45184326171875, 134.0327911376953, 95.31299591064453]\n",
      "\n",
      "Instance 201 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "fluent at index 14: [-1.145232915878296, 0.7501708269119263, 0.11611055582761765, 0.9194188117980957, 3.3519115447998047]\n",
      "Grand sum of 177 tensor sets is: [-70.86344909667969, 280.6137390136719, 148.5679473876953, 134.95220947265625, 98.66490936279297]\n",
      "\n",
      "Instance 202 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 19: [-0.7330878973007202, 2.9773895740509033, 0.34422025084495544, 1.5950552225112915, 0.1598040759563446]\n",
      "Grand sum of 178 tensor sets is: [-71.5965347290039, 283.59112548828125, 148.91217041015625, 136.54727172851562, 98.82471466064453]\n",
      "\n",
      "Instance 203 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "fluent at index 8: [-0.6722137928009033, 1.2240641117095947, 0.8690001964569092, -1.6122088432312012, -0.12656930088996887]\n",
      "Grand sum of 179 tensor sets is: [-72.26874542236328, 284.815185546875, 149.7811737060547, 134.93505859375, 98.6981430053711]\n",
      "\n",
      "Instance 204 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "fluent at index 7: [-0.5190034508705139, 0.7688255906105042, 0.9474347233772278, -0.9436076879501343, -1.4220739603042603]\n",
      "Grand sum of 180 tensor sets is: [-72.78775024414062, 285.5840148925781, 150.72860717773438, 133.991455078125, 97.27606964111328]\n",
      "\n",
      "Instance 205 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "fluent at index 15: [-0.771389365196228, 1.823752999305725, 0.7696713209152222, 1.2611379623413086, -0.7106645107269287]\n",
      "Grand sum of 181 tensor sets is: [-73.55914306640625, 287.40777587890625, 151.49827575683594, 135.25259399414062, 96.5654067993164]\n",
      "\n",
      "Instance 206 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 16: [0.009264426305890083, 0.596274733543396, 1.4317296743392944, 1.5392873287200928, -0.20548254251480103]\n",
      "Grand sum of 182 tensor sets is: [-73.54988098144531, 288.0040588378906, 152.9300079345703, 136.79188537597656, 96.35992431640625]\n",
      "\n",
      "Instance 207 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [104]\n",
      "Size of token embeddings is torch.Size([111, 13, 768])\n",
      "Shape of summed layers is: 111 x 768\n",
      "fluent at index 104: [-0.9928011894226074, 2.6513049602508545, 1.1878079175949097, 1.07876455783844, 0.6248687505722046]\n",
      "Grand sum of 183 tensor sets is: [-74.54267883300781, 290.6553649902344, 154.11781311035156, 137.8706512451172, 96.98479461669922]\n",
      "\n",
      "Instance 208 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "fluent at index 10: [-0.31170859932899475, 2.2781474590301514, 1.7930864095687866, 1.1040540933609009, 1.5407817363739014]\n",
      "Grand sum of 184 tensor sets is: [-74.85438537597656, 292.9335021972656, 155.91090393066406, 138.97470092773438, 98.52557373046875]\n",
      "\n",
      "Instance 209 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "fluent at index 9: [-0.17045089602470398, 1.8862074613571167, 0.025247592478990555, 2.0183541774749756, -0.21508267521858215]\n",
      "Grand sum of 185 tensor sets is: [-75.02483367919922, 294.8197021484375, 155.9361572265625, 140.99305725097656, 98.31049346923828]\n",
      "\n",
      "Instance 210 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 21: [-0.43938687443733215, 2.023350715637207, 0.023818209767341614, 1.07766592502594, -0.6382115483283997]\n",
      "Grand sum of 186 tensor sets is: [-75.46421813964844, 296.8430480957031, 155.95997619628906, 142.0707244873047, 97.67227935791016]\n",
      "\n",
      "Instance 211 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "fluent at index 6: [-0.6901421546936035, 1.7201801538467407, 1.9995750188827515, -0.31028062105178833, -0.47742390632629395]\n",
      "Grand sum of 187 tensor sets is: [-76.15435791015625, 298.563232421875, 157.9595489501953, 141.76043701171875, 97.19485473632812]\n",
      "\n",
      "Instance 212 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 9: [-0.473206490278244, 0.7734866142272949, 0.6797366142272949, 1.6513736248016357, -0.7089434862136841]\n",
      "Grand sum of 188 tensor sets is: [-76.6275634765625, 299.33673095703125, 158.6392822265625, 143.41180419921875, 96.48590850830078]\n",
      "\n",
      "Instance 213 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 214 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 215 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "fluent at index 9: [0.10959511250257492, 1.8959729671478271, 0.3334636688232422, 0.44567403197288513, 1.6827797889709473]\n",
      "Grand sum of 189 tensor sets is: [-76.5179672241211, 301.2326965332031, 158.97274780273438, 143.85748291015625, 98.16868591308594]\n",
      "\n",
      "Instance 216 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 190 tensor sets is: [-77.24243927001953, 303.63037109375, 158.8489227294922, 146.58889770507812, 100.17465209960938]\n",
      "\n",
      "Instance 217 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 8: [-1.8727253675460815, 2.7080795764923096, 1.771540641784668, 0.7096219062805176, 1.7477670907974243]\n",
      "Grand sum of 191 tensor sets is: [-79.11516571044922, 306.33843994140625, 160.62046813964844, 147.29852294921875, 101.92241668701172]\n",
      "\n",
      "Instance 218 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "fluent at index 11: [-0.4553775191307068, 0.2360820174217224, 0.7123676538467407, 0.798224925994873, -1.7336232662200928]\n",
      "Grand sum of 192 tensor sets is: [-79.57054138183594, 306.57452392578125, 161.3328399658203, 148.09674072265625, 100.18879699707031]\n",
      "\n",
      "Instance 219 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "fluent at index 19: [0.6510859131813049, 1.3317729234695435, 0.4164307713508606, -0.24348270893096924, 1.129040241241455]\n",
      "Grand sum of 193 tensor sets is: [-78.9194564819336, 307.90631103515625, 161.749267578125, 147.85325622558594, 101.31784057617188]\n",
      "\n",
      "Instance 220 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "fluent at index 21: [-0.3385103940963745, 1.440087080001831, 0.1915278285741806, 0.1638856828212738, -0.325092077255249]\n",
      "Grand sum of 194 tensor sets is: [-79.25796508789062, 309.3464050292969, 161.9407958984375, 148.0171356201172, 100.99275207519531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 221 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "fluent at index 6: [0.2222270369529724, 1.3280861377716064, 1.1462061405181885, -0.1737176477909088, -1.0523606538772583]\n",
      "Grand sum of 195 tensor sets is: [-79.03573608398438, 310.67449951171875, 163.08700561523438, 147.84341430664062, 99.94039154052734]\n",
      "\n",
      "Instance 222 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 21: [-0.43938687443733215, 2.023350715637207, 0.023818209767341614, 1.07766592502594, -0.6382115483283997]\n",
      "Grand sum of 196 tensor sets is: [-79.4751205444336, 312.6978454589844, 163.11082458496094, 148.92108154296875, 99.30217742919922]\n",
      "\n",
      "Instance 223 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "fluent at index 4: [-0.7343118786811829, 1.8212535381317139, 1.4935182332992554, 1.3293366432189941, 0.7721138000488281]\n",
      "Grand sum of 197 tensor sets is: [-80.20943450927734, 314.51910400390625, 164.60433959960938, 150.2504119873047, 100.07429504394531]\n",
      "\n",
      "Instance 224 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 225 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "fluent at index 9: [-0.2803017199039459, 1.9892325401306152, 1.4184094667434692, 0.4177549183368683, -0.3977794647216797]\n",
      "Grand sum of 198 tensor sets is: [-80.48973846435547, 316.5083312988281, 166.0227508544922, 150.6681671142578, 99.676513671875]\n",
      "\n",
      "Instance 226 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 227 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "fluent at index 3: [-0.030968453735113144, 0.9829655289649963, 2.122274398803711, 1.3029290437698364, -0.19842547178268433]\n",
      "Grand sum of 199 tensor sets is: [-80.52070617675781, 317.4913024902344, 168.14501953125, 151.97109985351562, 99.47808837890625]\n",
      "\n",
      "Instance 228 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "fluent at index 5: [-0.897228479385376, 2.989656686782837, -0.02505694329738617, 1.7585138082504272, 1.1813774108886719]\n",
      "Grand sum of 200 tensor sets is: [-81.41793823242188, 320.48095703125, 168.11996459960938, 153.7296142578125, 100.65946960449219]\n",
      "\n",
      "Instance 229 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "fluent at index 5: [-1.353732943534851, 1.716531753540039, 0.31949734687805176, 0.10603439807891846, 0.47357046604156494]\n",
      "Grand sum of 201 tensor sets is: [-82.77166748046875, 322.1974792480469, 168.43946838378906, 153.8356475830078, 101.13304138183594]\n",
      "\n",
      "Instance 230 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "fluent at index 3: [0.1555183231830597, 0.4839668273925781, 1.5968825817108154, -0.8414345979690552, 0.06009417772293091]\n",
      "Grand sum of 202 tensor sets is: [-82.61614990234375, 322.68145751953125, 170.03634643554688, 152.9942169189453, 101.1931381225586]\n",
      "\n",
      "Instance 231 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "fluent at index 5: [-0.9075266122817993, 1.1125481128692627, 1.562330961227417, 1.0039870738983154, 0.1734428107738495]\n",
      "Grand sum of 203 tensor sets is: [-83.52367401123047, 323.79400634765625, 171.5986785888672, 153.99819946289062, 101.3665771484375]\n",
      "\n",
      "Instance 232 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "fluent at index 11: [-0.015121199190616608, 1.7771668434143066, 1.110105037689209, 1.0444464683532715, 3.5383968353271484]\n",
      "Grand sum of 204 tensor sets is: [-83.5387954711914, 325.5711669921875, 172.7087860107422, 155.0426483154297, 104.90497589111328]\n",
      "\n",
      "Instance 233 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "fluent at index 21: [-0.3385103940963745, 1.440087080001831, 0.1915278285741806, 0.1638856828212738, -0.325092077255249]\n",
      "Grand sum of 205 tensor sets is: [-83.87730407714844, 327.0112609863281, 172.9003143310547, 155.20652770996094, 104.57988739013672]\n",
      "\n",
      "Instance 234 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 235 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "fluent at index 10: [0.062036290764808655, 1.8690439462661743, 0.8209178447723389, 0.5355299115180969, -0.2638755440711975]\n",
      "Grand sum of 206 tensor sets is: [-83.81526947021484, 328.88031005859375, 173.7212371826172, 155.74205017089844, 104.31600952148438]\n",
      "\n",
      "Instance 236 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "fluent at index 3: [-0.6910665035247803, 2.078314781188965, 2.0409302711486816, -0.5250962376594543, -0.07399791479110718]\n",
      "Grand sum of 207 tensor sets is: [-84.50633239746094, 330.9586181640625, 175.7621612548828, 155.21694946289062, 104.24201202392578]\n",
      "\n",
      "Instance 237 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([240, 13, 768])\n",
      "Shape of summed layers is: 240 x 768\n",
      "fluent at index 43: [-0.8000909686088562, 2.6079282760620117, 0.9423706531524658, 2.214717388153076, 2.5200376510620117]\n",
      "Grand sum of 208 tensor sets is: [-85.30642700195312, 333.5665588378906, 176.70452880859375, 157.43167114257812, 106.76204681396484]\n",
      "\n",
      "Instance 238 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 239 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "fluent at index 3: [-0.6910665035247803, 2.078314781188965, 2.0409302711486816, -0.5250962376594543, -0.07399791479110718]\n",
      "Grand sum of 209 tensor sets is: [-85.99748992919922, 335.6448669433594, 178.74545288085938, 156.9065704345703, 106.68804931640625]\n",
      "\n",
      "Instance 240 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "fluent at index 6: [-1.1587584018707275, 1.0121595859527588, 1.6445657014846802, -1.1836485862731934, 0.1752510666847229]\n",
      "Grand sum of 210 tensor sets is: [-87.15625, 336.6570129394531, 180.3900146484375, 155.72291564941406, 106.8633041381836]\n",
      "\n",
      "Instance 241 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "fluent at index 14: [-0.12442100793123245, 2.752383232116699, 1.3734681606292725, 0.6869966983795166, 0.625411331653595]\n",
      "Grand sum of 211 tensor sets is: [-87.28067016601562, 339.4093933105469, 181.76348876953125, 156.409912109375, 107.48871612548828]\n",
      "\n",
      "Instance 242 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "fluent at index 5: [-0.8275516033172607, 1.6491442918777466, 1.9111881256103516, 0.35921990871429443, 0.0692516565322876]\n",
      "Grand sum of 212 tensor sets is: [-88.10822296142578, 341.05853271484375, 183.6746826171875, 156.76913452148438, 107.55796813964844]\n",
      "\n",
      "Instance 243 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [128]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "fluent at index 128: [-0.1580011397600174, 0.427862286567688, 0.07095405459403992, 0.3251473903656006, 0.8616439700126648]\n",
      "Grand sum of 213 tensor sets is: [-88.26622772216797, 341.48638916015625, 183.74563598632812, 157.0942840576172, 108.41960906982422]\n",
      "\n",
      "Instance 244 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 214 tensor sets is: [-88.9906997680664, 343.8840637207031, 183.62181091308594, 159.82569885253906, 110.42557525634766]\n",
      "\n",
      "Instance 245 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "fluent at index 56: [-0.7443333864212036, 2.822413206100464, 0.739888072013855, 1.0621095895767212, -0.2773439586162567]\n",
      "Grand sum of 215 tensor sets is: [-89.73503112792969, 346.70648193359375, 184.3616943359375, 160.88780212402344, 110.14823150634766]\n",
      "\n",
      "Instance 246 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "fluent at index 14: [-0.233007550239563, 1.7410961389541626, 1.4315876960754395, 1.0955243110656738, 1.4279992580413818]\n",
      "Grand sum of 216 tensor sets is: [-89.9680404663086, 348.44757080078125, 185.7932891845703, 161.9833221435547, 111.57623291015625]\n",
      "\n",
      "Instance 247 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "fluent at index 31: [-0.14999398589134216, 0.9665215015411377, 0.990175724029541, 0.46508845686912537, 1.041846513748169]\n",
      "Grand sum of 217 tensor sets is: [-90.11803436279297, 349.4140930175781, 186.78346252441406, 162.4484100341797, 112.61808013916016]\n",
      "\n",
      "Instance 248 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "fluent at index 17: [-0.35967159271240234, -0.13052096962928772, 0.7008234262466431, 0.19130681455135345, -4.008251190185547]\n",
      "Grand sum of 218 tensor sets is: [-90.47770690917969, 349.2835693359375, 187.48428344726562, 162.63970947265625, 108.60983276367188]\n",
      "\n",
      "Instance 249 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 250 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "fluent at index 20: [-0.9591256976127625, 0.13152986764907837, -0.10783228278160095, 1.9658795595169067, 2.6039395332336426]\n",
      "Grand sum of 219 tensor sets is: [-91.43683624267578, 349.41510009765625, 187.37644958496094, 164.6055908203125, 111.21377563476562]\n",
      "\n",
      "Instance 251 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 252 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 220 tensor sets is: [-92.16130828857422, 351.8127746582031, 187.25262451171875, 167.33700561523438, 113.21974182128906]\n",
      "\n",
      "Instance 253 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "fluent at index 13: [-1.1908161640167236, 1.225885272026062, 0.4999619722366333, 0.005427330732345581, 0.35075297951698303]\n",
      "Grand sum of 221 tensor sets is: [-93.35212707519531, 353.0386657714844, 187.75259399414062, 167.34243774414062, 113.57049560546875]\n",
      "\n",
      "Instance 254 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [79]\n",
      "Size of token embeddings is torch.Size([137, 13, 768])\n",
      "Shape of summed layers is: 137 x 768\n",
      "fluent at index 79: [-1.207963228225708, 1.8231216669082642, 1.0856624841690063, 1.7312694787979126, 2.4853856563568115]\n",
      "Grand sum of 222 tensor sets is: [-94.56008911132812, 354.8617858886719, 188.8382568359375, 169.07369995117188, 116.05587768554688]\n",
      "\n",
      "Instance 255 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 256 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "fluent at index 6: [-1.2957334518432617, 2.616715431213379, 1.3152248859405518, 0.8722268342971802, -0.6353647708892822]\n",
      "Grand sum of 223 tensor sets is: [-95.85581970214844, 357.478515625, 190.1534881591797, 169.9459228515625, 115.4205093383789]\n",
      "\n",
      "Instance 257 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "fluent at index 3: [0.1555183231830597, 0.4839668273925781, 1.5968825817108154, -0.8414345979690552, 0.06009417772293091]\n",
      "Grand sum of 224 tensor sets is: [-95.70030212402344, 357.9624938964844, 191.7503662109375, 169.1044921875, 115.48060607910156]\n",
      "\n",
      "Instance 258 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 19: [0.7190461158752441, 2.0231549739837646, -0.7411234974861145, 1.6080385446548462, 1.510246992111206]\n",
      "Grand sum of 225 tensor sets is: [-94.98125457763672, 359.98565673828125, 191.00924682617188, 170.7125244140625, 116.99085235595703]\n",
      "\n",
      "Instance 259 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 226 tensor sets is: [-94.81372833251953, 360.5773010253906, 190.66415405273438, 171.18637084960938, 119.25004577636719]\n",
      "\n",
      "Instance 260 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "fluent at index 50: [-0.6216220855712891, 2.089048147201538, 0.39073646068573, 2.0657596588134766, 1.9189351797103882]\n",
      "Grand sum of 227 tensor sets is: [-95.43534851074219, 362.6663513183594, 191.0548858642578, 173.25213623046875, 121.16898345947266]\n",
      "\n",
      "Instance 261 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "fluent at index 5: [-0.4541952908039093, 1.7717361450195312, 1.1562212705612183, -0.3837934732437134, -0.5544348955154419]\n",
      "Grand sum of 228 tensor sets is: [-95.88954162597656, 364.4380798339844, 192.2111053466797, 172.86834716796875, 120.61454772949219]\n",
      "\n",
      "Instance 262 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "fluent at index 3: [-0.03477030247449875, 1.492963194847107, 0.9266045093536377, 0.11466608941555023, -0.42875000834465027]\n",
      "Grand sum of 229 tensor sets is: [-95.92430877685547, 365.9310302734375, 193.13771057128906, 172.98301696777344, 120.18579864501953]\n",
      "\n",
      "Instance 263 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "fluent at index 17: [0.23525984585285187, 0.9949068427085876, 1.9762914180755615, 0.4193154573440552, -1.327999472618103]\n",
      "Grand sum of 230 tensor sets is: [-95.68904876708984, 366.9259338378906, 195.11399841308594, 173.40232849121094, 118.85779571533203]\n",
      "\n",
      "Instance 264 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "fluent at index 6: [-1.6069509983062744, 2.675405979156494, 1.9503768682479858, 0.27614858746528625, 0.8510208129882812]\n",
      "Grand sum of 231 tensor sets is: [-97.2959976196289, 369.6013488769531, 197.0643768310547, 173.67848205566406, 119.70881652832031]\n",
      "\n",
      "Instance 265 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "fluent at index 18: [-1.3565970659255981, 2.884974956512451, 1.2380176782608032, 2.3258023262023926, 0.5483405590057373]\n",
      "Grand sum of 232 tensor sets is: [-98.65259552001953, 372.486328125, 198.30239868164062, 176.00428771972656, 120.25715637207031]\n",
      "\n",
      "Instance 266 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 233 tensor sets is: [-99.37706756591797, 374.8840026855469, 198.17857360839844, 178.73570251464844, 122.26312255859375]\n",
      "\n",
      "Instance 267 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "fluent at index 10: [0.31830400228500366, -2.028630495071411, -0.8084251284599304, 3.2415432929992676, 1.1300896406173706]\n",
      "Grand sum of 234 tensor sets is: [-99.05876159667969, 372.8553771972656, 197.37014770507812, 181.9772491455078, 123.3932113647461]\n",
      "\n",
      "Instance 268 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 235 tensor sets is: [-99.78323364257812, 375.2530517578125, 197.24632263183594, 184.7086639404297, 125.39917755126953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 269 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "fluent at index 6: [-0.018595576286315918, 1.5094243288040161, 0.14475467801094055, 1.1539905071258545, 1.8806277513504028]\n",
      "Grand sum of 236 tensor sets is: [-99.80182647705078, 376.7624816894531, 197.39108276367188, 185.86265563964844, 127.2798080444336]\n",
      "\n",
      "Instance 270 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 237 tensor sets is: [-99.6343002319336, 377.3541259765625, 197.04598999023438, 186.3365020751953, 129.53900146484375]\n",
      "\n",
      "Instance 271 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 9: [-0.6419892311096191, 0.20153425633907318, 0.7340121865272522, 2.3245677947998047, -2.75780987739563]\n",
      "Grand sum of 238 tensor sets is: [-100.27629089355469, 377.5556640625, 197.77999877929688, 188.66107177734375, 126.78118896484375]\n",
      "\n",
      "Instance 272 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "fluent at index 14: [-0.011638350784778595, 0.8321704864501953, 1.2744840383529663, 1.472572684288025, -1.373476505279541]\n",
      "Grand sum of 239 tensor sets is: [-100.28792572021484, 378.3878479003906, 199.0544891357422, 190.13365173339844, 125.40771484375]\n",
      "\n",
      "Instance 273 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [102, 103]\n",
      "Size of token embeddings is torch.Size([390, 13, 768])\n",
      "Shape of summed layers is: 390 x 768\n",
      "fluent at index 102: [0.8288633823394775, -1.226250171661377, -0.15247440338134766, 1.3158944845199585, -0.061471231281757355]\n",
      "fluent at index 103: [1.4940601587295532, -1.979896903038025, -0.9562875032424927, 2.866597890853882, -3.9406630992889404]\n",
      "Grand sum of 240 tensor sets is: [-99.12646484375, 376.7847595214844, 198.50010681152344, 192.2248992919922, 123.40664672851562]\n",
      "\n",
      "Instance 274 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 17: [-0.11565154790878296, 0.8124000430107117, 0.25219979882240295, 1.8271719217300415, -1.5777183771133423]\n",
      "Grand sum of 241 tensor sets is: [-99.24211883544922, 377.59716796875, 198.75230407714844, 194.0520782470703, 121.82892608642578]\n",
      "\n",
      "Instance 275 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 17: [-0.11565154790878296, 0.8124000430107117, 0.25219979882240295, 1.8271719217300415, -1.5777183771133423]\n",
      "Grand sum of 242 tensor sets is: [-99.35777282714844, 378.4095764160156, 199.00450134277344, 195.87925720214844, 120.25120544433594]\n",
      "\n",
      "Instance 276 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "fluent at index 30: [-0.07821676135063171, -0.2822090685367584, -0.8029299974441528, 1.4299633502960205, 2.9620816707611084]\n",
      "Grand sum of 243 tensor sets is: [-99.43598937988281, 378.12738037109375, 198.20156860351562, 197.30921936035156, 123.21328735351562]\n",
      "\n",
      "Instance 277 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "fluent at index 11: [0.2510562837123871, 2.1104683876037598, -0.06178417056798935, 0.17201951146125793, 2.175438165664673]\n",
      "Grand sum of 244 tensor sets is: [-99.1849365234375, 380.23785400390625, 198.13978576660156, 197.48123168945312, 125.38872528076172]\n",
      "\n",
      "Instance 278 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 279 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "fluent at index 9: [0.17104579508304596, 1.9170520305633545, 1.601174235343933, -0.7651408910751343, -1.2667100429534912]\n",
      "Grand sum of 245 tensor sets is: [-99.0138931274414, 382.1549072265625, 199.740966796875, 196.71609497070312, 124.12201690673828]\n",
      "\n",
      "Instance 280 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 281 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "fluent at index 7: [-0.6804423332214355, 0.7565412521362305, 0.060334086418151855, -0.1054019033908844, 0.5083379149436951]\n",
      "Grand sum of 246 tensor sets is: [-99.6943359375, 382.91143798828125, 199.80130004882812, 196.61068725585938, 124.63035583496094]\n",
      "\n",
      "Instance 282 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 247 tensor sets is: [-100.41880798339844, 385.3091125488281, 199.67747497558594, 199.34210205078125, 126.63632202148438]\n",
      "\n",
      "Instance 283 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "fluent at index 21: [0.27941834926605225, 0.6244243383407593, 0.9480469226837158, -1.350839376449585, 0.5017644762992859]\n",
      "Grand sum of 248 tensor sets is: [-100.13938903808594, 385.93353271484375, 200.62551879882812, 197.9912567138672, 127.1380844116211]\n",
      "\n",
      "Instance 284 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "fluent at index 33: [0.45701101422309875, 1.1745446920394897, 0.38981160521507263, 0.3651347756385803, 2.3178086280822754]\n",
      "Grand sum of 249 tensor sets is: [-99.68238067626953, 387.1080627441406, 201.0153350830078, 198.35638427734375, 129.4558868408203]\n",
      "\n",
      "Instance 285 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "fluent at index 10: [0.062036290764808655, 1.8690439462661743, 0.8209178447723389, 0.5355299115180969, -0.2638755440711975]\n",
      "Grand sum of 250 tensor sets is: [-99.62034606933594, 388.97711181640625, 201.8362579345703, 198.89190673828125, 129.1920166015625]\n",
      "\n",
      "Instance 286 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "fluent at index 4: [-0.6108767986297607, 2.1389901638031006, 1.0704896450042725, -0.2330477088689804, -1.2135226726531982]\n",
      "Grand sum of 251 tensor sets is: [-100.2312240600586, 391.1160888671875, 202.90675354003906, 198.6588592529297, 127.9784927368164]\n",
      "\n",
      "Instance 287 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 288 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 5: [-1.4790799617767334, -1.04352867603302, -0.736006498336792, 0.02554379403591156, 3.57106876373291]\n",
      "Grand sum of 252 tensor sets is: [-101.7103042602539, 390.07257080078125, 202.17074584960938, 198.6844024658203, 131.549560546875]\n",
      "\n",
      "Instance 289 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 290 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "fluent at index 3: [-0.34948787093162537, 0.9273014664649963, 1.7195940017700195, 0.5496623516082764, 0.9593936204910278]\n",
      "Grand sum of 253 tensor sets is: [-102.0597915649414, 390.9998779296875, 203.8903350830078, 199.23406982421875, 132.5089569091797]\n",
      "\n",
      "Instance 291 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "fluent at index 15: [-0.4680727422237396, 0.7222720980644226, -0.27987295389175415, -0.9590536952018738, 2.651059627532959]\n",
      "Grand sum of 254 tensor sets is: [-102.52786254882812, 391.7221374511719, 203.61045837402344, 198.27500915527344, 135.16001892089844]\n",
      "\n",
      "Instance 292 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 4: [0.1675255298614502, 0.5916377305984497, -0.3450886011123657, 0.473846971988678, 2.259190320968628]\n",
      "Grand sum of 255 tensor sets is: [-102.36033630371094, 392.31378173828125, 203.26536560058594, 198.7488555908203, 137.41920471191406]\n",
      "\n",
      "Instance 293 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "fluent at index 17: [-1.7082555294036865, -0.5132997632026672, -0.1937340348958969, -0.12609833478927612, 2.2378149032592773]\n",
      "Grand sum of 256 tensor sets is: [-104.06858825683594, 391.80047607421875, 203.07162475585938, 198.6227569580078, 139.65701293945312]\n",
      "\n",
      "Instance 294 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 295 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "fluent at index 14: [-0.6760275959968567, 1.9869985580444336, 0.9629586935043335, -0.889887273311615, 0.02791428565979004]\n",
      "Grand sum of 257 tensor sets is: [-104.74461364746094, 393.7874755859375, 204.03457641601562, 197.7328643798828, 139.68492126464844]\n",
      "\n",
      "Instance 296 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "fluent at index 7: [-1.6700704097747803, 2.8388941287994385, 1.5016274452209473, 0.8590525984764099, 1.1821386814117432]\n",
      "Grand sum of 258 tensor sets is: [-106.41468048095703, 396.6263732910156, 205.5362091064453, 198.5919189453125, 140.8670654296875]\n",
      "\n",
      "Instance 297 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "fluent at index 13: [0.024854548275470734, 2.2272820472717285, -0.09920509159564972, 1.601966142654419, 1.3899178504943848]\n",
      "Grand sum of 259 tensor sets is: [-106.38982391357422, 398.8536682128906, 205.43699645996094, 200.19387817382812, 142.25698852539062]\n",
      "\n",
      "Instance 298 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "fluent at index 17: [-0.9303343296051025, 2.916675567626953, 1.5440706014633179, 0.14967960119247437, 2.126128911972046]\n",
      "Grand sum of 260 tensor sets is: [-107.32015991210938, 401.7703552246094, 206.98106384277344, 200.3435516357422, 144.38311767578125]\n",
      "\n",
      "Instance 299 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 300 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "fluent at index 4: [-0.8885881900787354, 1.5638172626495361, 1.7247068881988525, -0.4723612070083618, -0.0926956832408905]\n",
      "Grand sum of 261 tensor sets is: [-108.20874786376953, 403.33416748046875, 208.7057647705078, 199.87118530273438, 144.29042053222656]\n",
      "\n",
      "Instance 301 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "fluent at index 9: [0.10188137739896774, -0.1797458976507187, 0.014310568571090698, 0.08042147755622864, -0.14700409770011902]\n",
      "Grand sum of 262 tensor sets is: [-108.10686492919922, 403.1544189453125, 208.72007751464844, 199.9516143798828, 144.14341735839844]\n",
      "\n",
      "Instance 302 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 6: [0.36788812279701233, -0.6601552963256836, -0.27428656816482544, -0.7580229640007019, -0.05495864152908325]\n",
      "Grand sum of 263 tensor sets is: [-107.73897552490234, 402.4942626953125, 208.44578552246094, 199.19358825683594, 144.0884552001953]\n",
      "\n",
      "Instance 303 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 304 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 305 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 306 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "fluent at index 25: [0.2589185833930969, 1.7973474264144897, 0.3747175335884094, 0.8852308988571167, -0.6731592416763306]\n",
      "Grand sum of 264 tensor sets is: [-107.48005676269531, 404.2915954589844, 208.82049560546875, 200.0788116455078, 143.41529846191406]\n",
      "\n",
      "Instance 307 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 308 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "fluent at index 17: [-0.11565154790878296, 0.8124000430107117, 0.25219979882240295, 1.8271719217300415, -1.5777183771133423]\n",
      "Grand sum of 265 tensor sets is: [-107.59571075439453, 405.10400390625, 209.07269287109375, 201.90599060058594, 141.83758544921875]\n",
      "\n",
      "Instance 309 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 310 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "fluent at index 8: [-0.6559779047966003, 1.7838635444641113, 1.0779391527175903, -0.0737592950463295, 0.3915660083293915]\n",
      "Grand sum of 266 tensor sets is: [-108.2516860961914, 406.88787841796875, 210.150634765625, 201.8322296142578, 142.22915649414062]\n",
      "\n",
      "Instance 311 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "fluent at index 24: [0.7420557737350464, 2.443380117416382, 2.284703254699707, 1.4868228435516357, -0.9565655589103699]\n",
      "Grand sum of 267 tensor sets is: [-107.50962829589844, 409.3312683105469, 212.43533325195312, 203.3190460205078, 141.27259826660156]\n",
      "\n",
      "Instance 312 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "\n",
      "Instance 313 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "fluent at index 6: [0.5990873575210571, 1.1857850551605225, 1.6720526218414307, 0.2313065379858017, -1.8369007110595703]\n",
      "Grand sum of 268 tensor sets is: [-106.91053771972656, 410.5170593261719, 214.10739135742188, 203.55035400390625, 139.43569946289062]\n",
      "\n",
      "Instance 314 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "fluent at index 10: [0.062036290764808655, 1.8690439462661743, 0.8209178447723389, 0.5355299115180969, -0.2638755440711975]\n",
      "Grand sum of 269 tensor sets is: [-106.84850311279297, 412.3861083984375, 214.92831420898438, 204.08587646484375, 139.1718292236328]\n",
      "\n",
      "Instance 315 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "fluent at index 14: [-0.724473237991333, 2.3976733684539795, -0.12382659316062927, 2.731417179107666, 2.005963087081909]\n",
      "Grand sum of 270 tensor sets is: [-107.5729751586914, 414.7837829589844, 214.8044891357422, 206.81729125976562, 141.17779541015625]\n",
      "\n",
      "Instance 316 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "fluent at index 3: [-0.3037385940551758, 1.8265531063079834, 1.7869932651519775, 0.3246701955795288, 0.42834755778312683]\n",
      "Grand sum of 271 tensor sets is: [-107.87671661376953, 416.6103210449219, 216.5914764404297, 207.1419677734375, 141.60614013671875]\n",
      "\n",
      "Instance 317 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "fluent at index 28: [-0.3473988473415375, -0.5546485781669617, 0.4236803352832794, -0.6854094862937927, 0.7023090124130249]\n",
      "Grand sum of 272 tensor sets is: [-108.22411346435547, 416.0556640625, 217.01515197753906, 206.45655822753906, 142.30845642089844]\n",
      "\n",
      "Instance 318 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "fluent at index 8: [-1.0269700288772583, 1.644490122795105, 1.5771178007125854, 0.8985011577606201, 0.13388870656490326]\n",
      "Grand sum of 273 tensor sets is: [-109.25108337402344, 417.7001647949219, 218.59226989746094, 207.3550567626953, 142.44235229492188]\n",
      "\n",
      "Instance 319 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "fluent at index 18: [-1.3565970659255981, 2.884974956512451, 1.2380176782608032, 2.3258023262023926, 0.5483405590057373]\n",
      "Grand sum of 274 tensor sets is: [-110.60768127441406, 420.58514404296875, 219.83029174804688, 209.6808624267578, 142.99069213867188]\n",
      "\n",
      "Instance 320 of fluent.\n",
      "Looking for vocab token: fluent\n",
      "Indices are [20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "fluent at index 20: [-0.013586113229393959, 0.270305335521698, 0.8982506990432739, -0.11722353100776672, 0.6659053564071655]\n",
      "Grand sum of 275 tensor sets is: [-110.62126922607422, 420.8554382324219, 220.72854614257812, 209.5636444091797, 143.65660095214844]\n",
      "Mean of tensors is: tensor([-0.4023,  1.5304,  0.8026,  0.7620,  0.5224]) (768 features in tensor)\n",
      "Saved the embedding for fluent.\n",
      "Saved the count of sentences used to create fluent embedding\n",
      "Run time for fluent was 51.4174487804994 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "don\n",
      "ut\n",
      "\n",
      "Instance 1 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6, 7]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "don at index 6: [1.06541907787323, -2.6380362510681152, -0.20763221383094788, -2.06862211227417, -1.424448847770691]\n",
      "ut at index 7: [0.08790595829486847, -1.7761238813400269, -1.0377156734466553, 1.0461851358413696, 1.6593916416168213]\n",
      "Grand sum of 1 tensor sets is: [0.576662540435791, -2.207080125808716, -0.6226739287376404, -0.5112184882164001, 0.11747139692306519]\n",
      "\n",
      "Instance 2 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "don at index 24: [1.1620228290557861, -0.9305701851844788, -0.39329373836517334, 1.6992113590240479, 1.674973964691162]\n",
      "Grand sum of 2 tensor sets is: [1.7386853694915771, -3.13765025138855, -1.015967607498169, 1.187992811203003, 1.792445421218872]\n",
      "\n",
      "Instance 3 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [198]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 198: [-0.788311779499054, -1.7266035079956055, 0.6087488532066345, -0.5217610001564026, -0.6835480332374573]\n",
      "Grand sum of 3 tensor sets is: [0.9503735899925232, -4.864253997802734, -0.4072187542915344, 0.6662318110466003, 1.1088974475860596]\n",
      "\n",
      "Instance 4 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [13, 14]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "don at index 13: [0.6319950819015503, -1.793749213218689, 0.323734313249588, -1.7261550426483154, 0.099943608045578]\n",
      "ut at index 14: [1.0866737365722656, -2.001518726348877, 0.0966167151927948, 0.16856393218040466, 3.158226728439331]\n",
      "Grand sum of 4 tensor sets is: [1.8097079992294312, -6.761888027191162, -0.19704324007034302, -0.11256372928619385, 2.737982749938965]\n",
      "\n",
      "Instance 5 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [257]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 257: [-0.6918386816978455, 2.5309019088745117, -1.248115062713623, 1.0737531185150146, 1.9748470783233643]\n",
      "Grand sum of 5 tensor sets is: [1.1178693771362305, -4.23098611831665, -1.4451582431793213, 0.9611893892288208, 4.71282958984375]\n",
      "\n",
      "Instance 6 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5, 6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "don at index 5: [-0.04980244114995003, -2.1307590007781982, 0.65675950050354, -1.3999519348144531, -1.4750920534133911]\n",
      "ut at index 6: [-0.41299572587013245, -1.6869480609893799, 0.7428168654441833, 0.7258172035217285, 0.8193536996841431]\n",
      "Grand sum of 6 tensor sets is: [0.8864703178405762, -6.1398396492004395, -0.7453700304031372, 0.6241220235824585, 4.384960174560547]\n",
      "\n",
      "Instance 7 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [329]\n",
      "Size of token embeddings is torch.Size([401, 13, 768])\n",
      "Shape of summed layers is: 401 x 768\n",
      "don at index 329: [-1.2677100896835327, -1.3994464874267578, -0.03901592642068863, 0.015571512281894684, -0.3860386610031128]\n",
      "Grand sum of 7 tensor sets is: [-0.38123977184295654, -7.539286136627197, -0.7843859791755676, 0.639693558216095, 3.9989213943481445]\n",
      "\n",
      "Instance 8 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [33, 34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "don at index 33: [0.7487749457359314, -1.8746854066848755, 0.5082172155380249, -3.068044900894165, -1.285144567489624]\n",
      "ut at index 34: [-0.0329873189330101, -2.1389975547790527, -0.47448840737342834, -0.5864568948745728, 1.0990371704101562]\n",
      "Grand sum of 8 tensor sets is: [-0.023345947265625, -9.546127319335938, -0.7675215601921082, -1.1875574588775635, 3.905867576599121]\n",
      "\n",
      "Instance 9 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "don at index 9: [0.36300861835479736, -1.887847900390625, -0.8876609802246094, 0.4553930461406708, 1.7706129550933838]\n",
      "Grand sum of 9 tensor sets is: [0.33966267108917236, -11.433975219726562, -1.6551826000213623, -0.7321643829345703, 5.676480293273926]\n",
      "\n",
      "Instance 10 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [-0.14172697067260742, 0.47182098031044006, -0.3521590828895569, 0.37289056181907654, 2.5498294830322266]\n",
      "Grand sum of 10 tensor sets is: [0.19793570041656494, -10.962154388427734, -2.0073416233062744, -0.3592738211154938, 8.226309776306152]\n",
      "\n",
      "Instance 11 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 12 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 13 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "don at index 9: [-0.5505640506744385, -0.0509270504117012, -2.32780122756958, -0.04287176579236984, -0.9788830876350403]\n",
      "Grand sum of 11 tensor sets is: [-0.35262835025787354, -11.013081550598145, -4.335143089294434, -0.4021455943584442, 7.247426509857178]\n",
      "\n",
      "Instance 14 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [-0.14172697067260742, 0.47182098031044006, -0.3521590828895569, 0.37289056181907654, 2.5498294830322266]\n",
      "Grand sum of 12 tensor sets is: [-0.49435532093048096, -10.541260719299316, -4.687302112579346, -0.029255032539367676, 9.797256469726562]\n",
      "\n",
      "Instance 15 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "don at index 2: [-0.43132197856903076, -1.894585132598877, -0.4968457818031311, 0.17008283734321594, 3.510101795196533]\n",
      "Grand sum of 13 tensor sets is: [-0.9256772994995117, -12.435846328735352, -5.184147834777832, 0.14082780480384827, 13.307357788085938]\n",
      "\n",
      "Instance 16 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [85, 86]\n",
      "Size of token embeddings is torch.Size([133, 13, 768])\n",
      "Shape of summed layers is: 133 x 768\n",
      "don at index 85: [0.20022818446159363, -3.884594440460205, -0.03351852297782898, -2.2825121879577637, -1.2541825771331787]\n",
      "ut at index 86: [-0.12009552121162415, -3.0042574405670166, 0.47087472677230835, -0.45220500230789185, 1.3738112449645996]\n",
      "Grand sum of 14 tensor sets is: [-0.8856109380722046, -15.880271911621094, -4.965469837188721, -1.2265307903289795, 13.367172241210938]\n",
      "\n",
      "Instance 17 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [16, 17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "don at index 16: [0.11229827255010605, -1.9154601097106934, 0.6002202033996582, -3.7690320014953613, 0.12684573233127594]\n",
      "ut at index 17: [0.13950993120670319, -2.0216593742370605, 0.15583306550979614, -0.553665280342102, 2.104123592376709]\n",
      "Grand sum of 15 tensor sets is: [-0.7597068548202515, -17.848831176757812, -4.5874433517456055, -3.3878793716430664, 14.482656478881836]\n",
      "\n",
      "Instance 18 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [9, 10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "don at index 9: [-0.030975572764873505, -1.217968463897705, 1.1836826801300049, 1.8730345964431763, 2.3838274478912354]\n",
      "ut at index 10: [1.2340714931488037, -2.6703057289123535, 0.7767378687858582, 1.9703564643859863, 3.1201024055480957]\n",
      "Grand sum of 16 tensor sets is: [-0.15815889835357666, -19.79296875, -3.6072330474853516, -1.4661839008331299, 17.234621047973633]\n",
      "\n",
      "Instance 19 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "don at index 12: [1.197250485420227, -1.8904184103012085, -0.922147274017334, 0.6673644781112671, 3.604978322982788]\n",
      "Grand sum of 17 tensor sets is: [1.0390915870666504, -21.683387756347656, -4.5293803215026855, -0.7988194227218628, 20.839599609375]\n",
      "\n",
      "Instance 20 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 21 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "don at index 5: [0.16139501333236694, -2.017814874649048, -1.275746464729309, -0.12344907969236374, 2.26247239112854]\n",
      "Grand sum of 18 tensor sets is: [1.200486660003662, -23.701202392578125, -5.805126667022705, -0.9222685098648071, 23.10207176208496]\n",
      "\n",
      "Instance 22 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [76, 77]\n",
      "Size of token embeddings is torch.Size([107, 13, 768])\n",
      "Shape of summed layers is: 107 x 768\n",
      "don at index 76: [0.2361682802438736, -3.8251242637634277, -0.40685465931892395, -0.7263984084129333, 0.9621621370315552]\n",
      "ut at index 77: [0.1159270778298378, -2.9770476818084717, 0.09465840458869934, 1.2357847690582275, 2.430643081665039]\n",
      "Grand sum of 19 tensor sets is: [1.376534342765808, -27.10228729248047, -5.961224555969238, -0.6675753593444824, 24.798473358154297]\n",
      "\n",
      "Instance 23 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [7, 8]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "don at index 7: [0.29123297333717346, -1.8105217218399048, 0.503405749797821, -1.7470898628234863, -2.021604299545288]\n",
      "ut at index 8: [-0.2519397437572479, -1.5005494356155396, 0.07548335194587708, -0.30363017320632935, -1.0224372148513794]\n",
      "Grand sum of 20 tensor sets is: [1.3961809873580933, -28.757822036743164, -5.671780109405518, -1.6929353475570679, 23.276453018188477]\n",
      "\n",
      "Instance 24 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [12, 13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "don at index 12: [0.16614678502082825, -1.538695216178894, 0.5998754501342773, -1.6057872772216797, 0.03396789729595184]\n",
      "ut at index 13: [0.2846982777118683, -1.4040623903274536, -1.3706308603286743, -0.13056893646717072, 2.720486879348755]\n",
      "Grand sum of 21 tensor sets is: [1.6216034889221191, -30.22920036315918, -6.05715799331665, -2.5611133575439453, 24.6536808013916]\n",
      "\n",
      "Instance 25 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [-0.14172697067260742, 0.47182098031044006, -0.3521590828895569, 0.37289056181907654, 2.5498294830322266]\n",
      "Grand sum of 22 tensor sets is: [1.4798765182495117, -29.75737953186035, -6.4093170166015625, -2.188222885131836, 27.203510284423828]\n",
      "\n",
      "Instance 26 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [22, 67, 68]\n",
      "Size of token embeddings is torch.Size([82, 13, 768])\n",
      "Shape of summed layers is: 82 x 768\n",
      "don at index 22: [-0.2782580852508545, -2.2851099967956543, 0.38649484515190125, -0.9677048325538635, -1.8330614566802979]\n",
      "ut at index 67: [0.07352443039417267, -1.3487515449523926, 0.3057580590248108, -3.213118553161621, -0.9644395709037781]\n",
      "don at index 68: [0.4439102113246918, -0.7763140201568604, -0.29565027356147766, 0.07469034194946289, 1.632868766784668]\n",
      "Grand sum of 23 tensor sets is: [1.5596020221710205, -31.22743797302246, -6.277116298675537, -3.556933879852295, 26.81529998779297]\n",
      "\n",
      "Instance 27 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [16, 17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "don at index 16: [0.014971263706684113, -2.963803768157959, -0.05703553557395935, -1.5965795516967773, -0.4689047932624817]\n",
      "ut at index 17: [0.22716599702835083, -2.5153419971466064, -0.7730573415756226, 0.5481663942337036, 2.2151236534118652]\n",
      "Grand sum of 24 tensor sets is: [1.6806706190109253, -33.967010498046875, -6.69216251373291, -4.081140518188477, 27.68840980529785]\n",
      "\n",
      "Instance 28 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [52, 40, 53]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "don at index 52: [-0.40063798427581787, -2.6909167766571045, 0.32780689001083374, -2.658057689666748, -2.4038352966308594]\n",
      "ut at index 40: [0.06244165450334549, -2.2988088130950928, -1.1550745964050293, -0.656804084777832, -0.13419444859027863]\n",
      "don at index 53: [-0.39210453629493713, -2.0671465396881104, 0.07117017358541489, -0.5987898707389832, -0.14413510262966156]\n",
      "Grand sum of 25 tensor sets is: [1.437237024307251, -36.31930160522461, -6.944194793701172, -5.385691165924072, 26.794355392456055]\n",
      "\n",
      "Instance 29 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [28, 29]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "don at index 28: [0.4950156807899475, -1.4864448308944702, 0.23298971354961395, -0.6367259621620178, -1.5642870664596558]\n",
      "ut at index 29: [-0.003813013434410095, -0.21117089688777924, -0.01785891316831112, 1.8685327768325806, 0.11257459223270416]\n",
      "Grand sum of 26 tensor sets is: [1.6828383207321167, -37.16810989379883, -6.836629390716553, -4.769787788391113, 26.068498611450195]\n",
      "\n",
      "Instance 30 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "don at index 24: [1.018189787864685, -1.4260450601577759, -0.5621400475502014, 1.261506199836731, 1.2730567455291748]\n",
      "Grand sum of 27 tensor sets is: [2.7010281085968018, -38.594154357910156, -7.398769378662109, -3.508281707763672, 27.341554641723633]\n",
      "\n",
      "Instance 31 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "don at index 47: [0.15471765398979187, -3.420179843902588, -0.4743551015853882, -1.5396950244903564, -0.31646665930747986]\n",
      "Grand sum of 28 tensor sets is: [2.855745792388916, -42.01433563232422, -7.873124599456787, -5.047976493835449, 27.025087356567383]\n",
      "\n",
      "Instance 32 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [40, 41]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "don at index 40: [0.3357926905155182, -1.2932593822479248, -0.6651858687400818, -1.7221393585205078, 0.419634610414505]\n",
      "ut at index 41: [0.14909836649894714, -1.9023582935333252, -0.8674076199531555, 0.8343127965927124, 2.794856309890747]\n",
      "Grand sum of 29 tensor sets is: [3.098191261291504, -43.612144470214844, -8.639421463012695, -5.491889953613281, 28.63233184814453]\n",
      "\n",
      "Instance 33 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "don at index 6: [0.3697374761104584, -1.6900986433029175, -1.2910408973693848, 0.2064872682094574, 2.17539119720459]\n",
      "Grand sum of 30 tensor sets is: [3.467928647994995, -45.302242279052734, -9.930461883544922, -5.285402774810791, 30.807723999023438]\n",
      "\n",
      "Instance 34 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 35 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [10, 11]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "don at index 10: [0.6216775178909302, -2.679137706756592, -0.48543569445610046, -3.8525614738464355, 0.510434091091156]\n",
      "ut at index 11: [0.35764163732528687, -2.1825294494628906, -0.5685971975326538, -0.911156952381134, 1.0792124271392822]\n",
      "Grand sum of 31 tensor sets is: [3.9575881958007812, -47.73307418823242, -10.457478523254395, -7.667262077331543, 31.60254669189453]\n",
      "\n",
      "Instance 36 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [25, 26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "don at index 25: [0.7152900099754333, -2.4662952423095703, 0.3588000535964966, -3.377166509628296, -0.608244776725769]\n",
      "ut at index 26: [0.3562658429145813, -2.224231481552124, 0.29723793268203735, 0.1782415211200714, 1.2691740989685059]\n",
      "Grand sum of 32 tensor sets is: [4.493366241455078, -50.078338623046875, -10.129459381103516, -9.266724586486816, 31.933012008666992]\n",
      "\n",
      "Instance 37 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 38 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "don at index 5: [0.47951775789260864, 0.8331218361854553, -1.7062050104141235, 0.5116692185401917, 1.413386344909668]\n",
      "Grand sum of 33 tensor sets is: [4.972884178161621, -49.245216369628906, -11.835664749145508, -8.75505542755127, 33.346397399902344]\n",
      "\n",
      "Instance 39 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 40 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [68]\n",
      "Size of token embeddings is torch.Size([105, 13, 768])\n",
      "Shape of summed layers is: 105 x 768\n",
      "don at index 68: [0.3079909086227417, -1.616295576095581, 0.684352695941925, 1.4314541816711426, -0.4940110445022583]\n",
      "Grand sum of 34 tensor sets is: [5.280875205993652, -50.86151123046875, -11.151311874389648, -7.323601245880127, 32.852386474609375]\n",
      "\n",
      "Instance 41 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "don at index 20: [0.14438320696353912, 1.0752084255218506, -1.0729846954345703, 0.5777962803840637, 0.8560961484909058]\n",
      "Grand sum of 35 tensor sets is: [5.425258636474609, -49.78630447387695, -12.224296569824219, -6.745804786682129, 33.70848083496094]\n",
      "\n",
      "Instance 42 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 43 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [27, 28]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "don at index 27: [0.2510680556297302, -1.6325701475143433, -0.6063812375068665, -0.09318245947360992, 1.0724461078643799]\n",
      "ut at index 28: [0.4336819648742676, -1.6720175743103027, -0.911238431930542, 1.6721011400222778, 2.7479121685028076]\n",
      "Grand sum of 36 tensor sets is: [5.767633438110352, -51.4385986328125, -12.98310661315918, -5.956345558166504, 35.61865997314453]\n",
      "\n",
      "Instance 44 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "don at index 24: [-0.4014967679977417, 1.18586266040802, -1.1322098970413208, -0.38003697991371155, 0.12558171153068542]\n",
      "Grand sum of 37 tensor sets is: [5.36613655090332, -50.25273513793945, -14.115316390991211, -6.3363823890686035, 35.744239807128906]\n",
      "\n",
      "Instance 45 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [446, 443]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 446: [0.15398499369621277, -3.6754698753356934, 1.0040132999420166, -0.29608720541000366, -5.517543792724609]\n",
      "ut at index 443: [1.0218833684921265, 0.4551970958709717, 0.31922274827957153, 0.5006868243217468, 1.831423282623291]\n",
      "Grand sum of 38 tensor sets is: [5.954070568084717, -51.86287307739258, -13.45369815826416, -6.2340826988220215, 33.901180267333984]\n",
      "\n",
      "Instance 46 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [22, 23]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "don at index 22: [0.40476715564727783, -2.4864423274993896, 1.1388463973999023, -1.621145486831665, -1.4283159971237183]\n",
      "ut at index 23: [-0.24637632071971893, -2.3017807006835938, 0.04464162141084671, -0.40888166427612305, 0.3021014630794525]\n",
      "Grand sum of 39 tensor sets is: [6.033266067504883, -54.25698471069336, -12.861953735351562, -7.249096393585205, 33.33807373046875]\n",
      "\n",
      "Instance 47 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "don at index 3: [0.4352305233478546, 0.2732018828392029, -2.0366950035095215, 1.5993694067001343, 0.9570112228393555]\n",
      "Grand sum of 40 tensor sets is: [6.468496799468994, -53.98378372192383, -14.898649215698242, -5.649726867675781, 34.29508590698242]\n",
      "\n",
      "Instance 48 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [33, 34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "don at index 33: [0.7487749457359314, -1.8746854066848755, 0.5082172155380249, -3.068044900894165, -1.285144567489624]\n",
      "ut at index 34: [-0.0329873189330101, -2.1389975547790527, -0.47448840737342834, -0.5864568948745728, 1.0990371704101562]\n",
      "Grand sum of 41 tensor sets is: [6.826390743255615, -55.990623474121094, -14.881784439086914, -7.476977825164795, 34.20203399658203]\n",
      "\n",
      "Instance 49 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [440]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 440: [0.4185546636581421, 0.03957942873239517, -0.48920005559921265, 0.7691627740859985, 2.5680460929870605]\n",
      "Grand sum of 42 tensor sets is: [7.244945526123047, -55.95104217529297, -15.370984077453613, -6.707815170288086, 36.77008056640625]\n",
      "\n",
      "Instance 50 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2, 8, 3, 9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "don at index 2: [-0.8030931949615479, -1.6264151334762573, -0.285945862531662, -0.49564436078071594, -2.3218324184417725]\n",
      "ut at index 8: [-0.6838630437850952, -0.9909270405769348, 0.27256083488464355, -1.469092607498169, -2.3502016067504883]\n",
      "don at index 3: [-0.05538393184542656, -2.4124293327331543, -0.8064107894897461, 1.2926768064498901, 1.2544492483139038]\n",
      "ut at index 9: [-0.012394480407238007, -0.8742939233779907, -0.4114999771118164, 0.9926590919494629, -0.3533049523830414]\n",
      "Grand sum of 43 tensor sets is: [6.856261730194092, -57.427059173583984, -15.678808212280273, -6.6276655197143555, 35.82735824584961]\n",
      "\n",
      "Instance 51 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 52 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [4, 28, 5, 29]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "don at index 4: [0.7750786542892456, -2.5374972820281982, 0.5960276126861572, -1.9394606351852417, 0.0641699880361557]\n",
      "ut at index 28: [1.1864802837371826, -1.8364298343658447, 0.4131788909435272, -2.7361738681793213, 0.19504472613334656]\n",
      "don at index 5: [1.1447968482971191, -2.5592188835144043, -0.49718692898750305, 0.6445250511169434, 2.275249481201172]\n",
      "ut at index 29: [0.5232567191123962, -2.05887508392334, -0.5767496824264526, -0.5945013165473938, 2.520383358001709]\n",
      "Grand sum of 44 tensor sets is: [7.763664722442627, -59.67506408691406, -15.694991111755371, -7.7840681076049805, 37.09107208251953]\n",
      "\n",
      "Instance 53 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [-0.14172697067260742, 0.47182098031044006, -0.3521590828895569, 0.37289056181907654, 2.5498294830322266]\n",
      "Grand sum of 45 tensor sets is: [7.6219377517700195, -59.203243255615234, -16.047149658203125, -7.411177635192871, 39.640899658203125]\n",
      "\n",
      "Instance 54 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "don at index 2: [0.0886261910200119, -0.5187808871269226, -1.453700304031372, -1.1070200204849243, 2.7636613845825195]\n",
      "Grand sum of 46 tensor sets is: [7.710564136505127, -59.722023010253906, -17.500850677490234, -8.518198013305664, 42.40456008911133]\n",
      "\n",
      "Instance 55 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [16, 17]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "don at index 16: [0.4688001871109009, -2.1609184741973877, 0.638131320476532, -0.6165052056312561, -1.8875819444656372]\n",
      "ut at index 17: [-0.05706050246953964, -1.2411857843399048, 0.18566644191741943, 1.6004455089569092, 1.310762643814087]\n",
      "Grand sum of 47 tensor sets is: [7.916433811187744, -61.42307662963867, -17.088951110839844, -8.026227951049805, 42.11614990234375]\n",
      "\n",
      "Instance 56 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([116, 13, 768])\n",
      "Shape of summed layers is: 116 x 768\n",
      "don at index 111: [-0.5407661199569702, -1.5664860010147095, -1.1712671518325806, 1.1351816654205322, 3.481750965118408]\n",
      "Grand sum of 48 tensor sets is: [7.375667572021484, -62.98956298828125, -18.260217666625977, -6.891046524047852, 45.597900390625]\n",
      "\n",
      "Instance 57 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2, 3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "don at index 2: [0.5782493948936462, -2.9345664978027344, 0.25326859951019287, -0.9970138072967529, 1.024877905845642]\n",
      "ut at index 3: [0.26609790325164795, -3.282137393951416, -0.7203695178031921, 1.1379865407943726, 2.1874780654907227]\n",
      "Grand sum of 49 tensor sets is: [7.7978410720825195, -66.09791564941406, -18.4937686920166, -6.820559978485107, 47.204078674316406]\n",
      "\n",
      "Instance 58 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [55]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "don at index 55: [0.20921938121318817, -3.149092674255371, -0.7871729731559753, -0.5466986894607544, 1.3803790807724]\n",
      "Grand sum of 50 tensor sets is: [8.007060050964355, -69.24700927734375, -19.280941009521484, -7.367258548736572, 48.58445739746094]\n",
      "\n",
      "Instance 59 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [40, 41]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "don at index 40: [0.3357926905155182, -1.2932593822479248, -0.6651858687400818, -1.7221393585205078, 0.419634610414505]\n",
      "ut at index 41: [0.14909836649894714, -1.9023582935333252, -0.8674076199531555, 0.8343127965927124, 2.794856309890747]\n",
      "Grand sum of 51 tensor sets is: [8.249505996704102, -70.84481811523438, -20.047237396240234, -7.811172008514404, 50.19170379638672]\n",
      "\n",
      "Instance 60 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [-0.14172697067260742, 0.47182098031044006, -0.3521590828895569, 0.37289056181907654, 2.5498294830322266]\n",
      "Grand sum of 52 tensor sets is: [8.107778549194336, -70.37299346923828, -20.399396896362305, -7.438281536102295, 52.74153137207031]\n",
      "\n",
      "Instance 61 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [257]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 257: [-0.6918386816978455, 2.5309019088745117, -1.248115062713623, 1.0737531185150146, 1.9748470783233643]\n",
      "Grand sum of 53 tensor sets is: [7.415939807891846, -67.84209442138672, -21.647512435913086, -6.364528656005859, 54.71637725830078]\n",
      "\n",
      "Instance 62 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [11, 12]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "don at index 11: [0.2810365557670593, -2.886162519454956, -0.37196779251098633, -2.1637635231018066, -0.7416479587554932]\n",
      "ut at index 12: [-0.010669983923435211, -2.908987522125244, -0.2748277485370636, -0.3262951970100403, 1.299889087677002]\n",
      "Grand sum of 54 tensor sets is: [7.551123142242432, -70.73966979980469, -21.970911026000977, -7.60955810546875, 54.99549865722656]\n",
      "\n",
      "Instance 63 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [212]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 212: [0.5604667067527771, 0.061389584094285965, -0.3265286087989807, -0.2628869116306305, -0.604041576385498]\n",
      "Grand sum of 55 tensor sets is: [8.111589431762695, -70.67828369140625, -22.297439575195312, -7.872445106506348, 54.391456604003906]\n",
      "\n",
      "Instance 64 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [47, 19, 24]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "don at index 47: [-0.2947031855583191, -2.469743013381958, 0.4243190884590149, -2.7963781356811523, -3.8624894618988037]\n",
      "ut at index 19: [0.6754516363143921, -2.7306134700775146, 0.3591626286506653, 1.3403102159500122, 3.6646170616149902]\n",
      "don at index 24: [1.3507686853408813, -3.059324026107788, 0.9467728137969971, -0.557802140712738, 3.79065203666687]\n",
      "Grand sum of 56 tensor sets is: [8.688761711120605, -73.43151092529297, -21.720687866210938, -8.54373550415039, 55.58905029296875]\n",
      "\n",
      "Instance 65 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "don at index 13: [0.7407242655754089, -3.0320334434509277, -0.38410452008247375, 3.523912191390991, 2.4086122512817383]\n",
      "Grand sum of 57 tensor sets is: [9.429486274719238, -76.46354675292969, -22.10479164123535, -5.01982307434082, 57.99766159057617]\n",
      "\n",
      "Instance 66 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "don at index 13: [0.7407242655754089, -3.0320334434509277, -0.38410452008247375, 3.523912191390991, 2.4086122512817383]\n",
      "Grand sum of 58 tensor sets is: [10.170210838317871, -79.4955825805664, -22.488895416259766, -1.495910882949829, 60.406272888183594]\n",
      "\n",
      "Instance 67 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [18, 19]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "don at index 18: [0.5973824262619019, -1.1535745859146118, 0.131153866648674, 0.5509191751480103, -0.22740057110786438]\n",
      "ut at index 19: [0.9444179534912109, -2.111344575881958, -0.7066078186035156, 2.815006971359253, 2.9151713848114014]\n",
      "Grand sum of 59 tensor sets is: [10.941110610961914, -81.12804412841797, -22.776622772216797, 0.18705224990844727, 61.750160217285156]\n",
      "\n",
      "Instance 68 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6, 7]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "don at index 6: [-0.2357296347618103, -2.7333314418792725, -0.3221378028392792, -1.6906704902648926, -1.6519677639007568]\n",
      "ut at index 7: [-0.6188623309135437, -1.9455623626708984, -0.2815909683704376, 0.39424484968185425, -1.113464593887329]\n",
      "Grand sum of 60 tensor sets is: [10.513814926147461, -83.46749114990234, -23.078487396240234, -0.4611605405807495, 60.3674430847168]\n",
      "\n",
      "Instance 69 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6, 7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "don at index 6: [0.7354114651679993, -2.1440248489379883, 0.8406842350959778, -2.2121052742004395, -2.129810333251953]\n",
      "ut at index 7: [0.940148651599884, -2.6817221641540527, -0.6096996068954468, 0.8936615586280823, -0.4848117530345917]\n",
      "Grand sum of 61 tensor sets is: [11.351594924926758, -85.88036346435547, -22.962995529174805, -1.1203824281692505, 59.06013107299805]\n",
      "\n",
      "Instance 70 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "don at index 4: [0.5170365571975708, -2.511070966720581, -1.424163818359375, 0.3065778613090515, 3.0947768688201904]\n",
      "Grand sum of 62 tensor sets is: [11.868631362915039, -88.39143371582031, -24.38715934753418, -0.813804566860199, 62.1549072265625]\n",
      "\n",
      "Instance 71 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [10, 11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "don at index 10: [0.21387451887130737, -1.2263014316558838, -0.28390827775001526, -0.9150680303573608, -1.239761471748352]\n",
      "ut at index 11: [0.27905187010765076, -1.7797138690948486, -1.2563680410385132, 1.7689166069030762, 2.5561132431030273]\n",
      "Grand sum of 63 tensor sets is: [12.115094184875488, -89.89443969726562, -25.157297134399414, -0.3868802785873413, 62.81308364868164]\n",
      "\n",
      "Instance 72 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [15, 16]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "don at index 15: [-0.033668152987957, -2.408372640609741, 0.7055983543395996, -2.627424716949463, -0.3052592873573303]\n",
      "ut at index 16: [0.9522087574005127, -2.748389720916748, -0.10951106250286102, 1.0868494510650635, 0.5106808543205261]\n",
      "Grand sum of 64 tensor sets is: [12.57436466217041, -92.47282409667969, -24.8592529296875, -1.157167911529541, 62.915794372558594]\n",
      "\n",
      "Instance 73 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "don at index 5: [-0.5131145119667053, -2.5901944637298584, 0.3312966227531433, 0.1565553843975067, 2.53139328956604]\n",
      "Grand sum of 65 tensor sets is: [12.061249732971191, -95.06301879882812, -24.527956008911133, -1.000612497329712, 65.44718933105469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 74 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [25, 38, 26]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "don at index 25: [0.12865662574768066, -0.43028172850608826, -0.07577422261238098, -0.7645759582519531, -1.105894923210144]\n",
      "ut at index 38: [0.8227853178977966, -0.013210296630859375, -0.8709388971328735, -0.9281275868415833, -2.9063570499420166]\n",
      "don at index 26: [0.989797055721283, -1.6185613870620728, -0.5203119516372681, 1.0635144710540771, 2.3602135181427]\n",
      "Grand sum of 66 tensor sets is: [12.708329200744629, -95.7503662109375, -25.016963958740234, -1.2103421688079834, 64.8965072631836]\n",
      "\n",
      "Instance 75 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [9, 10]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "don at index 9: [0.7483184933662415, -1.119639277458191, 0.6062742471694946, -1.1775404214859009, -0.7735169529914856]\n",
      "ut at index 10: [1.4594879150390625, -2.0205209255218506, -0.38311585783958435, 2.3135344982147217, 2.3049135208129883]\n",
      "Grand sum of 67 tensor sets is: [13.81223201751709, -97.32044982910156, -24.905384063720703, -0.642345130443573, 65.6622085571289]\n",
      "\n",
      "Instance 76 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [14, 29, 15]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "don at index 14: [-0.6731100082397461, -1.2483488321304321, -0.35417160391807556, -0.32683607935905457, -1.8932032585144043]\n",
      "ut at index 29: [-0.30351290106773376, -2.860690116882324, -0.661069393157959, -1.5337927341461182, -0.9000973701477051]\n",
      "don at index 15: [-0.1913979947566986, -2.027878999710083, -0.6667969226837158, 0.84623122215271, 2.4091036319732666]\n",
      "Grand sum of 68 tensor sets is: [13.422891616821289, -99.3660888671875, -25.466062545776367, -0.9804776906967163, 65.53414154052734]\n",
      "\n",
      "Instance 77 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "don at index 5: [0.7692846655845642, -1.9964779615402222, -1.1066031455993652, -0.12112860381603241, 1.5983774662017822]\n",
      "Grand sum of 69 tensor sets is: [14.19217586517334, -101.36256408691406, -26.57266616821289, -1.1016062498092651, 67.13252258300781]\n",
      "\n",
      "Instance 78 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "don at index 4: [-0.42498382925987244, -0.671596348285675, -1.1852537393569946, 1.129318118095398, 3.0815110206604004]\n",
      "Grand sum of 70 tensor sets is: [13.767191886901855, -102.0341567993164, -27.757919311523438, 0.027711868286132812, 70.21403503417969]\n",
      "\n",
      "Instance 79 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [20, 21]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "don at index 20: [0.007040061056613922, -2.482335329055786, 0.32584866881370544, -0.07982056587934494, -1.9558594226837158]\n",
      "ut at index 21: [-0.6375975608825684, -0.4468655586242676, 0.22895386815071106, 1.541576862335205, 1.2839162349700928]\n",
      "Grand sum of 71 tensor sets is: [13.451912879943848, -103.4987564086914, -27.480518341064453, 0.758590042591095, 69.87806701660156]\n",
      "\n",
      "Instance 80 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [8, 9]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "don at index 8: [0.08728832751512527, -1.2011536359786987, -0.5520994663238525, -0.8916661143302917, -1.184708833694458]\n",
      "ut at index 9: [-0.34696704149246216, -1.3969125747680664, -0.9875486493110657, 1.590114712715149, 1.604742169380188]\n",
      "Grand sum of 72 tensor sets is: [13.322073936462402, -104.79779052734375, -28.250343322753906, 1.1078143119812012, 70.08808135986328]\n",
      "\n",
      "Instance 81 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "don at index 12: [-0.12790533900260925, 0.09160960465669632, -0.6800025701522827, 0.7264894247055054, 2.0037426948547363]\n",
      "Grand sum of 73 tensor sets is: [13.194169044494629, -104.70618438720703, -28.93034553527832, 1.8343037366867065, 72.09182739257812]\n",
      "\n",
      "Instance 82 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6, 7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "don at index 6: [0.7354114651679993, -2.1440248489379883, 0.8406842350959778, -2.2121052742004395, -2.129810333251953]\n",
      "ut at index 7: [0.940148651599884, -2.6817221641540527, -0.6096996068954468, 0.8936615586280823, -0.4848117530345917]\n",
      "Grand sum of 74 tensor sets is: [14.031949043273926, -107.11905670166016, -28.81485366821289, 1.1750818490982056, 70.78451538085938]\n",
      "\n",
      "Instance 83 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [13, 14]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "don at index 13: [0.20468038320541382, -1.693149447441101, -0.21149992942810059, -0.847994863986969, 0.23929625749588013]\n",
      "ut at index 14: [0.13296358287334442, -2.3690528869628906, -0.4127228856086731, 1.2727887630462646, 3.488232374191284]\n",
      "Grand sum of 75 tensor sets is: [14.20077133178711, -109.15016174316406, -29.126964569091797, 1.3874788284301758, 72.64827728271484]\n",
      "\n",
      "Instance 84 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [107, 276]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 107: [-0.7828222513198853, -1.781507134437561, 0.9024642705917358, 1.3574938774108887, -1.3997414112091064]\n",
      "ut at index 276: [-0.5140088796615601, -1.8655489683151245, 0.5468668341636658, -0.2838279604911804, 2.2438652515411377]\n",
      "Grand sum of 76 tensor sets is: [13.552355766296387, -110.97368621826172, -28.402299880981445, 1.9243117570877075, 73.07034301757812]\n",
      "\n",
      "Instance 85 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "don at index 16: [0.32720786333084106, -2.1364498138427734, 0.2891390323638916, -1.430560827255249, -3.6399857997894287]\n",
      "Grand sum of 77 tensor sets is: [13.879563331604004, -113.11013793945312, -28.113161087036133, 0.4937509298324585, 69.43035888671875]\n",
      "\n",
      "Instance 86 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [24, 25]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "don at index 24: [1.1250345706939697, -2.7083356380462646, 0.7469278573989868, -2.474100112915039, -2.7829999923706055]\n",
      "ut at index 25: [0.0599542111158371, -2.0889761447906494, 0.6273910403251648, 0.2680168151855469, -0.13088393211364746]\n",
      "Grand sum of 78 tensor sets is: [14.472057342529297, -115.50879669189453, -27.426002502441406, -0.6092907190322876, 67.97341918945312]\n",
      "\n",
      "Instance 87 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [375]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 375: [0.5625423789024353, -0.2868376076221466, -0.8096054792404175, 1.0789543390274048, 1.2873749732971191]\n",
      "Grand sum of 79 tensor sets is: [15.034599304199219, -115.7956314086914, -28.235607147216797, 0.4696636199951172, 69.26079559326172]\n",
      "\n",
      "Instance 88 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "don at index 17: [0.9310728907585144, -1.5340113639831543, -1.069672703742981, -0.8753277659416199, 1.3633527755737305]\n",
      "Grand sum of 80 tensor sets is: [15.965672492980957, -117.32964324951172, -29.305280685424805, -0.4056641459465027, 70.6241455078125]\n",
      "\n",
      "Instance 89 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 90 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "don at index 13: [0.7407242655754089, -3.0320334434509277, -0.38410452008247375, 3.523912191390991, 2.4086122512817383]\n",
      "Grand sum of 81 tensor sets is: [16.706396102905273, -120.36167907714844, -29.68938446044922, 3.1182479858398438, 73.03276062011719]\n",
      "\n",
      "Instance 91 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "don at index 13: [0.48936039209365845, -1.0544302463531494, -0.5302718877792358, -1.4980175495147705, -1.2134044170379639]\n",
      "Grand sum of 82 tensor sets is: [17.195756912231445, -121.41610717773438, -30.219655990600586, 1.6202304363250732, 71.8193588256836]\n",
      "\n",
      "Instance 92 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 93 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [9, 10]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "don at index 9: [-0.1388152539730072, -2.773141622543335, 0.34635278582572937, -3.0922908782958984, -1.4951828718185425]\n",
      "ut at index 10: [-0.9595798850059509, -2.3768739700317383, -0.18115335702896118, 0.03767520934343338, 0.49459296464920044]\n",
      "Grand sum of 83 tensor sets is: [16.64655876159668, -123.9911117553711, -30.137056350708008, 0.09292256832122803, 71.31906127929688]\n",
      "\n",
      "Instance 94 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 95 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [33, 34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "don at index 33: [0.7487749457359314, -1.8746854066848755, 0.5082172155380249, -3.068044900894165, -1.285144567489624]\n",
      "ut at index 34: [-0.0329873189330101, -2.1389975547790527, -0.47448840737342834, -0.5864568948745728, 1.0990371704101562]\n",
      "Grand sum of 84 tensor sets is: [17.004451751708984, -125.99795532226562, -30.12019157409668, -1.7343283891677856, 71.22600555419922]\n",
      "\n",
      "Instance 96 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 97 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "don at index 2: [0.9746079444885254, -0.14755114912986755, -1.3107882738113403, 0.3635498583316803, 1.0291929244995117]\n",
      "Grand sum of 85 tensor sets is: [17.97905921936035, -126.1455078125, -31.430980682373047, -1.3707785606384277, 72.25519561767578]\n",
      "\n",
      "Instance 98 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "don at index 2: [0.8239725232124329, -0.1678398847579956, -0.5584408640861511, 1.2409847974777222, 1.374813199043274]\n",
      "Grand sum of 86 tensor sets is: [18.80303192138672, -126.31334686279297, -31.989421844482422, -0.12979376316070557, 73.63001251220703]\n",
      "\n",
      "Instance 99 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "don at index 4: [0.5170365571975708, -2.511070966720581, -1.424163818359375, 0.3065778613090515, 3.0947768688201904]\n",
      "Grand sum of 87 tensor sets is: [19.320068359375, -128.8244171142578, -33.4135856628418, 0.17678409814834595, 76.72479248046875]\n",
      "\n",
      "Instance 100 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [-0.14172697067260742, 0.47182098031044006, -0.3521590828895569, 0.37289056181907654, 2.5498294830322266]\n",
      "Grand sum of 88 tensor sets is: [19.178340911865234, -128.35260009765625, -33.765743255615234, 0.5496746301651001, 79.27462005615234]\n",
      "\n",
      "Instance 101 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2, 3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "don at index 2: [0.19102652370929718, -1.9017493724822998, -0.43987172842025757, -0.28757137060165405, 0.15530730783939362]\n",
      "ut at index 3: [0.1076449602842331, -2.0261642932891846, -1.3681341409683228, 1.786419153213501, 2.426692008972168]\n",
      "Grand sum of 89 tensor sets is: [19.32767677307129, -130.31655883789062, -34.66974639892578, 1.2990984916687012, 80.56562042236328]\n",
      "\n",
      "Instance 102 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "don at index 53: [0.8168477416038513, -1.372750163078308, 0.06778596341609955, 0.5160173773765564, 2.616507053375244]\n",
      "Grand sum of 90 tensor sets is: [20.14452362060547, -131.68931579589844, -34.601959228515625, 1.8151159286499023, 83.18212890625]\n",
      "\n",
      "Instance 103 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "don at index 15: [0.5603006482124329, -1.3166927099227905, -0.5842909812927246, 0.8968656659126282, 1.3962109088897705]\n",
      "Grand sum of 91 tensor sets is: [20.704824447631836, -133.00601196289062, -35.186248779296875, 2.7119815349578857, 84.57833862304688]\n",
      "\n",
      "Instance 104 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "don at index 8: [0.5276799201965332, -0.1352638304233551, -0.8777515888214111, 1.0546027421951294, 0.6179054379463196]\n",
      "Grand sum of 92 tensor sets is: [21.23250389099121, -133.1412811279297, -36.06399917602539, 3.7665843963623047, 85.19624328613281]\n",
      "\n",
      "Instance 105 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [9, 10]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "don at index 9: [0.7758761048316956, -1.8849899768829346, 0.2953639030456543, -1.1261863708496094, -0.9503711462020874]\n",
      "ut at index 10: [0.8196007013320923, -3.1699628829956055, -0.49984094500541687, 1.3828082084655762, 2.274813175201416]\n",
      "Grand sum of 93 tensor sets is: [22.030242919921875, -135.66876220703125, -36.166236877441406, 3.894895315170288, 85.85846710205078]\n",
      "\n",
      "Instance 106 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [12, 13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "don at index 12: [0.09079264849424362, -1.8884586095809937, -0.168696790933609, -1.867480993270874, -1.8230175971984863]\n",
      "ut at index 13: [0.8844726085662842, -2.0048933029174805, -0.9877229928970337, 0.7465624809265137, 0.6857075691223145]\n",
      "Grand sum of 94 tensor sets is: [22.51787567138672, -137.6154327392578, -36.74444580078125, 3.3344359397888184, 85.28981018066406]\n",
      "\n",
      "Instance 107 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [30, 31]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "don at index 30: [0.11601687967777252, -2.2524757385253906, -0.2486983835697174, -2.1503005027770996, 0.5836674571037292]\n",
      "ut at index 31: [-0.318869411945343, -2.7983546257019043, -0.6938642263412476, -0.11986923217773438, 3.5017080307006836]\n",
      "Grand sum of 95 tensor sets is: [22.41644859313965, -140.14085388183594, -37.215728759765625, 2.1993510723114014, 87.3324966430664]\n",
      "\n",
      "Instance 108 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 109 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "don at index 22: [-0.24073944985866547, 2.5714337825775146, -0.3264006972312927, 0.9804651737213135, 0.26912981271743774]\n",
      "Grand sum of 96 tensor sets is: [22.175708770751953, -137.56942749023438, -37.54212951660156, 3.179816246032715, 87.60162353515625]\n",
      "\n",
      "Instance 110 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [4, 7, 8]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "don at index 4: [0.08106571435928345, -2.005174398422241, 0.5317176580429077, -0.1935846358537674, -2.27455735206604]\n",
      "ut at index 7: [0.10672546923160553, -1.584251880645752, 0.3603205680847168, -1.1167000532150269, -4.130936145782471]\n",
      "don at index 8: [0.41229838132858276, -2.6583199501037598, 0.14627747237682343, 0.8879940509796143, 1.0894343852996826]\n",
      "Grand sum of 97 tensor sets is: [22.3757381439209, -139.65200805664062, -37.19602584838867, 3.039052724838257, 85.82960510253906]\n",
      "\n",
      "Instance 111 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6, 35, 54, 25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([84, 13, 768])\n",
      "Shape of summed layers is: 84 x 768\n",
      "don at index 6: [0.08097970485687256, -2.8403947353363037, 0.7373778820037842, -2.053750991821289, -1.5287296772003174]\n",
      "ut at index 35: [0.09113740175962448, -1.934160590171814, 0.3183201849460602, -1.678196907043457, -2.142035484313965]\n",
      "don at index 54: [-0.17813795804977417, -2.1451539993286133, 0.26895493268966675, -2.3349504470825195, -1.9847946166992188]\n",
      "ut at index 25: [0.6837242841720581, -1.4941788911819458, -0.6055026650428772, 0.6169036030769348, 2.544800281524658]\n",
      "Grand sum of 98 tensor sets is: [22.545164108276367, -141.75547790527344, -37.016239166259766, 1.6765540838241577, 85.05191802978516]\n",
      "\n",
      "Instance 112 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "don at index 4: [0.23941382765769958, -0.5954461693763733, -1.192845344543457, 0.5829442739486694, 0.4518643021583557]\n",
      "Grand sum of 99 tensor sets is: [22.784578323364258, -142.35092163085938, -38.209083557128906, 2.259498357772827, 85.5037841796875]\n",
      "\n",
      "Instance 113 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 114 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [7, 8]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "don at index 7: [0.8564081192016602, -2.23215389251709, 0.36189141869544983, -2.4817800521850586, -1.440755009651184]\n",
      "ut at index 8: [1.0970648527145386, -1.7125948667526245, -0.43254244327545166, -0.2827768623828888, 0.6955993175506592]\n",
      "Grand sum of 100 tensor sets is: [23.761314392089844, -144.32330322265625, -38.244407653808594, 0.8772199153900146, 85.1312026977539]\n",
      "\n",
      "Instance 115 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [-0.14172697067260742, 0.47182098031044006, -0.3521590828895569, 0.37289056181907654, 2.5498294830322266]\n",
      "Grand sum of 101 tensor sets is: [23.619586944580078, -143.8514862060547, -38.59656524658203, 1.2501105070114136, 87.6810302734375]\n",
      "\n",
      "Instance 116 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [40, 41]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "don at index 40: [0.3357926905155182, -1.2932593822479248, -0.6651858687400818, -1.7221393585205078, 0.419634610414505]\n",
      "ut at index 41: [0.14909836649894714, -1.9023582935333252, -0.8674076199531555, 0.8343127965927124, 2.794856309890747]\n",
      "Grand sum of 102 tensor sets is: [23.862031936645508, -145.4492950439453, -39.36286163330078, 0.8061972260475159, 89.28827667236328]\n",
      "\n",
      "Instance 117 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 118 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "don at index 7: [0.5401481986045837, -2.366635799407959, -0.4899621903896332, -2.01332688331604, -2.6190903186798096]\n",
      "Grand sum of 103 tensor sets is: [24.402179718017578, -147.81593322753906, -39.85282516479492, -1.207129716873169, 86.669189453125]\n",
      "\n",
      "Instance 119 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [107, 276]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 107: [-0.7828222513198853, -1.781507134437561, 0.9024642705917358, 1.3574938774108887, -1.3997414112091064]\n",
      "ut at index 276: [-0.5140088796615601, -1.8655489683151245, 0.5468668341636658, -0.2838279604911804, 2.2438652515411377]\n",
      "Grand sum of 104 tensor sets is: [23.753765106201172, -149.63946533203125, -39.12815856933594, -0.6702967882156372, 87.09124755859375]\n",
      "\n",
      "Instance 120 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 121 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2, 3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "don at index 2: [0.5782493948936462, -2.9345664978027344, 0.25326859951019287, -0.9970138072967529, 1.024877905845642]\n",
      "ut at index 3: [0.26609790325164795, -3.282137393951416, -0.7203695178031921, 1.1379865407943726, 2.1874780654907227]\n",
      "Grand sum of 105 tensor sets is: [24.175939559936523, -152.74781799316406, -39.36170959472656, -0.5998104214668274, 88.69742584228516]\n",
      "\n",
      "Instance 122 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "don at index 5: [-0.15915679931640625, -1.825394630432129, -0.6687279343605042, 0.25209397077560425, 2.319200038909912]\n",
      "Grand sum of 106 tensor sets is: [24.016782760620117, -154.57321166992188, -40.03043746948242, -0.34771645069122314, 91.0166244506836]\n",
      "\n",
      "Instance 123 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2, 3]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "don at index 2: [0.14219126105308533, -2.3203306198120117, -0.08992186188697815, 0.26374128460884094, 0.3346269428730011]\n",
      "ut at index 3: [0.1894630789756775, -2.635540723800659, 0.27793675661087036, 1.2978291511535645, 2.7473177909851074]\n",
      "Grand sum of 107 tensor sets is: [24.18260955810547, -157.0511474609375, -39.936431884765625, 0.43306875228881836, 92.5575942993164]\n",
      "\n",
      "Instance 124 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "don at index 16: [0.5826876163482666, -2.612400531768799, -0.4370241165161133, 0.4059343934059143, 1.7941620349884033]\n",
      "Grand sum of 108 tensor sets is: [24.765296936035156, -159.66354370117188, -40.37345504760742, 0.8390031456947327, 94.35175323486328]\n",
      "\n",
      "Instance 125 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [375]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 375: [0.5625423789024353, -0.2868376076221466, -0.8096054792404175, 1.0789543390274048, 1.2873749732971191]\n",
      "Grand sum of 109 tensor sets is: [25.327838897705078, -159.95037841796875, -41.18305969238281, 1.9179575443267822, 95.63912963867188]\n",
      "\n",
      "Instance 126 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2, 3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "don at index 2: [0.5782493948936462, -2.9345664978027344, 0.25326859951019287, -0.9970138072967529, 1.024877905845642]\n",
      "ut at index 3: [0.26609790325164795, -3.282137393951416, -0.7203695178031921, 1.1379865407943726, 2.1874780654907227]\n",
      "Grand sum of 110 tensor sets is: [25.75001335144043, -163.05873107910156, -41.41661071777344, 1.9884438514709473, 97.24530792236328]\n",
      "\n",
      "Instance 127 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [11, 24, 12, 25]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "don at index 11: [-0.028049014508724213, -1.9504233598709106, 0.14067873358726501, -0.8522046804428101, -0.25429993867874146]\n",
      "ut at index 24: [-0.07403268665075302, -1.7956650257110596, 1.2850154638290405, -0.932147741317749, -0.6927212476730347]\n",
      "don at index 12: [0.42460334300994873, -3.0614917278289795, -0.20499201118946075, 1.682119369506836, 3.878934860229492]\n",
      "ut at index 25: [0.7121375799179077, -2.6702914237976074, 0.7449662685394287, 0.562540590763092, 1.8063099384307861]\n",
      "Grand sum of 111 tensor sets is: [26.008678436279297, -165.42819213867188, -40.925193786621094, 2.103520631790161, 98.42986297607422]\n",
      "\n",
      "Instance 128 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "don at index 10: [1.0624487400054932, -1.3188806772232056, -0.9718278050422668, 1.6116831302642822, 0.40407681465148926]\n",
      "Grand sum of 112 tensor sets is: [27.07112693786621, -166.7470703125, -41.89702224731445, 3.7152037620544434, 98.83393859863281]\n",
      "\n",
      "Instance 129 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [10, 11]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "don at index 10: [0.7659896612167358, -1.2361595630645752, -1.2670791149139404, -0.9577925801277161, -1.281463623046875]\n",
      "ut at index 11: [-0.08969864249229431, -1.8446213006973267, -2.1869757175445557, 1.4679256677627563, 1.9410709142684937]\n",
      "Grand sum of 113 tensor sets is: [27.409273147583008, -168.28746032714844, -43.62405014038086, 3.9702703952789307, 99.16374206542969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 130 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "don at index 6: [-0.13467931747436523, 0.2830005884170532, -1.1367989778518677, 0.9941291809082031, -0.4078046679496765]\n",
      "Grand sum of 114 tensor sets is: [27.274593353271484, -168.00445556640625, -44.76084899902344, 4.964399337768555, 98.75593566894531]\n",
      "\n",
      "Instance 131 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "don at index 4: [-0.42498382925987244, -0.671596348285675, -1.1852537393569946, 1.129318118095398, 3.0815110206604004]\n",
      "Grand sum of 115 tensor sets is: [26.849609375, -168.67605590820312, -45.946102142333984, 6.093717575073242, 101.83744812011719]\n",
      "\n",
      "Instance 132 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [33, 34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "don at index 33: [0.7487749457359314, -1.8746854066848755, 0.5082172155380249, -3.068044900894165, -1.285144567489624]\n",
      "ut at index 34: [-0.0329873189330101, -2.1389975547790527, -0.47448840737342834, -0.5864568948745728, 1.0990371704101562]\n",
      "Grand sum of 116 tensor sets is: [27.207502365112305, -170.68289184570312, -45.929237365722656, 4.2664666175842285, 101.74439239501953]\n",
      "\n",
      "Instance 133 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 134 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [170, 240]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 170: [0.21222954988479614, -0.7334696054458618, 0.46047574281692505, 0.25820568203926086, 1.4583821296691895]\n",
      "ut at index 240: [1.4888808727264404, 0.305864155292511, -1.8490798473358154, -1.6738929748535156, -1.1718343496322632]\n",
      "Grand sum of 117 tensor sets is: [28.05805778503418, -170.89669799804688, -46.623538970947266, 3.5586228370666504, 101.88766479492188]\n",
      "\n",
      "Instance 135 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6, 7]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "don at index 6: [-0.1332254558801651, -2.7306699752807617, 0.04578429460525513, -1.4416425228118896, -1.5730950832366943]\n",
      "ut at index 7: [-0.363921195268631, -2.556577682495117, 0.4895380139350891, -0.1536264568567276, 1.217219591140747]\n",
      "Grand sum of 118 tensor sets is: [27.809484481811523, -173.5403289794922, -46.35587692260742, 2.760988235473633, 101.70972442626953]\n",
      "\n",
      "Instance 136 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "don at index 3: [1.116349220275879, -2.0871942043304443, -0.7241867780685425, 1.8150039911270142, 2.895801067352295]\n",
      "Grand sum of 119 tensor sets is: [28.92583465576172, -175.6275177001953, -47.08006286621094, 4.575992107391357, 104.60552215576172]\n",
      "\n",
      "Instance 137 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "don at index 5: [0.8283839225769043, -1.940810203552246, -0.932740330696106, -0.38608163595199585, 2.5709192752838135]\n",
      "Grand sum of 120 tensor sets is: [29.75421905517578, -177.56832885742188, -48.01280212402344, 4.189910411834717, 107.17644500732422]\n",
      "\n",
      "Instance 138 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [38, 39]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "don at index 38: [0.23478004336357117, -2.8494932651519775, 0.5056911706924438, -1.8021413087844849, -0.2829858660697937]\n",
      "ut at index 39: [0.35093849897384644, -3.002840757369995, -0.647901177406311, 0.7467737197875977, 2.108610153198242]\n",
      "Grand sum of 121 tensor sets is: [30.04707908630371, -180.49449157714844, -48.08390808105469, 3.662226676940918, 108.0892562866211]\n",
      "\n",
      "Instance 139 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [16, 17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "don at index 16: [0.11229827255010605, -1.9154601097106934, 0.6002202033996582, -3.7690320014953613, 0.12684573233127594]\n",
      "ut at index 17: [0.13950993120670319, -2.0216593742370605, 0.15583306550979614, -0.553665280342102, 2.104123592376709]\n",
      "Grand sum of 122 tensor sets is: [30.172983169555664, -182.4630584716797, -47.70588302612305, 1.500878095626831, 109.20474243164062]\n",
      "\n",
      "Instance 140 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "don at index 5: [0.7692846655845642, -1.9964779615402222, -1.1066031455993652, -0.12112860381603241, 1.5983774662017822]\n",
      "Grand sum of 123 tensor sets is: [30.94226837158203, -184.45953369140625, -48.81248474121094, 1.3797495365142822, 110.8031234741211]\n",
      "\n",
      "Instance 141 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [8, 9]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "don at index 8: [0.16940812766551971, -3.5267670154571533, 1.3014224767684937, -0.10824040323495865, -2.3405697345733643]\n",
      "ut at index 9: [0.5213379859924316, -2.62375545501709, -0.778445839881897, 0.9949233531951904, 0.34450995922088623]\n",
      "Grand sum of 124 tensor sets is: [31.287641525268555, -187.5347900390625, -48.550994873046875, 1.8230910301208496, 109.80509185791016]\n",
      "\n",
      "Instance 142 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [287, 293, 305, 327, 328]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 287: [1.1329355239868164, -2.213629961013794, 1.3196728229522705, 0.028406217694282532, -4.826488018035889]\n",
      "ut at index 293: [0.5194762349128723, -1.4699010848999023, 1.1147050857543945, 0.15766960382461548, -5.358469009399414]\n",
      "don at index 305: [0.21543021500110626, -2.2921054363250732, 1.1510918140411377, -1.9656918048858643, -2.431967258453369]\n",
      "ut at index 327: [0.41094502806663513, -1.6189523935317993, 2.0688834190368652, -0.6347804665565491, -1.1235648393630981]\n",
      "don at index 328: [1.1051775217056274, -0.8251170516014099, 0.1512872874736786, 2.3018808364868164, 1.9743163585662842]\n",
      "Grand sum of 125 tensor sets is: [31.964433670043945, -189.21873474121094, -47.38986587524414, 1.8005878925323486, 107.45185852050781]\n",
      "\n",
      "Instance 143 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [-0.14172697067260742, 0.47182098031044006, -0.3521590828895569, 0.37289056181907654, 2.5498294830322266]\n",
      "Grand sum of 126 tensor sets is: [31.82270622253418, -188.74691772460938, -47.74202346801758, 2.173478364944458, 110.0016860961914]\n",
      "\n",
      "Instance 144 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "don at index 16: [0.3699299693107605, -2.3579819202423096, -0.5975395441055298, -1.3178486824035645, 0.2558378279209137]\n",
      "Grand sum of 127 tensor sets is: [32.19263458251953, -191.1049041748047, -48.339561462402344, 0.8556296825408936, 110.25752258300781]\n",
      "\n",
      "Instance 145 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [66, 67, 69, 68]\n",
      "Size of token embeddings is torch.Size([225, 13, 768])\n",
      "Shape of summed layers is: 225 x 768\n",
      "don at index 66: [1.7846065759658813, -3.0518798828125, 0.28452086448669434, -2.4417741298675537, -2.861297607421875]\n",
      "ut at index 67: [1.1181931495666504, -2.6857588291168213, 0.8235681653022766, -1.058550477027893, -3.9639511108398438]\n",
      "don at index 69: [0.7916389107704163, -3.61775803565979, 1.2184500694274902, -1.3598158359527588, -3.7458434104919434]\n",
      "ut at index 68: [-0.4940941035747528, -0.6710219383239746, -0.7919847369194031, 2.043755531311035, -0.4542829394340515]\n",
      "Grand sum of 128 tensor sets is: [32.99272155761719, -193.61151123046875, -47.9559211730957, 0.1515333652496338, 107.50117492675781]\n",
      "\n",
      "Instance 146 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6, 7]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "don at index 6: [0.2692449390888214, -1.7226802110671997, -0.13733220100402832, -0.8295553922653198, -0.49460598826408386]\n",
      "ut at index 7: [0.7002031803131104, -1.3759750127792358, -0.8611531853675842, 0.9236180782318115, 1.2962377071380615]\n",
      "Grand sum of 129 tensor sets is: [33.477447509765625, -195.1608428955078, -48.455162048339844, 0.19856470823287964, 107.90199279785156]\n",
      "\n",
      "Instance 147 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [-0.14172697067260742, 0.47182098031044006, -0.3521590828895569, 0.37289056181907654, 2.5498294830322266]\n",
      "Grand sum of 130 tensor sets is: [33.33572006225586, -194.68902587890625, -48.80731964111328, 0.5714552402496338, 110.45182037353516]\n",
      "\n",
      "Instance 148 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [12, 13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "don at index 12: [0.09079264849424362, -1.8884586095809937, -0.168696790933609, -1.867480993270874, -1.8230175971984863]\n",
      "ut at index 13: [0.8844726085662842, -2.0048933029174805, -0.9877229928970337, 0.7465624809265137, 0.6857075691223145]\n",
      "Grand sum of 131 tensor sets is: [33.8233528137207, -196.6356964111328, -49.385528564453125, 0.010995984077453613, 109.88316345214844]\n",
      "\n",
      "Instance 149 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2, 3]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "don at index 2: [-0.11211634427309036, -1.2012290954589844, -0.031236663460731506, -0.637797474861145, -0.9408020377159119]\n",
      "ut at index 3: [0.1305137276649475, -1.6074929237365723, -0.8125274777412415, 0.1544751226902008, 2.606478691101074]\n",
      "Grand sum of 132 tensor sets is: [33.832550048828125, -198.04005432128906, -49.807411193847656, -0.2306651920080185, 110.71600341796875]\n",
      "\n",
      "Instance 150 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [446, 443]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 446: [0.15398499369621277, -3.6754698753356934, 1.0040132999420166, -0.29608720541000366, -5.517543792724609]\n",
      "ut at index 443: [1.0218833684921265, 0.4551970958709717, 0.31922274827957153, 0.5006868243217468, 1.831423282623291]\n",
      "Grand sum of 133 tensor sets is: [34.42048263549805, -199.6501922607422, -49.14579391479492, -0.1283653825521469, 108.87294006347656]\n",
      "\n",
      "Instance 151 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [17, 18]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "don at index 17: [0.9043378829956055, -2.585566759109497, -0.03737860918045044, -1.915137767791748, -1.0930029153823853]\n",
      "ut at index 18: [0.8958221077919006, -2.5093865394592285, -0.8253012299537659, 1.029409408569336, -0.5128284096717834]\n",
      "Grand sum of 134 tensor sets is: [35.32056427001953, -202.19766235351562, -49.57713317871094, -0.5712295770645142, 108.07002258300781]\n",
      "\n",
      "Instance 152 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [8, 9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "don at index 8: [0.10214294493198395, -2.0802388191223145, 0.22026492655277252, 0.8255242109298706, -0.3476841151714325]\n",
      "ut at index 9: [0.5386989712715149, -2.9644815921783447, -0.7211152911186218, 2.0142953395843506, 1.0676243305206299]\n",
      "Grand sum of 135 tensor sets is: [35.64098358154297, -204.7200164794922, -49.82755661010742, 0.8486801385879517, 108.42999267578125]\n",
      "\n",
      "Instance 153 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [15, 16]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "don at index 15: [0.6936078071594238, -1.422450065612793, -0.30303144454956055, -1.943579912185669, -0.877185583114624]\n",
      "ut at index 16: [0.5470091700553894, -0.9774281978607178, -0.4469681978225708, 0.13455656170845032, 0.8203142881393433]\n",
      "Grand sum of 136 tensor sets is: [36.26129150390625, -205.91995239257812, -50.20255661010742, -0.05583155155181885, 108.40155792236328]\n",
      "\n",
      "Instance 154 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [8, 9]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "don at index 8: [0.896664559841156, -3.1389591693878174, 0.6578758358955383, -1.3330429792404175, 1.128286361694336]\n",
      "ut at index 9: [1.428094506263733, -3.5510776042938232, -0.019178174436092377, 0.06719757616519928, 2.8037641048431396]\n",
      "Grand sum of 137 tensor sets is: [37.42367172241211, -209.2649688720703, -49.883209228515625, -0.6887542605400085, 110.36758422851562]\n",
      "\n",
      "Instance 155 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "don at index 7: [0.08592787384986877, -0.6758008599281311, -1.1533020734786987, -0.33180204033851624, 1.9060533046722412]\n",
      "Grand sum of 138 tensor sets is: [37.50959777832031, -209.94076538085938, -51.0365104675293, -1.0205563306808472, 112.27363586425781]\n",
      "\n",
      "Instance 156 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "don at index 12: [-0.800042986869812, -2.4601199626922607, -0.32954978942871094, -3.1421051025390625, 1.7570024728775024]\n",
      "Grand sum of 139 tensor sets is: [36.709556579589844, -212.40087890625, -51.366058349609375, -4.162661552429199, 114.0306396484375]\n",
      "\n",
      "Instance 157 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 158 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [10, 11]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "don at index 10: [0.262528657913208, -1.1681879758834839, 0.11725039780139923, -0.1966378092765808, 0.08446631580591202]\n",
      "ut at index 11: [0.7475185394287109, -2.2282958030700684, -0.8457651138305664, 1.61667799949646, 3.513216257095337]\n",
      "Grand sum of 140 tensor sets is: [37.21458053588867, -214.09912109375, -51.730316162109375, -3.452641487121582, 115.82948303222656]\n",
      "\n",
      "Instance 159 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [-0.14172697067260742, 0.47182098031044006, -0.3521590828895569, 0.37289056181907654, 2.5498294830322266]\n",
      "Grand sum of 141 tensor sets is: [37.072853088378906, -213.62730407714844, -52.08247375488281, -3.0797510147094727, 118.37931060791016]\n",
      "\n",
      "Instance 160 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "don at index 5: [0.16139501333236694, -2.017814874649048, -1.275746464729309, -0.12344907969236374, 2.26247239112854]\n",
      "Grand sum of 142 tensor sets is: [37.234249114990234, -215.64512634277344, -53.358219146728516, -3.203200101852417, 120.64178466796875]\n",
      "\n",
      "Instance 161 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 162 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "don at index 5: [0.16139501333236694, -2.017814874649048, -1.275746464729309, -0.12344907969236374, 2.26247239112854]\n",
      "Grand sum of 143 tensor sets is: [37.39564514160156, -217.66294860839844, -54.63396453857422, -3.3266491889953613, 122.90425872802734]\n",
      "\n",
      "Instance 163 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [14, 15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "don at index 14: [0.35918113589286804, -1.5355286598205566, 0.4383193254470825, 0.32819342613220215, -1.8536171913146973]\n",
      "ut at index 15: [0.22745630145072937, -1.8879423141479492, -0.5916155576705933, 2.1223862171173096, 1.3687055110931396]\n",
      "Grand sum of 144 tensor sets is: [37.68896484375, -219.3746795654297, -54.71061325073242, -2.1013593673706055, 122.66180419921875]\n",
      "\n",
      "Instance 164 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [22, 23]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "don at index 22: [1.1082838773727417, -2.236384391784668, -0.01523357629776001, -2.185786485671997, 0.855391263961792]\n",
      "ut at index 23: [0.07250308990478516, -1.6067143678665161, -0.6324765086174011, 0.22475269436836243, 1.9118175506591797]\n",
      "Grand sum of 145 tensor sets is: [38.27935791015625, -221.29623413085938, -55.03446960449219, -3.081876277923584, 124.04541015625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 165 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [17, 48, 18]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "don at index 17: [0.7156343460083008, -2.8311758041381836, 0.5119290947914124, -2.5858240127563477, -2.117389440536499]\n",
      "ut at index 48: [0.34373247623443604, -3.3603177070617676, -1.0746221542358398, -3.823596954345703, -1.869837760925293]\n",
      "don at index 18: [0.055920008569955826, -2.565570116043091, -0.3032285273075104, -1.034118890762329, 2.4623634815216064]\n",
      "Grand sum of 146 tensor sets is: [38.651119232177734, -224.2152557373047, -55.3231086730957, -5.563055992126465, 123.53712463378906]\n",
      "\n",
      "Instance 166 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6, 7]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "don at index 6: [0.7280679941177368, -2.292299747467041, -0.2815035581588745, -1.7646896839141846, -0.29935893416404724]\n",
      "ut at index 7: [1.059558629989624, -2.2517893314361572, -1.045933485031128, 0.5922040939331055, 2.4753105640411377]\n",
      "Grand sum of 147 tensor sets is: [39.5449333190918, -226.4873046875, -55.9868278503418, -6.149298667907715, 124.6250991821289]\n",
      "\n",
      "Instance 167 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 168 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [16, 17]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "don at index 16: [0.798382580280304, -1.5691659450531006, 0.18140625953674316, -3.229130744934082, -0.7068808078765869]\n",
      "ut at index 17: [0.044904690235853195, -1.510911226272583, -0.22475147247314453, -0.41741523146629333, 1.275543451309204]\n",
      "Grand sum of 148 tensor sets is: [39.966575622558594, -228.02734375, -56.00849914550781, -7.97257137298584, 124.90943145751953]\n",
      "\n",
      "Instance 169 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "don at index 5: [-0.17750507593154907, -0.6927163600921631, -0.8109767436981201, 0.506959080696106, 0.7003166079521179]\n",
      "Grand sum of 149 tensor sets is: [39.78907012939453, -228.72006225585938, -56.81947708129883, -7.465612411499023, 125.60974884033203]\n",
      "\n",
      "Instance 170 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [33, 34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "don at index 33: [0.7487749457359314, -1.8746854066848755, 0.5082172155380249, -3.068044900894165, -1.285144567489624]\n",
      "ut at index 34: [-0.0329873189330101, -2.1389975547790527, -0.47448840737342834, -0.5864568948745728, 1.0990371704101562]\n",
      "Grand sum of 150 tensor sets is: [40.14696502685547, -230.72689819335938, -56.8026123046875, -9.292863845825195, 125.51669311523438]\n",
      "\n",
      "Instance 171 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([89, 13, 768])\n",
      "Shape of summed layers is: 89 x 768\n",
      "don at index 32: [-0.85066819190979, -1.20965576171875, -0.6816798448562622, -0.08660152554512024, -0.7171002626419067]\n",
      "Grand sum of 151 tensor sets is: [39.296295166015625, -231.93655395507812, -57.484291076660156, -9.379465103149414, 124.79959106445312]\n",
      "\n",
      "Instance 172 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [4, 5]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "don at index 4: [-0.14798498153686523, -2.4731571674346924, -0.09102371335029602, -2.062427282333374, 0.794765293598175]\n",
      "ut at index 5: [-0.09392297267913818, -2.5459911823272705, -0.4760199189186096, -0.5381265878677368, 3.3458597660064697]\n",
      "Grand sum of 152 tensor sets is: [39.17534255981445, -234.4461212158203, -57.76781463623047, -10.679741859436035, 126.86990356445312]\n",
      "\n",
      "Instance 173 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "don at index 5: [-0.37601438164711, -0.6519495248794556, -0.7192163467407227, 2.1435790061950684, 1.4099366664886475]\n",
      "Grand sum of 153 tensor sets is: [38.7993278503418, -235.0980682373047, -58.487030029296875, -8.536163330078125, 128.27984619140625]\n",
      "\n",
      "Instance 174 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [24, 25]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "don at index 24: [0.7252271175384521, -1.1814944744110107, 0.9732414484024048, -0.9995427131652832, -0.0061468034982681274]\n",
      "ut at index 25: [1.6548370122909546, -1.7399364709854126, -0.0941336378455162, 0.4709262251853943, 1.5499491691589355]\n",
      "Grand sum of 154 tensor sets is: [39.98936080932617, -236.55877685546875, -58.04747772216797, -8.800471305847168, 129.05174255371094]\n",
      "\n",
      "Instance 175 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [30, 31]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "don at index 30: [0.14531613886356354, -1.3198797702789307, 0.38883548974990845, 0.33145204186439514, -0.6407531499862671]\n",
      "ut at index 31: [-0.4540005624294281, -0.967801034450531, -0.6398885250091553, 0.6765326857566833, 1.891493320465088]\n",
      "Grand sum of 155 tensor sets is: [39.835018157958984, -237.70262145996094, -58.173004150390625, -8.296479225158691, 129.67710876464844]\n",
      "\n",
      "Instance 176 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [10, 21, 11, 22]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "don at index 10: [-0.34375086426734924, -3.236130475997925, -0.6631886959075928, 0.07267601788043976, -0.29350733757019043]\n",
      "ut at index 21: [-0.7386229038238525, -2.35355281829834, 1.4945720434188843, -0.8407381772994995, -1.6465833187103271]\n",
      "don at index 11: [-0.36825284361839294, -3.517852306365967, -0.4754111170768738, 0.5576900839805603, 2.974224090576172]\n",
      "ut at index 22: [-0.2455742210149765, -2.742264986038208, 1.3702144622802734, 0.2925226092338562, 0.9157461524009705]\n",
      "Grand sum of 156 tensor sets is: [39.41096878051758, -240.66506958007812, -57.741458892822266, -8.275941848754883, 130.16458129882812]\n",
      "\n",
      "Instance 177 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "don at index 2: [1.480043888092041, -2.136979818344116, -0.7388366460800171, 1.9172601699829102, 3.4276604652404785]\n",
      "Grand sum of 157 tensor sets is: [40.891014099121094, -242.8020477294922, -58.48029708862305, -6.358681678771973, 133.5922393798828]\n",
      "\n",
      "Instance 178 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2, 12]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "don at index 2: [1.0181771516799927, -2.4232635498046875, -0.7179103493690491, 2.0740303993225098, 3.3590750694274902]\n",
      "ut at index 12: [1.0101863145828247, -3.2375574111938477, -0.5167855620384216, 1.417349100112915, 3.3928115367889404]\n",
      "Grand sum of 158 tensor sets is: [41.90519714355469, -245.63246154785156, -59.0976448059082, -4.612991809844971, 136.9681854248047]\n",
      "\n",
      "Instance 179 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "don at index 29: [0.5717961192131042, -1.8922594785690308, -0.5416176915168762, 1.1222009658813477, 1.418078064918518]\n",
      "Grand sum of 159 tensor sets is: [42.476993560791016, -247.52471923828125, -59.63926315307617, -3.490790843963623, 138.38626098632812]\n",
      "\n",
      "Instance 180 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "don at index 6: [-0.13467931747436523, 0.2830005884170532, -1.1367989778518677, 0.9941291809082031, -0.4078046679496765]\n",
      "Grand sum of 160 tensor sets is: [42.342315673828125, -247.24171447753906, -60.77606201171875, -2.49666166305542, 137.97845458984375]\n",
      "\n",
      "Instance 181 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 182 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [9, 10]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "don at index 9: [-0.009678229689598083, -0.6380492448806763, -0.49665921926498413, -1.6329911947250366, -3.669311046600342]\n",
      "ut at index 10: [0.26262176036834717, -0.3501674234867096, -2.0900285243988037, -1.738567590713501, 0.8266617059707642]\n",
      "Grand sum of 161 tensor sets is: [42.468788146972656, -247.73582458496094, -62.06940460205078, -4.182440757751465, 136.55712890625]\n",
      "\n",
      "Instance 183 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [-0.14172697067260742, 0.47182098031044006, -0.3521590828895569, 0.37289056181907654, 2.5498294830322266]\n",
      "Grand sum of 162 tensor sets is: [42.32706069946289, -247.26400756835938, -62.42156219482422, -3.8095502853393555, 139.10696411132812]\n",
      "\n",
      "Instance 184 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 185 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [4, 29, 5, 30]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "don at index 4: [-0.02514607459306717, -1.500693440437317, -0.28870174288749695, -0.16354142129421234, -0.8835636973381042]\n",
      "ut at index 29: [-0.3905711770057678, -0.26208657026290894, 0.20524010062217712, -0.28351402282714844, -2.431461811065674]\n",
      "don at index 5: [-0.0029862187802791595, -1.2497395277023315, -1.3891935348510742, 1.4940309524536133, 1.7771936655044556]\n",
      "ut at index 30: [0.007984831929206848, -0.9411785006523132, -1.166261911392212, 0.6649699211120605, 1.5070499181747437]\n",
      "Grand sum of 163 tensor sets is: [42.22438049316406, -248.25242614746094, -63.08129119873047, -3.381563901901245, 139.09927368164062]\n",
      "\n",
      "Instance 186 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "don at index 14: [0.7155311703681946, -2.1631174087524414, -0.11899793148040771, 1.0524749755859375, 1.5262393951416016]\n",
      "Grand sum of 164 tensor sets is: [42.939910888671875, -250.41554260253906, -63.20029067993164, -2.3290889263153076, 140.62551879882812]\n",
      "\n",
      "Instance 187 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [-0.14172697067260742, 0.47182098031044006, -0.3521590828895569, 0.37289056181907654, 2.5498294830322266]\n",
      "Grand sum of 165 tensor sets is: [42.79818344116211, -249.9437255859375, -63.55244827270508, -1.9561983346939087, 143.17535400390625]\n",
      "\n",
      "Instance 188 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [19, 20]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "don at index 19: [0.2398320436477661, -1.149245262145996, 0.19763922691345215, -0.5627824068069458, -0.2730967402458191]\n",
      "ut at index 20: [0.7504864931106567, -0.6472061276435852, -0.08317527174949646, 0.5578646063804626, 2.683077573776245]\n",
      "Grand sum of 166 tensor sets is: [43.29334259033203, -250.84194946289062, -63.495216369628906, -1.9586572647094727, 144.38034057617188]\n",
      "\n",
      "Instance 189 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [3, 4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 3: [-0.05081440880894661, -4.294591903686523, 0.4064108729362488, -1.7176940441131592, -1.3318278789520264]\n",
      "ut at index 4: [0.8703140020370483, -3.812018632888794, -0.5894112586975098, 0.5235399007797241, 1.246109962463379]\n",
      "Grand sum of 167 tensor sets is: [43.70309066772461, -254.89524841308594, -63.58671569824219, -2.555734395980835, 144.3374786376953]\n",
      "\n",
      "Instance 190 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "don at index 9: [0.36300861835479736, -1.887847900390625, -0.8876609802246094, 0.4553930461406708, 1.7706129550933838]\n",
      "Grand sum of 168 tensor sets is: [44.06610107421875, -256.7830810546875, -64.47438049316406, -2.100341320037842, 146.10809326171875]\n",
      "\n",
      "Instance 191 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [33, 34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "don at index 33: [0.7487749457359314, -1.8746854066848755, 0.5082172155380249, -3.068044900894165, -1.285144567489624]\n",
      "ut at index 34: [-0.0329873189330101, -2.1389975547790527, -0.47448840737342834, -0.5864568948745728, 1.0990371704101562]\n",
      "Grand sum of 169 tensor sets is: [44.42399597167969, -258.7899169921875, -64.45751953125, -3.9275922775268555, 146.01504516601562]\n",
      "\n",
      "Instance 192 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "don at index 6: [0.42979833483695984, 0.43603789806365967, -0.9433259963989258, 0.23466885089874268, -1.3206731081008911]\n",
      "Grand sum of 170 tensor sets is: [44.85379409790039, -258.3538818359375, -65.40084838867188, -3.6929235458374023, 144.69436645507812]\n",
      "\n",
      "Instance 193 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "don at index 17: [-0.22906915843486786, -0.7942038178443909, -1.2394667863845825, 2.720798969268799, 2.4989190101623535]\n",
      "Grand sum of 171 tensor sets is: [44.624725341796875, -259.1480712890625, -66.64031219482422, -0.9721245765686035, 147.1932830810547]\n",
      "\n",
      "Instance 194 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [11, 12]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "don at index 11: [0.5567238330841064, -2.677363157272339, 0.1226155161857605, -0.5302451252937317, -2.4335689544677734]\n",
      "ut at index 12: [-0.21299007534980774, -1.6047519445419312, 1.0772333145141602, 0.7873544096946716, -0.2115335911512375]\n",
      "Grand sum of 172 tensor sets is: [44.796592712402344, -261.28912353515625, -66.04039001464844, -0.8435699343681335, 145.8707275390625]\n",
      "\n",
      "Instance 195 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "don at index 2: [-0.2801568806171417, -2.333805799484253, -0.8575619459152222, -1.370595097541809, -2.165776491165161]\n",
      "Grand sum of 173 tensor sets is: [44.51643753051758, -263.6229248046875, -66.89794921875, -2.214164972305298, 143.7049560546875]\n",
      "\n",
      "Instance 196 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [68]\n",
      "Size of token embeddings is torch.Size([105, 13, 768])\n",
      "Shape of summed layers is: 105 x 768\n",
      "don at index 68: [0.3079909086227417, -1.616295576095581, 0.684352695941925, 1.4314541816711426, -0.4940110445022583]\n",
      "Grand sum of 174 tensor sets is: [44.82442855834961, -265.2392272949219, -66.2136001586914, -0.7827107906341553, 143.2109375]\n",
      "\n",
      "Instance 197 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [128]\n",
      "Size of token embeddings is torch.Size([272, 13, 768])\n",
      "Shape of summed layers is: 272 x 768\n",
      "don at index 128: [-0.49914297461509705, -2.4833881855010986, -1.3672475814819336, 1.013115644454956, -0.7563567757606506]\n",
      "Grand sum of 175 tensor sets is: [44.325286865234375, -267.7226257324219, -67.58084869384766, 0.23040485382080078, 142.45457458496094]\n",
      "\n",
      "Instance 198 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "don at index 12: [-0.12790533900260925, 0.09160960465669632, -0.6800025701522827, 0.7264894247055054, 2.0037426948547363]\n",
      "Grand sum of 176 tensor sets is: [44.19738006591797, -267.6310119628906, -68.26084899902344, 0.9568942785263062, 144.45831298828125]\n",
      "\n",
      "Instance 199 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "don at index 5: [0.16139501333236694, -2.017814874649048, -1.275746464729309, -0.12344907969236374, 2.26247239112854]\n",
      "Grand sum of 177 tensor sets is: [44.3587760925293, -269.6488342285156, -69.5365982055664, 0.8334451913833618, 146.7207794189453]\n",
      "\n",
      "Instance 200 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6, 35, 54, 25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([84, 13, 768])\n",
      "Shape of summed layers is: 84 x 768\n",
      "don at index 6: [0.08097970485687256, -2.8403947353363037, 0.7373778820037842, -2.053750991821289, -1.5287296772003174]\n",
      "ut at index 35: [0.09113740175962448, -1.934160590171814, 0.3183201849460602, -1.678196907043457, -2.142035484313965]\n",
      "don at index 54: [-0.17813795804977417, -2.1451539993286133, 0.26895493268966675, -2.3349504470825195, -1.9847946166992188]\n",
      "ut at index 25: [0.6837242841720581, -1.4941788911819458, -0.6055026650428772, 0.6169036030769348, 2.544800281524658]\n",
      "Grand sum of 178 tensor sets is: [44.528202056884766, -271.7523193359375, -69.3568115234375, -0.5290534496307373, 145.94308471679688]\n",
      "\n",
      "Instance 201 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [12, 13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "don at index 12: [0.09079264849424362, -1.8884586095809937, -0.168696790933609, -1.867480993270874, -1.8230175971984863]\n",
      "ut at index 13: [0.8844726085662842, -2.0048933029174805, -0.9877229928970337, 0.7465624809265137, 0.6857075691223145]\n",
      "Grand sum of 179 tensor sets is: [45.01583480834961, -273.6990051269531, -69.93502044677734, -1.0895127058029175, 145.3744354248047]\n",
      "\n",
      "Instance 202 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 203 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 204 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [8, 9]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "don at index 8: [0.5711906552314758, -1.601231336593628, -0.0026515722274780273, -0.753062903881073, 0.8424535989761353]\n",
      "ut at index 9: [1.332571268081665, -2.72910737991333, -0.4906378984451294, 1.648730754852295, 3.7500102519989014]\n",
      "Grand sum of 180 tensor sets is: [45.967716217041016, -275.8641662597656, -70.1816635131836, -0.6416788101196289, 147.67066955566406]\n",
      "\n",
      "Instance 205 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 206 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [52, 40, 53]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "don at index 52: [-0.40063798427581787, -2.6909167766571045, 0.32780689001083374, -2.658057689666748, -2.4038352966308594]\n",
      "ut at index 40: [0.06244165450334549, -2.2988088130950928, -1.1550745964050293, -0.656804084777832, -0.13419444859027863]\n",
      "don at index 53: [-0.39210453629493713, -2.0671465396881104, 0.07117017358541489, -0.5987898707389832, -0.14413510262966156]\n",
      "Grand sum of 181 tensor sets is: [45.724281311035156, -278.2164611816406, -70.4336929321289, -1.946229338645935, 146.776611328125]\n",
      "\n",
      "Instance 207 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [42, 44, 43]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "don at index 42: [0.8639466166496277, -3.979693651199341, 1.7496671676635742, -0.4039738178253174, -1.4949533939361572]\n",
      "ut at index 44: [0.0656195655465126, -3.2209877967834473, 1.6193400621414185, -0.9437539577484131, -2.8912711143493652]\n",
      "don at index 43: [-1.288833737373352, -1.1940556764602661, 0.5592324733734131, 2.990647315979004, 0.761457085609436]\n",
      "Grand sum of 182 tensor sets is: [45.60452651977539, -281.01470947265625, -69.12428283691406, -1.3985894918441772, 145.568359375]\n",
      "\n",
      "Instance 208 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [22, 23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "don at index 22: [0.14856338500976562, -2.5698843002319336, -1.0705775022506714, -2.9714345932006836, 1.6124967336654663]\n",
      "ut at index 23: [-0.13435737788677216, -2.7012243270874023, -1.218083143234253, -0.48357027769088745, 1.8085863590240479]\n",
      "Grand sum of 183 tensor sets is: [45.611629486083984, -283.6502685546875, -70.26861572265625, -3.126091957092285, 147.27890014648438]\n",
      "\n",
      "Instance 209 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [17, 18]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "don at index 17: [0.9043378829956055, -2.585566759109497, -0.03737860918045044, -1.915137767791748, -1.0930029153823853]\n",
      "ut at index 18: [0.8958221077919006, -2.5093865394592285, -0.8253012299537659, 1.029409408569336, -0.5128284096717834]\n",
      "Grand sum of 184 tensor sets is: [46.51171112060547, -286.19775390625, -70.69995880126953, -3.568956136703491, 146.47598266601562]\n",
      "\n",
      "Instance 210 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2, 3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "don at index 2: [0.14536817371845245, -1.7644200325012207, -0.40731024742126465, -0.27992957830429077, -0.3623782992362976]\n",
      "ut at index 3: [0.509689211845398, -2.3197240829467773, -0.958564281463623, 2.4688708782196045, 2.808277130126953]\n",
      "Grand sum of 185 tensor sets is: [46.83924102783203, -288.2398376464844, -71.38289642333984, -2.474485397338867, 147.6989288330078]\n",
      "\n",
      "Instance 211 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [16, 17]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "don at index 16: [0.5113610029220581, -2.291823387145996, -0.5829050540924072, -0.6815723180770874, -0.4591708481311798]\n",
      "ut at index 17: [0.0005117692053318024, -2.6755146980285645, -0.969950258731842, 0.7871543169021606, 1.5644841194152832]\n",
      "Grand sum of 186 tensor sets is: [47.095176696777344, -290.7235107421875, -72.1593246459961, -2.421694278717041, 148.2515869140625]\n",
      "\n",
      "Instance 212 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [1.0235028266906738, -2.4555857181549072, -1.390946626663208, 0.7966101765632629, -1.1063264608383179]\n",
      "Grand sum of 187 tensor sets is: [48.11867904663086, -293.1791076660156, -73.5502700805664, -1.6250841617584229, 147.145263671875]\n",
      "\n",
      "Instance 213 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "don at index 4: [-0.42498382925987244, -0.671596348285675, -1.1852537393569946, 1.129318118095398, 3.0815110206604004]\n",
      "Grand sum of 188 tensor sets is: [47.693695068359375, -293.8507080078125, -74.73552703857422, -0.4957660436630249, 150.22677612304688]\n",
      "\n",
      "Instance 214 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [11, 12]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "don at index 11: [0.29295486211776733, -2.3747611045837402, 0.08403418213129044, 0.4443156123161316, 1.210148572921753]\n",
      "ut at index 12: [-0.21686890721321106, -1.6503345966339111, -0.6912022829055786, 1.4281895160675049, 2.751040458679199]\n",
      "Grand sum of 189 tensor sets is: [47.73173904418945, -295.8632507324219, -75.03910827636719, 0.4404865503311157, 152.20736694335938]\n",
      "\n",
      "Instance 215 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [17, 18]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "don at index 17: [-0.3582637310028076, -2.515467405319214, -0.1839560866355896, -1.9134896993637085, -1.12726628780365]\n",
      "ut at index 18: [-0.004765644669532776, -2.5358870029449463, -0.8040287494659424, 0.8998234272003174, 0.6839289665222168]\n",
      "Grand sum of 190 tensor sets is: [47.55022430419922, -298.388916015625, -75.5331039428711, -0.06634658575057983, 151.98570251464844]\n",
      "\n",
      "Instance 216 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [28, 39, 29]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "don at index 28: [0.2908531427383423, -2.5326499938964844, 0.10997530817985535, -2.505429744720459, -1.2315208911895752]\n",
      "ut at index 39: [0.15403719246387482, -1.3239988088607788, 0.12495492398738861, -1.8413625955581665, -2.4794371128082275]\n",
      "don at index 29: [0.8729089498519897, -2.9670486450195312, -0.37434902787208557, 0.5034539699554443, 1.5987567901611328]\n",
      "Grand sum of 191 tensor sets is: [47.9894905090332, -300.6634826660156, -75.57957458496094, -1.347459316253662, 151.28163146972656]\n",
      "\n",
      "Instance 217 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [33, 34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "don at index 33: [0.7487749457359314, -1.8746854066848755, 0.5082172155380249, -3.068044900894165, -1.285144567489624]\n",
      "ut at index 34: [-0.0329873189330101, -2.1389975547790527, -0.47448840737342834, -0.5864568948745728, 1.0990371704101562]\n",
      "Grand sum of 192 tensor sets is: [48.34738540649414, -302.6703186035156, -75.56271362304688, -3.174710273742676, 151.18858337402344]\n",
      "\n",
      "Instance 218 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 219 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "don at index 20: [-0.1403857320547104, 0.5773478150367737, -1.7884016036987305, -0.1453612595796585, 0.43986842036247253]\n",
      "Grand sum of 193 tensor sets is: [48.207000732421875, -302.09295654296875, -77.35111236572266, -3.3200714588165283, 151.62844848632812]\n",
      "\n",
      "Instance 220 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 221 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [379, 385, 389, 390]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 379: [1.0459002256393433, -4.22147274017334, 0.8497065305709839, -1.5505129098892212, -1.58256196975708]\n",
      "ut at index 385: [-0.4756356179714203, -2.9433670043945312, 0.6853755712509155, -0.8191568851470947, -3.102907180786133]\n",
      "don at index 389: [0.5828842520713806, -5.535087585449219, 0.033682674169540405, -2.3451974391937256, -3.620495557785034]\n",
      "ut at index 390: [0.13467590510845184, 0.1944536417722702, -0.26512712240219116, -0.3697105050086975, -2.2941441535949707]\n",
      "Grand sum of 194 tensor sets is: [48.52895736694336, -305.2193298339844, -77.02519989013672, -4.591216087341309, 148.97842407226562]\n",
      "\n",
      "Instance 222 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [56, 51, 57]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "don at index 56: [0.934207558631897, -2.521864891052246, 0.6332056522369385, 0.08048388361930847, -2.3805487155914307]\n",
      "ut at index 51: [0.10275264829397202, 0.8519033193588257, -1.2553406953811646, 1.6945405006408691, 0.26760333776474]\n",
      "don at index 57: [1.4942022562026978, -3.207749366760254, 0.9194464683532715, 1.9561465978622437, -0.2891547381877899]\n",
      "Grand sum of 195 tensor sets is: [49.372676849365234, -306.8452453613281, -76.92609405517578, -3.347492218017578, 148.17771911621094]\n",
      "\n",
      "Instance 223 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [26, 27]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "don at index 26: [0.5238666534423828, -2.4404897689819336, 0.07640653848648071, -2.183881998062134, -0.6735173463821411]\n",
      "ut at index 27: [1.4434518814086914, -2.752779006958008, -0.596758246421814, -0.4497181475162506, 1.7702314853668213]\n",
      "Grand sum of 196 tensor sets is: [50.3563346862793, -309.44189453125, -77.18627166748047, -4.664292335510254, 148.72607421875]\n",
      "\n",
      "Instance 224 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [367, 369, 371, 375, 370, 372]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 367: [0.14688919484615326, -3.9528567790985107, 0.6928138732910156, -0.2702106833457947, -3.2873058319091797]\n",
      "ut at index 369: [0.30635836720466614, -3.8679392337799072, 1.5655579566955566, -0.9900332689285278, -4.266233921051025]\n",
      "don at index 371: [0.16114726662635803, -3.398419141769409, 1.4259390830993652, -1.5793602466583252, -5.420454978942871]\n",
      "ut at index 375: [-0.22662194073200226, -3.8973608016967773, 1.9030866622924805, -2.444549798965454, -4.895187854766846]\n",
      "don at index 370: [-0.3048100471496582, -1.6726022958755493, 1.688371181488037, 1.5519968271255493, -0.8763484954833984]\n",
      "ut at index 372: [0.004447042942047119, -1.8263200521469116, 1.6181491613388062, -1.3116765022277832, -1.364316463470459]\n",
      "Grand sum of 197 tensor sets is: [50.37090301513672, -312.5444641113281, -75.70394897460938, -5.504931449890137, 145.3744354248047]\n",
      "\n",
      "Instance 225 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 226 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [1, 12, 2]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "don at index 1: [0.9400479793548584, -1.7971988916397095, 1.4532331228256226, -2.124326705932617, -0.42722031474113464]\n",
      "ut at index 12: [1.2523612976074219, 0.2272033840417862, 2.38607120513916, -1.8141664266586304, -1.3013142347335815]\n",
      "don at index 2: [0.9815616607666016, -0.8414531350135803, -1.135738492012024, 0.704878568649292, 1.2726805210113525]\n",
      "Grand sum of 198 tensor sets is: [51.42889404296875, -313.3482666015625, -74.8027572631836, -6.582802772521973, 145.2224884033203]\n",
      "\n",
      "Instance 227 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [27, 32, 28, 33]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "don at index 27: [0.23884251713752747, -2.310232162475586, 0.20223036408424377, -2.686738967895508, -2.6655054092407227]\n",
      "ut at index 32: [-0.29946988821029663, -2.1011993885040283, 0.8735833764076233, -2.4017279148101807, -2.276420831680298]\n",
      "don at index 28: [-0.038565509021282196, -2.8649234771728516, -0.41867104172706604, -0.9629871845245361, 0.6786209344863892]\n",
      "ut at index 33: [-0.42257094383239746, -2.7987592220306396, 0.1822521984577179, -0.21184366941452026, 1.3243985176086426]\n",
      "Grand sum of 199 tensor sets is: [51.29845428466797, -315.8670349121094, -74.59291076660156, -8.148627281188965, 144.48776245117188]\n",
      "\n",
      "Instance 228 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "don at index 5: [0.3335427939891815, 0.5731894969940186, -0.5891692638397217, 1.051560401916504, 0.7642869353294373]\n",
      "Grand sum of 200 tensor sets is: [51.631996154785156, -315.2938537597656, -75.18208312988281, -7.097066879272461, 145.25204467773438]\n",
      "\n",
      "Instance 229 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "don at index 5: [0.8283839225769043, -1.940810203552246, -0.932740330696106, -0.38608163595199585, 2.5709192752838135]\n",
      "Grand sum of 201 tensor sets is: [52.46038055419922, -317.2346496582031, -76.11482238769531, -7.483148574829102, 147.82296752929688]\n",
      "\n",
      "Instance 230 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "don at index 12: [-0.12790533900260925, 0.09160960465669632, -0.6800025701522827, 0.7264894247055054, 2.0037426948547363]\n",
      "Grand sum of 202 tensor sets is: [52.33247375488281, -317.1430358886719, -76.7948226928711, -6.756659030914307, 149.8267059326172]\n",
      "\n",
      "Instance 231 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [19, 20]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "don at index 19: [-0.353626012802124, -1.0343419313430786, 0.3295068144798279, 0.3383883237838745, -0.8081254363059998]\n",
      "ut at index 20: [0.31165599822998047, -1.18910813331604, -0.31391263008117676, 1.8092236518859863, 1.821516513824463]\n",
      "Grand sum of 203 tensor sets is: [52.31148910522461, -318.2547607421875, -76.78702545166016, -5.682852745056152, 150.33340454101562]\n",
      "\n",
      "Instance 232 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5, 6]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "don at index 5: [0.3651699423789978, -3.720028877258301, 0.9283543229103088, -0.8152832388877869, -0.23328706622123718]\n",
      "ut at index 6: [0.6193146705627441, -2.8558738231658936, -0.6513699293136597, 0.3223634362220764, -0.7341071367263794]\n",
      "Grand sum of 204 tensor sets is: [52.80373001098633, -321.542724609375, -76.6485366821289, -5.929312705993652, 149.84970092773438]\n",
      "\n",
      "Instance 233 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "don at index 2: [0.6983523368835449, 1.0527124404907227, -1.6152453422546387, 1.7474806308746338, 4.499908924102783]\n",
      "Grand sum of 205 tensor sets is: [53.50208282470703, -320.4900207519531, -78.26377868652344, -4.181832313537598, 154.349609375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 234 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [1, 20, 29, 2, 21, 30]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "don at index 1: [0.49008291959762573, -2.443756580352783, 0.6835604906082153, -0.7390517592430115, -1.1947332620620728]\n",
      "ut at index 20: [0.9484843611717224, -2.050811767578125, 1.7871983051300049, 0.462886244058609, -1.589501976966858]\n",
      "don at index 29: [0.8837560415267944, -1.873544692993164, 1.9696608781814575, 0.4524229168891907, -1.8475282192230225]\n",
      "ut at index 2: [0.6672121286392212, -1.841200828552246, -0.44127923250198364, 1.9692953824996948, 0.9398894309997559]\n",
      "don at index 21: [0.9598469138145447, -0.9113637208938599, 0.9838619232177734, 1.7180838584899902, 0.3423491418361664]\n",
      "ut at index 30: [0.861038863658905, -0.9360331892967224, 1.238556146621704, 1.6102821826934814, 0.13517823815345764]\n",
      "Grand sum of 206 tensor sets is: [54.30381774902344, -322.1661376953125, -77.22685241699219, -3.269512414932251, 153.8138885498047]\n",
      "\n",
      "Instance 235 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [15, 16]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "don at index 15: [0.22667166590690613, -2.188335418701172, -0.46266284584999084, -3.013744592666626, -1.6737780570983887]\n",
      "ut at index 16: [0.19218501448631287, -2.118659734725952, -0.022911764681339264, -0.057432711124420166, 0.7335165739059448]\n",
      "Grand sum of 207 tensor sets is: [54.51324462890625, -324.31964111328125, -77.46964263916016, -4.805100917816162, 153.34376525878906]\n",
      "\n",
      "Instance 236 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [379, 385, 389, 390]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 379: [1.0459002256393433, -4.22147274017334, 0.8497065305709839, -1.5505129098892212, -1.58256196975708]\n",
      "ut at index 385: [-0.4756356179714203, -2.9433670043945312, 0.6853755712509155, -0.8191568851470947, -3.102907180786133]\n",
      "don at index 389: [0.5828842520713806, -5.535087585449219, 0.033682674169540405, -2.3451974391937256, -3.620495557785034]\n",
      "ut at index 390: [0.13467590510845184, 0.1944536417722702, -0.26512712240219116, -0.3697105050086975, -2.2941441535949707]\n",
      "Grand sum of 208 tensor sets is: [54.835201263427734, -327.4460144042969, -77.14373016357422, -6.076245307922363, 150.69374084472656]\n",
      "\n",
      "Instance 237 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "don at index 6: [1.3061071634292603, -1.6244429349899292, -0.9764434695243835, 0.9280456900596619, 3.2601332664489746]\n",
      "Grand sum of 209 tensor sets is: [56.14130783081055, -329.0704650878906, -78.12017059326172, -5.148199558258057, 153.95387268066406]\n",
      "\n",
      "Instance 238 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "don at index 4: [0.14359858632087708, -1.3389015197753906, 0.0011902009136974812, 0.29549580812454224, 1.7446770668029785]\n",
      "Grand sum of 210 tensor sets is: [56.284908294677734, -330.40936279296875, -78.11898040771484, -4.85270357131958, 155.69854736328125]\n",
      "\n",
      "Instance 239 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [10, 19]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "don at index 10: [-0.24483780562877655, -2.3763678073883057, -1.1950613260269165, 0.5448262095451355, 2.7726407051086426]\n",
      "ut at index 19: [0.573011040687561, -0.8311471939086914, -0.346263587474823, -0.578737735748291, 2.4329771995544434]\n",
      "Grand sum of 211 tensor sets is: [56.44899368286133, -332.01312255859375, -78.88964080810547, -4.869659423828125, 158.30136108398438]\n",
      "\n",
      "Instance 240 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [3, 4]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "don at index 3: [0.48590028285980225, -2.6102075576782227, 0.22871485352516174, 0.013059772551059723, -1.5925427675247192]\n",
      "ut at index 4: [-0.03867392614483833, -2.1585335731506348, 1.1886334419250488, 1.6700265407562256, 0.23718687891960144]\n",
      "Grand sum of 212 tensor sets is: [56.672607421875, -334.3974914550781, -78.18096923828125, -4.028116226196289, 157.62368774414062]\n",
      "\n",
      "Instance 241 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "don at index 5: [0.8283839225769043, -1.940810203552246, -0.932740330696106, -0.38608163595199585, 2.5709192752838135]\n",
      "Grand sum of 213 tensor sets is: [57.50099182128906, -336.3382873535156, -79.11370849609375, -4.41419792175293, 160.19461059570312]\n",
      "\n",
      "Instance 242 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [14, 15]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "don at index 14: [0.1760663539171219, -1.9948856830596924, -0.3736423850059509, -0.03965237736701965, -1.6098872423171997]\n",
      "ut at index 15: [0.2741619348526001, -2.4525508880615234, -0.5525531768798828, 1.2724087238311768, 0.4261317253112793]\n",
      "Grand sum of 214 tensor sets is: [57.726104736328125, -338.56201171875, -79.5768051147461, -3.7978196144104004, 159.6027374267578]\n",
      "\n",
      "Instance 243 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "don at index 12: [0.6677242517471313, -1.3226224184036255, -0.9619014263153076, 0.30760639905929565, 3.1308600902557373]\n",
      "Grand sum of 215 tensor sets is: [58.393829345703125, -339.8846435546875, -80.53870391845703, -3.49021315574646, 162.7335968017578]\n",
      "\n",
      "Instance 244 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [38, 39]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "don at index 38: [0.05219002813100815, -2.019069194793701, -0.29485538601875305, -1.5335700511932373, -0.8016791343688965]\n",
      "ut at index 39: [0.13977698981761932, -1.8392746448516846, -0.4088597595691681, 2.0820302963256836, 1.9498567581176758]\n",
      "Grand sum of 216 tensor sets is: [58.48981475830078, -341.8138122558594, -80.89056396484375, -3.2159829139709473, 163.30767822265625]\n",
      "\n",
      "Instance 245 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [367, 369, 371, 375, 370, 372]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 367: [0.14688919484615326, -3.9528567790985107, 0.6928138732910156, -0.2702106833457947, -3.2873058319091797]\n",
      "ut at index 369: [0.30635836720466614, -3.8679392337799072, 1.5655579566955566, -0.9900332689285278, -4.266233921051025]\n",
      "don at index 371: [0.16114726662635803, -3.398419141769409, 1.4259390830993652, -1.5793602466583252, -5.420454978942871]\n",
      "ut at index 375: [-0.22662194073200226, -3.8973608016967773, 1.9030866622924805, -2.444549798965454, -4.895187854766846]\n",
      "don at index 370: [-0.3048100471496582, -1.6726022958755493, 1.688371181488037, 1.5519968271255493, -0.8763484954833984]\n",
      "ut at index 372: [0.004447042942047119, -1.8263200521469116, 1.6181491613388062, -1.3116765022277832, -1.364316463470459]\n",
      "Grand sum of 217 tensor sets is: [58.5043830871582, -344.9163818359375, -79.40824127197266, -4.05662202835083, 159.95603942871094]\n",
      "\n",
      "Instance 246 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "don at index 5: [0.16139501333236694, -2.017814874649048, -1.275746464729309, -0.12344907969236374, 2.26247239112854]\n",
      "Grand sum of 218 tensor sets is: [58.66577911376953, -346.9342041015625, -80.68399047851562, -4.180070877075195, 162.218505859375]\n",
      "\n",
      "Instance 247 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "don at index 47: [0.15471765398979187, -3.420179843902588, -0.4743551015853882, -1.5396950244903564, -0.31646665930747986]\n",
      "Grand sum of 219 tensor sets is: [58.82049560546875, -350.3543701171875, -81.1583480834961, -5.719765663146973, 161.90203857421875]\n",
      "\n",
      "Instance 248 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "don at index 7: [0.8926615715026855, -1.7250059843063354, -1.3149614334106445, 0.49719876050949097, 0.7225908637046814]\n",
      "Grand sum of 220 tensor sets is: [59.713157653808594, -352.0793762207031, -82.47331237792969, -5.222567081451416, 162.6246337890625]\n",
      "\n",
      "Instance 249 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [12, 13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "don at index 12: [-0.12353476881980896, -2.1288933753967285, -0.2351672649383545, -0.14689642190933228, -1.2823121547698975]\n",
      "ut at index 13: [-0.7348330020904541, -0.4971764087677002, 0.856066107749939, 1.407426357269287, 2.007988452911377]\n",
      "Grand sum of 221 tensor sets is: [59.283973693847656, -353.3924255371094, -82.1628646850586, -4.592302322387695, 162.9874725341797]\n",
      "\n",
      "Instance 250 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [11, 12]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "don at index 11: [0.5743941068649292, -1.8554964065551758, 0.18992146849632263, -1.275678038597107, -0.12369228899478912]\n",
      "ut at index 12: [1.1313304901123047, -2.1788766384124756, -0.652736485004425, 1.6353429555892944, 2.1657450199127197]\n",
      "Grand sum of 222 tensor sets is: [60.136837005615234, -355.40960693359375, -82.39427185058594, -4.412469863891602, 164.0084991455078]\n",
      "\n",
      "Instance 251 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [107, 276]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 107: [-0.7828222513198853, -1.781507134437561, 0.9024642705917358, 1.3574938774108887, -1.3997414112091064]\n",
      "ut at index 276: [-0.5140088796615601, -1.8655489683151245, 0.5468668341636658, -0.2838279604911804, 2.2438652515411377]\n",
      "Grand sum of 223 tensor sets is: [59.48842239379883, -357.2331237792969, -81.66960906982422, -3.8756370544433594, 164.43055725097656]\n",
      "\n",
      "Instance 252 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [27, 28]\n",
      "Size of token embeddings is torch.Size([66, 13, 768])\n",
      "Shape of summed layers is: 66 x 768\n",
      "don at index 27: [0.18893465399742126, -3.163278579711914, 0.11568121612071991, -2.837069272994995, -0.4916655421257019]\n",
      "ut at index 28: [-0.2952541410923004, -3.253558397293091, -0.2289380431175232, -0.7285628318786621, 2.381535291671753]\n",
      "Grand sum of 224 tensor sets is: [59.43526077270508, -360.4415283203125, -81.72623443603516, -5.658452987670898, 165.37548828125]\n",
      "\n",
      "Instance 253 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 254 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [77, 78]\n",
      "Size of token embeddings is torch.Size([86, 13, 768])\n",
      "Shape of summed layers is: 86 x 768\n",
      "don at index 77: [0.18697896599769592, -2.1005377769470215, 0.6358475685119629, -3.077676296234131, -1.0552459955215454]\n",
      "ut at index 78: [0.005993150174617767, -1.6866750717163086, -0.0932936742901802, -1.1299222707748413, 0.5995501279830933]\n",
      "Grand sum of 225 tensor sets is: [59.53174591064453, -362.33514404296875, -81.4549560546875, -7.762252330780029, 165.14764404296875]\n",
      "\n",
      "Instance 255 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "don at index 2: [-0.14172697067260742, 0.47182098031044006, -0.3521590828895569, 0.37289056181907654, 2.5498294830322266]\n",
      "Grand sum of 226 tensor sets is: [59.390018463134766, -361.8633117675781, -81.80711364746094, -7.38936185836792, 167.69747924804688]\n",
      "\n",
      "Instance 256 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [4, 5]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "don at index 4: [0.24933674931526184, -2.1040167808532715, -0.01591205596923828, -1.344881296157837, 0.10887416452169418]\n",
      "ut at index 5: [0.8918631672859192, -2.366702079772949, -0.6532025933265686, 1.3920972347259521, 3.1587061882019043]\n",
      "Grand sum of 227 tensor sets is: [59.96061706542969, -364.0986633300781, -82.14167022705078, -7.365754127502441, 169.33126831054688]\n",
      "\n",
      "Instance 257 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [14, 15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "don at index 14: [0.48212629556655884, -1.7949944734573364, 0.15735647082328796, -0.5936071872711182, -1.3201385736465454]\n",
      "ut at index 15: [1.0238282680511475, -2.5090954303741455, -0.8274613618850708, 2.1300604343414307, 1.3724510669708252]\n",
      "Grand sum of 228 tensor sets is: [60.713592529296875, -366.2507019042969, -82.47672271728516, -6.597527503967285, 169.357421875]\n",
      "\n",
      "Instance 258 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [7, 10, 11]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "don at index 7: [0.23158656060695648, -1.8387932777404785, 0.41972020268440247, -0.3106703460216522, -3.6887784004211426]\n",
      "ut at index 10: [0.30454087257385254, -0.8451087474822998, 0.945574164390564, -0.820469856262207, -4.228892803192139]\n",
      "don at index 11: [0.8183389902114868, -2.067269802093506, 0.24263368546962738, 0.8640381097793579, 1.5029667615890503]\n",
      "Grand sum of 229 tensor sets is: [61.16508102416992, -367.83441162109375, -81.94075012207031, -6.686561584472656, 167.2191925048828]\n",
      "\n",
      "Instance 259 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "don at index 5: [0.16139501333236694, -2.017814874649048, -1.275746464729309, -0.12344907969236374, 2.26247239112854]\n",
      "Grand sum of 230 tensor sets is: [61.32647705078125, -369.85223388671875, -83.21649932861328, -6.8100104331970215, 169.48165893554688]\n",
      "\n",
      "Instance 260 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "don at index 53: [0.8168477416038513, -1.372750163078308, 0.06778596341609955, 0.5160173773765564, 2.616507053375244]\n",
      "Grand sum of 231 tensor sets is: [62.14332580566406, -371.2249755859375, -83.14871215820312, -6.29399299621582, 172.09815979003906]\n",
      "\n",
      "Instance 261 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [33, 34]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "don at index 33: [0.44699403643608093, -1.6733450889587402, 0.26822274923324585, -1.6513159275054932, -1.3425759077072144]\n",
      "ut at index 34: [0.7125877737998962, -1.1705025434494019, 0.24242928624153137, -0.012677252292633057, 1.4566538333892822]\n",
      "Grand sum of 232 tensor sets is: [62.72311782836914, -372.64691162109375, -82.89338684082031, -7.1259894371032715, 172.1551971435547]\n",
      "\n",
      "Instance 262 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [19, 20]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "don at index 19: [0.537172257900238, -1.6571310758590698, -0.0072304606437683105, -0.8909751176834106, -0.914620041847229]\n",
      "ut at index 20: [1.1577352285385132, -2.231863498687744, -0.756986141204834, 1.1140650510787964, 1.811047077178955]\n",
      "Grand sum of 233 tensor sets is: [63.57057189941406, -374.5914001464844, -83.27549743652344, -7.014444351196289, 172.60340881347656]\n",
      "\n",
      "Instance 263 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [12, 13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "don at index 12: [0.09079264849424362, -1.8884586095809937, -0.168696790933609, -1.867480993270874, -1.8230175971984863]\n",
      "ut at index 13: [0.8844726085662842, -2.0048933029174805, -0.9877229928970337, 0.7465624809265137, 0.6857075691223145]\n",
      "Grand sum of 234 tensor sets is: [64.0582046508789, -376.5380859375, -83.85370635986328, -7.57490348815918, 172.03475952148438]\n",
      "\n",
      "Instance 264 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 265 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [3, 4]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "don at index 3: [0.06750542670488358, -1.8542704582214355, 0.03363180160522461, -1.3424956798553467, 1.2478523254394531]\n",
      "ut at index 4: [0.5335521697998047, -2.435783624649048, -0.8648805618286133, 0.18163710832595825, 2.8560757637023926]\n",
      "Grand sum of 235 tensor sets is: [64.35873413085938, -378.68310546875, -84.26933288574219, -8.155332565307617, 174.08673095703125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 266 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "\n",
      "Instance 267 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [477, 480]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "don at index 477: [-0.17488613724708557, -0.9500951170921326, 0.6378838419914246, 0.6924277544021606, -0.21533790230751038]\n",
      "ut at index 480: [-0.6395050287246704, -0.07959727197885513, 1.0472100973129272, -0.8083470463752747, -1.445960521697998]\n",
      "Grand sum of 236 tensor sets is: [63.9515380859375, -379.19793701171875, -83.42678833007812, -8.213292121887207, 173.25608825683594]\n",
      "\n",
      "Instance 268 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [40, 41]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "don at index 40: [0.3357926905155182, -1.2932593822479248, -0.6651858687400818, -1.7221393585205078, 0.419634610414505]\n",
      "ut at index 41: [0.14909836649894714, -1.9023582935333252, -0.8674076199531555, 0.8343127965927124, 2.794856309890747]\n",
      "Grand sum of 237 tensor sets is: [64.19398498535156, -380.7957458496094, -84.19308471679688, -8.657205581665039, 174.8633270263672]\n",
      "\n",
      "Instance 269 of donut.\n",
      "Looking for vocab token: don\n",
      "Looking for vocab token: ut\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "don at index 5: [0.16139501333236694, -2.017814874649048, -1.275746464729309, -0.12344907969236374, 2.26247239112854]\n",
      "Grand sum of 238 tensor sets is: [64.35537719726562, -382.8135681152344, -85.46883392333984, -8.780654907226562, 177.12579345703125]\n",
      "Mean of tensors is: tensor([ 0.2704, -1.6085, -0.3591, -0.0369,  0.7442]) (768 features in tensor)\n",
      "Saved the embedding for donut.\n",
      "Saved the count of sentences used to create donut embedding\n",
      "Run time for donut was 58.14003141783178 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "triangular\n",
      "\n",
      "Instance 1 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "triangular at index 2: [-0.16546478867530823, -1.1797235012054443, -0.231679767370224, 0.6913056969642639, 4.464269638061523]\n",
      "Grand sum of 1 tensor sets is: [-0.16546478867530823, -1.1797235012054443, -0.231679767370224, 0.6913056969642639, 4.464269638061523]\n",
      "\n",
      "Instance 2 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "triangular at index 16: [-0.7053353190422058, 1.2484298944473267, -0.7391042113304138, -0.41725534200668335, 1.570774793624878]\n",
      "Grand sum of 2 tensor sets is: [-0.8708001375198364, 0.06870639324188232, -0.9707839488983154, 0.27405035495758057, 6.0350446701049805]\n",
      "\n",
      "Instance 3 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "triangular at index 42: [0.46209916472435, 0.5531732439994812, -0.24029850959777832, -0.26017630100250244, 2.622870922088623]\n",
      "Grand sum of 3 tensor sets is: [-0.40870097279548645, 0.6218796372413635, -1.2110824584960938, 0.013874053955078125, 8.657915115356445]\n",
      "\n",
      "Instance 4 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 5 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 6 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "triangular at index 30: [0.4665536880493164, 0.07564886659383774, 0.2879261076450348, 0.5423416495323181, 3.684621572494507]\n",
      "Grand sum of 4 tensor sets is: [0.057852715253829956, 0.6975284814834595, -0.9231563806533813, 0.5562157034873962, 12.342536926269531]\n",
      "\n",
      "Instance 7 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 8 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [59]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "triangular at index 59: [0.4034970998764038, 0.5806998014450073, -0.9229722023010254, 0.7239715456962585, 2.990565299987793]\n",
      "Grand sum of 5 tensor sets is: [0.46134981513023376, 1.2782282829284668, -1.8461285829544067, 1.2801872491836548, 15.333102226257324]\n",
      "\n",
      "Instance 9 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 10 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "triangular at index 29: [1.4880667924880981, 0.34590980410575867, -0.6137669682502747, -0.08251599967479706, 1.8082630634307861]\n",
      "Grand sum of 6 tensor sets is: [1.9494166374206543, 1.6241381168365479, -2.459895610809326, 1.1976712942123413, 17.14136505126953]\n",
      "\n",
      "Instance 11 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "triangular at index 31: [0.3395965099334717, -0.293903112411499, -0.8249999284744263, -0.4934167265892029, 6.147026062011719]\n",
      "Grand sum of 7 tensor sets is: [2.289013147354126, 1.3302350044250488, -3.284895420074463, 0.7042545676231384, 23.28839111328125]\n",
      "\n",
      "Instance 12 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 13 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [4, 18]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "triangular at index 4: [-0.25024157762527466, 1.6878039836883545, 0.718339204788208, 1.2434817552566528, 3.81105375289917]\n",
      "triangular at index 18: [-0.7117792963981628, 0.16715647280216217, 0.8011243343353271, 0.60542893409729, 1.5084853172302246]\n",
      "Grand sum of 8 tensor sets is: [1.8080027103424072, 2.2577152252197266, -2.5251636505126953, 1.6287099123001099, 25.94816017150879]\n",
      "\n",
      "Instance 14 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 15 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "triangular at index 7: [-0.15561175346374512, 0.13831545412540436, -0.44410544633865356, 0.45053189992904663, 1.426090955734253]\n",
      "Grand sum of 9 tensor sets is: [1.652390956878662, 2.3960306644439697, -2.969269037246704, 2.0792417526245117, 27.374250411987305]\n",
      "\n",
      "Instance 16 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "triangular at index 18: [1.3579100370407104, 0.45352452993392944, -0.07583681493997574, 0.9484891295433044, 1.6471613645553589]\n",
      "Grand sum of 10 tensor sets is: [3.010301113128662, 2.849555253982544, -3.0451059341430664, 3.027730941772461, 29.021411895751953]\n",
      "\n",
      "Instance 17 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "triangular at index 7: [0.02661307156085968, 0.6220481991767883, 0.12204666435718536, -1.2043707370758057, 2.2182774543762207]\n",
      "Grand sum of 11 tensor sets is: [3.036914110183716, 3.4716033935546875, -2.9230592250823975, 1.8233602046966553, 31.239688873291016]\n",
      "\n",
      "Instance 18 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "triangular at index 9: [-0.23496705293655396, 0.7302971482276917, -0.9055674076080322, 0.1201322078704834, 1.352978229522705]\n",
      "Grand sum of 12 tensor sets is: [2.8019471168518066, 4.201900482177734, -3.8286266326904297, 1.9434924125671387, 32.59266662597656]\n",
      "\n",
      "Instance 19 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [1, 4]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "triangular at index 1: [0.09899376332759857, 0.20597469806671143, -0.30977505445480347, 0.4028167128562927, 4.504881858825684]\n",
      "triangular at index 4: [0.09761407971382141, 1.1112433671951294, 0.9982641935348511, 1.2289046049118042, 2.7868270874023438]\n",
      "Grand sum of 13 tensor sets is: [2.9002511501312256, 4.860509395599365, -3.484382152557373, 2.7593531608581543, 36.238521575927734]\n",
      "\n",
      "Instance 20 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "triangular at index 30: [0.4665536880493164, 0.07564886659383774, 0.2879261076450348, 0.5423416495323181, 3.684621572494507]\n",
      "Grand sum of 14 tensor sets is: [3.366804838180542, 4.936158180236816, -3.196455955505371, 3.301694869995117, 39.92314147949219]\n",
      "\n",
      "Instance 21 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [178]\n",
      "Size of token embeddings is torch.Size([372, 13, 768])\n",
      "Shape of summed layers is: 372 x 768\n",
      "triangular at index 178: [0.8380002379417419, 2.15647554397583, -0.04370000958442688, 1.1494063138961792, 0.47584837675094604]\n",
      "Grand sum of 15 tensor sets is: [4.20480489730835, 7.0926337242126465, -3.2401559352874756, 4.451101303100586, 40.398990631103516]\n",
      "\n",
      "Instance 22 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "triangular at index 27: [-0.2553118169307709, -0.9843860268592834, -0.71193528175354, 1.0907797813415527, 0.40805602073669434]\n",
      "Grand sum of 16 tensor sets is: [3.949493169784546, 6.108247756958008, -3.9520912170410156, 5.541881084442139, 40.807044982910156]\n",
      "\n",
      "Instance 23 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "triangular at index 17: [0.4075946509838104, 0.750005841255188, -0.7630523443222046, -0.11095599830150604, 2.1645212173461914]\n",
      "Grand sum of 17 tensor sets is: [4.3570876121521, 6.858253479003906, -4.71514368057251, 5.430924892425537, 42.97156524658203]\n",
      "\n",
      "Instance 24 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 25 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 26 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [2, 13]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "triangular at index 2: [0.8213323950767517, -0.5979160070419312, -0.14756488800048828, 1.948339819908142, 2.8964786529541016]\n",
      "triangular at index 13: [0.46634575724601746, 1.3989344835281372, 1.0482176542282104, 0.9008944034576416, 3.2497878074645996]\n",
      "Grand sum of 18 tensor sets is: [5.000926494598389, 7.258762836456299, -4.264817237854004, 6.855542182922363, 46.04469680786133]\n",
      "\n",
      "Instance 27 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "triangular at index 12: [0.27001240849494934, 1.058639407157898, -1.0843664407730103, 0.19306811690330505, 1.0896544456481934]\n",
      "Grand sum of 19 tensor sets is: [5.270938873291016, 8.317401885986328, -5.349183559417725, 7.048610210418701, 47.13434982299805]\n",
      "\n",
      "Instance 28 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "triangular at index 9: [-0.36284294724464417, 1.1369216442108154, 0.011932107619941235, -0.5543888211250305, 3.776628017425537]\n",
      "Grand sum of 20 tensor sets is: [4.908095836639404, 9.454323768615723, -5.337251663208008, 6.494221210479736, 50.91097640991211]\n",
      "\n",
      "Instance 29 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "triangular at index 12: [-0.2649421989917755, 1.1197035312652588, -0.6757487654685974, 1.2900285720825195, 3.3943424224853516]\n",
      "Grand sum of 21 tensor sets is: [4.643153667449951, 10.574027061462402, -6.01300048828125, 7.784249782562256, 54.305320739746094]\n",
      "\n",
      "Instance 30 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "triangular at index 2: [0.8779438138008118, 0.3341333568096161, -0.033227428793907166, 0.8927270174026489, 3.782667875289917]\n",
      "Grand sum of 22 tensor sets is: [5.521097660064697, 10.908160209655762, -6.046227931976318, 8.676977157592773, 58.087989807128906]\n",
      "\n",
      "Instance 31 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "triangular at index 18: [0.04625679552555084, 0.11285018920898438, 0.452323853969574, -2.726911783218384, 5.849559783935547]\n",
      "Grand sum of 23 tensor sets is: [5.567354679107666, 11.021010398864746, -5.5939040184021, 5.950065612792969, 63.93754959106445]\n",
      "\n",
      "Instance 32 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "triangular at index 16: [0.9176470041275024, 0.3125839829444885, -0.005690779536962509, -1.00125253200531, 2.547666311264038]\n",
      "Grand sum of 24 tensor sets is: [6.485001564025879, 11.33359432220459, -5.599594593048096, 4.948812961578369, 66.48521423339844]\n",
      "\n",
      "Instance 33 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [69]\n",
      "Size of token embeddings is torch.Size([89, 13, 768])\n",
      "Shape of summed layers is: 89 x 768\n",
      "triangular at index 69: [1.1323131322860718, -0.16919741034507751, -1.2660813331604004, -0.4495640993118286, 2.7181568145751953]\n",
      "Grand sum of 25 tensor sets is: [7.61731481552124, 11.164397239685059, -6.865675926208496, 4.49924898147583, 69.203369140625]\n",
      "\n",
      "Instance 34 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 35 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "triangular at index 25: [0.5475919246673584, -0.00962837040424347, -0.7276976704597473, -0.22902457416057587, -0.1474124789237976]\n",
      "Grand sum of 26 tensor sets is: [8.16490650177002, 11.154768943786621, -7.593373775482178, 4.270224571228027, 69.05595397949219]\n",
      "\n",
      "Instance 36 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "triangular at index 3: [0.30459320545196533, -0.8177891969680786, -0.9480879306793213, 1.6044285297393799, 1.1614007949829102]\n",
      "Grand sum of 27 tensor sets is: [8.469499588012695, 10.336979866027832, -8.541461944580078, 5.874652862548828, 70.21735382080078]\n",
      "\n",
      "Instance 37 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "triangular at index 6: [0.6374201774597168, -0.6777157783508301, -1.1703178882598877, -0.04927563667297363, 0.8590230941772461]\n",
      "Grand sum of 28 tensor sets is: [9.10692024230957, 9.659263610839844, -9.711779594421387, 5.825377464294434, 71.07637786865234]\n",
      "\n",
      "Instance 38 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "triangular at index 2: [0.6159254312515259, 0.4206129312515259, -0.5773710608482361, 2.041887044906616, 1.58616042137146]\n",
      "Grand sum of 29 tensor sets is: [9.722846031188965, 10.079876899719238, -10.28915023803711, 7.867264747619629, 72.66253662109375]\n",
      "\n",
      "Instance 39 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "triangular at index 22: [-0.11923614144325256, -0.4832737445831299, -0.6990131735801697, -0.05017196387052536, 4.565415859222412]\n",
      "Grand sum of 30 tensor sets is: [9.603610038757324, 9.596603393554688, -10.988162994384766, 7.8170928955078125, 77.22795104980469]\n",
      "\n",
      "Instance 40 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 41 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 42 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [223, 243, 381]\n",
      "Size of token embeddings is torch.Size([451, 13, 768])\n",
      "Shape of summed layers is: 451 x 768\n",
      "triangular at index 223: [0.3369094431400299, 2.025197982788086, 0.998874306678772, -2.2336974143981934, 3.5241432189941406]\n",
      "triangular at index 243: [0.1293928474187851, 2.100369930267334, 1.7558435201644897, -2.3528335094451904, 2.8551740646362305]\n",
      "triangular at index 381: [0.1858687847852707, 1.941922903060913, 1.45643150806427, -2.6453351974487305, 1.7826786041259766]\n",
      "Grand sum of 31 tensor sets is: [9.821000099182129, 11.619100570678711, -9.584446907043457, 5.406471252441406, 79.94861602783203]\n",
      "\n",
      "Instance 43 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "triangular at index 15: [1.0859161615371704, -0.8923108577728271, -1.1672399044036865, 1.2036503553390503, 1.1766464710235596]\n",
      "Grand sum of 32 tensor sets is: [10.906916618347168, 10.726789474487305, -10.751687049865723, 6.610121726989746, 81.12525939941406]\n",
      "\n",
      "Instance 44 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "triangular at index 9: [0.04289593547582626, 0.024339057505130768, -1.3651493787765503, 0.08049830794334412, 1.8615827560424805]\n",
      "Grand sum of 33 tensor sets is: [10.949812889099121, 10.751128196716309, -12.116836547851562, 6.690619945526123, 82.9868392944336]\n",
      "\n",
      "Instance 45 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "triangular at index 5: [-0.10824078321456909, 0.19835473597049713, -1.3417600393295288, 0.3831801116466522, -0.4513474106788635]\n",
      "Grand sum of 34 tensor sets is: [10.841571807861328, 10.949482917785645, -13.458596229553223, 7.073800086975098, 82.53549194335938]\n",
      "\n",
      "Instance 46 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "triangular at index 13: [0.2693103551864624, 0.42434635758399963, -0.6827039122581482, 0.30582359433174133, 3.106966972351074]\n",
      "Grand sum of 35 tensor sets is: [11.110881805419922, 11.373828887939453, -14.141300201416016, 7.379623889923096, 85.6424560546875]\n",
      "\n",
      "Instance 47 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "triangular at index 8: [0.6827980279922485, 1.0258251428604126, -0.48292624950408936, 0.9369284510612488, 1.7739999294281006]\n",
      "Grand sum of 36 tensor sets is: [11.793680191040039, 12.399654388427734, -14.624226570129395, 8.31655216217041, 87.41645812988281]\n",
      "\n",
      "Instance 48 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "triangular at index 4: [0.539082407951355, -0.6642187833786011, -0.8767457008361816, 0.5183369517326355, 2.0234954357147217]\n",
      "Grand sum of 37 tensor sets is: [12.332762718200684, 11.735435485839844, -15.500972747802734, 8.83488941192627, 89.43995666503906]\n",
      "\n",
      "Instance 49 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([129, 13, 768])\n",
      "Shape of summed layers is: 129 x 768\n",
      "triangular at index 5: [0.49367764592170715, -0.2702009677886963, 0.406734824180603, 1.388440489768982, 1.4180622100830078]\n",
      "Grand sum of 38 tensor sets is: [12.826440811157227, 11.465234756469727, -15.09423828125, 10.223329544067383, 90.85801696777344]\n",
      "\n",
      "Instance 50 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "triangular at index 3: [-0.30726248025894165, -0.31465771794319153, -1.0857293605804443, 1.9685065746307373, 0.775833010673523]\n",
      "Grand sum of 39 tensor sets is: [12.51917839050293, 11.1505765914917, -16.179967880249023, 12.1918363571167, 91.63385009765625]\n",
      "\n",
      "Instance 51 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "triangular at index 34: [0.36672353744506836, 0.0332988016307354, -0.07369044423103333, 0.25183403491973877, 1.377685546875]\n",
      "Grand sum of 40 tensor sets is: [12.885902404785156, 11.18387508392334, -16.253658294677734, 12.443670272827148, 93.01153564453125]\n",
      "\n",
      "Instance 52 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "triangular at index 11: [-0.09341977536678314, 2.472241163253784, -0.6845421195030212, -0.0812077522277832, 2.1165404319763184]\n",
      "Grand sum of 41 tensor sets is: [12.792482376098633, 13.656116485595703, -16.938199996948242, 12.362462997436523, 95.1280746459961]\n",
      "\n",
      "Instance 53 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "triangular at index 6: [0.41009682416915894, -0.27421247959136963, -0.5229393839836121, 2.210153341293335, 0.8818018436431885]\n",
      "Grand sum of 42 tensor sets is: [13.202579498291016, 13.381903648376465, -17.461139678955078, 14.572616577148438, 96.00988006591797]\n",
      "\n",
      "Instance 54 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "triangular at index 29: [0.22418048977851868, -0.3401494324207306, -0.49946773052215576, 0.42272844910621643, 1.2250123023986816]\n",
      "Grand sum of 43 tensor sets is: [13.426759719848633, 13.041753768920898, -17.960607528686523, 14.995345115661621, 97.23489379882812]\n",
      "\n",
      "Instance 55 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "triangular at index 4: [0.8667111396789551, 0.858390748500824, 0.8910181522369385, -0.7457985877990723, 3.134554147720337]\n",
      "Grand sum of 44 tensor sets is: [14.29347038269043, 13.900144577026367, -17.069589614868164, 14.24954605102539, 100.36944580078125]\n",
      "\n",
      "Instance 56 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 57 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 58 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "triangular at index 17: [1.0212507247924805, 0.5549895763397217, -0.17866197228431702, -2.0485470294952393, -0.5694982409477234]\n",
      "Grand sum of 45 tensor sets is: [15.31472110748291, 14.455134391784668, -17.24825096130371, 12.20099925994873, 99.7999496459961]\n",
      "\n",
      "Instance 59 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "triangular at index 10: [0.7108726501464844, -0.40186551213264465, -0.9397203326225281, 1.3448104858398438, 0.8567172884941101]\n",
      "Grand sum of 46 tensor sets is: [16.025592803955078, 14.053268432617188, -18.187971115112305, 13.545809745788574, 100.65666961669922]\n",
      "\n",
      "Instance 60 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 61 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [223, 243, 381]\n",
      "Size of token embeddings is torch.Size([451, 13, 768])\n",
      "Shape of summed layers is: 451 x 768\n",
      "triangular at index 223: [0.3369094431400299, 2.025197982788086, 0.998874306678772, -2.2336974143981934, 3.5241432189941406]\n",
      "triangular at index 243: [0.1293928474187851, 2.100369930267334, 1.7558435201644897, -2.3528335094451904, 2.8551740646362305]\n",
      "triangular at index 381: [0.1858687847852707, 1.941922903060913, 1.45643150806427, -2.6453351974487305, 1.7826786041259766]\n",
      "Grand sum of 47 tensor sets is: [16.242982864379883, 16.07576560974121, -16.78425407409668, 11.135188102722168, 103.37733459472656]\n",
      "\n",
      "Instance 62 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "triangular at index 17: [0.10416871309280396, 0.4149513840675354, 0.5788573026657104, -0.2882154583930969, 4.240555763244629]\n",
      "Grand sum of 48 tensor sets is: [16.347150802612305, 16.4907169342041, -16.20539665222168, 10.846972465515137, 107.61788940429688]\n",
      "\n",
      "Instance 63 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "triangular at index 6: [0.9748547673225403, -0.34305015206336975, -1.0780586004257202, 0.8753386735916138, 2.7529208660125732]\n",
      "Grand sum of 49 tensor sets is: [17.322006225585938, 16.147666931152344, -17.28345489501953, 11.722311019897461, 110.37081146240234]\n",
      "\n",
      "Instance 64 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "triangular at index 9: [1.485251545906067, -0.01919494941830635, -1.3218904733657837, 1.215544581413269, 2.518510103225708]\n",
      "Grand sum of 50 tensor sets is: [18.80725860595703, 16.12847137451172, -18.605344772338867, 12.93785572052002, 112.88932037353516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 65 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "triangular at index 17: [-0.6687009334564209, 1.3138468265533447, -1.1893693208694458, 1.065585970878601, 2.289466619491577]\n",
      "Grand sum of 51 tensor sets is: [18.13855743408203, 17.442317962646484, -19.794713973999023, 14.00344181060791, 115.17878723144531]\n",
      "\n",
      "Instance 66 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "triangular at index 7: [0.008947528898715973, 0.018686708062887192, -0.5437599420547485, 1.7020134925842285, 0.9271041750907898]\n",
      "Grand sum of 52 tensor sets is: [18.147504806518555, 17.46100425720215, -20.33847427368164, 15.705455780029297, 116.10588836669922]\n",
      "\n",
      "Instance 67 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "triangular at index 6: [0.32985007762908936, -0.8171787261962891, -1.139940857887268, 0.009042009711265564, 0.9962129592895508]\n",
      "Grand sum of 53 tensor sets is: [18.477354049682617, 16.64382553100586, -21.47841453552246, 15.714497566223145, 117.10210418701172]\n",
      "\n",
      "Instance 68 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "triangular at index 20: [0.11100339889526367, 0.19975465536117554, -0.26587751507759094, 0.5976439714431763, 3.9673688411712646]\n",
      "Grand sum of 54 tensor sets is: [18.58835792541504, 16.84358024597168, -21.744291305541992, 16.31214141845703, 121.06947326660156]\n",
      "\n",
      "Instance 69 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([92, 13, 768])\n",
      "Shape of summed layers is: 92 x 768\n",
      "triangular at index 5: [1.0730773210525513, 0.17403675615787506, -1.1381874084472656, 0.9166314005851746, 2.8035898208618164]\n",
      "Grand sum of 55 tensor sets is: [19.661436080932617, 17.017616271972656, -22.882478713989258, 17.22877311706543, 123.87306213378906]\n",
      "\n",
      "Instance 70 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "triangular at index 14: [-0.3974883556365967, 0.674558162689209, -1.6065231561660767, -0.2941299080848694, 2.1140968799591064]\n",
      "Grand sum of 56 tensor sets is: [19.263948440551758, 17.692174911499023, -24.489002227783203, 16.934642791748047, 125.9871597290039]\n",
      "\n",
      "Instance 71 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 72 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 73 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "triangular at index 5: [0.9759436249732971, -0.9647029638290405, 0.18988355994224548, 1.3880696296691895, 4.274033546447754]\n",
      "Grand sum of 57 tensor sets is: [20.239892959594727, 16.72747230529785, -24.299118041992188, 18.322711944580078, 130.26119995117188]\n",
      "\n",
      "Instance 74 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 75 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [88]\n",
      "Size of token embeddings is torch.Size([91, 13, 768])\n",
      "Shape of summed layers is: 91 x 768\n",
      "triangular at index 88: [0.2343878448009491, 0.7733334898948669, 0.07759527117013931, -1.3729350566864014, 0.8749860525131226]\n",
      "Grand sum of 58 tensor sets is: [20.474281311035156, 17.500804901123047, -24.22152328491211, 16.949777603149414, 131.1361846923828]\n",
      "\n",
      "Instance 76 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "triangular at index 18: [0.09577447921037674, -0.2397850751876831, -0.27145954966545105, -0.24372783303260803, 0.8172231912612915]\n",
      "Grand sum of 59 tensor sets is: [20.57005500793457, 17.26102066040039, -24.492982864379883, 16.7060489654541, 131.9534149169922]\n",
      "\n",
      "Instance 77 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "triangular at index 11: [-0.46015965938568115, 0.8498056530952454, 0.47783195972442627, 0.6256619095802307, 3.5911078453063965]\n",
      "Grand sum of 60 tensor sets is: [20.109895706176758, 18.11082649230957, -24.01515007019043, 17.331710815429688, 135.54452514648438]\n",
      "\n",
      "Instance 78 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "triangular at index 5: [0.6949735283851624, 0.2470470666885376, -1.8812415599822998, 0.34430402517318726, 0.8944829702377319]\n",
      "Grand sum of 61 tensor sets is: [20.804868698120117, 18.357873916625977, -25.896390914916992, 17.676013946533203, 136.4390106201172]\n",
      "\n",
      "Instance 79 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "triangular at index 14: [0.6839725375175476, -0.3043402135372162, -0.16872459650039673, 1.9605456590652466, 0.11057481169700623]\n",
      "Grand sum of 62 tensor sets is: [21.488842010498047, 18.05353355407715, -26.065114974975586, 19.636560440063477, 136.54959106445312]\n",
      "\n",
      "Instance 80 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [52]\n",
      "Size of token embeddings is torch.Size([125, 13, 768])\n",
      "Shape of summed layers is: 125 x 768\n",
      "triangular at index 52: [0.7748144268989563, -0.6735210418701172, -1.3454002141952515, -0.8319444060325623, 4.318869590759277]\n",
      "Grand sum of 63 tensor sets is: [22.263656616210938, 17.38001251220703, -27.41051483154297, 18.804616928100586, 140.8684539794922]\n",
      "\n",
      "Instance 81 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "triangular at index 4: [-0.2024122029542923, 0.5064204335212708, -0.19408980011940002, -0.35632115602493286, 3.4273319244384766]\n",
      "Grand sum of 64 tensor sets is: [22.06124496459961, 17.886432647705078, -27.604604721069336, 18.44829559326172, 144.29579162597656]\n",
      "\n",
      "Instance 82 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 83 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "triangular at index 6: [0.3234233856201172, -0.4175339937210083, 1.80967378616333, 0.6308819055557251, 1.8180840015411377]\n",
      "Grand sum of 65 tensor sets is: [22.384668350219727, 17.46889877319336, -25.794931411743164, 19.079177856445312, 146.11387634277344]\n",
      "\n",
      "Instance 84 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "triangular at index 23: [0.5926252603530884, 1.675563097000122, -1.3347678184509277, 0.7030825614929199, 3.5132358074188232]\n",
      "Grand sum of 66 tensor sets is: [22.977293014526367, 19.14446258544922, -27.12969970703125, 19.78226089477539, 149.62710571289062]\n",
      "\n",
      "Instance 85 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 86 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "triangular at index 5: [-0.06400454044342041, 0.32520973682403564, 0.10891048610210419, 0.4548386037349701, 2.18261981010437]\n",
      "Grand sum of 67 tensor sets is: [22.913288116455078, 19.46967315673828, -27.020790100097656, 20.237098693847656, 151.80972290039062]\n",
      "\n",
      "Instance 87 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 88 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "triangular at index 22: [-0.2906656265258789, 1.0253691673278809, 0.16137176752090454, 0.20940625667572021, 1.8638265132904053]\n",
      "Grand sum of 68 tensor sets is: [22.622623443603516, 20.49504280090332, -26.859418869018555, 20.446504592895508, 153.67355346679688]\n",
      "\n",
      "Instance 89 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "triangular at index 2: [1.1049126386642456, 0.6106314659118652, -0.9463047981262207, 1.044912338256836, 2.492161750793457]\n",
      "Grand sum of 69 tensor sets is: [23.727535247802734, 21.105674743652344, -27.805723190307617, 21.491416931152344, 156.16571044921875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 90 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "triangular at index 20: [-0.5715634822845459, -0.6303589344024658, 0.2849917709827423, 0.7768264412879944, 2.184788942337036]\n",
      "Grand sum of 70 tensor sets is: [23.15597152709961, 20.47531509399414, -27.52073097229004, 22.26824378967285, 158.35049438476562]\n",
      "\n",
      "Instance 91 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "triangular at index 14: [0.03619369864463806, 1.588854432106018, -0.5139371752738953, 1.7720428705215454, 1.6980743408203125]\n",
      "Grand sum of 71 tensor sets is: [23.19216537475586, 22.06416893005371, -28.03466796875, 24.040287017822266, 160.04856872558594]\n",
      "\n",
      "Instance 92 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 93 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "triangular at index 31: [0.3888474404811859, 0.2105647772550583, 0.8649678230285645, 1.8995071649551392, 1.2190948724746704]\n",
      "Grand sum of 72 tensor sets is: [23.581012725830078, 22.274734497070312, -27.169700622558594, 25.939794540405273, 161.26766967773438]\n",
      "\n",
      "Instance 94 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "triangular at index 26: [-0.18269437551498413, 0.8724011182785034, -1.241683006286621, 1.4868152141571045, 1.2607324123382568]\n",
      "Grand sum of 73 tensor sets is: [23.398319244384766, 23.14713478088379, -28.41138458251953, 27.42660903930664, 162.5283966064453]\n",
      "\n",
      "Instance 95 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 96 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "triangular at index 50: [0.29591289162635803, 1.6056689023971558, -0.2186521589756012, 0.4738021790981293, 1.292970895767212]\n",
      "Grand sum of 74 tensor sets is: [23.694232940673828, 24.752803802490234, -28.630037307739258, 27.90041160583496, 163.8213653564453]\n",
      "\n",
      "Instance 97 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 98 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [8, 21]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "triangular at index 8: [0.1864812821149826, -0.5706586241722107, -0.7196075916290283, -0.49710801243782043, 2.908482074737549]\n",
      "triangular at index 21: [0.18839722871780396, 0.17914658784866333, 0.8189123272895813, -1.4315799474716187, 2.6204938888549805]\n",
      "Grand sum of 75 tensor sets is: [23.881671905517578, 24.55704689025879, -28.580385208129883, 26.936067581176758, 166.58584594726562]\n",
      "\n",
      "Instance 99 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 100 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "triangular at index 21: [-0.6368641257286072, 2.003263473510742, 1.0492644309997559, 0.1818641573190689, 4.27656364440918]\n",
      "Grand sum of 76 tensor sets is: [23.244808197021484, 26.56031036376953, -27.53112030029297, 27.117931365966797, 170.86241149902344]\n",
      "\n",
      "Instance 101 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "triangular at index 10: [0.22762523591518402, -1.1188018321990967, 0.8095977902412415, 0.7350152134895325, 2.38533091545105]\n",
      "Grand sum of 77 tensor sets is: [23.47243309020996, 25.441509246826172, -26.72152328491211, 27.852947235107422, 173.24774169921875]\n",
      "\n",
      "Instance 102 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "triangular at index 14: [0.22075589001178741, 0.8468289375305176, -0.4343000650405884, 1.3513263463974, 1.7108097076416016]\n",
      "Grand sum of 78 tensor sets is: [23.69318962097168, 26.28833770751953, -27.15582275390625, 29.204273223876953, 174.95855712890625]\n",
      "\n",
      "Instance 103 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "triangular at index 9: [0.44840845465660095, 1.791375756263733, 0.526193380355835, 2.2535462379455566, 2.6341183185577393]\n",
      "Grand sum of 79 tensor sets is: [24.141597747802734, 28.079713821411133, -26.629629135131836, 31.45781898498535, 177.59268188476562]\n",
      "\n",
      "Instance 104 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "triangular at index 26: [-0.003886885941028595, 0.08910766243934631, -0.2744758427143097, -2.174426794052124, 3.5434300899505615]\n",
      "Grand sum of 80 tensor sets is: [24.137710571289062, 28.168821334838867, -26.904104232788086, 29.28339195251465, 181.1361083984375]\n",
      "\n",
      "Instance 105 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 106 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "triangular at index 3: [-0.2840827405452728, -1.0199369192123413, -0.5460447072982788, 0.859316885471344, 7.559670925140381]\n",
      "Grand sum of 81 tensor sets is: [23.853628158569336, 27.148883819580078, -27.450149536132812, 30.142709732055664, 188.69578552246094]\n",
      "\n",
      "Instance 107 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "triangular at index 10: [0.974157452583313, 1.648194670677185, -0.39946094155311584, 2.5930614471435547, 2.3605411052703857]\n",
      "Grand sum of 82 tensor sets is: [24.82778549194336, 28.79707908630371, -27.849611282348633, 32.73577117919922, 191.0563201904297]\n",
      "\n",
      "Instance 108 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "triangular at index 4: [0.49545595049858093, -0.47990134358406067, -1.0764033794403076, 0.6926431655883789, 3.34159255027771]\n",
      "Grand sum of 83 tensor sets is: [25.3232421875, 28.317176818847656, -28.926013946533203, 33.42841339111328, 194.39791870117188]\n",
      "\n",
      "Instance 109 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "triangular at index 28: [0.5720692873001099, 0.02673506550490856, 0.029736340045928955, 1.1201914548873901, -0.48734259605407715]\n",
      "Grand sum of 84 tensor sets is: [25.89531135559082, 28.34391212463379, -28.896278381347656, 34.54860305786133, 193.91058349609375]\n",
      "\n",
      "Instance 110 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "triangular at index 4: [0.5685973763465881, -0.26017487049102783, -0.8540503978729248, 0.0024338960647583008, 2.220717430114746]\n",
      "Grand sum of 85 tensor sets is: [26.463909149169922, 28.083736419677734, -29.750328063964844, 34.5510368347168, 196.1313018798828]\n",
      "\n",
      "Instance 111 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "triangular at index 14: [0.05825309827923775, 0.4045545756816864, 0.17881989479064941, -1.3016440868377686, 3.561605930328369]\n",
      "Grand sum of 86 tensor sets is: [26.52216148376465, 28.488290786743164, -29.571508407592773, 33.249393463134766, 199.69290161132812]\n",
      "\n",
      "Instance 112 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "triangular at index 18: [-0.5596067309379578, 1.3987982273101807, -0.6481500267982483, 0.031953178346157074, -0.008091092109680176]\n",
      "Grand sum of 87 tensor sets is: [25.962554931640625, 29.887088775634766, -30.21965789794922, 33.28134536743164, 199.684814453125]\n",
      "\n",
      "Instance 113 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 114 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "triangular at index 18: [-0.4286268949508667, 2.69063138961792, 1.4431523084640503, -0.4911562502384186, 3.1360738277435303]\n",
      "Grand sum of 88 tensor sets is: [25.53392791748047, 32.577720642089844, -28.776506423950195, 32.79018783569336, 202.82089233398438]\n",
      "\n",
      "Instance 115 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "triangular at index 38: [-0.1971329152584076, 0.5481020212173462, -0.9453508257865906, -1.4041551351547241, 6.003443717956543]\n",
      "Grand sum of 89 tensor sets is: [25.336795806884766, 33.125823974609375, -29.72185707092285, 31.386032104492188, 208.8243408203125]\n",
      "\n",
      "Instance 116 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [52]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "triangular at index 52: [0.8791459798812866, -0.7348611950874329, -1.1876606941223145, -1.0808988809585571, 2.784306287765503]\n",
      "Grand sum of 90 tensor sets is: [26.2159423828125, 32.39096450805664, -30.909517288208008, 30.305133819580078, 211.608642578125]\n",
      "\n",
      "Instance 117 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "triangular at index 14: [-0.2653721570968628, -1.3062036037445068, -1.5234458446502686, 0.39990225434303284, 3.4967048168182373]\n",
      "Grand sum of 91 tensor sets is: [25.950571060180664, 31.084760665893555, -32.43296432495117, 30.705036163330078, 215.1053466796875]\n",
      "\n",
      "Instance 118 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 119 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "triangular at index 12: [-0.07597260922193527, 0.5866830945014954, 0.05533730983734131, 1.3216769695281982, 2.544872999191284]\n",
      "Grand sum of 92 tensor sets is: [25.874597549438477, 31.671443939208984, -32.377628326416016, 32.02671432495117, 217.6502227783203]\n",
      "\n",
      "Instance 120 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "triangular at index 23: [-0.24320469796657562, 1.5400128364562988, 0.24507254362106323, -0.17763754725456238, 3.018275499343872]\n",
      "Grand sum of 93 tensor sets is: [25.631393432617188, 33.211456298828125, -32.1325569152832, 31.849077224731445, 220.6685028076172]\n",
      "\n",
      "Instance 121 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [69]\n",
      "Size of token embeddings is torch.Size([89, 13, 768])\n",
      "Shape of summed layers is: 89 x 768\n",
      "triangular at index 69: [1.1323131322860718, -0.16919741034507751, -1.2660813331604004, -0.4495640993118286, 2.7181568145751953]\n",
      "Grand sum of 94 tensor sets is: [26.76370620727539, 33.042259216308594, -33.39863967895508, 31.399513244628906, 223.38665771484375]\n",
      "\n",
      "Instance 122 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "triangular at index 4: [-0.783320426940918, 1.6781823635101318, -0.3518609404563904, -1.5387299060821533, 6.224607467651367]\n",
      "Grand sum of 95 tensor sets is: [25.980384826660156, 34.72043991088867, -33.7504997253418, 29.860782623291016, 229.61126708984375]\n",
      "\n",
      "Instance 123 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 124 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "triangular at index 9: [1.1027247905731201, -0.4349217414855957, -0.6510917544364929, 0.9733908772468567, 2.2291386127471924]\n",
      "Grand sum of 96 tensor sets is: [27.08310890197754, 34.285518646240234, -34.40159225463867, 30.83417320251465, 231.8404083251953]\n",
      "\n",
      "Instance 125 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 126 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "triangular at index 3: [0.11530959606170654, -0.13657239079475403, -0.49724915623664856, -0.11074113845825195, 3.6405014991760254]\n",
      "Grand sum of 97 tensor sets is: [27.19841766357422, 34.14894485473633, -34.898841857910156, 30.723432540893555, 235.4809112548828]\n",
      "\n",
      "Instance 127 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [178]\n",
      "Size of token embeddings is torch.Size([372, 13, 768])\n",
      "Shape of summed layers is: 372 x 768\n",
      "triangular at index 178: [0.8380002379417419, 2.15647554397583, -0.04370000958442688, 1.1494063138961792, 0.47584837675094604]\n",
      "Grand sum of 98 tensor sets is: [28.03641700744629, 36.305419921875, -34.942543029785156, 31.872838973999023, 235.95675659179688]\n",
      "\n",
      "Instance 128 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "triangular at index 3: [-0.1271587312221527, 0.33038800954818726, -1.284047245979309, 0.23293882608413696, 4.310938358306885]\n",
      "Grand sum of 99 tensor sets is: [27.909257888793945, 36.635807037353516, -36.22658920288086, 32.105777740478516, 240.2677001953125]\n",
      "\n",
      "Instance 129 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "triangular at index 10: [0.9084395170211792, 0.9467024803161621, 0.3305157423019409, 1.6591355800628662, 0.0013366341590881348]\n",
      "Grand sum of 100 tensor sets is: [28.817697525024414, 37.5825080871582, -35.89607238769531, 33.76491165161133, 240.26904296875]\n",
      "\n",
      "Instance 130 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "triangular at index 9: [-0.13939562439918518, -0.16551437973976135, 0.8351524472236633, 0.6756964921951294, 0.19593997299671173]\n",
      "Grand sum of 101 tensor sets is: [28.678302764892578, 37.4169921875, -35.06092071533203, 34.440608978271484, 240.46498107910156]\n",
      "\n",
      "Instance 131 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "triangular at index 14: [0.13679863512516022, 0.3128633201122284, -1.1331201791763306, 1.7408068180084229, 2.573251724243164]\n",
      "Grand sum of 102 tensor sets is: [28.815101623535156, 37.729854583740234, -36.19404220581055, 36.18141555786133, 243.03823852539062]\n",
      "\n",
      "Instance 132 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 133 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "triangular at index 29: [0.12096607685089111, -0.653303861618042, -0.26110514998435974, 0.7263636589050293, 1.988327980041504]\n",
      "Grand sum of 103 tensor sets is: [28.936067581176758, 37.0765495300293, -36.45514678955078, 36.907779693603516, 245.0265655517578]\n",
      "\n",
      "Instance 134 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [59]\n",
      "Size of token embeddings is torch.Size([153, 13, 768])\n",
      "Shape of summed layers is: 153 x 768\n",
      "triangular at index 59: [1.3555219173431396, 0.8589680790901184, 1.0647013187408447, 2.8400473594665527, 2.796666383743286]\n",
      "Grand sum of 104 tensor sets is: [30.291589736938477, 37.935516357421875, -35.390445709228516, 39.747825622558594, 247.82322692871094]\n",
      "\n",
      "Instance 135 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [1]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "triangular at index 1: [-0.0747448056936264, 0.9634788036346436, -0.17521774768829346, 0.7659935355186462, 5.306362628936768]\n",
      "Grand sum of 105 tensor sets is: [30.21684455871582, 38.89899444580078, -35.5656623840332, 40.51382064819336, 253.1295928955078]\n",
      "\n",
      "Instance 136 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "triangular at index 23: [1.243483304977417, 0.9089840054512024, -0.25873756408691406, 1.4935131072998047, 2.185299873352051]\n",
      "Grand sum of 106 tensor sets is: [31.4603271484375, 39.807979583740234, -35.82440185546875, 42.00733184814453, 255.3148956298828]\n",
      "\n",
      "Instance 137 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "triangular at index 9: [0.8697747588157654, 0.17257754504680634, 0.17686958611011505, 1.1621990203857422, 2.5403547286987305]\n",
      "Grand sum of 107 tensor sets is: [32.330101013183594, 39.98055648803711, -35.64753341674805, 43.169532775878906, 257.8552551269531]\n",
      "\n",
      "Instance 138 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "triangular at index 9: [0.6541860103607178, 0.38713687658309937, -0.47237977385520935, -0.7435468435287476, 3.8852624893188477]\n",
      "Grand sum of 108 tensor sets is: [32.98428726196289, 40.36769485473633, -36.11991500854492, 42.425987243652344, 261.7405090332031]\n",
      "\n",
      "Instance 139 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "triangular at index 6: [0.5218700170516968, 1.9932998418807983, -1.1480023860931396, 0.6339601874351501, 1.828546166419983]\n",
      "Grand sum of 109 tensor sets is: [33.50615692138672, 42.36099624633789, -37.26791763305664, 43.0599479675293, 263.5690612792969]\n",
      "\n",
      "Instance 140 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "triangular at index 4: [1.1200504302978516, -0.5652493834495544, -1.4098694324493408, -0.8066501617431641, 1.4031658172607422]\n",
      "Grand sum of 110 tensor sets is: [34.62620544433594, 41.795745849609375, -38.67778778076172, 42.2532958984375, 264.97222900390625]\n",
      "\n",
      "Instance 141 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "triangular at index 7: [1.261627435684204, 0.06540757417678833, -0.60129714012146, -1.6002732515335083, 1.4386602640151978]\n",
      "Grand sum of 111 tensor sets is: [35.88783264160156, 41.86115264892578, -39.279083251953125, 40.65302276611328, 266.410888671875]\n",
      "\n",
      "Instance 142 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "triangular at index 44: [0.3528807461261749, 0.47140297293663025, 0.16386759281158447, 0.9385541677474976, -0.38972610235214233]\n",
      "Grand sum of 112 tensor sets is: [36.24071502685547, 42.33255386352539, -39.11521530151367, 41.591575622558594, 266.0211486816406]\n",
      "\n",
      "Instance 143 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "triangular at index 15: [-0.40575021505355835, -1.7055550813674927, -0.2766052484512329, -0.6041529178619385, 2.9572880268096924]\n",
      "Grand sum of 113 tensor sets is: [35.834964752197266, 40.62699890136719, -39.39181900024414, 40.987422943115234, 268.9784240722656]\n",
      "\n",
      "Instance 144 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "triangular at index 11: [1.2515907287597656, 0.6712456941604614, -1.2776849269866943, 1.156861662864685, 3.310722589492798]\n",
      "Grand sum of 114 tensor sets is: [37.08655548095703, 41.29824447631836, -40.66950225830078, 42.144283294677734, 272.2891540527344]\n",
      "\n",
      "Instance 145 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "triangular at index 27: [-0.3636075556278229, -1.1316297054290771, 1.3605477809906006, -0.7175995111465454, 1.8428078889846802]\n",
      "Grand sum of 115 tensor sets is: [36.72294616699219, 40.1666145324707, -39.308956146240234, 41.42668533325195, 274.1319580078125]\n",
      "\n",
      "Instance 146 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "triangular at index 15: [0.5899044871330261, 0.26362478733062744, 0.3905320167541504, 0.22703438997268677, 2.1583900451660156]\n",
      "Grand sum of 116 tensor sets is: [37.31285095214844, 40.430240631103516, -38.91842269897461, 41.65372085571289, 276.29034423828125]\n",
      "\n",
      "Instance 147 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [178]\n",
      "Size of token embeddings is torch.Size([372, 13, 768])\n",
      "Shape of summed layers is: 372 x 768\n",
      "triangular at index 178: [0.8380002379417419, 2.15647554397583, -0.04370000958442688, 1.1494063138961792, 0.47584837675094604]\n",
      "Grand sum of 117 tensor sets is: [38.15085220336914, 42.58671569824219, -38.96212387084961, 42.80312728881836, 276.7662048339844]\n",
      "\n",
      "Instance 148 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 149 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "triangular at index 12: [-0.1385912001132965, -0.8803243041038513, -0.6135165691375732, 0.26378172636032104, 3.8342504501342773]\n",
      "Grand sum of 118 tensor sets is: [38.01226043701172, 41.706390380859375, -39.57564163208008, 43.06690979003906, 280.6004638671875]\n",
      "\n",
      "Instance 150 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "triangular at index 16: [-0.5102534890174866, 1.0937609672546387, -0.6672203540802002, 0.1787562370300293, 0.49235910177230835]\n",
      "Grand sum of 119 tensor sets is: [37.50200653076172, 42.80015182495117, -40.242862701416016, 43.24566650390625, 281.09283447265625]\n",
      "\n",
      "Instance 151 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "triangular at index 29: [0.26534029841423035, 0.33615005016326904, -0.24231097102165222, 0.3785925507545471, 1.234834909439087]\n",
      "Grand sum of 120 tensor sets is: [37.7673454284668, 43.13630294799805, -40.485172271728516, 43.62425994873047, 282.3276672363281]\n",
      "\n",
      "Instance 152 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 153 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "triangular at index 23: [1.5411807298660278, 2.210970401763916, 0.18482272326946259, -0.15762823820114136, 4.261000156402588]\n",
      "Grand sum of 121 tensor sets is: [39.30852508544922, 45.34727478027344, -40.300350189208984, 43.46663284301758, 286.5886535644531]\n",
      "\n",
      "Instance 154 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "triangular at index 15: [0.48634305596351624, 1.2842711210250854, 0.40780508518218994, 1.0523598194122314, 1.5377708673477173]\n",
      "Grand sum of 122 tensor sets is: [39.79486846923828, 46.63154602050781, -39.89254379272461, 44.51899337768555, 288.1264343261719]\n",
      "\n",
      "Instance 155 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "triangular at index 12: [0.29397860169410706, 0.20974449813365936, 0.2938573956489563, 0.6722894310951233, 2.036146879196167]\n",
      "Grand sum of 123 tensor sets is: [40.08884811401367, 46.84128952026367, -39.59868621826172, 45.1912841796875, 290.1625671386719]\n",
      "\n",
      "Instance 156 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "triangular at index 27: [-0.31374943256378174, 2.130399703979492, 1.1781004667282104, -0.3377971947193146, 4.574601650238037]\n",
      "Grand sum of 124 tensor sets is: [39.77509689331055, 48.97168731689453, -38.42058563232422, 44.853485107421875, 294.7371826171875]\n",
      "\n",
      "Instance 157 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [58]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "triangular at index 58: [0.7104688882827759, 0.21296700835227966, -0.39563503861427307, 0.3381279408931732, 2.0587377548217773]\n",
      "Grand sum of 125 tensor sets is: [40.485565185546875, 49.184654235839844, -38.816219329833984, 45.191612243652344, 296.7959289550781]\n",
      "\n",
      "Instance 158 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "triangular at index 17: [-0.04363274574279785, 0.07308327406644821, -0.2670201063156128, 0.36332154273986816, 2.3689117431640625]\n",
      "Grand sum of 126 tensor sets is: [40.441932678222656, 49.25773620605469, -39.0832405090332, 45.554935455322266, 299.16485595703125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 159 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "triangular at index 35: [0.03678636625409126, 0.6465833187103271, -0.860395610332489, 0.08023275434970856, 1.1526246070861816]\n",
      "Grand sum of 127 tensor sets is: [40.47871780395508, 49.904319763183594, -39.94363784790039, 45.635169982910156, 300.3174743652344]\n",
      "\n",
      "Instance 160 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "triangular at index 2: [0.28204023838043213, 1.2646836042404175, -0.7129592895507812, 0.31068000197410583, 1.4323824644088745]\n",
      "Grand sum of 128 tensor sets is: [40.76075744628906, 51.169002532958984, -40.65659713745117, 45.94585037231445, 301.7498474121094]\n",
      "\n",
      "Instance 161 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 162 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "triangular at index 20: [0.37866073846817017, 0.2562742829322815, 0.17104202508926392, 0.440723717212677, 3.0538768768310547]\n",
      "Grand sum of 129 tensor sets is: [41.13941955566406, 51.42527770996094, -40.48555374145508, 46.386573791503906, 304.8037109375]\n",
      "\n",
      "Instance 163 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [69]\n",
      "Size of token embeddings is torch.Size([89, 13, 768])\n",
      "Shape of summed layers is: 89 x 768\n",
      "triangular at index 69: [1.1323131322860718, -0.16919741034507751, -1.2660813331604004, -0.4495640993118286, 2.7181568145751953]\n",
      "Grand sum of 130 tensor sets is: [42.271732330322266, 51.256080627441406, -41.75163650512695, 45.937007904052734, 307.5218811035156]\n",
      "\n",
      "Instance 164 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "triangular at index 7: [0.8855369091033936, 0.9951333999633789, -0.9109383225440979, -0.34155476093292236, 0.9351945519447327]\n",
      "Grand sum of 131 tensor sets is: [43.15726852416992, 52.25121307373047, -42.662574768066406, 45.59545135498047, 308.4570617675781]\n",
      "\n",
      "Instance 165 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "triangular at index 11: [0.3601166009902954, 1.2030905485153198, -0.11355231702327728, -0.3592492341995239, 1.1895651817321777]\n",
      "Grand sum of 132 tensor sets is: [43.51738357543945, 53.45430374145508, -42.776126861572266, 45.236202239990234, 309.6466369628906]\n",
      "\n",
      "Instance 166 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "triangular at index 11: [-0.6439287662506104, 1.9292508363723755, 0.29363033175468445, 0.6326887607574463, 3.684730052947998]\n",
      "Grand sum of 133 tensor sets is: [42.87345504760742, 55.3835563659668, -42.48249816894531, 45.868892669677734, 313.33135986328125]\n",
      "\n",
      "Instance 167 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "triangular at index 25: [-0.0710052028298378, 0.6023865342140198, 0.12494364380836487, -1.1393985748291016, 3.2669262886047363]\n",
      "Grand sum of 134 tensor sets is: [42.80244827270508, 55.98594284057617, -42.3575553894043, 44.7294921875, 316.5982971191406]\n",
      "\n",
      "Instance 168 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "triangular at index 4: [1.1815465688705444, 0.1617567241191864, -0.09403626620769501, -1.1050119400024414, 4.159312725067139]\n",
      "Grand sum of 135 tensor sets is: [43.98399353027344, 56.147701263427734, -42.45159149169922, 43.624481201171875, 320.7575988769531]\n",
      "\n",
      "Instance 169 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "triangular at index 15: [0.7216952443122864, 0.8900349736213684, -0.005121268332004547, 0.15219895541667938, 1.673092007637024]\n",
      "Grand sum of 136 tensor sets is: [44.7056884765625, 57.03773498535156, -42.45671463012695, 43.77667999267578, 322.4306945800781]\n",
      "\n",
      "Instance 170 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "triangular at index 20: [0.05360856652259827, 0.6664936542510986, -0.1650993973016739, -0.3403145670890808, 3.698992967605591]\n",
      "Grand sum of 137 tensor sets is: [44.75929641723633, 57.704227447509766, -42.6218147277832, 43.43636703491211, 326.12969970703125]\n",
      "\n",
      "Instance 171 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 172 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 173 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "triangular at index 29: [-0.3209545910358429, 0.6154507398605347, -0.1740175038576126, -0.5211881995201111, 4.339684963226318]\n",
      "Grand sum of 138 tensor sets is: [44.4383430480957, 58.319679260253906, -42.795833587646484, 42.91518020629883, 330.4693908691406]\n",
      "\n",
      "Instance 174 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "triangular at index 46: [-0.34121036529541016, 0.3691977262496948, -0.7429322004318237, -3.3096253871917725, 2.898557186126709]\n",
      "Grand sum of 139 tensor sets is: [44.09713363647461, 58.68887710571289, -43.53876495361328, 39.60555648803711, 333.3679504394531]\n",
      "\n",
      "Instance 175 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "triangular at index 18: [-0.3481352925300598, 0.49101462960243225, -0.0010802745819091797, 0.6764310598373413, 1.721238613128662]\n",
      "Grand sum of 140 tensor sets is: [43.74899673461914, 59.179893493652344, -43.53984451293945, 40.281986236572266, 335.0892028808594]\n",
      "\n",
      "Instance 176 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "triangular at index 10: [0.2789982855319977, 0.1103970855474472, -0.6655013561248779, 0.945903480052948, 1.6954615116119385]\n",
      "Grand sum of 141 tensor sets is: [44.02799606323242, 59.29029083251953, -44.205345153808594, 41.22789001464844, 336.78466796875]\n",
      "\n",
      "Instance 177 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "triangular at index 12: [0.1318667232990265, 1.5139306783676147, 0.2149125635623932, 1.3890539407730103, -0.2975750267505646]\n",
      "Grand sum of 142 tensor sets is: [44.15986251831055, 60.804222106933594, -43.99043273925781, 42.616943359375, 336.4870910644531]\n",
      "\n",
      "Instance 178 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 179 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 180 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "triangular at index 21: [0.3063783347606659, 0.7598958015441895, -0.0809611976146698, 0.11384880542755127, 4.272244930267334]\n",
      "Grand sum of 143 tensor sets is: [44.46623992919922, 61.564117431640625, -44.07139205932617, 42.73079299926758, 340.75933837890625]\n",
      "\n",
      "Instance 181 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "triangular at index 2: [-0.12941378355026245, 0.002118106931447983, -0.6784583926200867, 1.880926251411438, 2.7154321670532227]\n",
      "Grand sum of 144 tensor sets is: [44.33682632446289, 61.56623458862305, -44.74985122680664, 44.61172103881836, 343.4747619628906]\n",
      "\n",
      "Instance 182 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 183 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "triangular at index 19: [0.36408960819244385, 0.8785044550895691, -0.19358393549919128, -0.8860582709312439, 2.365731716156006]\n",
      "Grand sum of 145 tensor sets is: [44.7009162902832, 62.444740295410156, -44.94343566894531, 43.72566223144531, 345.8404846191406]\n",
      "\n",
      "Instance 184 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "triangular at index 18: [-0.14953723549842834, 0.422778844833374, -0.012078627943992615, 0.5550790429115295, 2.6493606567382812]\n",
      "Grand sum of 146 tensor sets is: [44.5513801574707, 62.86751937866211, -44.95551300048828, 44.28074264526367, 348.4898376464844]\n",
      "\n",
      "Instance 185 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "triangular at index 6: [-0.4750414490699768, 0.7147523164749146, 0.04871475696563721, -1.737088918685913, 2.234835624694824]\n",
      "Grand sum of 147 tensor sets is: [44.07633972167969, 63.582271575927734, -44.90679931640625, 42.54365539550781, 350.72467041015625]\n",
      "\n",
      "Instance 186 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 187 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 188 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "triangular at index 7: [0.15282882750034332, 0.59297114610672, -0.7462480068206787, 1.71091628074646, 1.0818262100219727]\n",
      "Grand sum of 148 tensor sets is: [44.22916793823242, 64.17523956298828, -45.653045654296875, 44.25457000732422, 351.8064880371094]\n",
      "\n",
      "Instance 189 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [69]\n",
      "Size of token embeddings is torch.Size([89, 13, 768])\n",
      "Shape of summed layers is: 89 x 768\n",
      "triangular at index 69: [1.1323131322860718, -0.16919741034507751, -1.2660813331604004, -0.4495640993118286, 2.7181568145751953]\n",
      "Grand sum of 149 tensor sets is: [45.361480712890625, 64.00604248046875, -46.91912841796875, 43.80500411987305, 354.524658203125]\n",
      "\n",
      "Instance 190 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "triangular at index 4: [0.3365210294723511, 0.08983249962329865, -0.6856796145439148, -0.7695534825325012, 2.6274099349975586]\n",
      "Grand sum of 150 tensor sets is: [45.698001861572266, 64.09587860107422, -47.60480880737305, 43.03544998168945, 357.1520690917969]\n",
      "\n",
      "Instance 191 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 192 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "triangular at index 33: [0.3067803382873535, 0.2050468474626541, 0.7367655634880066, 0.5325127840042114, 2.6796953678131104]\n",
      "Grand sum of 151 tensor sets is: [46.004783630371094, 64.3009262084961, -46.8680419921875, 43.567962646484375, 359.8317565917969]\n",
      "\n",
      "Instance 193 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "\n",
      "Instance 194 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "triangular at index 20: [0.2556711435317993, 1.4517695903778076, 0.5214912295341492, -0.710204005241394, 3.9024276733398438]\n",
      "Grand sum of 152 tensor sets is: [46.26045608520508, 65.75269317626953, -46.34654998779297, 42.857757568359375, 363.73419189453125]\n",
      "\n",
      "Instance 195 of triangular.\n",
      "Looking for vocab token: triangular\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "triangular at index 28: [-0.6125909090042114, 1.8873311281204224, -0.676765501499176, -2.5831570625305176, 1.5316228866577148]\n",
      "Grand sum of 153 tensor sets is: [45.647865295410156, 67.64002227783203, -47.0233154296875, 40.274600982666016, 365.26580810546875]\n",
      "Mean of tensors is: tensor([ 0.2984,  0.4421, -0.3073,  0.2632,  2.3874]) (768 features in tensor)\n",
      "Saved the embedding for triangular.\n",
      "Saved the count of sentences used to create triangular embedding\n",
      "Run time for triangular was 37.60199949145317 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "jumping\n",
      "\n",
      "Instance 1 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 9: [-0.40322545170783997, -0.07400412857532501, -0.654420018196106, -1.2205466032028198, 0.3332587778568268]\n",
      "Grand sum of 1 tensor sets is: [-0.40322545170783997, -0.07400412857532501, -0.654420018196106, -1.2205466032028198, 0.3332587778568268]\n",
      "\n",
      "Instance 2 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 2 tensor sets is: [-1.068884253501892, 2.161625385284424, -1.8257708549499512, -2.1418094635009766, -0.5422632694244385]\n",
      "\n",
      "Instance 3 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 3: [-0.49774909019470215, 1.5053365230560303, -0.3491864502429962, -1.1604585647583008, 2.993807792663574]\n",
      "Grand sum of 3 tensor sets is: [-1.5666333436965942, 3.666961908340454, -2.174957275390625, -3.3022680282592773, 2.4515445232391357]\n",
      "\n",
      "Instance 4 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 31: [0.7308530807495117, 1.446509599685669, 0.9715222120285034, -0.756840705871582, 1.6107447147369385]\n",
      "Grand sum of 4 tensor sets is: [-0.8357802629470825, 5.113471508026123, -1.2034350633621216, -4.059108734130859, 4.062289237976074]\n",
      "\n",
      "Instance 5 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 5 tensor sets is: [-0.9537373781204224, 5.541770935058594, -1.0204765796661377, -6.254716873168945, 6.564051628112793]\n",
      "\n",
      "Instance 6 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([110, 13, 768])\n",
      "Shape of summed layers is: 110 x 768\n",
      "jumping at index 14: [0.4780179560184479, 1.9421477317810059, -0.6466348767280579, -1.617964267730713, 3.2116150856018066]\n",
      "Grand sum of 6 tensor sets is: [-0.4757194221019745, 7.4839186668396, -1.6671113967895508, -7.872681140899658, 9.775667190551758]\n",
      "\n",
      "Instance 7 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 22: [0.501754641532898, 0.690273642539978, -2.429314613342285, 2.8545894622802734, 4.087538242340088]\n",
      "Grand sum of 7 tensor sets is: [0.026035219430923462, 8.174192428588867, -4.096426010131836, -5.018091678619385, 13.863204956054688]\n",
      "\n",
      "Instance 8 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 19: [-0.1137247383594513, -0.2970207631587982, -0.861227810382843, -1.131150245666504, 1.160927653312683]\n",
      "Grand sum of 8 tensor sets is: [-0.08768951892852783, 7.877171516418457, -4.957653999328613, -6.149241924285889, 15.02413272857666]\n",
      "\n",
      "Instance 9 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 10 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [61]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "jumping at index 61: [0.8735477328300476, 1.8020164966583252, -2.197086811065674, -0.8528189659118652, -0.12462358176708221]\n",
      "Grand sum of 9 tensor sets is: [0.7858582139015198, 9.679187774658203, -7.154740810394287, -7.002060890197754, 14.89950942993164]\n",
      "\n",
      "Instance 11 of jumping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 31: [-0.007205277681350708, 1.0726356506347656, -0.6527242660522461, -1.6997889280319214, 0.5011343955993652]\n",
      "Grand sum of 10 tensor sets is: [0.7786529064178467, 10.751823425292969, -7.807465076446533, -8.701849937438965, 15.400644302368164]\n",
      "\n",
      "Instance 12 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 6: [0.06544824689626694, 1.4440839290618896, 0.6749966740608215, -1.6313462257385254, 2.187748908996582]\n",
      "Grand sum of 11 tensor sets is: [0.8441011309623718, 12.195907592773438, -7.132468223571777, -10.333196640014648, 17.588394165039062]\n",
      "\n",
      "Instance 13 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 3: [0.3770534098148346, 0.8444465398788452, 1.4878417253494263, -1.174741506576538, 2.6642799377441406]\n",
      "Grand sum of 12 tensor sets is: [1.2211545705795288, 13.040353775024414, -5.644626617431641, -11.507938385009766, 20.252674102783203]\n",
      "\n",
      "Instance 14 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 15 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 13: [-0.36897796392440796, 1.865752935409546, 0.5097132921218872, -2.120054006576538, 2.924896001815796]\n",
      "Grand sum of 13 tensor sets is: [0.8521766066551208, 14.906106948852539, -5.134913444519043, -13.627992630004883, 23.177570343017578]\n",
      "\n",
      "Instance 16 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 9: [0.26350364089012146, 1.2025578022003174, 0.05262425169348717, -1.9842904806137085, 3.070491313934326]\n",
      "Grand sum of 14 tensor sets is: [1.11568021774292, 16.108665466308594, -5.082289218902588, -15.612282752990723, 26.248062133789062]\n",
      "\n",
      "Instance 17 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 15: [-0.6566887497901917, 0.9134571552276611, -1.4263105392456055, 1.034759283065796, 1.7500241994857788]\n",
      "Grand sum of 15 tensor sets is: [0.45899146795272827, 17.022123336791992, -6.508599758148193, -14.577523231506348, 27.99808692932129]\n",
      "\n",
      "Instance 18 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 33: [0.8752151131629944, 0.6746745109558105, -0.4674273133277893, 0.18612663447856903, 0.02450484037399292]\n",
      "Grand sum of 16 tensor sets is: [1.3342065811157227, 17.69679832458496, -6.976027011871338, -14.391396522521973, 28.022592544555664]\n",
      "\n",
      "Instance 19 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "jumping at index 22: [-0.5126543045043945, 1.706146478652954, 0.4283892512321472, -0.6090923547744751, 0.4085220992565155]\n",
      "Grand sum of 17 tensor sets is: [0.8215522766113281, 19.402944564819336, -6.547637939453125, -15.000489234924316, 28.431114196777344]\n",
      "\n",
      "Instance 20 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 14: [-0.17192088067531586, 0.10264050960540771, 2.0860166549682617, -1.1420845985412598, 2.9547336101531982]\n",
      "Grand sum of 18 tensor sets is: [0.6496313810348511, 19.505584716796875, -4.461621284484863, -16.142574310302734, 31.385847091674805]\n",
      "\n",
      "Instance 21 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 19 tensor sets is: [1.5920917987823486, 19.57121467590332, -4.8318939208984375, -16.612319946289062, 34.012176513671875]\n",
      "\n",
      "Instance 22 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [1]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 1: [0.2840344309806824, 1.6026077270507812, -0.8006553649902344, 0.825473964214325, 7.433827877044678]\n",
      "Grand sum of 20 tensor sets is: [1.8761262893676758, 21.1738224029541, -5.632549285888672, -15.786846160888672, 41.44600296020508]\n",
      "\n",
      "Instance 23 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 21: [0.7428349256515503, 2.014119863510132, -0.02664097771048546, 0.730730414390564, 0.6547770500183105]\n",
      "Grand sum of 21 tensor sets is: [2.6189613342285156, 23.187942504882812, -5.6591901779174805, -15.056116104125977, 42.10078048706055]\n",
      "\n",
      "Instance 24 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 6: [0.06544824689626694, 1.4440839290618896, 0.6749966740608215, -1.6313462257385254, 2.187748908996582]\n",
      "Grand sum of 22 tensor sets is: [2.6844096183776855, 24.63202667236328, -4.984193325042725, -16.687461853027344, 44.28852844238281]\n",
      "\n",
      "Instance 25 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "jumping at index 3: [0.21992124617099762, 1.011819839477539, 0.6035152077674866, 0.5072473883628845, 4.14814567565918]\n",
      "Grand sum of 23 tensor sets is: [2.9043309688568115, 25.64384651184082, -4.380678176879883, -16.180213928222656, 48.436676025390625]\n",
      "\n",
      "Instance 26 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "jumping at index 14: [0.2895587086677551, -0.44744011759757996, -0.00665755569934845, -0.25807830691337585, 0.013190805912017822]\n",
      "Grand sum of 24 tensor sets is: [3.193889617919922, 25.196407318115234, -4.387335777282715, -16.438291549682617, 48.449867248535156]\n",
      "\n",
      "Instance 27 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 8: [0.6574286818504333, 3.0194385051727295, -0.10047688335180283, 1.1424096822738647, 1.6274151802062988]\n",
      "Grand sum of 25 tensor sets is: [3.851318359375, 28.215845108032227, -4.487812519073486, -15.295882225036621, 50.0772819519043]\n",
      "\n",
      "Instance 28 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 26 tensor sets is: [3.73336124420166, 28.64414405822754, -4.304853916168213, -17.49148941040039, 52.579044342041016]\n",
      "\n",
      "Instance 29 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "jumping at index 41: [0.8461534976959229, 0.4237762987613678, 0.4794570803642273, 0.27685242891311646, 2.071887731552124]\n",
      "Grand sum of 27 tensor sets is: [4.579514503479004, 29.067920684814453, -3.825396776199341, -17.214637756347656, 54.65093231201172]\n",
      "\n",
      "Instance 30 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 28 tensor sets is: [5.685540199279785, 30.00636100769043, -4.42887020111084, -17.516128540039062, 56.570945739746094]\n",
      "\n",
      "Instance 31 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 29: [-0.47710520029067993, 0.4986730217933655, 0.5965859293937683, -3.2913360595703125, 4.461522579193115]\n",
      "Grand sum of 29 tensor sets is: [5.20843505859375, 30.505033493041992, -3.8322842121124268, -20.807464599609375, 61.032466888427734]\n",
      "\n",
      "Instance 32 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 24: [-0.33908772468566895, 2.3500239849090576, -0.9748744368553162, -0.3873941898345947, 2.824113130569458]\n",
      "Grand sum of 30 tensor sets is: [4.86934757232666, 32.85505676269531, -4.807158470153809, -21.19485855102539, 63.8565788269043]\n",
      "\n",
      "Instance 33 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 16: [-0.5653939247131348, 2.123201370239258, -0.0965069979429245, -1.9013254642486572, 3.1319475173950195]\n",
      "Grand sum of 31 tensor sets is: [4.303953647613525, 34.97825622558594, -4.903665542602539, -23.09618377685547, 66.988525390625]\n",
      "\n",
      "Instance 34 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "jumping at index 18: [1.4917160272598267, 2.554391860961914, -0.6392304301261902, -0.48336631059646606, 1.3659656047821045]\n",
      "Grand sum of 32 tensor sets is: [5.7956695556640625, 37.53264617919922, -5.542895793914795, -23.57954978942871, 68.3544921875]\n",
      "\n",
      "Instance 35 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [54]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "jumping at index 54: [0.1787436604499817, -0.8351538181304932, 0.20780354738235474, -3.2420449256896973, 0.8534033298492432]\n",
      "Grand sum of 33 tensor sets is: [5.9744133949279785, 36.69749069213867, -5.335092067718506, -26.82159423828125, 69.20789337158203]\n",
      "\n",
      "Instance 36 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 25: [-0.256424218416214, 1.2988150119781494, 0.5757050514221191, -1.7893929481506348, -0.6766921281814575]\n",
      "Grand sum of 34 tensor sets is: [5.717988967895508, 37.996307373046875, -4.759387016296387, -28.610986709594727, 68.53120422363281]\n",
      "\n",
      "Instance 37 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 16: [1.1507742404937744, 3.1085755825042725, -0.7731150984764099, 2.437532424926758, -1.6698050498962402]\n",
      "Grand sum of 35 tensor sets is: [6.868762969970703, 41.104881286621094, -5.532502174377441, -26.17345428466797, 66.86139678955078]\n",
      "\n",
      "Instance 38 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 5: [-0.7684081196784973, 0.6065731048583984, 0.24227701127529144, -1.5059888362884521, 2.2626097202301025]\n",
      "Grand sum of 36 tensor sets is: [6.1003546714782715, 41.711456298828125, -5.290225028991699, -27.679443359375, 69.12400817871094]\n",
      "\n",
      "Instance 39 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 3: [0.35860660672187805, 0.5878005027770996, 0.19286030530929565, 0.530090868473053, 2.693881034851074]\n",
      "Grand sum of 37 tensor sets is: [6.458961486816406, 42.29925537109375, -5.097364902496338, -27.14935302734375, 71.81788635253906]\n",
      "\n",
      "Instance 40 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([80, 13, 768])\n",
      "Shape of summed layers is: 80 x 768\n",
      "jumping at index 65: [1.390573501586914, 1.5001411437988281, -0.0752289816737175, -0.05625516176223755, 2.340376377105713]\n",
      "Grand sum of 38 tensor sets is: [7.84953498840332, 43.79939651489258, -5.17259407043457, -27.205608367919922, 74.15826416015625]\n",
      "\n",
      "Instance 41 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 6: [0.34219199419021606, 1.0776844024658203, 0.4460078179836273, -0.6103689074516296, 0.0489945262670517]\n",
      "Grand sum of 39 tensor sets is: [8.191726684570312, 44.87708282470703, -4.72658634185791, -27.815977096557617, 74.20726013183594]\n",
      "\n",
      "Instance 42 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 24: [0.10686120390892029, -0.9652096629142761, 1.400240421295166, -1.7094333171844482, -1.782267689704895]\n",
      "Grand sum of 40 tensor sets is: [8.298587799072266, 43.91187286376953, -3.326345920562744, -29.525409698486328, 72.42499542236328]\n",
      "\n",
      "Instance 43 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 13: [-0.564190685749054, 1.660815954208374, -0.4017788767814636, -3.3169186115264893, 3.9124813079833984]\n",
      "Grand sum of 41 tensor sets is: [7.734396934509277, 45.572689056396484, -3.7281248569488525, -32.84232711791992, 76.33747863769531]\n",
      "\n",
      "Instance 44 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 45 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 9: [0.16338014602661133, 0.8527259230613708, -0.1277007758617401, -3.067582130432129, 4.225024223327637]\n",
      "Grand sum of 42 tensor sets is: [7.897777080535889, 46.4254150390625, -3.855825662612915, -35.909908294677734, 80.5625]\n",
      "\n",
      "Instance 46 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "jumping at index 33: [-0.1761552393436432, 1.0799784660339355, 1.032780647277832, -2.120934009552002, -0.15945973992347717]\n",
      "Grand sum of 43 tensor sets is: [7.721621990203857, 47.505393981933594, -2.823045015335083, -38.03084182739258, 80.40303802490234]\n",
      "\n",
      "Instance 47 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 44 tensor sets is: [7.055963039398193, 49.741024017333984, -3.9943957328796387, -38.95210647583008, 79.52751922607422]\n",
      "\n",
      "Instance 48 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 12: [0.3190326690673828, 0.5524039268493652, -0.38581448793411255, -0.033746227622032166, 0.153439462184906]\n",
      "Grand sum of 45 tensor sets is: [7.374995708465576, 50.293426513671875, -4.3802103996276855, -38.9858512878418, 79.68096160888672]\n",
      "\n",
      "Instance 49 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 50 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 11: [0.09804250299930573, 0.7348319888114929, -1.423635721206665, -1.9636154174804688, 0.6949124932289124]\n",
      "Grand sum of 46 tensor sets is: [7.473038196563721, 51.02825927734375, -5.80384635925293, -40.949466705322266, 80.3758773803711]\n",
      "\n",
      "Instance 51 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 52 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 53 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "jumping at index 45: [-0.31133657693862915, 1.745922565460205, -0.09286122024059296, -0.7268761396408081, -0.17157810926437378]\n",
      "Grand sum of 47 tensor sets is: [7.161701679229736, 52.7741813659668, -5.896707534790039, -41.67634201049805, 80.20429992675781]\n",
      "\n",
      "Instance 54 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 4: [0.5636948943138123, 1.4957839250564575, -1.852734923362732, -0.2770429849624634, 2.8797504901885986]\n",
      "Grand sum of 48 tensor sets is: [7.725396633148193, 54.26996612548828, -7.7494425773620605, -41.95338439941406, 83.08405303955078]\n",
      "\n",
      "Instance 55 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 26: [0.8327994346618652, 1.6612904071807861, -0.33770477771759033, -2.4402124881744385, 0.31993281841278076]\n",
      "Grand sum of 49 tensor sets is: [8.558196067810059, 55.93125534057617, -8.08714771270752, -44.39359664916992, 83.40398406982422]\n",
      "\n",
      "Instance 56 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 19: [-0.807694673538208, 1.4181638956069946, -0.7411438822746277, -0.6394084095954895, 4.016451835632324]\n",
      "Grand sum of 50 tensor sets is: [7.75050163269043, 57.34941864013672, -8.828291893005371, -45.03300476074219, 87.4204330444336]\n",
      "\n",
      "Instance 57 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 16: [-0.0748460665345192, 0.7877259850502014, 1.0045288801193237, -2.404214859008789, 1.0948781967163086]\n",
      "Grand sum of 51 tensor sets is: [7.675655364990234, 58.13714599609375, -7.823762893676758, -47.437217712402344, 88.51531219482422]\n",
      "\n",
      "Instance 58 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 12: [-0.6962324976921082, 1.6388288736343384, 0.25388118624687195, -1.6404521465301514, 1.3081507682800293]\n",
      "Grand sum of 52 tensor sets is: [6.9794230461120605, 59.77597427368164, -7.569881916046143, -49.07767105102539, 89.8234634399414]\n",
      "\n",
      "Instance 59 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 60 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 8: [-0.17108765244483948, 0.1780025064945221, -0.09354811906814575, -1.8215746879577637, -1.5655995607376099]\n",
      "Grand sum of 53 tensor sets is: [6.808335304260254, 59.953975677490234, -7.663430213928223, -50.89924621582031, 88.25786590576172]\n",
      "\n",
      "Instance 61 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 33: [0.4492918848991394, 0.2314612716436386, 0.8005500435829163, -2.390084743499756, 0.43798643350601196]\n",
      "Grand sum of 54 tensor sets is: [7.257627010345459, 60.1854362487793, -6.862880229949951, -53.289329528808594, 88.69585418701172]\n",
      "\n",
      "Instance 62 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 38: [0.09518691897392273, 0.4562031030654907, 0.10854917764663696, -2.3361563682556152, 0.17606481909751892]\n",
      "Grand sum of 55 tensor sets is: [7.352813720703125, 60.641639709472656, -6.754331111907959, -55.625484466552734, 88.87191772460938]\n",
      "\n",
      "Instance 63 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "jumping at index 15: [0.2379295825958252, -0.07109874486923218, -0.8726658821105957, 0.45765432715415955, 1.0942498445510864]\n",
      "Grand sum of 56 tensor sets is: [7.590743064880371, 60.57054138183594, -7.626996994018555, -55.16783142089844, 89.96617126464844]\n",
      "\n",
      "Instance 64 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 15: [0.03658110648393631, 1.8201918601989746, 0.6510325074195862, -2.5760581493377686, 2.846942663192749]\n",
      "Grand sum of 57 tensor sets is: [7.627324104309082, 62.39073181152344, -6.975964546203613, -57.74388885498047, 92.8131103515625]\n",
      "\n",
      "Instance 65 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 6: [0.06544824689626694, 1.4440839290618896, 0.6749966740608215, -1.6313462257385254, 2.187748908996582]\n",
      "Grand sum of 58 tensor sets is: [7.692772388458252, 63.834815979003906, -6.300967693328857, -59.37523651123047, 95.00086212158203]\n",
      "\n",
      "Instance 66 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 67 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [84]\n",
      "Size of token embeddings is torch.Size([88, 13, 768])\n",
      "Shape of summed layers is: 88 x 768\n",
      "jumping at index 84: [0.027018696069717407, 1.5432895421981812, -1.008208990097046, -1.7889597415924072, 1.473524808883667]\n",
      "Grand sum of 59 tensor sets is: [7.719790935516357, 65.37810516357422, -7.309176445007324, -61.1641960144043, 96.4743881225586]\n",
      "\n",
      "Instance 68 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 19: [-0.46527615189552307, 0.6065371036529541, 0.8289234042167664, -1.5898692607879639, 2.0311319828033447]\n",
      "Grand sum of 60 tensor sets is: [7.254514694213867, 65.9846420288086, -6.480253219604492, -62.754066467285156, 98.50552368164062]\n",
      "\n",
      "Instance 69 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 7: [-0.040702130645513535, -0.04923711717128754, 0.6714779734611511, -1.2296276092529297, 0.8382266163825989]\n",
      "Grand sum of 61 tensor sets is: [7.213812351226807, 65.9354019165039, -5.808775424957275, -63.98369598388672, 99.34375]\n",
      "\n",
      "Instance 70 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 13: [0.4827694892883301, 1.686653733253479, -0.156693696975708, -0.726878821849823, 2.5696334838867188]\n",
      "Grand sum of 62 tensor sets is: [7.696581840515137, 67.62205505371094, -5.9654693603515625, -64.7105712890625, 101.91338348388672]\n",
      "\n",
      "Instance 71 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 15: [0.6160544753074646, 0.1404692679643631, 0.47749853134155273, -1.2258670330047607, 1.7250100374221802]\n",
      "Grand sum of 63 tensor sets is: [8.312636375427246, 67.76252746582031, -5.48797082901001, -65.93643951416016, 103.63839721679688]\n",
      "\n",
      "Instance 72 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 20: [0.8306000232696533, 1.5905482769012451, 0.6902852058410645, -0.6776464581489563, 1.4527207612991333]\n",
      "Grand sum of 64 tensor sets is: [9.14323616027832, 69.35307312011719, -4.797685623168945, -66.61408233642578, 105.09111785888672]\n",
      "\n",
      "Instance 73 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 8: [0.7695786952972412, 1.6677597761154175, -1.409134030342102, -0.8242355585098267, 1.5043379068374634]\n",
      "Grand sum of 65 tensor sets is: [9.91281509399414, 71.02083587646484, -6.206819534301758, -67.43831634521484, 106.595458984375]\n",
      "\n",
      "Instance 74 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 21: [-0.18674913048744202, -0.17176184058189392, -0.16208890080451965, -0.9802609086036682, 1.8111200332641602]\n",
      "Grand sum of 66 tensor sets is: [9.726065635681152, 70.84907531738281, -6.368908405303955, -68.4185791015625, 108.40657806396484]\n",
      "\n",
      "Instance 75 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 12: [-0.06622061878442764, 1.0201084613800049, -0.22119823098182678, -0.4958290457725525, 2.8896727561950684]\n",
      "Grand sum of 67 tensor sets is: [9.659845352172852, 71.86918640136719, -6.59010648727417, -68.9144058227539, 111.29624938964844]\n",
      "\n",
      "Instance 76 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "jumping at index 39: [1.3248474597930908, 1.5518373250961304, 0.11071968078613281, 0.9242246747016907, 2.296543836593628]\n",
      "Grand sum of 68 tensor sets is: [10.984692573547363, 73.4210205078125, -6.479386806488037, -67.99018096923828, 113.5927963256836]\n",
      "\n",
      "Instance 77 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 69 tensor sets is: [10.3190336227417, 75.65664672851562, -7.650737762451172, -68.91144561767578, 112.71727752685547]\n",
      "\n",
      "Instance 78 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 8: [-0.8657119870185852, 1.168102502822876, 0.8637903332710266, -0.8752450346946716, 1.0683084726333618]\n",
      "Grand sum of 70 tensor sets is: [9.45332145690918, 76.82475280761719, -6.786947250366211, -69.78668975830078, 113.78558349609375]\n",
      "\n",
      "Instance 79 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 71 tensor sets is: [10.559347152709961, 77.76319122314453, -7.390420436859131, -70.08818054199219, 115.70559692382812]\n",
      "\n",
      "Instance 80 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 24: [1.26104736328125, 2.3113796710968018, -0.2855009436607361, -1.8827667236328125, 1.0226706266403198]\n",
      "Grand sum of 72 tensor sets is: [11.820394515991211, 80.07456970214844, -7.675921440124512, -71.970947265625, 116.72826385498047]\n",
      "\n",
      "Instance 81 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 6: [0.06544824689626694, 1.4440839290618896, 0.6749966740608215, -1.6313462257385254, 2.187748908996582]\n",
      "Grand sum of 73 tensor sets is: [11.885842323303223, 81.5186538696289, -7.000924587249756, -73.602294921875, 118.916015625]\n",
      "\n",
      "Instance 82 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 5: [0.6109642386436462, -0.9422043561935425, 1.4636837244033813, -2.1630308628082275, -1.9515529870986938]\n",
      "Grand sum of 74 tensor sets is: [12.496806144714355, 80.57644653320312, -5.537240982055664, -75.76532745361328, 116.96446228027344]\n",
      "\n",
      "Instance 83 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 20: [0.14338982105255127, 0.45982760190963745, 1.3688515424728394, -0.7747311592102051, 0.704642117023468]\n",
      "Grand sum of 75 tensor sets is: [12.640195846557617, 81.0362777709961, -4.168389320373535, -76.5400619506836, 117.66910552978516]\n",
      "\n",
      "Instance 84 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 9: [1.193150520324707, 1.2042450904846191, -0.31694772839546204, -0.4504854083061218, -1.5143028497695923]\n",
      "Grand sum of 76 tensor sets is: [13.833346366882324, 82.24052429199219, -4.485337257385254, -76.99054718017578, 116.15480041503906]\n",
      "\n",
      "Instance 85 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 86 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 30: [-0.32450634241104126, 0.9223372340202332, -0.19259221851825714, 0.5194743871688843, 1.0734971761703491]\n",
      "Grand sum of 77 tensor sets is: [13.50883960723877, 83.1628646850586, -4.677929401397705, -76.4710693359375, 117.2282943725586]\n",
      "\n",
      "Instance 87 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 8: [0.5004468560218811, 2.5058088302612305, -0.3854568600654602, 2.3078184127807617, -0.7958950996398926]\n",
      "Grand sum of 78 tensor sets is: [14.009286880493164, 85.66867065429688, -5.0633864402771, -74.16325378417969, 116.4323959350586]\n",
      "\n",
      "Instance 88 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 36: [-0.2497486174106598, 0.938951849937439, 1.115768313407898, -3.3475723266601562, 0.16577012836933136]\n",
      "Grand sum of 79 tensor sets is: [13.759538650512695, 86.60762023925781, -3.947618007659912, -77.51082611083984, 116.5981674194336]\n",
      "\n",
      "Instance 89 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 14: [-1.0818276405334473, 0.36672091484069824, 0.04948905110359192, -1.1078600883483887, 1.8771653175354004]\n",
      "Grand sum of 80 tensor sets is: [12.677711486816406, 86.9743423461914, -3.8981289863586426, -78.61868286132812, 118.47533416748047]\n",
      "\n",
      "Instance 90 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 91 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 13: [0.7015203833580017, 0.547173261642456, -0.20832276344299316, -2.0396575927734375, 0.9829097986221313]\n",
      "Grand sum of 81 tensor sets is: [13.379231452941895, 87.52151489257812, -4.106451988220215, -80.65834045410156, 119.45824432373047]\n",
      "\n",
      "Instance 92 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 29: [-0.15594276785850525, 1.9735653400421143, 0.31457170844078064, -2.578415632247925, 1.826525330543518]\n",
      "Grand sum of 82 tensor sets is: [13.223288536071777, 89.49507904052734, -3.7918803691864014, -83.23675537109375, 121.2847671508789]\n",
      "\n",
      "Instance 93 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 6: [-0.2946733236312866, 1.872504472732544, 0.07900216430425644, -1.6809992790222168, -1.0722038745880127]\n",
      "Grand sum of 83 tensor sets is: [12.92861557006836, 91.36758422851562, -3.7128782272338867, -84.91775512695312, 120.21256256103516]\n",
      "\n",
      "Instance 94 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 84 tensor sets is: [14.03464126586914, 92.30602264404297, -4.316351413726807, -85.21924591064453, 122.13257598876953]\n",
      "\n",
      "Instance 95 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "jumping at index 41: [0.43993884325027466, 2.0343072414398193, -0.9357567429542542, -0.7765213251113892, 1.4903416633605957]\n",
      "Grand sum of 85 tensor sets is: [14.474579811096191, 94.34033203125, -5.252108097076416, -85.99576568603516, 123.62291717529297]\n",
      "\n",
      "Instance 96 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 7: [-0.4807931184768677, 0.9681137204170227, -0.3249076008796692, -0.4936544597148895, 2.678513765335083]\n",
      "Grand sum of 86 tensor sets is: [13.993786811828613, 95.3084487915039, -5.5770158767700195, -86.48941802978516, 126.30142974853516]\n",
      "\n",
      "Instance 97 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 24: [0.41840168833732605, 1.2470417022705078, 1.258008360862732, -0.8887654542922974, -1.5346323251724243]\n",
      "Grand sum of 87 tensor sets is: [14.412188529968262, 96.55548858642578, -4.319007396697998, -87.37818145751953, 124.76679992675781]\n",
      "\n",
      "Instance 98 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 37: [0.2727662920951843, 2.694479465484619, 0.2659541368484497, -2.2482542991638184, 1.8691554069519043]\n",
      "Grand sum of 88 tensor sets is: [14.684954643249512, 99.24996948242188, -4.053053379058838, -89.62643432617188, 126.63595581054688]\n",
      "\n",
      "Instance 99 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "jumping at index 50: [-0.32268255949020386, 0.8589609861373901, -0.5832332968711853, 0.40337073802948, 3.015742778778076]\n",
      "Grand sum of 89 tensor sets is: [14.362272262573242, 100.10893249511719, -4.636286735534668, -89.22306060791016, 129.65170288085938]\n",
      "\n",
      "Instance 100 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "jumping at index 11: [-0.6291496753692627, 2.3746650218963623, 0.36614230275154114, 0.6321062445640564, 1.6213245391845703]\n",
      "Grand sum of 90 tensor sets is: [13.733122825622559, 102.48359680175781, -4.270144462585449, -88.59095764160156, 131.2730255126953]\n",
      "\n",
      "Instance 101 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "jumping at index 41: [-0.7579550743103027, 0.7489339709281921, 0.8583462238311768, -1.3837218284606934, 3.9166929721832275]\n",
      "Grand sum of 91 tensor sets is: [12.975168228149414, 103.23252868652344, -3.4117982387542725, -89.97467803955078, 135.18971252441406]\n",
      "\n",
      "Instance 102 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 13: [1.09164297580719, 0.8669785261154175, 0.11258916556835175, -0.6409502625465393, 1.699051856994629]\n",
      "Grand sum of 92 tensor sets is: [14.066811561584473, 104.0995101928711, -3.2992091178894043, -90.61563110351562, 136.88876342773438]\n",
      "\n",
      "Instance 103 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 17: [0.15006336569786072, 1.4632740020751953, 0.288159042596817, -0.3081951141357422, 1.6607553958892822]\n",
      "Grand sum of 93 tensor sets is: [14.216875076293945, 105.56278228759766, -3.01104998588562, -90.923828125, 138.5495147705078]\n",
      "\n",
      "Instance 104 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 24: [-0.0017863381654024124, 2.5134410858154297, 0.10022083669900894, -0.5900221467018127, 2.378202199935913]\n",
      "Grand sum of 94 tensor sets is: [14.215088844299316, 108.07622528076172, -2.9108290672302246, -91.51384735107422, 140.92771911621094]\n",
      "\n",
      "Instance 105 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 3: [-0.32126089930534363, 3.5600180625915527, -0.45368751883506775, -3.2601845264434814, 3.8135921955108643]\n",
      "Grand sum of 95 tensor sets is: [13.893828392028809, 111.63624572753906, -3.364516496658325, -94.77403259277344, 144.74131774902344]\n",
      "\n",
      "Instance 106 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 107 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [69]\n",
      "Size of token embeddings is torch.Size([95, 13, 768])\n",
      "Shape of summed layers is: 95 x 768\n",
      "jumping at index 69: [-0.10371744632720947, 1.198954701423645, -0.4352591633796692, -0.3180001378059387, 1.1706628799438477]\n",
      "Grand sum of 96 tensor sets is: [13.79011058807373, 112.83519744873047, -3.7997756004333496, -95.09203338623047, 145.9119873046875]\n",
      "\n",
      "Instance 108 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 10: [0.0793408453464508, 0.856426477432251, 0.49546563625335693, -0.6121639013290405, 0.7048224210739136]\n",
      "Grand sum of 97 tensor sets is: [13.869451522827148, 113.6916275024414, -3.304309844970703, -95.7042007446289, 146.61680603027344]\n",
      "\n",
      "Instance 109 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [52]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "jumping at index 52: [-0.492106169462204, -0.05630010366439819, -0.7370527386665344, -1.2302906513214111, 3.164332389831543]\n",
      "Grand sum of 98 tensor sets is: [13.377345085144043, 113.63533020019531, -4.041362762451172, -96.93449401855469, 149.78114318847656]\n",
      "\n",
      "Instance 110 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 11: [-0.2800106704235077, 0.8600803017616272, -0.5554116368293762, 0.18709246814250946, 2.4371914863586426]\n",
      "Grand sum of 99 tensor sets is: [13.097334861755371, 114.49540710449219, -4.596774578094482, -96.74739837646484, 152.2183380126953]\n",
      "\n",
      "Instance 111 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 17: [0.16809627413749695, 1.2905778884887695, 0.8419980406761169, -3.889540195465088, 0.6916037201881409]\n",
      "Grand sum of 100 tensor sets is: [13.26543140411377, 115.7859878540039, -3.7547764778137207, -100.6369400024414, 152.90994262695312]\n",
      "\n",
      "Instance 112 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 4: [0.08506937325000763, 0.3036542236804962, -0.17166432738304138, -2.374246120452881, 6.750240325927734]\n",
      "Grand sum of 101 tensor sets is: [13.35050106048584, 116.08964538574219, -3.926440715789795, -103.01118469238281, 159.66018676757812]\n",
      "\n",
      "Instance 113 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 30: [0.3588889539241791, 2.7350821495056152, -0.11414387822151184, 0.9632048606872559, -0.19784778356552124]\n",
      "Grand sum of 102 tensor sets is: [13.709389686584473, 118.8247299194336, -4.040584564208984, -102.04798126220703, 159.46234130859375]\n",
      "\n",
      "Instance 114 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 21: [-0.3690270781517029, 0.6089944839477539, 0.37507274746894836, -1.3476811647415161, 0.4101061224937439]\n",
      "Grand sum of 103 tensor sets is: [13.340362548828125, 119.43372344970703, -3.6655118465423584, -103.39566040039062, 159.87245178222656]\n",
      "\n",
      "Instance 115 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 15: [0.6160544753074646, 0.1404692679643631, 0.47749853134155273, -1.2258670330047607, 1.7250100374221802]\n",
      "Grand sum of 104 tensor sets is: [13.956417083740234, 119.5741958618164, -3.1880133152008057, -104.62152862548828, 161.5974578857422]\n",
      "\n",
      "Instance 116 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 12: [0.3894614577293396, 1.9601532220840454, -0.783705472946167, 0.03516092151403427, 2.5473992824554443]\n",
      "Grand sum of 105 tensor sets is: [14.345878601074219, 121.53434753417969, -3.9717187881469727, -104.58636474609375, 164.1448516845703]\n",
      "\n",
      "Instance 117 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 16: [-0.5653939247131348, 2.123201370239258, -0.0965069979429245, -1.9013254642486572, 3.1319475173950195]\n",
      "Grand sum of 106 tensor sets is: [13.780485153198242, 123.65754699707031, -4.068225860595703, -106.4876937866211, 167.27679443359375]\n",
      "\n",
      "Instance 118 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 11: [-0.4793427586555481, 0.3149203658103943, -0.2583857774734497, -0.5991156101226807, -0.3822079002857208]\n",
      "Grand sum of 107 tensor sets is: [13.301142692565918, 123.97246551513672, -4.326611518859863, -107.08680725097656, 166.89459228515625]\n",
      "\n",
      "Instance 119 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "jumping at index 33: [0.4191218614578247, 1.5188541412353516, 0.290391743183136, -0.006210684776306152, 3.995936870574951]\n",
      "Grand sum of 108 tensor sets is: [13.720264434814453, 125.49131774902344, -4.036219596862793, -107.093017578125, 170.89053344726562]\n",
      "\n",
      "Instance 120 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16, 41]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "jumping at index 16: [0.583098828792572, 0.32475197315216064, -0.438927561044693, -1.4328900575637817, 2.1097593307495117]\n",
      "jumping at index 41: [0.44500699639320374, 0.25617456436157227, -0.4467530846595764, -1.7014697790145874, 3.5690412521362305]\n",
      "Grand sum of 109 tensor sets is: [14.234317779541016, 125.78178405761719, -4.479060173034668, -108.66019439697266, 173.7299346923828]\n",
      "\n",
      "Instance 121 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 16: [-0.5514659881591797, 2.123021364212036, -0.8042794466018677, -0.6271752119064331, 0.2717481255531311]\n",
      "Grand sum of 110 tensor sets is: [13.682851791381836, 127.9048080444336, -5.283339500427246, -109.28736877441406, 174.00167846679688]\n",
      "\n",
      "Instance 122 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 123 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "jumping at index 27: [-0.15651367604732513, 0.9097501039505005, -0.05328729748725891, 0.03974585980176926, -0.04501986503601074]\n",
      "Grand sum of 111 tensor sets is: [13.526338577270508, 128.81455993652344, -5.336627006530762, -109.24761962890625, 173.9566650390625]\n",
      "\n",
      "Instance 124 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 14: [0.33089345693588257, 0.20039021968841553, -0.3313356637954712, 0.8254762887954712, 0.18893074989318848]\n",
      "Grand sum of 112 tensor sets is: [13.857232093811035, 129.01495361328125, -5.667962551116943, -108.4221420288086, 174.14559936523438]\n",
      "\n",
      "Instance 125 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "jumping at index 13: [0.19530701637268066, -0.2875779867172241, 0.30430787801742554, -0.6332203149795532, 1.6073148250579834]\n",
      "Grand sum of 113 tensor sets is: [14.052538871765137, 128.7273712158203, -5.363654613494873, -109.05535888671875, 175.75291442871094]\n",
      "\n",
      "Instance 126 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "jumping at index 29: [0.5149160623550415, 1.8001139163970947, 0.07961207628250122, 0.5757966041564941, -1.2400494813919067]\n",
      "Grand sum of 114 tensor sets is: [14.567455291748047, 130.52748107910156, -5.2840423583984375, -108.47956085205078, 174.5128631591797]\n",
      "\n",
      "Instance 127 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "jumping at index 40: [-0.4805834889411926, 2.146409034729004, -0.5749302506446838, -2.506399393081665, 4.864457130432129]\n",
      "Grand sum of 115 tensor sets is: [14.086872100830078, 132.67388916015625, -5.858972549438477, -110.9859619140625, 179.3773193359375]\n",
      "\n",
      "Instance 128 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 16: [1.1507742404937744, 3.1085755825042725, -0.7731150984764099, 2.437532424926758, -1.6698050498962402]\n",
      "Grand sum of 116 tensor sets is: [15.237646102905273, 135.782470703125, -6.632087707519531, -108.54843139648438, 177.70751953125]\n",
      "\n",
      "Instance 129 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 24: [1.1416395902633667, 2.1582112312316895, -0.24287886917591095, -2.0771872997283936, 0.8757995367050171]\n",
      "Grand sum of 117 tensor sets is: [16.37928581237793, 137.94068908691406, -6.874966621398926, -110.62561798095703, 178.58331298828125]\n",
      "\n",
      "Instance 130 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 118 tensor sets is: [15.713626861572266, 140.1763153076172, -8.046317100524902, -111.54688262939453, 177.70779418945312]\n",
      "\n",
      "Instance 131 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 4: [0.29690825939178467, 1.0628178119659424, -2.2467432022094727, 1.024605393409729, 1.1743786334991455]\n",
      "Grand sum of 119 tensor sets is: [16.010534286499023, 141.2391357421875, -10.293060302734375, -110.52227783203125, 178.88217163085938]\n",
      "\n",
      "Instance 132 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 133 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [1.1660417318344116, -0.32877299189567566, -0.41697362065315247, -1.8962494134902954, 1.8680942058563232]\n",
      "Grand sum of 120 tensor sets is: [17.176576614379883, 140.91036987304688, -10.710034370422363, -112.41852569580078, 180.75025939941406]\n",
      "\n",
      "Instance 134 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 10: [-0.5549705624580383, 0.9383563995361328, -1.0433672666549683, -1.9000530242919922, 3.4019625186920166]\n",
      "Grand sum of 121 tensor sets is: [16.621606826782227, 141.84872436523438, -11.753401756286621, -114.3185806274414, 184.1522216796875]\n",
      "\n",
      "Instance 135 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 6: [0.22928059101104736, 0.793168306350708, 0.6741667985916138, 0.4814719557762146, 0.16463333368301392]\n",
      "Grand sum of 122 tensor sets is: [16.850887298583984, 142.6418914794922, -11.079235076904297, -113.83711242675781, 184.3168487548828]\n",
      "\n",
      "Instance 136 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "jumping at index 35: [-0.09366627037525177, -0.00831650197505951, 1.7814502716064453, 0.15884782373905182, 0.01697629690170288]\n",
      "Grand sum of 123 tensor sets is: [16.757221221923828, 142.63357543945312, -9.297784805297852, -113.67826080322266, 184.33383178710938]\n",
      "\n",
      "Instance 137 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 10: [-0.5549705624580383, 0.9383563995361328, -1.0433672666549683, -1.9000530242919922, 3.4019625186920166]\n",
      "Grand sum of 124 tensor sets is: [16.202251434326172, 143.57192993164062, -10.34115219116211, -115.57831573486328, 187.7357940673828]\n",
      "\n",
      "Instance 138 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 139 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 125 tensor sets is: [17.308277130126953, 144.5103759765625, -10.944625854492188, -115.87980651855469, 189.6558074951172]\n",
      "\n",
      "Instance 140 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "jumping at index 6: [-1.1854922771453857, 0.9047653675079346, -0.45881813764572144, -0.9964531660079956, 0.7876385450363159]\n",
      "Grand sum of 126 tensor sets is: [16.122785568237305, 145.41514587402344, -11.403444290161133, -116.87625885009766, 190.44345092773438]\n",
      "\n",
      "Instance 141 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 10: [-0.2393244057893753, 1.1680576801300049, 0.7454730272293091, -2.577298402786255, 3.0318217277526855]\n",
      "Grand sum of 127 tensor sets is: [15.883460998535156, 146.5832061767578, -10.657971382141113, -119.45355987548828, 193.4752655029297]\n",
      "\n",
      "Instance 142 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 6: [0.09963464736938477, 1.1052923202514648, 0.516441822052002, 0.053052403032779694, 0.8247783184051514]\n",
      "Grand sum of 128 tensor sets is: [15.983095169067383, 147.68849182128906, -10.141529083251953, -119.40050506591797, 194.300048828125]\n",
      "\n",
      "Instance 143 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 144 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 129 tensor sets is: [17.089120864868164, 148.62693786621094, -10.745002746582031, -119.70199584960938, 196.22006225585938]\n",
      "\n",
      "Instance 145 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 15: [-0.7591613531112671, 2.2543773651123047, 0.1942283660173416, 0.12496408820152283, 0.23771819472312927]\n",
      "Grand sum of 130 tensor sets is: [16.329959869384766, 150.88131713867188, -10.550774574279785, -119.57703399658203, 196.45777893066406]\n",
      "\n",
      "Instance 146 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 147 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 3: [0.8727676868438721, 2.2045843601226807, -0.22009031474590302, 1.7267429828643799, 0.1501975655555725]\n",
      "Grand sum of 131 tensor sets is: [17.202728271484375, 153.08590698242188, -10.770864486694336, -117.85028839111328, 196.60797119140625]\n",
      "\n",
      "Instance 148 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "jumping at index 33: [0.9813221096992493, 0.4732794761657715, -0.8393315076828003, -0.016215525567531586, 2.882002353668213]\n",
      "Grand sum of 132 tensor sets is: [18.184049606323242, 153.55918884277344, -11.610196113586426, -117.86650085449219, 199.48997497558594]\n",
      "\n",
      "Instance 149 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 2: [0.9722787141799927, 2.9546611309051514, -0.6150832772254944, 0.039239875972270966, 1.5624899864196777]\n",
      "Grand sum of 133 tensor sets is: [19.156328201293945, 156.51385498046875, -12.225279808044434, -117.82726287841797, 201.05245971679688]\n",
      "\n",
      "Instance 150 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 4: [-0.5077928900718689, 1.8150602579116821, -0.7251145839691162, -3.824812650680542, 0.024647340178489685]\n",
      "Grand sum of 134 tensor sets is: [18.648534774780273, 158.32891845703125, -12.950394630432129, -121.6520767211914, 201.0771026611328]\n",
      "\n",
      "Instance 151 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "jumping at index 31: [0.27980518341064453, 0.5203657150268555, 1.0192334651947021, -2.2588305473327637, 2.6327223777770996]\n",
      "Grand sum of 135 tensor sets is: [18.928340911865234, 158.8492889404297, -11.931160926818848, -123.91090393066406, 203.70982360839844]\n",
      "\n",
      "Instance 152 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 25: [-0.5749988555908203, 0.96495521068573, -0.027700988575816154, -1.1943520307540894, 0.4515261948108673]\n",
      "Grand sum of 136 tensor sets is: [18.353342056274414, 159.81423950195312, -11.9588623046875, -125.10525512695312, 204.16134643554688]\n",
      "\n",
      "Instance 153 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 10: [0.29077041149139404, 1.5912235975265503, -0.5000864863395691, 1.153328537940979, 1.970700740814209]\n",
      "Grand sum of 137 tensor sets is: [18.64411163330078, 161.40545654296875, -12.458949089050293, -123.9519271850586, 206.13204956054688]\n",
      "\n",
      "Instance 154 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "jumping at index 6: [1.7468879222869873, 1.4604244232177734, 0.4305437207221985, 0.5053858757019043, -0.4705253839492798]\n",
      "Grand sum of 138 tensor sets is: [20.39099884033203, 162.86587524414062, -12.02840518951416, -123.44654083251953, 205.66152954101562]\n",
      "\n",
      "Instance 155 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 38: [-0.472095787525177, 0.9956002235412598, -0.061650052666664124, -1.0481332540512085, 2.2737977504730225]\n",
      "Grand sum of 139 tensor sets is: [19.918903350830078, 163.86148071289062, -12.090055465698242, -124.49467468261719, 207.93533325195312]\n",
      "\n",
      "Instance 156 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "jumping at index 2: [0.2025507539510727, 0.8241167664527893, -0.10931092500686646, -0.620173454284668, 3.3327476978302]\n",
      "Grand sum of 140 tensor sets is: [20.1214542388916, 164.6855926513672, -12.199366569519043, -125.1148452758789, 211.26808166503906]\n",
      "\n",
      "Instance 157 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 21: [-0.2076295018196106, 1.723698377609253, 0.8697625994682312, -0.10409270972013474, -0.32757091522216797]\n",
      "Grand sum of 141 tensor sets is: [19.9138240814209, 166.40928649902344, -11.329604148864746, -125.21894073486328, 210.9405059814453]\n",
      "\n",
      "Instance 158 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [1]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 1: [-0.2980111241340637, -0.9807189106941223, -1.7440299987792969, 0.237186461687088, 1.7595906257629395]\n",
      "Grand sum of 142 tensor sets is: [19.615812301635742, 165.42857360839844, -13.073634147644043, -124.98175048828125, 212.70010375976562]\n",
      "\n",
      "Instance 159 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 23: [0.9030454754829407, 1.438037633895874, -0.5462108254432678, 2.137892723083496, 0.9635491967201233]\n",
      "Grand sum of 143 tensor sets is: [20.518857955932617, 166.86660766601562, -13.619845390319824, -122.84385681152344, 213.6636505126953]\n",
      "\n",
      "Instance 160 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 12: [0.17282016575336456, 2.0164871215820312, -0.09337562322616577, -1.824743628501892, 0.47685542702674866]\n",
      "Grand sum of 144 tensor sets is: [20.691679000854492, 168.88308715820312, -13.713220596313477, -124.6686019897461, 214.1405029296875]\n",
      "\n",
      "Instance 161 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 29: [0.09273163229227066, 2.736220598220825, -0.46155858039855957, 1.9286001920700073, -0.46970024704933167]\n",
      "Grand sum of 145 tensor sets is: [20.78441047668457, 171.6193084716797, -14.174778938293457, -122.74000549316406, 213.67080688476562]\n",
      "\n",
      "Instance 162 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 163 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [61]\n",
      "Size of token embeddings is torch.Size([89, 13, 768])\n",
      "Shape of summed layers is: 89 x 768\n",
      "jumping at index 61: [-0.25091132521629333, 1.3592171669006348, -0.7771826982498169, -1.079670786857605, 1.1261017322540283]\n",
      "Grand sum of 146 tensor sets is: [20.533498764038086, 172.97853088378906, -14.951961517333984, -123.8196792602539, 214.79690551757812]\n",
      "\n",
      "Instance 164 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 10: [-0.032793257385492325, 0.17198500037193298, -1.1545881032943726, -0.16980856657028198, 3.8582303524017334]\n",
      "Grand sum of 147 tensor sets is: [20.50070571899414, 173.1505126953125, -16.106550216674805, -123.98948669433594, 218.65513610839844]\n",
      "\n",
      "Instance 165 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "jumping at index 34: [-0.21404029428958893, 1.5169827938079834, -0.8941611051559448, -1.193468451499939, 3.677826404571533]\n",
      "Grand sum of 148 tensor sets is: [20.286664962768555, 174.66749572753906, -17.00071144104004, -125.18295288085938, 222.3329620361328]\n",
      "\n",
      "Instance 166 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "jumping at index 4: [-0.947573721408844, 0.18246500194072723, 0.6035608649253845, 0.015035703778266907, 0.7032198309898376]\n",
      "Grand sum of 149 tensor sets is: [19.33909034729004, 174.84996032714844, -16.39715003967285, -125.16791534423828, 223.0361785888672]\n",
      "\n",
      "Instance 167 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 150 tensor sets is: [19.410011291503906, 177.11819458007812, -17.064319610595703, -125.02217102050781, 222.8997039794922]\n",
      "\n",
      "Instance 168 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 23: [0.8369346857070923, 1.6976287364959717, -0.4858229160308838, 1.1681506633758545, 1.2212140560150146]\n",
      "Grand sum of 151 tensor sets is: [20.246946334838867, 178.81582641601562, -17.550142288208008, -123.85401916503906, 224.12091064453125]\n",
      "\n",
      "Instance 169 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 21: [-0.5478537082672119, 2.0627315044403076, -0.2121724933385849, -0.1981753706932068, -1.1838682889938354]\n",
      "Grand sum of 152 tensor sets is: [19.699092864990234, 180.87855529785156, -17.762313842773438, -124.05219268798828, 222.93704223632812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 170 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22, 368]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 22: [-0.20570963621139526, 2.138561964035034, 1.0356311798095703, -1.6269596815109253, 3.897801399230957]\n",
      "jumping at index 368: [-0.39903581142425537, 2.2914578914642334, 0.5616462230682373, -0.8992323279380798, 5.037720203399658]\n",
      "Grand sum of 153 tensor sets is: [19.39672088623047, 183.09356689453125, -16.963674545288086, -125.31529235839844, 227.40480041503906]\n",
      "\n",
      "Instance 171 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 9: [0.8075505495071411, 1.7592334747314453, -0.18597471714019775, 0.8553537726402283, 1.677126407623291]\n",
      "Grand sum of 154 tensor sets is: [20.20427131652832, 184.85279846191406, -17.149648666381836, -124.4599380493164, 229.08192443847656]\n",
      "\n",
      "Instance 172 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 22: [-0.5682231187820435, -0.27498918771743774, 0.8935573101043701, -0.5903955698013306, -0.4039151966571808]\n",
      "Grand sum of 155 tensor sets is: [19.63604736328125, 184.5778045654297, -16.256092071533203, -125.05033111572266, 228.67800903320312]\n",
      "\n",
      "Instance 173 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 22: [-0.5682231187820435, -0.27498918771743774, 0.8935573101043701, -0.5903955698013306, -0.4039151966571808]\n",
      "Grand sum of 156 tensor sets is: [19.06782341003418, 184.3028106689453, -15.362534523010254, -125.6407241821289, 228.2740936279297]\n",
      "\n",
      "Instance 174 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 20: [0.8306000232696533, 1.5905482769012451, 0.6902852058410645, -0.6776464581489563, 1.4527207612991333]\n",
      "Grand sum of 157 tensor sets is: [19.89842414855957, 185.8933563232422, -14.672248840332031, -126.31836700439453, 229.72682189941406]\n",
      "\n",
      "Instance 175 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 4: [0.03523094207048416, 1.9300131797790527, 1.0302810668945312, -0.08661799132823944, 3.2672150135040283]\n",
      "Grand sum of 158 tensor sets is: [19.93365478515625, 187.8233642578125, -13.6419677734375, -126.40498352050781, 232.99403381347656]\n",
      "\n",
      "Instance 176 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 177 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 178 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 15: [0.20820140838623047, 1.4047718048095703, -1.4325467348098755, -0.8144528865814209, -0.4482024013996124]\n",
      "Grand sum of 159 tensor sets is: [20.141857147216797, 189.22813415527344, -15.074514389038086, -127.21943664550781, 232.54583740234375]\n",
      "\n",
      "Instance 179 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 12: [0.42048171162605286, 1.082655906677246, -0.6899277567863464, -0.9414874911308289, 3.3672358989715576]\n",
      "Grand sum of 160 tensor sets is: [20.562339782714844, 190.310791015625, -15.764442443847656, -128.16091918945312, 235.91307067871094]\n",
      "\n",
      "Instance 180 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 31: [-0.13343891501426697, 1.2713137865066528, 1.0668705701828003, -2.5192129611968994, 1.9799137115478516]\n",
      "Grand sum of 161 tensor sets is: [20.42890167236328, 191.5821075439453, -14.697571754455566, -130.6801300048828, 237.8929901123047]\n",
      "\n",
      "Instance 181 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 21: [-0.3690270781517029, 0.6089944839477539, 0.37507274746894836, -1.3476811647415161, 0.4101061224937439]\n",
      "Grand sum of 162 tensor sets is: [20.05987548828125, 192.19110107421875, -14.32249927520752, -132.02781677246094, 238.3031005859375]\n",
      "\n",
      "Instance 182 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 24: [0.6015436053276062, 0.8735209703445435, -0.01144581288099289, -2.791327476501465, 3.745861053466797]\n",
      "Grand sum of 163 tensor sets is: [20.661418914794922, 193.0646209716797, -14.333945274353027, -134.8191375732422, 242.04896545410156]\n",
      "\n",
      "Instance 183 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 25: [0.11807628720998764, 2.2863895893096924, -0.23081083595752716, -1.7752939462661743, 1.3847893476486206]\n",
      "Grand sum of 164 tensor sets is: [20.779495239257812, 195.35101318359375, -14.564756393432617, -136.5944366455078, 243.4337615966797]\n",
      "\n",
      "Instance 184 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 185 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 186 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 14: [0.13062319159507751, 1.3569992780685425, 0.36972522735595703, 0.8724042177200317, 1.0764461755752563]\n",
      "Grand sum of 165 tensor sets is: [20.910118103027344, 196.7080078125, -14.19503116607666, -135.72203063964844, 244.5102081298828]\n",
      "\n",
      "Instance 187 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 3: [0.5362332463264465, 0.6914659738540649, 0.35454288125038147, -2.4041213989257812, 2.570303440093994]\n",
      "Grand sum of 166 tensor sets is: [21.446352005004883, 197.39947509765625, -13.84048843383789, -138.12615966796875, 247.08050537109375]\n",
      "\n",
      "Instance 188 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 189 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "jumping at index 4: [-0.46443885564804077, -0.15741363167762756, 0.09064899384975433, -1.4744569063186646, 2.586580276489258]\n",
      "Grand sum of 167 tensor sets is: [20.98191261291504, 197.2420654296875, -13.749839782714844, -139.60061645507812, 249.66708374023438]\n",
      "\n",
      "Instance 190 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 191 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 4: [-0.3150598108768463, 1.4955193996429443, -0.3685046434402466, -0.11961132287979126, 1.5442783832550049]\n",
      "Grand sum of 168 tensor sets is: [20.666852951049805, 198.73757934570312, -14.1183443069458, -139.72023010253906, 251.21136474609375]\n",
      "\n",
      "Instance 192 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 30: [-0.8231475353240967, 0.20416110754013062, -0.8184887170791626, -1.1787834167480469, 4.866078853607178]\n",
      "Grand sum of 169 tensor sets is: [19.843706130981445, 198.94174194335938, -14.936833381652832, -140.89901733398438, 256.07745361328125]\n",
      "\n",
      "Instance 193 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 31: [0.7308530807495117, 1.446509599685669, 0.9715222120285034, -0.756840705871582, 1.6107447147369385]\n",
      "Grand sum of 170 tensor sets is: [20.57455825805664, 200.38824462890625, -13.965311050415039, -141.65585327148438, 257.6882019042969]\n",
      "\n",
      "Instance 194 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 26: [-1.3388234376907349, 0.8144710659980774, -0.8507291078567505, -1.6241178512573242, 1.2128567695617676]\n",
      "Grand sum of 171 tensor sets is: [19.235734939575195, 201.2027130126953, -14.8160400390625, -143.27996826171875, 258.90106201171875]\n",
      "\n",
      "Instance 195 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "jumping at index 38: [-0.6348196268081665, 1.5799990892410278, -0.6289281845092773, -2.0646324157714844, 2.4352383613586426]\n",
      "Grand sum of 172 tensor sets is: [18.600915908813477, 202.78271484375, -15.444968223571777, -145.3446044921875, 261.3363037109375]\n",
      "\n",
      "Instance 196 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 15: [-0.6905327439308167, 0.829268217086792, 0.014935581013560295, -0.7797529697418213, 0.029058843851089478]\n",
      "Grand sum of 173 tensor sets is: [17.910383224487305, 203.6119842529297, -15.430032730102539, -146.12435913085938, 261.3653564453125]\n",
      "\n",
      "Instance 197 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 198 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 6: [-0.48968973755836487, 0.06409335881471634, -0.4988228976726532, -0.9576442241668701, 1.705112099647522]\n",
      "Grand sum of 174 tensor sets is: [17.42069435119629, 203.6760711669922, -15.928855895996094, -147.08200073242188, 263.0704650878906]\n",
      "\n",
      "Instance 199 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 175 tensor sets is: [18.52672004699707, 204.61451721191406, -16.532329559326172, -147.3834991455078, 264.990478515625]\n",
      "\n",
      "Instance 200 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [-0.7685133814811707, 3.383265495300293, -0.3741854727268219, -0.3176172971725464, 1.4229514598846436]\n",
      "Grand sum of 176 tensor sets is: [17.758207321166992, 207.99778747558594, -16.90651512145996, -147.70111083984375, 266.4134216308594]\n",
      "\n",
      "Instance 201 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 4: [1.4469467401504517, 0.37802958488464355, -0.10234155505895615, -1.119220495223999, 0.4996023178100586]\n",
      "Grand sum of 177 tensor sets is: [19.205154418945312, 208.37582397460938, -17.00885581970215, -148.82032775878906, 266.91302490234375]\n",
      "\n",
      "Instance 202 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 8: [0.244048610329628, 1.8777669668197632, 0.8131482601165771, -1.7628785371780396, 1.1625949144363403]\n",
      "Grand sum of 178 tensor sets is: [19.449203491210938, 210.2535858154297, -16.195707321166992, -150.5832061767578, 268.07562255859375]\n",
      "\n",
      "Instance 203 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 12: [1.0125645399093628, 2.7222745418548584, -0.14607128500938416, 1.3489975929260254, 0.9897739887237549]\n",
      "Grand sum of 179 tensor sets is: [20.461767196655273, 212.97586059570312, -16.341777801513672, -149.2342071533203, 269.0653991699219]\n",
      "\n",
      "Instance 204 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "jumping at index 16: [-0.2525404691696167, 1.289644479751587, 1.2085336446762085, 0.7401221990585327, -1.3150646686553955]\n",
      "Grand sum of 180 tensor sets is: [20.209226608276367, 214.2655029296875, -15.133244514465332, -148.49407958984375, 267.7503356933594]\n",
      "\n",
      "Instance 205 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 8: [0.21189865469932556, 1.164669156074524, -1.1851407289505005, -1.978887677192688, 0.3412487804889679]\n",
      "Grand sum of 181 tensor sets is: [20.421125411987305, 215.43017578125, -16.31838607788086, -150.47296142578125, 268.0915832519531]\n",
      "\n",
      "Instance 206 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 26: [-0.017404578626155853, 0.19761568307876587, -0.024275988340377808, 0.2726829946041107, -1.6054202318191528]\n",
      "Grand sum of 182 tensor sets is: [20.40372085571289, 215.62779235839844, -16.342662811279297, -150.2002716064453, 266.4861755371094]\n",
      "\n",
      "Instance 207 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "jumping at index 47: [-1.2253904342651367, 1.5305206775665283, -1.2060036659240723, 0.4314015507698059, -1.129608392715454]\n",
      "Grand sum of 183 tensor sets is: [19.178329467773438, 217.15830993652344, -17.54866600036621, -149.7688751220703, 265.3565673828125]\n",
      "\n",
      "Instance 208 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 11: [-0.1723564863204956, 2.025228977203369, 0.6171449422836304, -0.44516146183013916, -0.21314875781536102]\n",
      "Grand sum of 184 tensor sets is: [19.00597381591797, 219.18353271484375, -16.931520462036133, -150.2140350341797, 265.1434326171875]\n",
      "\n",
      "Instance 209 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 25: [0.1843196153640747, 0.7985811233520508, 1.223022699356079, -2.914849281311035, 1.937854528427124]\n",
      "Grand sum of 185 tensor sets is: [19.19029426574707, 219.98211669921875, -15.708498001098633, -153.12889099121094, 267.081298828125]\n",
      "\n",
      "Instance 210 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 42: [-0.18891483545303345, 1.7175328731536865, -0.33852165937423706, -0.4984990358352661, 6.681241035461426]\n",
      "Grand sum of 186 tensor sets is: [19.001379013061523, 221.69964599609375, -16.047019958496094, -153.6273956298828, 273.7625427246094]\n",
      "\n",
      "Instance 211 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 8: [-0.8032883405685425, 0.009353488683700562, 0.31805500388145447, -0.8119117021560669, 3.112666368484497]\n",
      "Grand sum of 187 tensor sets is: [18.198091506958008, 221.70899963378906, -15.728964805603027, -154.43930053710938, 276.8752136230469]\n",
      "\n",
      "Instance 212 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 15: [-0.7233967781066895, 1.0848630666732788, -0.9290522336959839, -0.8288624882698059, -0.1333564966917038]\n",
      "Grand sum of 188 tensor sets is: [17.474695205688477, 222.7938690185547, -16.658016204833984, -155.26815795898438, 276.7418518066406]\n",
      "\n",
      "Instance 213 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 6: [-0.052176423370838165, 0.7891625165939331, 0.42298653721809387, 1.3353652954101562, 0.07889989018440247]\n",
      "Grand sum of 189 tensor sets is: [17.42251968383789, 223.58303833007812, -16.235029220581055, -153.93280029296875, 276.82073974609375]\n",
      "\n",
      "Instance 214 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 19: [0.1014455109834671, 0.3237786591053009, -0.09773789346218109, 0.16271919012069702, 3.8811216354370117]\n",
      "Grand sum of 190 tensor sets is: [17.52396583557129, 223.9068145751953, -16.332767486572266, -153.77008056640625, 280.7018737792969]\n",
      "\n",
      "Instance 215 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "jumping at index 7: [0.7762936949729919, 1.3862603902816772, -1.1954485177993774, -3.3069493770599365, 3.418936252593994]\n",
      "Grand sum of 191 tensor sets is: [18.30025863647461, 225.29307556152344, -17.528215408325195, -157.0770263671875, 284.1208190917969]\n",
      "\n",
      "Instance 216 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 10: [-0.20740783214569092, 1.5748913288116455, -0.4444296658039093, -0.6041855812072754, 1.0489037036895752]\n",
      "Grand sum of 192 tensor sets is: [18.092851638793945, 226.8679656982422, -17.972644805908203, -157.68121337890625, 285.1697082519531]\n",
      "\n",
      "Instance 217 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 218 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([510, 13, 768])\n",
      "Shape of summed layers is: 510 x 768\n",
      "jumping at index 179: [0.017274722456932068, -1.2504990100860596, 0.0063507407903671265, 0.6808996796607971, 2.0948965549468994]\n",
      "Grand sum of 193 tensor sets is: [18.110126495361328, 225.61746215820312, -17.966293334960938, -157.0003204345703, 287.2646179199219]\n",
      "\n",
      "Instance 219 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 220 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "jumping at index 9: [1.4413654804229736, 1.2944469451904297, -0.4670338034629822, 0.06064808368682861, 0.14609447121620178]\n",
      "Grand sum of 194 tensor sets is: [19.55149269104004, 226.9119110107422, -18.433326721191406, -156.93966674804688, 287.41070556640625]\n",
      "\n",
      "Instance 221 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 20: [-0.4420677423477173, 0.9316521883010864, -1.4566571712493896, -1.2003214359283447, 1.872593879699707]\n",
      "Grand sum of 195 tensor sets is: [19.109424591064453, 227.84356689453125, -19.889984130859375, -158.13998413085938, 289.2832946777344]\n",
      "\n",
      "Instance 222 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 223 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 10: [0.25226566195487976, 2.1739721298217773, -1.3589003086090088, -0.6962292194366455, -0.00540691614151001]\n",
      "Grand sum of 196 tensor sets is: [19.361690521240234, 230.0175323486328, -21.248884201049805, -158.83621215820312, 289.27789306640625]\n",
      "\n",
      "Instance 224 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 12: [0.40518349409103394, 2.0303120613098145, 0.7397937178611755, -2.0889804363250732, 6.030000686645508]\n",
      "Grand sum of 197 tensor sets is: [19.766874313354492, 232.0478515625, -20.509090423583984, -160.92518615722656, 295.3078918457031]\n",
      "\n",
      "Instance 225 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 19: [0.18739143013954163, 0.8278087973594666, -0.5879422426223755, -0.7855809330940247, 0.29398414492607117]\n",
      "Grand sum of 198 tensor sets is: [19.954265594482422, 232.8756561279297, -21.09703254699707, -161.7107696533203, 295.60186767578125]\n",
      "\n",
      "Instance 226 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 13: [-1.0220251083374023, 1.9174655675888062, -0.01546846330165863, -1.6250168085098267, 1.83118736743927]\n",
      "Grand sum of 199 tensor sets is: [18.932239532470703, 234.79312133789062, -21.11250114440918, -163.33578491210938, 297.43304443359375]\n",
      "\n",
      "Instance 227 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 5: [0.38425564765930176, 0.8588300943374634, -0.6681759357452393, 0.9828866124153137, 0.9002067446708679]\n",
      "Grand sum of 200 tensor sets is: [19.316495895385742, 235.65194702148438, -21.780677795410156, -162.3529052734375, 298.333251953125]\n",
      "\n",
      "Instance 228 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 229 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 8: [1.3289868831634521, 1.8876534700393677, -0.2457594871520996, -1.1900070905685425, 3.1660828590393066]\n",
      "Grand sum of 201 tensor sets is: [20.645483016967773, 237.5395965576172, -22.026437759399414, -163.54290771484375, 301.49932861328125]\n",
      "\n",
      "Instance 230 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "jumping at index 44: [0.7090746760368347, -0.3626098334789276, 0.5395419001579285, -2.183037519454956, 0.8457897901535034]\n",
      "Grand sum of 202 tensor sets is: [21.354557037353516, 237.17698669433594, -21.486896514892578, -165.7259521484375, 302.3451232910156]\n",
      "\n",
      "Instance 231 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 12: [-0.29102325439453125, -0.501024603843689, 0.19791598618030548, -2.0410029888153076, 0.9708582758903503]\n",
      "Grand sum of 203 tensor sets is: [21.063533782958984, 236.67596435546875, -21.28898048400879, -167.76695251464844, 303.31597900390625]\n",
      "\n",
      "Instance 232 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "jumping at index 9: [0.72926265001297, 0.4988693296909332, 1.0930073261260986, -2.351012706756592, 5.7154693603515625]\n",
      "Grand sum of 204 tensor sets is: [21.792797088623047, 237.17483520507812, -20.195972442626953, -170.1179656982422, 309.03143310546875]\n",
      "\n",
      "Instance 233 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "jumping at index 49: [-0.7190667986869812, 1.0419305562973022, 0.4722580015659332, -2.099818229675293, 4.598810195922852]\n",
      "Grand sum of 205 tensor sets is: [21.07373046875, 238.21676635742188, -19.72371482849121, -172.21778869628906, 313.6302490234375]\n",
      "\n",
      "Instance 234 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 235 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 17: [0.57061368227005, -0.05347082018852234, 0.21368345618247986, -1.5818334817886353, -1.3103513717651367]\n",
      "Grand sum of 206 tensor sets is: [21.644344329833984, 238.16329956054688, -19.51003074645996, -173.79962158203125, 312.31988525390625]\n",
      "\n",
      "Instance 236 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 7: [-0.5774355530738831, 1.6515085697174072, -0.3186616003513336, -1.1020933389663696, 2.196591377258301]\n",
      "Grand sum of 207 tensor sets is: [21.06690788269043, 239.81480407714844, -19.828691482543945, -174.90171813964844, 314.5164794921875]\n",
      "\n",
      "Instance 237 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 15: [-0.9725984334945679, 2.3438472747802734, 0.7162073850631714, 0.9592764377593994, 0.4654974937438965]\n",
      "Grand sum of 208 tensor sets is: [20.094308853149414, 242.1586456298828, -19.112483978271484, -173.94244384765625, 314.9819641113281]\n",
      "\n",
      "Instance 238 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 209 tensor sets is: [21.200334548950195, 243.0970916748047, -19.715957641601562, -174.2439422607422, 316.9019775390625]\n",
      "\n",
      "Instance 239 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 18: [-0.0036564739421010017, 1.7429933547973633, -0.4588537812232971, 1.4890605211257935, 1.9569238424301147]\n",
      "Grand sum of 210 tensor sets is: [21.196678161621094, 244.840087890625, -20.17481231689453, -172.7548828125, 318.85888671875]\n",
      "\n",
      "Instance 240 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 14: [0.2739541232585907, 1.3222277164459229, 1.7527332305908203, -2.8373758792877197, 2.0119850635528564]\n",
      "Grand sum of 211 tensor sets is: [21.470632553100586, 246.16232299804688, -18.42207908630371, -175.59225463867188, 320.8708801269531]\n",
      "\n",
      "Instance 241 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 5: [0.865239143371582, 1.8325697183609009, -1.0863181352615356, 1.0887845754623413, 3.883211135864258]\n",
      "Grand sum of 212 tensor sets is: [22.335872650146484, 247.99488830566406, -19.508398056030273, -174.5034637451172, 324.75408935546875]\n",
      "\n",
      "Instance 242 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 19: [-0.6447800993919373, 1.8102850914001465, -0.8399112820625305, 0.09740263223648071, 3.1582157611846924]\n",
      "Grand sum of 213 tensor sets is: [21.69109344482422, 249.80517578125, -20.348308563232422, -174.40606689453125, 327.91229248046875]\n",
      "\n",
      "Instance 243 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 37: [0.3139370381832123, 1.38559091091156, -0.7730951905250549, -0.4732200503349304, 0.7572450637817383]\n",
      "Grand sum of 214 tensor sets is: [22.005029678344727, 251.19076538085938, -21.12140464782715, -174.87928771972656, 328.6695251464844]\n",
      "\n",
      "Instance 244 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 15: [0.6160544753074646, 0.1404692679643631, 0.47749853134155273, -1.2258670330047607, 1.7250100374221802]\n",
      "Grand sum of 215 tensor sets is: [22.621084213256836, 251.33123779296875, -20.643905639648438, -176.1051483154297, 330.39453125]\n",
      "\n",
      "Instance 245 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 18: [0.4491266906261444, 1.9792677164077759, -0.6818959712982178, -0.5187094211578369, -0.17665934562683105]\n",
      "Grand sum of 216 tensor sets is: [23.07021141052246, 253.3105010986328, -21.325801849365234, -176.6238555908203, 330.2178649902344]\n",
      "\n",
      "Instance 246 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 15: [0.6160544753074646, 0.1404692679643631, 0.47749853134155273, -1.2258670330047607, 1.7250100374221802]\n",
      "Grand sum of 217 tensor sets is: [23.68626594543457, 253.4509735107422, -20.848302841186523, -177.84971618652344, 331.94287109375]\n",
      "\n",
      "Instance 247 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 8: [-0.7408187985420227, 0.8162459135055542, -0.15173017978668213, 1.1693453788757324, 1.063128113746643]\n",
      "Grand sum of 218 tensor sets is: [22.94544792175293, 254.2672119140625, -21.000032424926758, -176.6803741455078, 333.0060119628906]\n",
      "\n",
      "Instance 248 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 11: [0.5511431097984314, 1.725875973701477, 0.4837196171283722, -0.08816338330507278, 0.14515084028244019]\n",
      "Grand sum of 219 tensor sets is: [23.496591567993164, 255.9930877685547, -20.516313552856445, -176.76853942871094, 333.1511535644531]\n",
      "\n",
      "Instance 249 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 6: [-0.19781246781349182, 1.635204792022705, 0.29873237013816833, -3.9760184288024902, 2.3175292015075684]\n",
      "Grand sum of 220 tensor sets is: [23.298778533935547, 257.6282958984375, -20.217580795288086, -180.7445526123047, 335.46868896484375]\n",
      "\n",
      "Instance 250 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "jumping at index 6: [0.4590442478656769, 1.2986584901809692, 1.2145541906356812, -2.389770984649658, 2.4861631393432617]\n",
      "Grand sum of 221 tensor sets is: [23.757822036743164, 258.92694091796875, -19.003026962280273, -183.1343231201172, 337.9548645019531]\n",
      "\n",
      "Instance 251 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 252 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 19: [-0.03830130770802498, 0.9205437302589417, -1.3995954990386963, 0.07167693972587585, -0.016575992107391357]\n",
      "Grand sum of 222 tensor sets is: [23.719520568847656, 259.84747314453125, -20.40262222290039, -183.06265258789062, 337.93829345703125]\n",
      "\n",
      "Instance 253 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 20: [-0.3915223479270935, 2.11877703666687, -0.2656952440738678, -1.7873687744140625, 1.3359220027923584]\n",
      "Grand sum of 223 tensor sets is: [23.327999114990234, 261.96624755859375, -20.668317794799805, -184.8500213623047, 339.2742004394531]\n",
      "\n",
      "Instance 254 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 255 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 6: [0.8473896980285645, 0.6671295166015625, -1.2538506984710693, -1.207625150680542, 2.1065826416015625]\n",
      "Grand sum of 224 tensor sets is: [24.17538833618164, 262.63336181640625, -21.922168731689453, -186.05764770507812, 341.38079833984375]\n",
      "\n",
      "Instance 256 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 18: [-0.3042337894439697, 1.8053268194198608, -1.3143559694290161, -0.7036722302436829, -0.03866496682167053]\n",
      "Grand sum of 225 tensor sets is: [23.87115478515625, 264.4386901855469, -23.23652458190918, -186.76132202148438, 341.3421325683594]\n",
      "\n",
      "Instance 257 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 4: [-0.37767621874809265, 1.1676713228225708, -0.01689457893371582, -0.9262669086456299, 2.933094024658203]\n",
      "Grand sum of 226 tensor sets is: [23.493478775024414, 265.6063537597656, -23.253419876098633, -187.68759155273438, 344.2752380371094]\n",
      "\n",
      "Instance 258 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 227 tensor sets is: [24.599504470825195, 266.5447998046875, -23.85689353942871, -187.9890899658203, 346.19525146484375]\n",
      "\n",
      "Instance 259 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 44: [0.24182161688804626, 1.9961259365081787, 1.5576120615005493, -1.100635290145874, 3.2186315059661865]\n",
      "Grand sum of 228 tensor sets is: [24.841325759887695, 268.5409240722656, -22.29928207397461, -189.0897216796875, 349.41387939453125]\n",
      "\n",
      "Instance 260 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 22: [1.028536081314087, 2.1545543670654297, -0.5725434422492981, -0.4432326555252075, 0.9129090905189514]\n",
      "Grand sum of 229 tensor sets is: [25.869861602783203, 270.6954650878906, -22.871826171875, -189.532958984375, 350.3267822265625]\n",
      "\n",
      "Instance 261 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 10: [0.9708243608474731, 2.751143455505371, -0.3381441533565521, -1.8654727935791016, -0.03615090250968933]\n",
      "Grand sum of 230 tensor sets is: [26.840686798095703, 273.44659423828125, -23.209970474243164, -191.3984375, 350.2906188964844]\n",
      "\n",
      "Instance 262 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 263 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 264 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 231 tensor sets is: [27.946712493896484, 274.3850402832031, -23.813444137573242, -191.69993591308594, 352.21063232421875]\n",
      "\n",
      "Instance 265 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 29: [-0.3421928882598877, 0.11100958287715912, 0.5914461016654968, -0.009703859686851501, -0.47926098108291626]\n",
      "Grand sum of 232 tensor sets is: [27.60451889038086, 274.4960632324219, -23.22199821472168, -191.7096405029297, 351.73138427734375]\n",
      "\n",
      "Instance 266 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "jumping at index 26: [0.7614971399307251, 0.796021044254303, 0.1561165153980255, -1.489020586013794, 0.6708604693412781]\n",
      "Grand sum of 233 tensor sets is: [28.366016387939453, 275.2920837402344, -23.065881729125977, -193.1986541748047, 352.4022521972656]\n",
      "\n",
      "Instance 267 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 28: [-0.1708115041255951, 0.6239558458328247, 0.35236361622810364, -1.3615808486938477, 0.18529817461967468]\n",
      "Grand sum of 234 tensor sets is: [28.195205688476562, 275.9160461425781, -22.713518142700195, -194.56024169921875, 352.5875549316406]\n",
      "\n",
      "Instance 268 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 5: [0.23235131800174713, -0.5252195000648499, -1.3044050931930542, -2.0490546226501465, 0.39547058939933777]\n",
      "Grand sum of 235 tensor sets is: [28.42755699157715, 275.3908386230469, -24.01792335510254, -196.6092987060547, 352.9830322265625]\n",
      "\n",
      "Instance 269 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 16: [-0.12060099840164185, 1.1788747310638428, -0.2228856235742569, -1.8320858478546143, 2.812713384628296]\n",
      "Grand sum of 236 tensor sets is: [28.306955337524414, 276.5697021484375, -24.240808486938477, -198.44139099121094, 355.7957458496094]\n",
      "\n",
      "Instance 270 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 271 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 272 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 3: [1.0646519660949707, 0.5890346169471741, -0.3572833836078644, -0.8389418125152588, 2.205312728881836]\n",
      "Grand sum of 237 tensor sets is: [29.371606826782227, 277.1587219238281, -24.59809112548828, -199.28033447265625, 358.0010681152344]\n",
      "\n",
      "Instance 273 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 5: [1.2591335773468018, -0.333186537027359, -1.441343069076538, -1.9029951095581055, 1.148162841796875]\n",
      "Grand sum of 238 tensor sets is: [30.630741119384766, 276.8255310058594, -26.0394344329834, -201.18333435058594, 359.14923095703125]\n",
      "\n",
      "Instance 274 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 275 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 276 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 277 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 6: [-1.535577654838562, 0.8074339032173157, -1.8324731588363647, -1.2702969312667847, -0.6791281700134277]\n",
      "Grand sum of 239 tensor sets is: [29.095163345336914, 277.6329650878906, -27.87190818786621, -202.45362854003906, 358.4700927734375]\n",
      "\n",
      "Instance 278 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 31: [-0.01993192732334137, -0.12655606865882874, 0.09280774742364883, -0.8887022733688354, 1.2399303913116455]\n",
      "Grand sum of 240 tensor sets is: [29.075231552124023, 277.50640869140625, -27.77910041809082, -203.3423309326172, 359.71002197265625]\n",
      "\n",
      "Instance 279 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 11: [-1.6423420906066895, 0.32226288318634033, 0.07518839091062546, -0.89845210313797, 3.964219093322754]\n",
      "Grand sum of 241 tensor sets is: [27.432889938354492, 277.82867431640625, -27.70391273498535, -204.24078369140625, 363.67425537109375]\n",
      "\n",
      "Instance 280 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 281 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 17: [0.028341826051473618, 0.6384172439575195, -0.6717864274978638, -0.36811643838882446, 1.570326328277588]\n",
      "Grand sum of 242 tensor sets is: [27.461231231689453, 278.46710205078125, -28.375699996948242, -204.60890197753906, 365.24456787109375]\n",
      "\n",
      "Instance 282 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 13: [1.156754732131958, 2.929917335510254, -0.4584571123123169, -0.3646091818809509, 0.20421753823757172]\n",
      "Grand sum of 243 tensor sets is: [28.61798667907715, 281.39703369140625, -28.834157943725586, -204.9735107421875, 365.44879150390625]\n",
      "\n",
      "Instance 283 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 40: [-0.3950863182544708, 0.7483572959899902, 0.7743229866027832, -2.87843918800354, -0.8713144063949585]\n",
      "Grand sum of 244 tensor sets is: [28.222900390625, 282.1453857421875, -28.05983543395996, -207.85194396972656, 364.5774841308594]\n",
      "\n",
      "Instance 284 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 20: [-0.0825444832444191, 0.9914910793304443, 0.8257978558540344, -0.7984102368354797, 1.4393467903137207]\n",
      "Grand sum of 245 tensor sets is: [28.140356063842773, 283.1368713378906, -27.234037399291992, -208.65036010742188, 366.016845703125]\n",
      "\n",
      "Instance 285 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14, 29]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 14: [1.0079007148742676, 1.2240114212036133, 0.46773669123649597, -2.029087781906128, 4.881296634674072]\n",
      "jumping at index 29: [0.7940207719802856, 1.5245461463928223, 0.9727023243904114, -1.7935235500335693, 4.158766746520996]\n",
      "Grand sum of 246 tensor sets is: [29.041316986083984, 284.5111389160156, -26.513818740844727, -210.56166076660156, 370.536865234375]\n",
      "\n",
      "Instance 286 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 11: [0.0020392443984746933, 1.328076720237732, 1.0905930995941162, -1.0023878812789917, 3.5253400802612305]\n",
      "Grand sum of 247 tensor sets is: [29.04335594177246, 285.8392028808594, -25.42322540283203, -211.5640411376953, 374.06219482421875]\n",
      "\n",
      "Instance 287 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 6: [-0.21178561449050903, 1.2369532585144043, 0.21475516259670258, -0.5508235692977905, 0.5707107782363892]\n",
      "Grand sum of 248 tensor sets is: [28.83156967163086, 287.0761413574219, -25.20846939086914, -212.1148681640625, 374.6329040527344]\n",
      "\n",
      "Instance 288 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 289 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 7: [0.026459813117980957, 2.0776872634887695, 0.09368988871574402, -2.4591431617736816, -1.2715775966644287]\n",
      "Grand sum of 249 tensor sets is: [28.858030319213867, 289.1538391113281, -25.11478042602539, -214.57400512695312, 373.361328125]\n",
      "\n",
      "Instance 290 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([94, 13, 768])\n",
      "Shape of summed layers is: 94 x 768\n",
      "jumping at index 30: [-0.14843669533729553, 0.5151054263114929, 0.3396646976470947, 0.3097222149372101, -0.7819839715957642]\n",
      "Grand sum of 250 tensor sets is: [28.709592819213867, 289.6689453125, -24.775115966796875, -214.2642822265625, 372.579345703125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 291 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 9: [0.3173485994338989, 0.4030633568763733, 0.2037791609764099, -2.3856143951416016, 2.06575608253479]\n",
      "Grand sum of 251 tensor sets is: [29.026941299438477, 290.072021484375, -24.57133674621582, -216.64990234375, 374.6451110839844]\n",
      "\n",
      "Instance 292 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 2: [-0.3164072334766388, 1.7057174444198608, -0.2652356028556824, -1.1034669876098633, 4.375460147857666]\n",
      "Grand sum of 252 tensor sets is: [28.710533142089844, 291.7777404785156, -24.836572647094727, -217.7533721923828, 379.02056884765625]\n",
      "\n",
      "Instance 293 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "jumping at index 44: [-0.6368817090988159, 0.6037139296531677, 0.8334202766418457, -1.6647071838378906, 3.1260898113250732]\n",
      "Grand sum of 253 tensor sets is: [28.073652267456055, 292.3814392089844, -24.00315284729004, -219.41807556152344, 382.14666748046875]\n",
      "\n",
      "Instance 294 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 21: [0.7695298194885254, 1.2868025302886963, 0.6560549139976501, -2.1136720180511475, 2.2851383686065674]\n",
      "Grand sum of 254 tensor sets is: [28.843181610107422, 293.6682434082031, -23.347097396850586, -221.53175354003906, 384.4317932128906]\n",
      "\n",
      "Instance 295 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 15: [-0.6566887497901917, 0.9134571552276611, -1.4263105392456055, 1.034759283065796, 1.7500241994857788]\n",
      "Grand sum of 255 tensor sets is: [28.186492919921875, 294.5816955566406, -24.773406982421875, -220.4969940185547, 386.18182373046875]\n",
      "\n",
      "Instance 296 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "jumping at index 32: [-0.4248978793621063, 1.0217556953430176, 1.0469549894332886, 0.32224035263061523, 0.24101762473583221]\n",
      "Grand sum of 256 tensor sets is: [27.761594772338867, 295.60345458984375, -23.726451873779297, -220.1747589111328, 386.4228515625]\n",
      "\n",
      "Instance 297 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 298 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 299 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 26: [1.803173303604126, 1.5757948160171509, -0.07559484243392944, 0.46830835938453674, -1.0865663290023804]\n",
      "Grand sum of 257 tensor sets is: [29.564767837524414, 297.17926025390625, -23.802045822143555, -219.70645141601562, 385.3362731933594]\n",
      "\n",
      "Instance 300 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 6: [0.5185101628303528, 0.6884564161300659, 0.030459482222795486, -1.2817790508270264, 3.0716114044189453]\n",
      "Grand sum of 258 tensor sets is: [30.08327865600586, 297.8677062988281, -23.77158546447754, -220.9882354736328, 388.40789794921875]\n",
      "\n",
      "Instance 301 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 6: [0.5185101628303528, 0.6884564161300659, 0.030459482222795486, -1.2817790508270264, 3.0716114044189453]\n",
      "Grand sum of 259 tensor sets is: [30.601789474487305, 298.55615234375, -23.741125106811523, -222.27001953125, 391.4795227050781]\n",
      "\n",
      "Instance 302 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 303 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 5: [0.344720721244812, 0.947982907295227, -0.5458080768585205, -0.5512498617172241, 0.17662286758422852]\n",
      "Grand sum of 260 tensor sets is: [30.946510314941406, 299.504150390625, -24.28693389892578, -222.82127380371094, 391.6561584472656]\n",
      "\n",
      "Instance 304 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 11: [-0.9404209852218628, 1.3332116603851318, -0.29651665687561035, -0.6964857578277588, 0.19971689581871033]\n",
      "Grand sum of 261 tensor sets is: [30.00609016418457, 300.8373718261719, -24.583450317382812, -223.51776123046875, 391.8558654785156]\n",
      "\n",
      "Instance 305 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 262 tensor sets is: [29.340431213378906, 303.072998046875, -25.75480079650879, -224.43902587890625, 390.9803466796875]\n",
      "\n",
      "Instance 306 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 9: [0.3141728937625885, 0.741439938545227, 0.39474308490753174, -1.30452561378479, 0.024016082286834717]\n",
      "Grand sum of 263 tensor sets is: [29.654603958129883, 303.814453125, -25.360057830810547, -225.74354553222656, 391.0043640136719]\n",
      "\n",
      "Instance 307 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 9: [0.2748852074146271, 0.6810227632522583, -0.6859410405158997, -2.3581478595733643, 2.095066547393799]\n",
      "Grand sum of 264 tensor sets is: [29.929489135742188, 304.4954833984375, -26.04599952697754, -228.10169982910156, 393.09942626953125]\n",
      "\n",
      "Instance 308 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 3: [0.2940194010734558, 0.4260557293891907, 0.8459377288818359, -2.4301395416259766, 3.153611183166504]\n",
      "Grand sum of 265 tensor sets is: [30.223508834838867, 304.9215393066406, -25.200061798095703, -230.53184509277344, 396.2530517578125]\n",
      "\n",
      "Instance 309 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 310 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "jumping at index 6: [-0.3046567440032959, -0.17612189054489136, 1.4409953355789185, 0.8520542979240417, -0.02900826930999756]\n",
      "Grand sum of 266 tensor sets is: [29.918851852416992, 304.74542236328125, -23.759065628051758, -229.67979431152344, 396.2240295410156]\n",
      "\n",
      "Instance 311 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 4: [-0.6647039651870728, 2.3671391010284424, 0.9967594742774963, 1.0236510038375854, 0.3222602307796478]\n",
      "Grand sum of 267 tensor sets is: [29.254148483276367, 307.112548828125, -22.762306213378906, -228.65614318847656, 396.5462951660156]\n",
      "\n",
      "Instance 312 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 19: [-0.03830130770802498, 0.9205437302589417, -1.3995954990386963, 0.07167693972587585, -0.016575992107391357]\n",
      "Grand sum of 268 tensor sets is: [29.21584701538086, 308.0330810546875, -24.161901473999023, -228.58447265625, 396.52972412109375]\n",
      "\n",
      "Instance 313 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 22: [1.0782780647277832, 1.2397994995117188, -1.3052810430526733, -1.6761860847473145, 1.3050289154052734]\n",
      "Grand sum of 269 tensor sets is: [30.294124603271484, 309.27288818359375, -25.467182159423828, -230.2606658935547, 397.8347473144531]\n",
      "\n",
      "Instance 314 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 11: [0.5164148807525635, 0.3055945932865143, -0.1220366582274437, -0.8410702347755432, 2.997211217880249]\n",
      "Grand sum of 270 tensor sets is: [30.81053924560547, 309.5784912109375, -25.589218139648438, -231.1017303466797, 400.83197021484375]\n",
      "\n",
      "Instance 315 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 6: [0.3713620901107788, 2.2174668312072754, 0.2864765524864197, -1.212193489074707, 0.8455988168716431]\n",
      "Grand sum of 271 tensor sets is: [31.181901931762695, 311.79595947265625, -25.30274200439453, -232.3139190673828, 401.6775817871094]\n",
      "\n",
      "Instance 316 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 42: [1.2275276184082031, 1.7038905620574951, 0.26633670926094055, -0.9195811748504639, 1.6106586456298828]\n",
      "Grand sum of 272 tensor sets is: [32.40943145751953, 313.4998474121094, -25.036405563354492, -233.23350524902344, 403.2882385253906]\n",
      "\n",
      "Instance 317 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 273 tensor sets is: [32.48035430908203, 315.76806640625, -25.703575134277344, -233.0877685546875, 403.1517639160156]\n",
      "\n",
      "Instance 318 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 7: [0.965900182723999, 0.5040457248687744, -1.7846057415008545, -2.4284205436706543, -0.42143571376800537]\n",
      "Grand sum of 274 tensor sets is: [33.44625473022461, 316.2721252441406, -27.48818016052246, -235.5161895751953, 402.7303161621094]\n",
      "\n",
      "Instance 319 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 25: [-0.28817299008369446, 1.1709997653961182, 1.2159225940704346, -2.1184980869293213, -1.290386438369751]\n",
      "Grand sum of 275 tensor sets is: [33.1580810546875, 317.443115234375, -26.27225685119629, -237.6346893310547, 401.43994140625]\n",
      "\n",
      "Instance 320 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 19: [-0.8337855339050293, 2.3236610889434814, 0.4349084794521332, -2.0158002376556396, 1.7753019332885742]\n",
      "Grand sum of 276 tensor sets is: [32.32429504394531, 319.76678466796875, -25.83734893798828, -239.65048217773438, 403.2152404785156]\n",
      "\n",
      "Instance 321 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 30: [0.34389543533325195, 1.4628427028656006, 0.9208675622940063, -1.1320428848266602, 3.680652141571045]\n",
      "Grand sum of 277 tensor sets is: [32.668190002441406, 321.2296142578125, -24.916481018066406, -240.78253173828125, 406.8959045410156]\n",
      "\n",
      "Instance 322 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 19: [-0.0017080381512641907, 0.06253494322299957, 0.21520426869392395, -1.2323834896087646, 2.6607093811035156]\n",
      "Grand sum of 278 tensor sets is: [32.666481018066406, 321.2921447753906, -24.701276779174805, -242.01490783691406, 409.5566101074219]\n",
      "\n",
      "Instance 323 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 324 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 7: [0.7289232015609741, 0.29681822657585144, 2.033900737762451, -1.3999868631362915, 0.9845004081726074]\n",
      "Grand sum of 279 tensor sets is: [33.39540481567383, 321.5889587402344, -22.667375564575195, -243.41490173339844, 410.5411071777344]\n",
      "\n",
      "Instance 325 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 10: [-0.4888693392276764, 0.23552954196929932, 0.21059627830982208, -2.340134859085083, 2.166100025177002]\n",
      "Grand sum of 280 tensor sets is: [32.90653610229492, 321.8244934082031, -22.45677947998047, -245.75503540039062, 412.70721435546875]\n",
      "\n",
      "Instance 326 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 6: [0.6593703627586365, 1.4159178733825684, 0.6952877044677734, -3.0633275508880615, 2.078873634338379]\n",
      "Grand sum of 281 tensor sets is: [33.5659065246582, 323.24041748046875, -21.761491775512695, -248.818359375, 414.7861022949219]\n",
      "\n",
      "Instance 327 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 15: [-0.28758442401885986, 1.6068073511123657, 0.6604201793670654, -0.9338761568069458, 2.0165493488311768]\n",
      "Grand sum of 282 tensor sets is: [33.2783203125, 324.84722900390625, -21.101072311401367, -249.7522430419922, 416.8026428222656]\n",
      "\n",
      "Instance 328 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 9: [0.6053367853164673, 2.363027334213257, 0.22195285558700562, -1.3588027954101562, 1.0469493865966797]\n",
      "Grand sum of 283 tensor sets is: [33.8836555480957, 327.21026611328125, -20.879119873046875, -251.11105346679688, 417.8495788574219]\n",
      "\n",
      "Instance 329 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 20: [-0.32337281107902527, 1.4531755447387695, 0.726128876209259, -1.8891661167144775, 4.302511215209961]\n",
      "Grand sum of 284 tensor sets is: [33.56028366088867, 328.6634521484375, -20.152990341186523, -253.00021362304688, 422.152099609375]\n",
      "\n",
      "Instance 330 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "jumping at index 45: [-0.10815970599651337, 0.6295220255851746, -1.1006016731262207, -1.994206428527832, 3.1421971321105957]\n",
      "Grand sum of 285 tensor sets is: [33.452125549316406, 329.29296875, -21.253591537475586, -254.99441528320312, 425.2943115234375]\n",
      "\n",
      "Instance 331 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 286 tensor sets is: [33.523048400878906, 331.5611877441406, -21.920761108398438, -254.8486785888672, 425.1578369140625]\n",
      "\n",
      "Instance 332 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [51]\n",
      "Size of token embeddings is torch.Size([70, 13, 768])\n",
      "Shape of summed layers is: 70 x 768\n",
      "jumping at index 51: [-0.24922111630439758, 1.92631995677948, -1.6122609376907349, 0.49859678745269775, 0.05495068430900574]\n",
      "Grand sum of 287 tensor sets is: [33.273826599121094, 333.4875183105469, -23.533021926879883, -254.35008239746094, 425.2127990722656]\n",
      "\n",
      "Instance 333 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [54]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "jumping at index 54: [0.7193306088447571, 0.41497451066970825, -0.9622527956962585, -0.35206976532936096, 2.8684191703796387]\n",
      "Grand sum of 288 tensor sets is: [33.99315643310547, 333.9024963378906, -24.495275497436523, -254.7021484375, 428.0812072753906]\n",
      "\n",
      "Instance 334 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 335 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [63]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "jumping at index 63: [0.2463420331478119, -0.4425605535507202, 0.5982028245925903, -1.110627293586731, 0.8066800832748413]\n",
      "Grand sum of 289 tensor sets is: [34.239498138427734, 333.4599304199219, -23.897071838378906, -255.81277465820312, 428.88787841796875]\n",
      "\n",
      "Instance 336 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 480: [1.3795037269592285, 1.0536975860595703, 0.6297832727432251, -1.5170445442199707, 6.460476875305176]\n",
      "Grand sum of 290 tensor sets is: [35.61900329589844, 334.5136413574219, -23.267288208007812, -257.329833984375, 435.3483581542969]\n",
      "\n",
      "Instance 337 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 291 tensor sets is: [35.50104522705078, 334.94195556640625, -23.08432960510254, -259.52545166015625, 437.8501281738281]\n",
      "\n",
      "Instance 338 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([70, 13, 768])\n",
      "Shape of summed layers is: 70 x 768\n",
      "jumping at index 40: [-0.04988720268011093, -0.3866336941719055, 1.2067005634307861, -0.9860159158706665, 1.3245792388916016]\n",
      "Grand sum of 292 tensor sets is: [35.45115661621094, 334.5553283691406, -21.877628326416016, -260.511474609375, 439.1747131347656]\n",
      "\n",
      "Instance 339 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 23: [-1.2256977558135986, -0.10246216505765915, -0.8274610042572021, 1.5985569953918457, -0.2410077452659607]\n",
      "Grand sum of 293 tensor sets is: [34.225460052490234, 334.452880859375, -22.705089569091797, -258.91290283203125, 438.9337158203125]\n",
      "\n",
      "Instance 340 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 341 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "jumping at index 4: [-0.8262749314308167, 0.28137871623039246, 0.3543461561203003, -0.5128006339073181, -2.1571426391601562]\n",
      "Grand sum of 294 tensor sets is: [33.39918518066406, 334.7342529296875, -22.350744247436523, -259.4256896972656, 436.7765808105469]\n",
      "\n",
      "Instance 342 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 22: [1.773796558380127, 0.9845874309539795, -1.336754560470581, -0.1296086609363556, -1.4986335039138794]\n",
      "Grand sum of 295 tensor sets is: [35.17298126220703, 335.7188415527344, -23.687498092651367, -259.5552978515625, 435.2779541015625]\n",
      "\n",
      "Instance 343 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 344 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 4: [0.6760945320129395, 2.413386106491089, -0.14840616285800934, 0.29665425419807434, -0.6431856155395508]\n",
      "Grand sum of 296 tensor sets is: [35.84907531738281, 338.1322326660156, -23.835905075073242, -259.2586364746094, 434.634765625]\n",
      "\n",
      "Instance 345 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [-0.45801663398742676, -0.9925666451454163, 0.12019787728786469, -2.552450656890869, 2.2982001304626465]\n",
      "Grand sum of 297 tensor sets is: [35.39105987548828, 337.1396789550781, -23.715707778930664, -261.81109619140625, 436.9329528808594]\n",
      "\n",
      "Instance 346 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 15: [-0.017409756779670715, 0.9924570322036743, -0.9816808700561523, -1.2458471059799194, -0.23029640316963196]\n",
      "Grand sum of 298 tensor sets is: [35.37364959716797, 338.13214111328125, -24.6973876953125, -263.05694580078125, 436.7026672363281]\n",
      "\n",
      "Instance 347 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 2: [0.9584184885025024, 2.173999547958374, 0.3377169668674469, 0.6446294188499451, 0.9912510514259338]\n",
      "Grand sum of 299 tensor sets is: [36.332069396972656, 340.30615234375, -24.359670639038086, -262.4123229980469, 437.69390869140625]\n",
      "\n",
      "Instance 348 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 349 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 22: [-0.24587778747081757, 1.390885829925537, -0.029454633593559265, 3.1882083415985107, -0.4377194046974182]\n",
      "Grand sum of 300 tensor sets is: [36.0861930847168, 341.6970520019531, -24.38912582397461, -259.22412109375, 437.2561950683594]\n",
      "\n",
      "Instance 350 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 301 tensor sets is: [37.02865219116211, 341.7626953125, -24.759397506713867, -259.6938781738281, 439.8825378417969]\n",
      "\n",
      "Instance 351 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 352 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 5: [0.9030729532241821, 1.4517674446105957, -0.1318405419588089, 0.3970757722854614, -1.1581923961639404]\n",
      "Grand sum of 302 tensor sets is: [37.931724548339844, 343.2144775390625, -24.891237258911133, -259.29681396484375, 438.7243347167969]\n",
      "\n",
      "Instance 353 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 19: [0.7466686367988586, 1.0819398164749146, -0.4928421378135681, 0.49207592010498047, 0.23325912654399872]\n",
      "Grand sum of 303 tensor sets is: [38.67839431762695, 344.2964172363281, -25.384078979492188, -258.80474853515625, 438.95758056640625]\n",
      "\n",
      "Instance 354 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "jumping at index 2: [-0.5519217252731323, 2.7869625091552734, -0.21034225821495056, 1.0666815042495728, 0.43819940090179443]\n",
      "Grand sum of 304 tensor sets is: [38.12647247314453, 347.0833740234375, -25.59442138671875, -257.7380676269531, 439.3957824707031]\n",
      "\n",
      "Instance 355 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 30: [-0.8231475353240967, 0.20416110754013062, -0.8184887170791626, -1.1787834167480469, 4.866078853607178]\n",
      "Grand sum of 305 tensor sets is: [37.30332565307617, 347.28753662109375, -26.41291046142578, -258.9168395996094, 444.2618713378906]\n",
      "\n",
      "Instance 356 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 32: [0.4753398895263672, 1.1946961879730225, 0.4708554446697235, -2.674074649810791, 2.3926868438720703]\n",
      "Grand sum of 306 tensor sets is: [37.778663635253906, 348.48223876953125, -25.942054748535156, -261.5909118652344, 446.6545715332031]\n",
      "\n",
      "Instance 357 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 23: [-0.41986674070358276, 0.1705174744129181, -1.4068139791488647, -0.6178423166275024, 0.22995424270629883]\n",
      "Grand sum of 307 tensor sets is: [37.358795166015625, 348.65277099609375, -27.34886932373047, -262.208740234375, 446.884521484375]\n",
      "\n",
      "Instance 358 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 31: [0.7308530807495117, 1.446509599685669, 0.9715222120285034, -0.756840705871582, 1.6107447147369385]\n",
      "Grand sum of 308 tensor sets is: [38.08964920043945, 350.0992736816406, -26.377347946166992, -262.965576171875, 448.4952697753906]\n",
      "\n",
      "Instance 359 of jumping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 360 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 27: [-0.007940918207168579, 0.7053402066230774, -0.4584462642669678, 0.2142457813024521, 0.11285242438316345]\n",
      "Grand sum of 309 tensor sets is: [38.08170700073242, 350.80462646484375, -26.83579444885254, -262.7513427734375, 448.6081237792969]\n",
      "\n",
      "Instance 361 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 362 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 24: [-0.8834428191184998, 1.9338691234588623, 0.17610053718090057, -0.47182896733283997, 1.3118094205856323]\n",
      "Grand sum of 310 tensor sets is: [37.198265075683594, 352.7384948730469, -26.65969467163086, -263.2231750488281, 449.919921875]\n",
      "\n",
      "Instance 363 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 7: [-0.047619935125112534, 1.4382808208465576, -0.8172945976257324, -1.7165710926055908, 1.1618987321853638]\n",
      "Grand sum of 311 tensor sets is: [37.1506462097168, 354.1767883300781, -27.47698974609375, -264.93975830078125, 451.0818176269531]\n",
      "\n",
      "Instance 364 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 4: [0.9899927973747253, -0.5018044710159302, -0.15792939066886902, -0.43202143907546997, 2.954936981201172]\n",
      "Grand sum of 312 tensor sets is: [38.14064025878906, 353.67498779296875, -27.634918212890625, -265.37176513671875, 454.0367431640625]\n",
      "\n",
      "Instance 365 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 12: [-0.9644930362701416, 1.9963992834091187, -1.446050763130188, -1.1936978101730347, -1.1843496561050415]\n",
      "Grand sum of 313 tensor sets is: [37.1761474609375, 355.67138671875, -29.080968856811523, -266.5654602050781, 452.8523864746094]\n",
      "\n",
      "Instance 366 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [51]\n",
      "Size of token embeddings is torch.Size([133, 13, 768])\n",
      "Shape of summed layers is: 133 x 768\n",
      "jumping at index 51: [2.254894971847534, 0.6514367461204529, 0.1732276976108551, -0.3837506175041199, 1.772889494895935]\n",
      "Grand sum of 314 tensor sets is: [39.4310417175293, 356.32281494140625, -28.90774154663086, -266.94921875, 454.6252746582031]\n",
      "\n",
      "Instance 367 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [-0.45801663398742676, -0.9925666451454163, 0.12019787728786469, -2.552450656890869, 2.2982001304626465]\n",
      "Grand sum of 315 tensor sets is: [38.973026275634766, 355.33026123046875, -28.78754425048828, -269.5016784667969, 456.9234619140625]\n",
      "\n",
      "Instance 368 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 316 tensor sets is: [39.91548538208008, 355.3959045410156, -29.15781593322754, -269.971435546875, 459.5498046875]\n",
      "\n",
      "Instance 369 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 11: [0.8994139432907104, 0.7518905401229858, -1.1915013790130615, -0.39370644092559814, 6.981278419494629]\n",
      "Grand sum of 317 tensor sets is: [40.81489944458008, 356.1477966308594, -30.34931755065918, -270.3651428222656, 466.5310974121094]\n",
      "\n",
      "Instance 370 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 371 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "jumping at index 3: [-0.6367586851119995, 0.7376865744590759, 0.2881425619125366, -0.9740330576896667, 1.1594419479370117]\n",
      "Grand sum of 318 tensor sets is: [40.17814254760742, 356.885498046875, -30.061174392700195, -271.33917236328125, 467.6905517578125]\n",
      "\n",
      "Instance 372 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 25: [-0.5749988555908203, 0.96495521068573, -0.027700988575816154, -1.1943520307540894, 0.4515261948108673]\n",
      "Grand sum of 319 tensor sets is: [39.60314178466797, 357.8504638671875, -30.08887481689453, -272.5335388183594, 468.14208984375]\n",
      "\n",
      "Instance 373 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 374 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 320 tensor sets is: [40.54560089111328, 357.9161071777344, -30.45914649963379, -273.0032958984375, 470.7684326171875]\n",
      "\n",
      "Instance 375 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 7: [1.1555569171905518, 0.6806050539016724, -0.12059309333562851, -0.25982481241226196, 0.5149611234664917]\n",
      "Grand sum of 321 tensor sets is: [41.70115661621094, 358.5967102050781, -30.579740524291992, -273.26312255859375, 471.28338623046875]\n",
      "\n",
      "Instance 376 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "jumping at index 2: [0.2025507539510727, 0.8241167664527893, -0.10931092500686646, -0.620173454284668, 3.3327476978302]\n",
      "Grand sum of 322 tensor sets is: [41.90370559692383, 359.42083740234375, -30.689050674438477, -273.88330078125, 474.6161193847656]\n",
      "\n",
      "Instance 377 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 8: [-0.15180633962154388, 0.7557868361473083, -1.8513259887695312, -2.3993852138519287, 2.699646472930908]\n",
      "Grand sum of 323 tensor sets is: [41.75189971923828, 360.1766357421875, -32.540374755859375, -276.2826843261719, 477.3157653808594]\n",
      "\n",
      "Instance 378 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 40: [0.2736690044403076, 1.240371584892273, -0.9730778336524963, -1.6442360877990723, 0.6099278926849365]\n",
      "Grand sum of 324 tensor sets is: [42.025569915771484, 361.4169921875, -33.51345443725586, -277.9269104003906, 477.9256896972656]\n",
      "\n",
      "Instance 379 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 11: [-0.030061550438404083, 1.4842138290405273, -0.9361082315444946, 0.9927908182144165, 3.542884349822998]\n",
      "Grand sum of 325 tensor sets is: [41.99551010131836, 362.9012145996094, -34.449562072753906, -276.9341125488281, 481.46856689453125]\n",
      "\n",
      "Instance 380 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 28: [-0.35210275650024414, 1.434852123260498, 0.6825548410415649, 0.8300846219062805, 0.6295202970504761]\n",
      "Grand sum of 326 tensor sets is: [41.64340591430664, 364.3360595703125, -33.767005920410156, -276.1040344238281, 482.09808349609375]\n",
      "\n",
      "Instance 381 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 327 tensor sets is: [40.97774887084961, 366.5716857910156, -34.938358306884766, -277.0252990722656, 481.2225646972656]\n",
      "\n",
      "Instance 382 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 11: [0.5511431097984314, 1.725875973701477, 0.4837196171283722, -0.08816338330507278, 0.14515084028244019]\n",
      "Grand sum of 328 tensor sets is: [41.528892517089844, 368.2975769042969, -34.45463943481445, -277.11346435546875, 481.3677062988281]\n",
      "\n",
      "Instance 383 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 384 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "jumping at index 8: [0.49616509675979614, 3.3042900562286377, -0.4205639064311981, 0.8127344846725464, 0.40006938576698303]\n",
      "Grand sum of 329 tensor sets is: [42.02505874633789, 371.60186767578125, -34.87520217895508, -276.30072021484375, 481.76776123046875]\n",
      "\n",
      "Instance 385 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 6: [0.10726036876440048, 0.38513463735580444, -0.7329921126365662, -0.7071418762207031, 2.39089035987854]\n",
      "Grand sum of 330 tensor sets is: [42.132320404052734, 371.98699951171875, -35.608192443847656, -277.00787353515625, 484.1586608886719]\n",
      "\n",
      "Instance 386 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 3: [-0.49774909019470215, 1.5053365230560303, -0.3491864502429962, -1.1604585647583008, 2.993807792663574]\n",
      "Grand sum of 331 tensor sets is: [41.63457107543945, 373.4923400878906, -35.95737838745117, -278.1683349609375, 487.1524658203125]\n",
      "\n",
      "Instance 387 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 388 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [-0.979627788066864, 0.44269412755966187, 1.3982096910476685, -1.8071774244308472, 0.3163454234600067]\n",
      "Grand sum of 332 tensor sets is: [40.65494155883789, 373.9350280761719, -34.55916976928711, -279.97552490234375, 487.46881103515625]\n",
      "\n",
      "Instance 389 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 37: [0.5562046766281128, 0.7651055455207825, 0.6862711310386658, -0.9315854907035828, -0.7601066827774048]\n",
      "Grand sum of 333 tensor sets is: [41.21114730834961, 374.70013427734375, -33.87289810180664, -280.9071044921875, 486.7087097167969]\n",
      "\n",
      "Instance 390 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 391 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 26: [-0.2899157404899597, 0.49383658170700073, 1.232061505317688, -1.9491629600524902, 0.6583724021911621]\n",
      "Grand sum of 334 tensor sets is: [40.92123031616211, 375.1939697265625, -32.64083480834961, -282.85626220703125, 487.3670959472656]\n",
      "\n",
      "Instance 392 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 31: [0.7308530807495117, 1.446509599685669, 0.9715222120285034, -0.756840705871582, 1.6107447147369385]\n",
      "Grand sum of 335 tensor sets is: [41.65208435058594, 376.6404724121094, -31.669313430786133, -283.61309814453125, 488.97784423828125]\n",
      "\n",
      "Instance 393 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 30: [-0.8231475353240967, 0.20416110754013062, -0.8184887170791626, -1.1787834167480469, 4.866078853607178]\n",
      "Grand sum of 336 tensor sets is: [40.82893753051758, 376.8446350097656, -32.48780059814453, -284.7918701171875, 493.84393310546875]\n",
      "\n",
      "Instance 394 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 13: [0.4827694892883301, 1.686653733253479, -0.156693696975708, -0.726878821849823, 2.5696334838867188]\n",
      "Grand sum of 337 tensor sets is: [41.31170654296875, 378.5312805175781, -32.644493103027344, -285.51873779296875, 496.41357421875]\n",
      "\n",
      "Instance 395 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 9: [0.2065403014421463, 1.786158561706543, -1.370908498764038, 0.36864715814590454, 1.605574131011963]\n",
      "Grand sum of 338 tensor sets is: [41.518245697021484, 380.31744384765625, -34.01539993286133, -285.15008544921875, 498.0191345214844]\n",
      "\n",
      "Instance 396 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 339 tensor sets is: [42.624271392822266, 381.2558898925781, -34.618873596191406, -285.4515686035156, 499.93914794921875]\n",
      "\n",
      "Instance 397 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [60]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "jumping at index 60: [-0.6318047642707825, 1.4997633695602417, 0.48061320185661316, -0.4231744408607483, 2.3953123092651367]\n",
      "Grand sum of 340 tensor sets is: [41.99246597290039, 382.7556457519531, -34.13825988769531, -285.874755859375, 502.33447265625]\n",
      "\n",
      "Instance 398 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 13: [-0.39785465598106384, 1.2886803150177002, 1.294082522392273, -0.6674246788024902, -0.9445896148681641]\n",
      "Grand sum of 341 tensor sets is: [41.59461212158203, 384.0443115234375, -32.84417724609375, -286.54217529296875, 501.389892578125]\n",
      "\n",
      "Instance 399 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 400 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 342 tensor sets is: [41.476654052734375, 384.4726257324219, -32.66122055053711, -288.73779296875, 503.89166259765625]\n",
      "\n",
      "Instance 401 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 8: [-0.9172607064247131, 1.6757869720458984, -0.21527281403541565, 1.550938367843628, 1.1468324661254883]\n",
      "Grand sum of 343 tensor sets is: [40.55939483642578, 386.1484069824219, -32.87649154663086, -287.1868591308594, 505.0384826660156]\n",
      "\n",
      "Instance 402 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 344 tensor sets is: [41.66542053222656, 387.08685302734375, -33.47996520996094, -287.48834228515625, 506.95849609375]\n",
      "\n",
      "Instance 403 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 10: [0.30654072761535645, 0.7520685791969299, -0.9763495326042175, -1.4619957208633423, 1.4253723621368408]\n",
      "Grand sum of 345 tensor sets is: [41.971961975097656, 387.83892822265625, -34.45631408691406, -288.9503479003906, 508.3838806152344]\n",
      "\n",
      "Instance 404 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 33: [-0.3336968719959259, 0.6368481516838074, 1.1602840423583984, -0.09760838747024536, 2.2313826084136963]\n",
      "Grand sum of 346 tensor sets is: [41.63826370239258, 388.47576904296875, -33.29602813720703, -289.0479431152344, 510.6152648925781]\n",
      "\n",
      "Instance 405 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 406 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 7: [-0.49931061267852783, -1.3099291324615479, 0.5012228488922119, -0.8854540586471558, 2.374804973602295]\n",
      "Grand sum of 347 tensor sets is: [41.138954162597656, 387.16583251953125, -32.794803619384766, -289.93341064453125, 512.9900512695312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 407 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 408 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 22: [-0.5339784026145935, 1.1113027334213257, -0.010615558363497257, -1.3126296997070312, 1.4165459871292114]\n",
      "Grand sum of 348 tensor sets is: [40.604976654052734, 388.2771301269531, -32.805419921875, -291.24603271484375, 514.4066162109375]\n",
      "\n",
      "Instance 409 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 410 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "jumping at index 28: [0.356874018907547, 1.7767714262008667, 0.6363214254379272, -0.6036278009414673, 3.382119655609131]\n",
      "Grand sum of 349 tensor sets is: [40.961849212646484, 390.05389404296875, -32.169097900390625, -291.84967041015625, 517.7887573242188]\n",
      "\n",
      "Instance 411 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 7: [0.6365556120872498, 0.6982996463775635, -0.24062395095825195, -0.8391608595848083, 2.3937783241271973]\n",
      "Grand sum of 350 tensor sets is: [41.59840393066406, 390.752197265625, -32.40972137451172, -292.6888427734375, 520.1825561523438]\n",
      "\n",
      "Instance 412 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 16: [0.3390462398529053, 0.20663362741470337, -0.29009759426116943, -0.7332460284233093, -1.5891926288604736]\n",
      "Grand sum of 351 tensor sets is: [41.93745040893555, 390.9588317871094, -32.6998176574707, -293.4220886230469, 518.5933837890625]\n",
      "\n",
      "Instance 413 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 2: [-0.6048601269721985, 1.1253846883773804, -0.9435784220695496, -2.507587194442749, 2.76242995262146]\n",
      "Grand sum of 352 tensor sets is: [41.33259201049805, 392.084228515625, -33.643394470214844, -295.9296875, 521.3558349609375]\n",
      "\n",
      "Instance 414 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 415 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 353 tensor sets is: [41.40351486206055, 394.3524475097656, -34.31056594848633, -295.783935546875, 521.2193603515625]\n",
      "\n",
      "Instance 416 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 10: [-1.142169713973999, 1.4794520139694214, -0.07253777980804443, 1.000716209411621, -0.2900722026824951]\n",
      "Grand sum of 354 tensor sets is: [40.26134490966797, 395.8319091796875, -34.38310241699219, -294.7832336425781, 520.9292602539062]\n",
      "\n",
      "Instance 417 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 33: [0.26829203963279724, -0.44466662406921387, 1.0125365257263184, -2.15714955329895, 1.9544180631637573]\n",
      "Grand sum of 355 tensor sets is: [40.52963638305664, 395.3872375488281, -33.370567321777344, -296.94036865234375, 522.8836669921875]\n",
      "\n",
      "Instance 418 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [1]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 1: [-0.8448672294616699, 1.2316532135009766, -1.7833545207977295, 0.41922903060913086, 3.8432135581970215]\n",
      "Grand sum of 356 tensor sets is: [39.68476867675781, 396.618896484375, -35.15392303466797, -296.5211486816406, 526.7268676757812]\n",
      "\n",
      "Instance 419 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 357 tensor sets is: [39.75569152832031, 398.8871154785156, -35.82109451293945, -296.3753967285156, 526.5903930664062]\n",
      "\n",
      "Instance 420 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "jumping at index 4: [-0.34915977716445923, 1.7563753128051758, -0.03190874680876732, -0.38795405626296997, 1.2782108783721924]\n",
      "Grand sum of 358 tensor sets is: [39.406532287597656, 400.64349365234375, -35.853004455566406, -296.7633361816406, 527.8685913085938]\n",
      "\n",
      "Instance 421 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 19: [1.5234252214431763, 1.7801682949066162, -0.5788780450820923, -0.40615737438201904, 2.44614315032959]\n",
      "Grand sum of 359 tensor sets is: [40.92995834350586, 402.4236755371094, -36.431880950927734, -297.16949462890625, 530.3147583007812]\n",
      "\n",
      "Instance 422 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 17: [-0.003356620669364929, 2.364286422729492, -0.23615077137947083, -2.958430290222168, 2.704827308654785]\n",
      "Grand sum of 360 tensor sets is: [40.92660140991211, 404.7879638671875, -36.668033599853516, -300.1279296875, 533.0195922851562]\n",
      "\n",
      "Instance 423 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 424 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 10: [-0.8708974123001099, 1.741455078125, -0.37855738401412964, 1.3001129627227783, 0.11707276105880737]\n",
      "Grand sum of 361 tensor sets is: [40.055702209472656, 406.5294189453125, -37.046592712402344, -298.82781982421875, 533.1366577148438]\n",
      "\n",
      "Instance 425 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 362 tensor sets is: [41.16172790527344, 407.4678649902344, -37.65006637573242, -299.1293029785156, 535.056640625]\n",
      "\n",
      "Instance 426 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [55]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "jumping at index 55: [0.3598889708518982, 1.3010042905807495, -0.7338337898254395, -1.6636782884597778, 0.29565954208374023]\n",
      "Grand sum of 363 tensor sets is: [41.5216178894043, 408.76885986328125, -38.3838996887207, -300.79296875, 535.352294921875]\n",
      "\n",
      "Instance 427 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 7: [0.2865865230560303, 2.2952051162719727, -0.4844112694263458, -0.2023710459470749, 3.2676215171813965]\n",
      "Grand sum of 364 tensor sets is: [41.808204650878906, 411.0640563964844, -38.86831283569336, -300.9953308105469, 538.6199340820312]\n",
      "\n",
      "Instance 428 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 365 tensor sets is: [42.91423034667969, 412.00250244140625, -39.47178649902344, -301.29681396484375, 540.5399169921875]\n",
      "\n",
      "Instance 429 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 7: [1.489169955253601, -0.29198113083839417, 0.14019423723220825, 1.5384668111801147, 2.2323226928710938]\n",
      "Grand sum of 366 tensor sets is: [44.40340042114258, 411.71051025390625, -39.33159255981445, -299.75836181640625, 542.772216796875]\n",
      "\n",
      "Instance 430 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28, 46]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "jumping at index 28: [0.02953055500984192, -0.002577841281890869, -0.038096968084573746, -2.793562650680542, 3.6429266929626465]\n",
      "jumping at index 46: [-0.5987898111343384, 0.3651660978794098, 0.36364954710006714, -2.4402916431427, 2.061063051223755]\n",
      "Grand sum of 367 tensor sets is: [44.118770599365234, 411.8918151855469, -39.16881561279297, -302.3752746582031, 545.6242065429688]\n",
      "\n",
      "Instance 431 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 12: [-0.2296231985092163, 1.7531684637069702, -0.8488551378250122, -1.4074299335479736, 0.06776565313339233]\n",
      "Grand sum of 368 tensor sets is: [43.8891487121582, 413.6449890136719, -40.017669677734375, -303.78271484375, 545.6919555664062]\n",
      "\n",
      "Instance 432 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 2: [0.06248552352190018, 1.9218076467514038, -1.0344635248184204, -1.3754682540893555, 1.0722882747650146]\n",
      "Grand sum of 369 tensor sets is: [43.95163345336914, 415.5668029785156, -41.05213165283203, -305.1581726074219, 546.7642211914062]\n",
      "\n",
      "Instance 433 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 370 tensor sets is: [44.02255630493164, 417.83502197265625, -41.719303131103516, -305.0124206542969, 546.6277465820312]\n",
      "\n",
      "Instance 434 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 435 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 22: [-1.150949239730835, -0.2840854227542877, -1.2030965089797974, 1.8091100454330444, -0.3096684515476227]\n",
      "Grand sum of 371 tensor sets is: [42.87160873413086, 417.5509338378906, -42.922401428222656, -303.20330810546875, 546.3180541992188]\n",
      "\n",
      "Instance 436 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 437 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 3: [0.7130095362663269, 0.7675206065177917, -0.7802754044532776, -1.8382742404937744, 1.5186264514923096]\n",
      "Grand sum of 372 tensor sets is: [43.584617614746094, 418.3184509277344, -43.70267868041992, -305.0415954589844, 547.836669921875]\n",
      "\n",
      "Instance 438 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 5: [0.3693222105503082, 1.190385103225708, -0.16140280663967133, -0.9568738341331482, 3.2676916122436523]\n",
      "Grand sum of 373 tensor sets is: [43.953941345214844, 419.50885009765625, -43.86408233642578, -305.99847412109375, 551.1043701171875]\n",
      "\n",
      "Instance 439 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 9: [-0.3668481111526489, 1.7165303230285645, 0.3017057180404663, -2.6144607067108154, 3.2336339950561523]\n",
      "Grand sum of 374 tensor sets is: [43.587093353271484, 421.2253723144531, -43.5623779296875, -308.6129455566406, 554.3380126953125]\n",
      "\n",
      "Instance 440 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 31: [0.7308530807495117, 1.446509599685669, 0.9715222120285034, -0.756840705871582, 1.6107447147369385]\n",
      "Grand sum of 375 tensor sets is: [44.31794738769531, 422.671875, -42.59085464477539, -309.3697814941406, 555.94873046875]\n",
      "\n",
      "Instance 441 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 11: [-0.09630665183067322, 1.8174721002578735, -0.3396620750427246, 1.3685109615325928, 0.0793202817440033]\n",
      "Grand sum of 376 tensor sets is: [44.221641540527344, 424.4893493652344, -42.93051528930664, -308.00128173828125, 556.028076171875]\n",
      "\n",
      "Instance 442 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 377 tensor sets is: [45.327667236328125, 425.42779541015625, -43.53398895263672, -308.3027648925781, 557.9480590820312]\n",
      "\n",
      "Instance 443 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 19: [-0.9799162149429321, 0.470222532749176, -0.3274955749511719, -1.6297084093093872, -0.02197897434234619]\n",
      "Grand sum of 378 tensor sets is: [44.34775161743164, 425.89801025390625, -43.86148452758789, -309.9324645996094, 557.9260864257812]\n",
      "\n",
      "Instance 444 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 16: [-0.3645324110984802, 0.7241820096969604, 0.2624979615211487, -1.5901477336883545, 2.62630558013916]\n",
      "Grand sum of 379 tensor sets is: [43.983219146728516, 426.6221923828125, -43.5989875793457, -311.5226135253906, 560.5523681640625]\n",
      "\n",
      "Instance 445 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 11: [0.5511431097984314, 1.725875973701477, 0.4837196171283722, -0.08816338330507278, 0.14515084028244019]\n",
      "Grand sum of 380 tensor sets is: [44.53436279296875, 428.34808349609375, -43.11526870727539, -311.61077880859375, 560.697509765625]\n",
      "\n",
      "Instance 446 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 10: [-0.4601379334926605, 2.0473973751068115, 0.18874631822109222, -1.5242586135864258, 5.070862293243408]\n",
      "Grand sum of 381 tensor sets is: [44.07422637939453, 430.3954772949219, -42.92652130126953, -313.1350402832031, 565.7683715820312]\n",
      "\n",
      "Instance 447 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "jumping at index 32: [0.43088409304618835, -0.15727943181991577, 1.4738770723342896, -1.9845349788665771, -0.5888403654098511]\n",
      "Grand sum of 382 tensor sets is: [44.50511169433594, 430.2381896972656, -41.45264434814453, -315.11956787109375, 565.1795043945312]\n",
      "\n",
      "Instance 448 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 15: [0.5826307535171509, 1.5053067207336426, 0.8907631635665894, -3.2909512519836426, 1.6961206197738647]\n",
      "Grand sum of 383 tensor sets is: [45.08774185180664, 431.7434997558594, -40.56188201904297, -318.4105224609375, 566.8756103515625]\n",
      "\n",
      "Instance 449 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 25: [0.23205681145191193, 0.45604151487350464, 0.2590055763721466, -0.9496663808822632, 0.9568642377853394]\n",
      "Grand sum of 384 tensor sets is: [45.31979751586914, 432.1995544433594, -40.30287551879883, -319.3601989746094, 567.8324584960938]\n",
      "\n",
      "Instance 450 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 14: [0.42430341243743896, 2.6723227500915527, -0.4201800227165222, -0.10889202356338501, -0.4931271970272064]\n",
      "Grand sum of 385 tensor sets is: [45.744102478027344, 434.87188720703125, -40.72305679321289, -319.4690856933594, 567.33935546875]\n",
      "\n",
      "Instance 451 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "jumping at index 5: [0.7370656728744507, 2.41003680229187, -0.3156198561191559, -0.37944406270980835, 3.9311439990997314]\n",
      "Grand sum of 386 tensor sets is: [46.48116683959961, 437.28192138671875, -41.03867721557617, -319.8485412597656, 571.2705078125]\n",
      "\n",
      "Instance 452 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 28: [-0.35210275650024414, 1.434852123260498, 0.6825548410415649, 0.8300846219062805, 0.6295202970504761]\n",
      "Grand sum of 387 tensor sets is: [46.12906265258789, 438.7167663574219, -40.35612106323242, -319.0184631347656, 571.9000244140625]\n",
      "\n",
      "Instance 453 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 25: [-0.5749988555908203, 0.96495521068573, -0.027700988575816154, -1.1943520307540894, 0.4515261948108673]\n",
      "Grand sum of 388 tensor sets is: [45.55406188964844, 439.6817321777344, -40.38382339477539, -320.21282958984375, 572.3515625]\n",
      "\n",
      "Instance 454 of jumping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 10: [0.41390806436538696, 3.00718092918396, 0.5206271409988403, 0.6376316547393799, 2.1285760402679443]\n",
      "Grand sum of 389 tensor sets is: [45.96797180175781, 442.68890380859375, -39.863197326660156, -319.5751953125, 574.4801635742188]\n",
      "\n",
      "Instance 455 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 29: [1.0793029069900513, 1.1062235832214355, 0.6292182803153992, -0.35578596591949463, 3.0110855102539062]\n",
      "Grand sum of 390 tensor sets is: [47.04727554321289, 443.7951354980469, -39.233978271484375, -319.93096923828125, 577.4912719726562]\n",
      "\n",
      "Instance 456 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 9: [-0.05615738034248352, 0.5796412229537964, 0.07483813166618347, -1.7033904790878296, 2.6733222007751465]\n",
      "Grand sum of 391 tensor sets is: [46.991119384765625, 444.3747863769531, -39.159141540527344, -321.6343688964844, 580.1646118164062]\n",
      "\n",
      "Instance 457 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([66, 13, 768])\n",
      "Shape of summed layers is: 66 x 768\n",
      "jumping at index 22: [0.05686721205711365, 1.5027997493743896, -0.682458221912384, 0.004268445074558258, 2.1839847564697266]\n",
      "Grand sum of 392 tensor sets is: [47.0479850769043, 445.8775939941406, -39.84159851074219, -321.6300964355469, 582.3485717773438]\n",
      "\n",
      "Instance 458 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 7: [-0.8517338037490845, 1.2897001504898071, 1.137540578842163, -2.3070058822631836, 1.2953829765319824]\n",
      "Grand sum of 393 tensor sets is: [46.196250915527344, 447.16729736328125, -38.70405960083008, -323.9371032714844, 583.6439819335938]\n",
      "\n",
      "Instance 459 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 27: [0.5241286158561707, 0.7382777333259583, -1.223789930343628, -2.185116767883301, -0.9127934575080872]\n",
      "Grand sum of 394 tensor sets is: [46.72037887573242, 447.90557861328125, -39.92784881591797, -326.1222229003906, 582.731201171875]\n",
      "\n",
      "Instance 460 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 25: [-0.5749988555908203, 0.96495521068573, -0.027700988575816154, -1.1943520307540894, 0.4515261948108673]\n",
      "Grand sum of 395 tensor sets is: [46.14537811279297, 448.87054443359375, -39.95555114746094, -327.31658935546875, 583.1827392578125]\n",
      "\n",
      "Instance 461 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 6: [0.8722321391105652, 2.986689567565918, -1.032314419746399, 0.2731873095035553, -1.8955973386764526]\n",
      "Grand sum of 396 tensor sets is: [47.017608642578125, 451.85723876953125, -40.98786544799805, -327.04339599609375, 581.2871704101562]\n",
      "\n",
      "Instance 462 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "jumping at index 24: [-0.29616740345954895, 0.7504483461380005, -0.8280429840087891, -1.467978596687317, -0.650956928730011]\n",
      "Grand sum of 397 tensor sets is: [46.721439361572266, 452.6076965332031, -41.81591033935547, -328.5113830566406, 580.63623046875]\n",
      "\n",
      "Instance 463 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 5: [0.22558525204658508, 1.0147815942764282, -0.34749624133110046, -0.8538079261779785, -0.7040113210678101]\n",
      "Grand sum of 398 tensor sets is: [46.947025299072266, 453.6224670410156, -42.16340637207031, -329.3652038574219, 579.9321899414062]\n",
      "\n",
      "Instance 464 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 12: [0.14864864945411682, 2.8004796504974365, -0.2929815649986267, -0.8400757312774658, 1.4223129749298096]\n",
      "Grand sum of 399 tensor sets is: [47.095672607421875, 456.4229431152344, -42.45638656616211, -330.2052917480469, 581.3544921875]\n",
      "\n",
      "Instance 465 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 29: [-0.05382450670003891, 1.811747431755066, 1.125011682510376, -1.112833023071289, -0.12574955821037292]\n",
      "Grand sum of 400 tensor sets is: [47.041847229003906, 458.23468017578125, -41.33137512207031, -331.318115234375, 581.228759765625]\n",
      "\n",
      "Instance 466 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 24: [1.0165672302246094, 0.4198017418384552, -1.537676453590393, -0.5035353899002075, 1.2611029148101807]\n",
      "Grand sum of 401 tensor sets is: [48.058414459228516, 458.65447998046875, -42.86905288696289, -331.8216552734375, 582.4898681640625]\n",
      "\n",
      "Instance 467 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 15: [-0.7688130140304565, 1.8245904445648193, -0.4480343461036682, 1.4686135053634644, -1.5475170612335205]\n",
      "Grand sum of 402 tensor sets is: [47.28960037231445, 460.47906494140625, -43.31708908081055, -330.35302734375, 580.9423217773438]\n",
      "\n",
      "Instance 468 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 403 tensor sets is: [47.36052322387695, 462.7472839355469, -43.98426055908203, -330.207275390625, 580.8058471679688]\n",
      "\n",
      "Instance 469 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 17: [-0.15914931893348694, -1.1213958263397217, 0.08499854058027267, -2.2281696796417236, -1.1857545375823975]\n",
      "Grand sum of 404 tensor sets is: [47.20137405395508, 461.6258850097656, -43.899261474609375, -332.4354553222656, 579.6201171875]\n",
      "\n",
      "Instance 470 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 8: [0.6574286818504333, 3.0194385051727295, -0.10047688335180283, 1.1424096822738647, 1.6274151802062988]\n",
      "Grand sum of 405 tensor sets is: [47.858802795410156, 464.64532470703125, -43.99973678588867, -331.2930603027344, 581.24755859375]\n",
      "\n",
      "Instance 471 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 8: [0.21826595067977905, 0.9751220941543579, 0.6302961111068726, 0.04217822849750519, -1.1166507005691528]\n",
      "Grand sum of 406 tensor sets is: [48.07706832885742, 465.6204528808594, -43.369441986083984, -331.2508850097656, 580.1309204101562]\n",
      "\n",
      "Instance 472 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 18: [-0.6898192167282104, 1.2285830974578857, 0.5890569090843201, -1.2650409936904907, -1.7966768741607666]\n",
      "Grand sum of 407 tensor sets is: [47.38724899291992, 466.8490295410156, -42.7803840637207, -332.51593017578125, 578.334228515625]\n",
      "\n",
      "Instance 473 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 25: [0.3753299117088318, 0.8361589312553406, 0.15275083482265472, -2.133328914642334, -0.7971711754798889]\n",
      "Grand sum of 408 tensor sets is: [47.762577056884766, 467.6851806640625, -42.62763214111328, -334.6492614746094, 577.5370483398438]\n",
      "\n",
      "Instance 474 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "jumping at index 17: [0.507732629776001, 0.8290050029754639, -0.819172739982605, -1.7770025730133057, -1.1936365365982056]\n",
      "Grand sum of 409 tensor sets is: [48.27030944824219, 468.5141906738281, -43.44680404663086, -336.42626953125, 576.3433837890625]\n",
      "\n",
      "Instance 475 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 3: [-0.6137627959251404, 0.781744658946991, -0.7665266990661621, -1.7509901523590088, 2.46859073638916]\n",
      "Grand sum of 410 tensor sets is: [47.65654754638672, 469.2959289550781, -44.21332931518555, -338.17724609375, 578.8119506835938]\n",
      "\n",
      "Instance 476 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [178]\n",
      "Size of token embeddings is torch.Size([358, 13, 768])\n",
      "Shape of summed layers is: 358 x 768\n",
      "jumping at index 178: [0.8881500959396362, 0.3055417239665985, 0.614541232585907, 0.18408533930778503, 1.2056875228881836]\n",
      "Grand sum of 411 tensor sets is: [48.54469680786133, 469.6014709472656, -43.59878921508789, -337.9931640625, 580.0176391601562]\n",
      "\n",
      "Instance 477 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 18: [0.1386469304561615, 0.24434038996696472, 0.06900618970394135, 0.09423576295375824, 0.05875128507614136]\n",
      "Grand sum of 412 tensor sets is: [48.68334197998047, 469.8458251953125, -43.529781341552734, -337.89892578125, 580.076416015625]\n",
      "\n",
      "Instance 478 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 17: [-0.23041583597660065, 1.4957374334335327, -1.0819886922836304, -0.6807569265365601, 1.8914124965667725]\n",
      "Grand sum of 413 tensor sets is: [48.45292663574219, 471.341552734375, -44.61177062988281, -338.5796813964844, 581.9678344726562]\n",
      "\n",
      "Instance 479 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "jumping at index 14: [-0.9396581649780273, 0.19968456029891968, -0.34355753660202026, -1.5485084056854248, 4.014360427856445]\n",
      "Grand sum of 414 tensor sets is: [47.513267517089844, 471.5412292480469, -44.95532989501953, -340.1282043457031, 585.982177734375]\n",
      "\n",
      "Instance 480 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 3: [-0.6176549792289734, 1.0155812501907349, -0.8218780159950256, 0.06817685067653656, 5.00993537902832]\n",
      "Grand sum of 415 tensor sets is: [46.89561080932617, 472.55682373046875, -45.77720642089844, -340.0600280761719, 590.9921264648438]\n",
      "\n",
      "Instance 481 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 22: [1.0373203754425049, 0.3330126404762268, -0.6624323129653931, -0.8866279721260071, 3.320871353149414]\n",
      "Grand sum of 416 tensor sets is: [47.93292999267578, 472.88983154296875, -46.439640045166016, -340.9466552734375, 594.31298828125]\n",
      "\n",
      "Instance 482 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 13: [0.58217453956604, 1.0735939741134644, 0.24773989617824554, -0.1702514886856079, 2.207261085510254]\n",
      "Grand sum of 417 tensor sets is: [48.515106201171875, 473.96343994140625, -46.191898345947266, -341.1169128417969, 596.520263671875]\n",
      "\n",
      "Instance 483 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 20: [0.4713534712791443, 1.1849751472473145, -1.0347917079925537, -3.118577480316162, 2.8649306297302246]\n",
      "Grand sum of 418 tensor sets is: [48.98645782470703, 475.1484069824219, -47.226688385009766, -344.2355041503906, 599.3851928710938]\n",
      "\n",
      "Instance 484 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 16: [-0.3372457027435303, 0.5208417773246765, -0.8959277868270874, -1.2774791717529297, 0.6329605579376221]\n",
      "Grand sum of 419 tensor sets is: [48.64921188354492, 475.66925048828125, -48.122615814208984, -345.5129699707031, 600.0181274414062]\n",
      "\n",
      "Instance 485 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "jumping at index 5: [-0.003258185461163521, 0.9375171661376953, -0.6108846664428711, -0.5314362645149231, 2.0065736770629883]\n",
      "Grand sum of 420 tensor sets is: [48.64595413208008, 476.6067810058594, -48.73350143432617, -346.0444030761719, 602.0247192382812]\n",
      "\n",
      "Instance 486 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "jumping at index 13: [0.011737070977687836, 0.5451023578643799, -0.10462789237499237, -3.350163221359253, 3.6497621536254883]\n",
      "Grand sum of 421 tensor sets is: [48.657691955566406, 477.1518859863281, -48.838130950927734, -349.3945617675781, 605.6744995117188]\n",
      "\n",
      "Instance 487 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "jumping at index 3: [-0.9601868391036987, 2.022099018096924, -0.2257467806339264, -0.20845185220241547, 4.423677444458008]\n",
      "Grand sum of 422 tensor sets is: [47.697505950927734, 479.1739807128906, -49.06387710571289, -349.60302734375, 610.0982055664062]\n",
      "\n",
      "Instance 488 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 15: [0.9980736374855042, 0.2947922945022583, 0.2901363968849182, -2.6335010528564453, -0.11388713121414185]\n",
      "Grand sum of 423 tensor sets is: [48.695579528808594, 479.4687805175781, -48.773738861083984, -352.2365417480469, 609.9843139648438]\n",
      "\n",
      "Instance 489 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 21: [0.7718690037727356, 2.773264169692993, -0.3619028627872467, -1.7255364656448364, 0.2237861156463623]\n",
      "Grand sum of 424 tensor sets is: [49.46744918823242, 482.2420349121094, -49.135643005371094, -353.9620666503906, 610.2081298828125]\n",
      "\n",
      "Instance 490 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 491 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 425 tensor sets is: [49.53837203979492, 484.51025390625, -49.80281448364258, -353.8163146972656, 610.0716552734375]\n",
      "\n",
      "Instance 492 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 426 tensor sets is: [50.480831146240234, 484.5758972167969, -50.17308807373047, -354.28607177734375, 612.697998046875]\n",
      "\n",
      "Instance 493 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 494 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 9: [-0.46862098574638367, -0.08691218495368958, 0.7748419046401978, -1.7943437099456787, 3.2533373832702637]\n",
      "Grand sum of 427 tensor sets is: [50.012210845947266, 484.4889831542969, -49.39824676513672, -356.0804138183594, 615.9513549804688]\n",
      "\n",
      "Instance 495 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 428 tensor sets is: [49.346553802490234, 486.724609375, -50.56959915161133, -357.0016784667969, 615.0758056640625]\n",
      "\n",
      "Instance 496 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 15: [0.9116542339324951, 2.0502688884735107, -1.362322449684143, -0.0034791380167007446, 0.8063337802886963]\n",
      "Grand sum of 429 tensor sets is: [50.258209228515625, 488.7748718261719, -51.931922912597656, -357.0051574707031, 615.8821411132812]\n",
      "\n",
      "Instance 497 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 21: [0.7617359757423401, 0.983979344367981, 0.7910224199295044, 0.22759662568569183, -0.998921275138855]\n",
      "Grand sum of 430 tensor sets is: [51.01994705200195, 489.75885009765625, -51.140899658203125, -356.7775573730469, 614.8832397460938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 498 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 431 tensor sets is: [51.09086990356445, 492.0270690917969, -51.80807113647461, -356.6318054199219, 614.7467651367188]\n",
      "\n",
      "Instance 499 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 432 tensor sets is: [50.42521286010742, 494.2626953125, -52.97942352294922, -357.5530700683594, 613.8712158203125]\n",
      "\n",
      "Instance 500 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "jumping at index 27: [0.5980889201164246, 2.457974433898926, 1.5332069396972656, -0.7693079710006714, 1.0993611812591553]\n",
      "Grand sum of 433 tensor sets is: [51.02330017089844, 496.7206726074219, -51.44621658325195, -358.3223876953125, 614.9705810546875]\n",
      "\n",
      "Instance 501 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 32: [-0.1133442372083664, 1.6936848163604736, 0.2657865285873413, -1.8542299270629883, 2.594773292541504]\n",
      "Grand sum of 434 tensor sets is: [50.90995407104492, 498.41436767578125, -51.1804313659668, -360.1766052246094, 617.5653686523438]\n",
      "\n",
      "Instance 502 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 435 tensor sets is: [50.98087692260742, 500.6825866699219, -51.84760284423828, -360.0308532714844, 617.4288940429688]\n",
      "\n",
      "Instance 503 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 11: [-0.9404209852218628, 1.3332116603851318, -0.29651665687561035, -0.6964857578277588, 0.19971689581871033]\n",
      "Grand sum of 436 tensor sets is: [50.04045486450195, 502.01580810546875, -52.14411926269531, -360.7273254394531, 617.6286010742188]\n",
      "\n",
      "Instance 504 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [-0.47513747215270996, 1.1971732378005981, -0.01404598355293274, -2.416454792022705, -0.4665341377258301]\n",
      "Grand sum of 437 tensor sets is: [49.5653190612793, 503.2129821777344, -52.158164978027344, -363.1437683105469, 617.1620483398438]\n",
      "\n",
      "Instance 505 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 13: [-1.2181040048599243, 0.5366047620773315, 0.461384117603302, -0.1483810842037201, 1.989253044128418]\n",
      "Grand sum of 438 tensor sets is: [48.34721374511719, 503.74957275390625, -51.696781158447266, -363.2921447753906, 619.1513061523438]\n",
      "\n",
      "Instance 506 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 21: [-0.3690270781517029, 0.6089944839477539, 0.37507274746894836, -1.3476811647415161, 0.4101061224937439]\n",
      "Grand sum of 439 tensor sets is: [47.978187561035156, 504.35858154296875, -51.32170867919922, -364.63983154296875, 619.5614013671875]\n",
      "\n",
      "Instance 507 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 508 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 3: [-0.4532877802848816, 1.488753318786621, -0.573617160320282, -1.487159013748169, 2.4298830032348633]\n",
      "Grand sum of 440 tensor sets is: [47.524898529052734, 505.8473205566406, -51.89532470703125, -366.1269836425781, 621.9912719726562]\n",
      "\n",
      "Instance 509 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 15: [0.7082720994949341, 0.1087665930390358, -1.198645830154419, -2.3023359775543213, 3.9410338401794434]\n",
      "Grand sum of 441 tensor sets is: [48.23316955566406, 505.9560852050781, -53.093971252441406, -368.4293212890625, 625.9323120117188]\n",
      "\n",
      "Instance 510 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "jumping at index 8: [0.34598228335380554, 0.19110192358493805, 0.9019846320152283, -1.18418550491333, 2.002478837966919]\n",
      "Grand sum of 442 tensor sets is: [48.57915115356445, 506.1471862792969, -52.191986083984375, -369.6134948730469, 627.934814453125]\n",
      "\n",
      "Instance 511 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([193, 13, 768])\n",
      "Shape of summed layers is: 193 x 768\n",
      "jumping at index 27: [0.22805579006671906, 0.1837853640317917, 2.0763676166534424, -2.245046377182007, 1.9957702159881592]\n",
      "Grand sum of 443 tensor sets is: [48.80720520019531, 506.3309631347656, -50.11561965942383, -371.8585510253906, 629.9306030273438]\n",
      "\n",
      "Instance 512 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 15: [1.2867498397827148, 1.5023304224014282, -1.9367573261260986, -1.9633769989013672, 1.987320899963379]\n",
      "Grand sum of 444 tensor sets is: [50.093955993652344, 507.8332824707031, -52.05237579345703, -373.8219299316406, 631.9179077148438]\n",
      "\n",
      "Instance 513 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "jumping at index 27: [1.259615421295166, 2.5040576457977295, -1.0272173881530762, -1.1464414596557617, -0.4818947911262512]\n",
      "Grand sum of 445 tensor sets is: [51.353572845458984, 510.33734130859375, -53.079593658447266, -374.9683837890625, 631.43603515625]\n",
      "\n",
      "Instance 514 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 10: [0.37560632824897766, 3.608839750289917, -0.4934995174407959, 1.6822770833969116, 1.9120714664459229]\n",
      "Grand sum of 446 tensor sets is: [51.72917938232422, 513.9461669921875, -53.57309341430664, -373.2861022949219, 633.3480834960938]\n",
      "\n",
      "Instance 515 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 8: [0.6398435831069946, 1.2639909982681274, -0.8479986190795898, 1.4572721719741821, -1.2677658796310425]\n",
      "Grand sum of 447 tensor sets is: [52.369022369384766, 515.2101440429688, -54.42109298706055, -371.8288269042969, 632.080322265625]\n",
      "\n",
      "Instance 516 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [-0.45801663398742676, -0.9925666451454163, 0.12019787728786469, -2.552450656890869, 2.2982001304626465]\n",
      "Grand sum of 448 tensor sets is: [51.911006927490234, 514.2175903320312, -54.30089569091797, -374.38128662109375, 634.3785400390625]\n",
      "\n",
      "Instance 517 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 5: [1.2584494352340698, 0.08207134902477264, -1.471253514289856, -2.004434585571289, 2.2799265384674072]\n",
      "Grand sum of 449 tensor sets is: [53.169456481933594, 514.2996826171875, -55.77214813232422, -376.3857116699219, 636.658447265625]\n",
      "\n",
      "Instance 518 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 519 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 520 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "jumping at index 32: [0.8322970867156982, 2.60025691986084, 0.05396302789449692, -0.6246660351753235, 2.890263795852661]\n",
      "Grand sum of 450 tensor sets is: [54.00175476074219, 516.8999633789062, -55.71818542480469, -377.0103759765625, 639.5487060546875]\n",
      "\n",
      "Instance 521 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 10: [0.40876418352127075, 1.4453545808792114, 0.4293631911277771, -1.0986448526382446, 1.8530449867248535]\n",
      "Grand sum of 451 tensor sets is: [54.410518646240234, 518.3453369140625, -55.288822174072266, -378.1090087890625, 641.4017333984375]\n",
      "\n",
      "Instance 522 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 32: [0.1503613293170929, 1.985371470451355, 1.3540946245193481, -0.8115835785865784, 1.9047222137451172]\n",
      "Grand sum of 452 tensor sets is: [54.56087875366211, 520.3306884765625, -53.93472671508789, -378.92059326171875, 643.3064575195312]\n",
      "\n",
      "Instance 523 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "jumping at index 29: [0.8247905969619751, 0.829727053642273, 2.2230865955352783, -0.29282262921333313, 0.8198807835578918]\n",
      "Grand sum of 453 tensor sets is: [55.38566970825195, 521.160400390625, -51.711639404296875, -379.2134094238281, 644.1263427734375]\n",
      "\n",
      "Instance 524 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 28: [0.13635331392288208, 0.7542291879653931, -0.19848626852035522, -1.0856671333312988, -1.4151092767715454]\n",
      "Grand sum of 454 tensor sets is: [55.52202224731445, 521.9146118164062, -51.910125732421875, -380.299072265625, 642.7112426757812]\n",
      "\n",
      "Instance 525 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 10: [0.6906849145889282, 0.9834524989128113, -0.16834811866283417, 0.36109232902526855, 1.1231926679611206]\n",
      "Grand sum of 455 tensor sets is: [56.21270751953125, 522.8980712890625, -52.07847213745117, -379.93798828125, 643.8344116210938]\n",
      "\n",
      "Instance 526 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 527 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 15: [-0.7233967781066895, 1.0848630666732788, -0.9290522336959839, -0.8288624882698059, -0.1333564966917038]\n",
      "Grand sum of 456 tensor sets is: [55.48931121826172, 523.98291015625, -53.00752258300781, -380.766845703125, 643.7010498046875]\n",
      "\n",
      "Instance 528 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 529 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 4: [-0.12242195010185242, 3.1608970165252686, -0.742293655872345, -0.6167941689491272, -0.4478747546672821]\n",
      "Grand sum of 457 tensor sets is: [55.36688995361328, 527.143798828125, -53.74981689453125, -381.3836364746094, 643.253173828125]\n",
      "\n",
      "Instance 530 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 5: [-0.2856781482696533, 0.8299692869186401, -1.2213295698165894, -0.6824219226837158, -0.14937853813171387]\n",
      "Grand sum of 458 tensor sets is: [55.08121109008789, 527.9737548828125, -54.97114562988281, -382.0660705566406, 643.1038208007812]\n",
      "\n",
      "Instance 531 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "jumping at index 49: [0.2361944615840912, 0.8255914449691772, 1.558007001876831, -2.3282737731933594, 2.235154151916504]\n",
      "Grand sum of 459 tensor sets is: [55.317405700683594, 528.79931640625, -53.41313934326172, -384.39434814453125, 645.3389892578125]\n",
      "\n",
      "Instance 532 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 21: [-0.3690270781517029, 0.6089944839477539, 0.37507274746894836, -1.3476811647415161, 0.4101061224937439]\n",
      "Grand sum of 460 tensor sets is: [54.94837951660156, 529.4083251953125, -53.03806686401367, -385.7420349121094, 645.7490844726562]\n",
      "\n",
      "Instance 533 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 19: [-0.06789587438106537, 1.8065898418426514, -0.39498525857925415, 0.9822128415107727, 0.8224893808364868]\n",
      "Grand sum of 461 tensor sets is: [54.88048553466797, 531.2149047851562, -53.43305206298828, -384.75982666015625, 646.5715942382812]\n",
      "\n",
      "Instance 534 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 535 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 27: [0.31335633993148804, 1.2122870683670044, -0.51052325963974, -0.8241062760353088, 1.0282716751098633]\n",
      "Grand sum of 462 tensor sets is: [55.19384002685547, 532.4271850585938, -53.94357681274414, -385.58392333984375, 647.599853515625]\n",
      "\n",
      "Instance 536 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "jumping at index 37: [0.6931467056274414, -0.07643603533506393, -0.4240565896034241, -0.5438852906227112, 2.833073616027832]\n",
      "Grand sum of 463 tensor sets is: [55.886985778808594, 532.3507690429688, -54.36763381958008, -386.1278076171875, 650.4329223632812]\n",
      "\n",
      "Instance 537 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 25: [0.366245299577713, 1.3865324258804321, 0.2946639060974121, -0.821914792060852, -2.1762912273406982]\n",
      "Grand sum of 464 tensor sets is: [56.253231048583984, 533.7373046875, -54.07297134399414, -386.9497375488281, 648.2566528320312]\n",
      "\n",
      "Instance 538 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 10: [0.06360933929681778, 1.0550849437713623, 0.07189503312110901, -0.40750712156295776, 0.7623006105422974]\n",
      "Grand sum of 465 tensor sets is: [56.31684112548828, 534.7924194335938, -54.001075744628906, -387.35723876953125, 649.0189819335938]\n",
      "\n",
      "Instance 539 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 30: [0.7810582518577576, 1.3950201272964478, 0.7973799705505371, 0.1125517413020134, 1.5685694217681885]\n",
      "Grand sum of 466 tensor sets is: [57.097900390625, 536.1874389648438, -53.203697204589844, -387.24468994140625, 650.5875244140625]\n",
      "\n",
      "Instance 540 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 541 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 5: [-0.05880635231733322, 0.5244035720825195, -1.6817277669906616, -1.2514073848724365, -0.13945361971855164]\n",
      "Grand sum of 467 tensor sets is: [57.039093017578125, 536.7118530273438, -54.88542556762695, -388.49609375, 650.4480590820312]\n",
      "\n",
      "Instance 542 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 7: [-0.2999269366264343, 0.7618333101272583, 0.45763611793518066, -1.0844483375549316, 0.6465109586715698]\n",
      "Grand sum of 468 tensor sets is: [56.739166259765625, 537.4736938476562, -54.42778778076172, -389.5805358886719, 651.0945434570312]\n",
      "\n",
      "Instance 543 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 18: [0.17627504467964172, 1.519415020942688, 0.030483238399028778, -2.5828685760498047, -0.2133426070213318]\n",
      "Grand sum of 469 tensor sets is: [56.91543960571289, 538.9931030273438, -54.39730453491211, -392.16339111328125, 650.8812255859375]\n",
      "\n",
      "Instance 544 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 5: [1.0817692279815674, 1.2930623292922974, -0.5074647665023804, -1.7865616083145142, 2.591181993484497]\n",
      "Grand sum of 470 tensor sets is: [57.99720764160156, 540.2861938476562, -54.90476989746094, -393.949951171875, 653.472412109375]\n",
      "\n",
      "Instance 545 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 8: [0.3195161819458008, 1.370841145515442, -0.48696407675743103, 0.15748047828674316, 1.1803944110870361]\n",
      "Grand sum of 471 tensor sets is: [58.31672286987305, 541.6570434570312, -55.3917350769043, -393.79248046875, 654.65283203125]\n",
      "\n",
      "Instance 546 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 28: [-0.011298546567559242, 1.116099238395691, 0.21288913488388062, -2.187828540802002, -0.535056471824646]\n",
      "Grand sum of 472 tensor sets is: [58.305423736572266, 542.7731323242188, -55.1788444519043, -395.9803161621094, 654.1177978515625]\n",
      "\n",
      "Instance 547 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 9: [0.06890272349119186, 2.4441182613372803, -1.383460283279419, 1.511051058769226, -0.8887498378753662]\n",
      "Grand sum of 473 tensor sets is: [58.374324798583984, 545.2172241210938, -56.56230545043945, -394.4692687988281, 653.2290649414062]\n",
      "\n",
      "Instance 548 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 45: [-0.15476387739181519, 1.7557933330535889, 0.8504877686500549, 0.2611792981624603, -0.25059974193573]\n",
      "Grand sum of 474 tensor sets is: [58.21956253051758, 546.9730224609375, -55.71181869506836, -394.2080993652344, 652.9784545898438]\n",
      "\n",
      "Instance 549 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([70, 13, 768])\n",
      "Shape of summed layers is: 70 x 768\n",
      "jumping at index 26: [0.40516379475593567, 1.2930289506912231, 1.2107336521148682, -1.6328039169311523, 1.3744447231292725]\n",
      "Grand sum of 475 tensor sets is: [58.624725341796875, 548.2660522460938, -54.50108337402344, -395.8409118652344, 654.3529052734375]\n",
      "\n",
      "Instance 550 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 12: [0.9648904800415039, 1.0379713773727417, -0.23671703040599823, 1.406444787979126, 1.0361661911010742]\n",
      "Grand sum of 476 tensor sets is: [59.58961486816406, 549.3040161132812, -54.73780059814453, -394.4344787597656, 655.3890991210938]\n",
      "\n",
      "Instance 551 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 477 tensor sets is: [60.695640563964844, 550.242431640625, -55.34127426147461, -394.7359619140625, 657.30908203125]\n",
      "\n",
      "Instance 552 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 10: [0.03979998826980591, 0.8407474756240845, 0.4120742678642273, -0.7345166206359863, 1.1916065216064453]\n",
      "Grand sum of 478 tensor sets is: [60.73543930053711, 551.0831909179688, -54.92919921875, -395.4704895019531, 658.5006713867188]\n",
      "\n",
      "Instance 553 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 14: [0.1920614391565323, 1.3378868103027344, 1.724614143371582, -1.1135865449905396, 1.1592721939086914]\n",
      "Grand sum of 479 tensor sets is: [60.9275016784668, 552.4210815429688, -53.204586029052734, -396.5840759277344, 659.6599731445312]\n",
      "\n",
      "Instance 554 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 6: [-0.8489975333213806, 0.5571465492248535, 0.22664006054401398, -1.475115180015564, 1.6467523574829102]\n",
      "Grand sum of 480 tensor sets is: [60.0785026550293, 552.9782104492188, -52.97794723510742, -398.0592041015625, 661.3067016601562]\n",
      "\n",
      "Instance 555 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 13: [0.5071360468864441, 1.7205870151519775, -0.36368584632873535, -2.2541635036468506, 0.652209997177124]\n",
      "Grand sum of 481 tensor sets is: [60.58563995361328, 554.6987915039062, -53.34163284301758, -400.3133544921875, 661.9589233398438]\n",
      "\n",
      "Instance 556 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 23: [0.6795129776000977, 1.904273271560669, 0.3472420871257782, -3.1934118270874023, -0.5946508049964905]\n",
      "Grand sum of 482 tensor sets is: [61.26515197753906, 556.6030883789062, -52.99439239501953, -403.50677490234375, 661.3642578125]\n",
      "\n",
      "Instance 557 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "jumping at index 3: [-0.08956360071897507, 0.3564414083957672, -1.471219539642334, 1.3423619270324707, 1.611574411392212]\n",
      "Grand sum of 483 tensor sets is: [61.17558670043945, 556.9595336914062, -54.46561050415039, -402.1643981933594, 662.975830078125]\n",
      "\n",
      "Instance 558 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 9: [-0.9958388209342957, 1.7592986822128296, 0.10672171413898468, 1.3108099699020386, -1.5949057340621948]\n",
      "Grand sum of 484 tensor sets is: [60.17974853515625, 558.7188110351562, -54.358890533447266, -400.85357666015625, 661.3809204101562]\n",
      "\n",
      "Instance 559 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12, 38]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 12: [0.21665692329406738, -0.44917505979537964, 0.6656479239463806, -0.2941742539405823, 0.2589629590511322]\n",
      "jumping at index 38: [0.44575947523117065, 1.0578687191009521, 2.0079147815704346, -0.5437869429588318, 0.8373279571533203]\n",
      "Grand sum of 485 tensor sets is: [60.510955810546875, 559.0231323242188, -53.02210998535156, -401.2725524902344, 661.9290771484375]\n",
      "\n",
      "Instance 560 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 13: [0.4827694892883301, 1.686653733253479, -0.156693696975708, -0.726878821849823, 2.5696334838867188]\n",
      "Grand sum of 486 tensor sets is: [60.99372482299805, 560.7097778320312, -53.178802490234375, -401.9994201660156, 664.4987182617188]\n",
      "\n",
      "Instance 561 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 562 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 563 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 564 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 487 tensor sets is: [61.06464767456055, 562.97802734375, -53.84597396850586, -401.8536682128906, 664.3622436523438]\n",
      "\n",
      "Instance 565 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "jumping at index 40: [0.14695711433887482, 0.25779712200164795, 0.48320919275283813, -1.156320571899414, 3.0227527618408203]\n",
      "Grand sum of 488 tensor sets is: [61.211605072021484, 563.23583984375, -53.36276626586914, -403.0099792480469, 667.385009765625]\n",
      "\n",
      "Instance 566 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 489 tensor sets is: [62.1540641784668, 563.3014526367188, -53.73303985595703, -403.479736328125, 670.0113525390625]\n",
      "\n",
      "Instance 567 of jumping.\n",
      "Looking for vocab token: jumping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([88, 13, 768])\n",
      "Shape of summed layers is: 88 x 768\n",
      "jumping at index 21: [-0.3245994746685028, 2.333422899246216, 1.1247057914733887, 0.5039874911308289, 1.0143842697143555]\n",
      "Grand sum of 490 tensor sets is: [61.829463958740234, 565.6348876953125, -52.608333587646484, -402.9757385253906, 671.0257568359375]\n",
      "\n",
      "Instance 568 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 26: [-0.42238935828208923, 0.5552464127540588, -0.7650701999664307, -2.3228096961975098, 2.3747477531433105]\n",
      "Grand sum of 491 tensor sets is: [61.407073974609375, 566.1901245117188, -53.37340545654297, -405.2985534667969, 673.4005126953125]\n",
      "\n",
      "Instance 569 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 8: [0.7695786952972412, 1.6677597761154175, -1.409134030342102, -0.8242355585098267, 1.5043379068374634]\n",
      "Grand sum of 492 tensor sets is: [62.17665100097656, 567.85791015625, -54.78253936767578, -406.122802734375, 674.9048461914062]\n",
      "\n",
      "Instance 570 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 28: [-0.9462165832519531, 2.024439811706543, 0.8879432082176208, 0.4038691818714142, -1.0476324558258057]\n",
      "Grand sum of 493 tensor sets is: [61.23043441772461, 569.88232421875, -53.894596099853516, -405.71893310546875, 673.8572387695312]\n",
      "\n",
      "Instance 571 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 5: [-0.2813948094844818, 1.3490266799926758, 0.3217652142047882, -0.8482803702354431, 1.04289972782135]\n",
      "Grand sum of 494 tensor sets is: [60.949039459228516, 571.2313232421875, -53.57283020019531, -406.56719970703125, 674.900146484375]\n",
      "\n",
      "Instance 572 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 20: [0.623625636100769, 0.8757541179656982, 0.29376739263534546, -1.909332513809204, 1.6750247478485107]\n",
      "Grand sum of 495 tensor sets is: [61.57266616821289, 572.1070556640625, -53.2790641784668, -408.4765319824219, 676.5751953125]\n",
      "\n",
      "Instance 573 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "jumping at index 48: [0.044173747301101685, 1.6423026323318481, 0.30019715428352356, -1.2085715532302856, 1.4189908504486084]\n",
      "Grand sum of 496 tensor sets is: [61.61684036254883, 573.7493286132812, -52.97886657714844, -409.6850891113281, 677.9942016601562]\n",
      "\n",
      "Instance 574 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 575 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 6: [-0.3194109797477722, 0.4454615116119385, 0.7153238654136658, -1.2001971006393433, 0.6943337917327881]\n",
      "Grand sum of 497 tensor sets is: [61.297428131103516, 574.1947631835938, -52.26354217529297, -410.8852844238281, 678.6885375976562]\n",
      "\n",
      "Instance 576 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 16: [0.618104100227356, 3.1389126777648926, 0.7841926217079163, -1.27850341796875, -0.7227831482887268]\n",
      "Grand sum of 498 tensor sets is: [61.915531158447266, 577.3336791992188, -51.47935104370117, -412.1637878417969, 677.9657592773438]\n",
      "\n",
      "Instance 577 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 29: [0.4677334129810333, 0.2794056832790375, 0.5418503880500793, -0.4395442605018616, 3.2915420532226562]\n",
      "Grand sum of 499 tensor sets is: [62.38326644897461, 577.6130981445312, -50.9375, -412.60333251953125, 681.25732421875]\n",
      "\n",
      "Instance 578 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 16: [0.04637395963072777, 0.20675864815711975, -0.14388243854045868, -0.3705325722694397, 1.9591120481491089]\n",
      "Grand sum of 500 tensor sets is: [62.42964172363281, 577.8198852539062, -51.081382751464844, -412.973876953125, 683.2164306640625]\n",
      "\n",
      "Instance 579 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 16: [-0.25965893268585205, 0.9811601638793945, 0.35886088013648987, -0.6556838154792786, -2.4393768310546875]\n",
      "Grand sum of 501 tensor sets is: [62.16998291015625, 578.801025390625, -50.7225227355957, -413.6295471191406, 680.7770385742188]\n",
      "\n",
      "Instance 580 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3, 10, 17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 3: [-0.17034338414669037, 2.7018625736236572, 1.2240403890609741, -0.6846176981925964, 3.1626124382019043]\n",
      "jumping at index 10: [-0.2818530797958374, 3.519939661026001, 1.1440834999084473, -0.6537734866142273, 2.5399932861328125]\n",
      "jumping at index 17: [-0.3136737644672394, 3.4380433559417725, 1.5041412115097046, -1.26947021484375, 0.5970445275306702]\n",
      "Grand sum of 502 tensor sets is: [61.91469192504883, 582.02099609375, -49.431766510009766, -414.49884033203125, 682.8768920898438]\n",
      "\n",
      "Instance 581 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 582 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 12: [1.0344082117080688, 0.7891104817390442, -1.5418812036514282, -1.8397482633590698, 2.197208881378174]\n",
      "Grand sum of 503 tensor sets is: [62.949100494384766, 582.8101196289062, -50.97364807128906, -416.3385925292969, 685.0740966796875]\n",
      "\n",
      "Instance 583 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 19: [-0.5431351065635681, 0.8182930946350098, 0.1370011270046234, -1.79579758644104, 2.0890893936157227]\n",
      "Grand sum of 504 tensor sets is: [62.40596389770508, 583.62841796875, -50.836647033691406, -418.1343994140625, 687.1632080078125]\n",
      "\n",
      "Instance 584 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 26: [0.1144317016005516, 0.5382003784179688, -0.6033670902252197, 1.1935975551605225, 1.3774161338806152]\n",
      "Grand sum of 505 tensor sets is: [62.5203971862793, 584.1666259765625, -51.44001388549805, -416.9407958984375, 688.5406494140625]\n",
      "\n",
      "Instance 585 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 14: [0.6503039002418518, 0.9430342316627502, -0.17698165774345398, -2.8693740367889404, 2.2470202445983887]\n",
      "Grand sum of 506 tensor sets is: [63.17070007324219, 585.1096801757812, -51.61699676513672, -419.8101806640625, 690.7876586914062]\n",
      "\n",
      "Instance 586 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 12: [0.03668082505464554, 0.44488054513931274, 0.4568869471549988, -0.1970619261264801, 1.0276637077331543]\n",
      "Grand sum of 507 tensor sets is: [63.20738220214844, 585.5545654296875, -51.16011047363281, -420.0072326660156, 691.8153076171875]\n",
      "\n",
      "Instance 587 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "jumping at index 8: [-0.06541421264410019, 0.9601256847381592, 0.2545764744281769, -1.710065245628357, 2.762267589569092]\n",
      "Grand sum of 508 tensor sets is: [63.1419677734375, 586.5147094726562, -50.90553283691406, -421.71728515625, 694.5775756835938]\n",
      "\n",
      "Instance 588 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 509 tensor sets is: [62.47631072998047, 588.7503662109375, -52.07688522338867, -422.6385498046875, 693.7020263671875]\n",
      "\n",
      "Instance 589 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 39: [0.2464001476764679, 1.802355170249939, -0.5162352919578552, -1.8679118156433105, 2.181856632232666]\n",
      "Grand sum of 510 tensor sets is: [62.72270965576172, 590.552734375, -52.59312057495117, -424.5064697265625, 695.8839111328125]\n",
      "\n",
      "Instance 590 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 591 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 592 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 9: [0.39121970534324646, 1.1900920867919922, 0.4300711452960968, -1.2445728778839111, 1.8124983310699463]\n",
      "Grand sum of 511 tensor sets is: [63.113929748535156, 591.7427978515625, -52.163047790527344, -425.75103759765625, 697.6964111328125]\n",
      "\n",
      "Instance 593 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 8: [0.28366267681121826, 0.6469993591308594, 0.7806885242462158, -2.4956822395324707, -1.5827734470367432]\n",
      "Grand sum of 512 tensor sets is: [63.39759063720703, 592.3897705078125, -51.38235855102539, -428.2467346191406, 696.1136474609375]\n",
      "\n",
      "Instance 594 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 5: [-0.033455267548561096, 0.9363731145858765, 0.5300517678260803, -0.4060046374797821, 0.571731686592102]\n",
      "Grand sum of 513 tensor sets is: [63.3641357421875, 593.326171875, -50.8523063659668, -428.6527404785156, 696.6853637695312]\n",
      "\n",
      "Instance 595 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 18: [0.4006098210811615, 2.2538938522338867, 0.36684924364089966, -1.7493938207626343, 3.218351125717163]\n",
      "Grand sum of 514 tensor sets is: [63.76474380493164, 595.580078125, -50.48545837402344, -430.4021301269531, 699.9036865234375]\n",
      "\n",
      "Instance 596 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 515 tensor sets is: [63.83566665649414, 597.8483276367188, -51.15262985229492, -430.2563781738281, 699.7672119140625]\n",
      "\n",
      "Instance 597 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 16: [0.8916189670562744, 1.4723405838012695, -0.2798442244529724, -0.6044361591339111, 1.8632020950317383]\n",
      "Grand sum of 516 tensor sets is: [64.72728729248047, 599.3206787109375, -51.432472229003906, -430.8608093261719, 701.6304321289062]\n",
      "\n",
      "Instance 598 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 9: [0.5621753931045532, 0.21783074736595154, -1.0344760417938232, -0.9842127561569214, 4.31997537612915]\n",
      "Grand sum of 517 tensor sets is: [65.28945922851562, 599.5385131835938, -52.466949462890625, -431.84503173828125, 705.9503784179688]\n",
      "\n",
      "Instance 599 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 11: [-0.31638965010643005, 2.6393611431121826, 0.8669160008430481, -1.5598551034927368, 3.032790184020996]\n",
      "Grand sum of 518 tensor sets is: [64.97306823730469, 602.1778564453125, -51.600032806396484, -433.4048767089844, 708.983154296875]\n",
      "\n",
      "Instance 600 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 601 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 13: [0.3978334069252014, 1.2572133541107178, 0.1472611427307129, -2.1559650897979736, 4.0010085105896]\n",
      "Grand sum of 519 tensor sets is: [65.37090301513672, 603.43505859375, -51.4527702331543, -435.56085205078125, 712.9841918945312]\n",
      "\n",
      "Instance 602 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 16: [7.090717554092407e-05, 0.8243117332458496, -0.019730806350708008, -1.9268070459365845, -0.5992358922958374]\n",
      "Grand sum of 520 tensor sets is: [65.3709716796875, 604.2593994140625, -51.47249984741211, -437.4876708984375, 712.3849487304688]\n",
      "\n",
      "Instance 603 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 9: [0.43863171339035034, 1.9573334455490112, 0.3702397048473358, 1.1997441053390503, 1.8223700523376465]\n",
      "Grand sum of 521 tensor sets is: [65.80960083007812, 606.2167358398438, -51.10226058959961, -436.2879333496094, 714.2073364257812]\n",
      "\n",
      "Instance 604 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 8: [0.3636462688446045, 1.596794843673706, -0.13381262123584747, -0.8808067440986633, -1.2105835676193237]\n",
      "Grand sum of 522 tensor sets is: [66.17324829101562, 607.8135375976562, -51.2360725402832, -437.1687316894531, 712.9967651367188]\n",
      "\n",
      "Instance 605 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "jumping at index 38: [0.29628318548202515, 2.995112180709839, -1.788697361946106, -1.2922747135162354, 3.2591066360473633]\n",
      "Grand sum of 523 tensor sets is: [66.46952819824219, 610.8086547851562, -53.0247688293457, -438.46099853515625, 716.255859375]\n",
      "\n",
      "Instance 606 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [66]\n",
      "Size of token embeddings is torch.Size([132, 13, 768])\n",
      "Shape of summed layers is: 132 x 768\n",
      "jumping at index 66: [1.756547451019287, 0.9054629802703857, 0.2213851809501648, -0.36098840832710266, 0.9731219410896301]\n",
      "Grand sum of 524 tensor sets is: [68.22607421875, 611.714111328125, -52.803382873535156, -438.8219909667969, 717.22900390625]\n",
      "\n",
      "Instance 607 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "jumping at index 29: [1.3658008575439453, 0.36943817138671875, -1.091871738433838, 0.506424605846405, 3.398524761199951]\n",
      "Grand sum of 525 tensor sets is: [69.59187316894531, 612.0835571289062, -53.89525604248047, -438.3155517578125, 720.6275024414062]\n",
      "\n",
      "Instance 608 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 11: [0.8716813921928406, 0.931671679019928, -0.20241400599479675, -0.762059211730957, 2.7271761894226074]\n",
      "Grand sum of 526 tensor sets is: [70.46355438232422, 613.0152587890625, -54.09767150878906, -439.0776062011719, 723.3546752929688]\n",
      "\n",
      "Instance 609 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [480]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 480: [1.3795037269592285, 1.0536975860595703, 0.6297832727432251, -1.5170445442199707, 6.460476875305176]\n",
      "Grand sum of 527 tensor sets is: [71.84305572509766, 614.0689697265625, -53.46788787841797, -440.59466552734375, 729.8151245117188]\n",
      "\n",
      "Instance 610 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 10: [-0.7057556509971619, 1.3948158025741577, -0.3837472200393677, -1.0658704042434692, 1.9858787059783936]\n",
      "Grand sum of 528 tensor sets is: [71.13729858398438, 615.4638061523438, -53.85163497924805, -441.6605224609375, 731.801025390625]\n",
      "\n",
      "Instance 611 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 612 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 6: [0.1519475281238556, 2.0216879844665527, -1.0106357336044312, 1.2319797277450562, 1.2269713878631592]\n",
      "Grand sum of 529 tensor sets is: [71.28924560546875, 617.4854736328125, -54.86227035522461, -440.42852783203125, 733.0280151367188]\n",
      "\n",
      "Instance 613 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 20: [0.5943713784217834, 1.091870665550232, 1.8711761236190796, -1.5461626052856445, 0.9274155497550964]\n",
      "Grand sum of 530 tensor sets is: [71.88361358642578, 618.5773315429688, -52.991092681884766, -441.9747009277344, 733.9554443359375]\n",
      "\n",
      "Instance 614 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "jumping at index 27: [-0.10241074860095978, 1.5796289443969727, -0.45565325021743774, 0.6306630373001099, 0.35171571373939514]\n",
      "Grand sum of 531 tensor sets is: [71.78120422363281, 620.156982421875, -53.446746826171875, -441.3440246582031, 734.3071899414062]\n",
      "\n",
      "Instance 615 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 14: [0.9318267107009888, 1.7157783508300781, 0.23885612189769745, -2.0368146896362305, 1.231608271598816]\n",
      "Grand sum of 532 tensor sets is: [72.71302795410156, 621.8727416992188, -53.207889556884766, -443.3808288574219, 735.538818359375]\n",
      "\n",
      "Instance 616 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 533 tensor sets is: [72.04737091064453, 624.1083984375, -54.379241943359375, -444.3020935058594, 734.6632690429688]\n",
      "\n",
      "Instance 617 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 618 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 11: [0.9587279558181763, 0.0902765542268753, 0.3971436619758606, 0.580862820148468, 0.44726794958114624]\n",
      "Grand sum of 534 tensor sets is: [73.00609588623047, 624.1986694335938, -53.98209762573242, -443.7212219238281, 735.1105346679688]\n",
      "\n",
      "Instance 619 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 620 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 15: [0.81144779920578, 0.9064406752586365, -1.2644243240356445, -1.4379044771194458, 0.7403135895729065]\n",
      "Grand sum of 535 tensor sets is: [73.81754302978516, 625.1051025390625, -55.24652099609375, -445.15911865234375, 735.850830078125]\n",
      "\n",
      "Instance 621 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 21: [0.09804414212703705, 1.0451117753982544, -0.12348753213882446, -0.188088521361351, 0.4855348467826843]\n",
      "Grand sum of 536 tensor sets is: [73.91558837890625, 626.1502075195312, -55.37001037597656, -445.3471984863281, 736.3363647460938]\n",
      "\n",
      "Instance 622 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 623 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "jumping at index 22: [0.3773754835128784, 1.059276819229126, -0.8481236696243286, -3.577765703201294, 5.145302772521973]\n",
      "Grand sum of 537 tensor sets is: [74.29296112060547, 627.20947265625, -56.218135833740234, -448.9249572753906, 741.481689453125]\n",
      "\n",
      "Instance 624 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 8: [-0.3512381911277771, 0.5889648795127869, 0.21263626217842102, -1.2544150352478027, 2.1352455615997314]\n",
      "Grand sum of 538 tensor sets is: [73.94172668457031, 627.7984619140625, -56.00550079345703, -450.17938232421875, 743.616943359375]\n",
      "\n",
      "Instance 625 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 5: [1.186049222946167, 2.5387156009674072, -1.4036777019500732, -1.795129418373108, 0.9076576232910156]\n",
      "Grand sum of 539 tensor sets is: [75.12777709960938, 630.337158203125, -57.4091796875, -451.9745178222656, 744.5245971679688]\n",
      "\n",
      "Instance 626 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 627 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 540 tensor sets is: [74.46212005615234, 632.5728149414062, -58.58053207397461, -452.8957824707031, 743.6490478515625]\n",
      "\n",
      "Instance 628 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 13: [-0.5174090266227722, 1.6086159944534302, -0.0302891843020916, 0.9521172046661377, 0.9784283638000488]\n",
      "Grand sum of 541 tensor sets is: [73.94470977783203, 634.1814575195312, -58.61082077026367, -451.94366455078125, 744.6275024414062]\n",
      "\n",
      "Instance 629 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [-0.6738111972808838, 1.8261176347732544, -0.16661083698272705, -2.086642265319824, 3.3058290481567383]\n",
      "Grand sum of 542 tensor sets is: [73.2708969116211, 636.007568359375, -58.77743148803711, -454.0303039550781, 747.933349609375]\n",
      "\n",
      "Instance 630 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 11: [1.0533418655395508, 1.0029748678207397, 0.002351216971874237, -0.17362888157367706, 1.3529125452041626]\n",
      "Grand sum of 543 tensor sets is: [74.3242416381836, 637.0105590820312, -58.775081634521484, -454.20391845703125, 749.2862548828125]\n",
      "\n",
      "Instance 631 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 5: [0.752097487449646, 0.28910166025161743, -0.48430049419403076, -2.7297773361206055, 1.2552284002304077]\n",
      "Grand sum of 544 tensor sets is: [75.07633972167969, 637.2996826171875, -59.25938034057617, -456.9336853027344, 750.54150390625]\n",
      "\n",
      "Instance 632 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "jumping at index 6: [1.0179640054702759, 0.22594770789146423, 0.5465586185455322, -0.21392551064491272, -0.5984818935394287]\n",
      "Grand sum of 545 tensor sets is: [76.09430694580078, 637.525634765625, -58.71282196044922, -457.1476135253906, 749.9429931640625]\n",
      "\n",
      "Instance 633 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 10: [-0.5549705624580383, 0.9383563995361328, -1.0433672666549683, -1.9000530242919922, 3.4019625186920166]\n",
      "Grand sum of 546 tensor sets is: [75.53933715820312, 638.4639892578125, -59.756187438964844, -459.04766845703125, 753.344970703125]\n",
      "\n",
      "Instance 634 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "jumping at index 2: [0.09421185404062271, 1.4450531005859375, 0.4424070715904236, -0.22370056807994843, 0.6335158348083496]\n",
      "Grand sum of 547 tensor sets is: [75.63355255126953, 639.9090576171875, -59.31378173828125, -459.2713623046875, 753.978515625]\n",
      "\n",
      "Instance 635 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 5: [-0.3438323736190796, 0.8824130296707153, -0.1750958114862442, -1.6159868240356445, 1.5929253101348877]\n",
      "Grand sum of 548 tensor sets is: [75.28971862792969, 640.7914428710938, -59.48887634277344, -460.8873596191406, 755.5714111328125]\n",
      "\n",
      "Instance 636 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "jumping at index 4: [-0.34915977716445923, 1.7563753128051758, -0.03190874680876732, -0.38795405626296997, 1.2782108783721924]\n",
      "Grand sum of 549 tensor sets is: [74.94055938720703, 642.5477905273438, -59.52078628540039, -461.2752990722656, 756.849609375]\n",
      "\n",
      "Instance 637 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "jumping at index 2: [0.11349363625049591, 1.7226794958114624, 0.362892210483551, -2.6617279052734375, 2.581666946411133]\n",
      "Grand sum of 550 tensor sets is: [75.0540542602539, 644.2704467773438, -59.157894134521484, -463.93701171875, 759.4312744140625]\n",
      "\n",
      "Instance 638 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 551 tensor sets is: [75.1249771118164, 646.5386962890625, -59.82506561279297, -463.791259765625, 759.2947998046875]\n",
      "\n",
      "Instance 639 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 11: [1.5169346332550049, 1.2707178592681885, -0.042192719876766205, -1.5398794412612915, 1.5790891647338867]\n",
      "Grand sum of 552 tensor sets is: [76.64191436767578, 647.8093872070312, -59.86725997924805, -465.3311462402344, 760.8739013671875]\n",
      "\n",
      "Instance 640 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 14: [-0.7765771150588989, 0.5308334231376648, -0.7662265300750732, 2.1405153274536133, 0.4633333086967468]\n",
      "Grand sum of 553 tensor sets is: [75.8653335571289, 648.3402099609375, -60.633487701416016, -463.1906433105469, 761.3372192382812]\n",
      "\n",
      "Instance 641 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [1]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 1: [-0.0017812363803386688, 1.6662158966064453, -1.4603081941604614, -1.4625296592712402, 2.8292226791381836]\n",
      "Grand sum of 554 tensor sets is: [75.86355590820312, 650.0064086914062, -62.09379577636719, -464.6531677246094, 764.1664428710938]\n",
      "\n",
      "Instance 642 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "jumping at index 29: [-0.3245052695274353, 2.4119184017181396, -0.3778262138366699, -0.7315150499343872, 1.0605647563934326]\n",
      "Grand sum of 555 tensor sets is: [75.53904724121094, 652.4183349609375, -62.471622467041016, -465.3846740722656, 765.2269897460938]\n",
      "\n",
      "Instance 643 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 3: [1.2128956317901611, 0.3600894808769226, -0.6847660541534424, -1.3020079135894775, 3.4509799480438232]\n",
      "Grand sum of 556 tensor sets is: [76.75194549560547, 652.7784423828125, -63.15638732910156, -466.6866760253906, 768.677978515625]\n",
      "\n",
      "Instance 644 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 645 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "jumping at index 7: [0.13826917111873627, 1.93502676486969, -1.10598623752594, -1.281659483909607, -1.4464547634124756]\n",
      "Grand sum of 557 tensor sets is: [76.89021301269531, 654.7134399414062, -64.26237487792969, -467.96832275390625, 767.2315063476562]\n",
      "\n",
      "Instance 646 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6, 14]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 6: [-0.05740147829055786, 0.7067599892616272, -0.24496296048164368, 0.6869840621948242, -0.9504783153533936]\n",
      "jumping at index 14: [0.0006105154752731323, 1.7893271446228027, 0.26109158992767334, 0.7454938888549805, -0.5694966316223145]\n",
      "Grand sum of 558 tensor sets is: [76.86181640625, 655.9614868164062, -64.25431060791016, -467.2520751953125, 766.4714965820312]\n",
      "\n",
      "Instance 647 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 648 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 12: [0.35771653056144714, 1.500307559967041, -0.7807797789573669, -2.139669895172119, 4.945598602294922]\n",
      "Grand sum of 559 tensor sets is: [77.21953582763672, 657.4617919921875, -65.03508758544922, -469.3917541503906, 771.4171142578125]\n",
      "\n",
      "Instance 649 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [68]\n",
      "Size of token embeddings is torch.Size([75, 13, 768])\n",
      "Shape of summed layers is: 75 x 768\n",
      "jumping at index 68: [0.8417532444000244, 0.8263330459594727, 1.4532074928283691, -1.956573486328125, 0.31005722284317017]\n",
      "Grand sum of 560 tensor sets is: [78.06128692626953, 658.2881469726562, -63.581878662109375, -471.34832763671875, 771.7271728515625]\n",
      "\n",
      "Instance 650 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 45: [-0.15476387739181519, 1.7557933330535889, 0.8504877686500549, 0.2611792981624603, -0.25059974193573]\n",
      "Grand sum of 561 tensor sets is: [77.90652465820312, 660.0439453125, -62.73139190673828, -471.087158203125, 771.4765625]\n",
      "\n",
      "Instance 651 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 652 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 562 tensor sets is: [77.78856658935547, 660.4722290039062, -62.54843521118164, -473.28277587890625, 773.9783325195312]\n",
      "\n",
      "Instance 653 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5, 16]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 5: [-0.4834904372692108, 2.3970212936401367, 0.871749758720398, -3.1980199813842773, 2.401571273803711]\n",
      "jumping at index 16: [0.26767444610595703, 1.579436182975769, 1.351487159729004, -2.4260034561157227, 3.521365165710449]\n",
      "Grand sum of 563 tensor sets is: [77.68065643310547, 662.46044921875, -61.43681716918945, -476.09478759765625, 776.9398193359375]\n",
      "\n",
      "Instance 654 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 8: [0.6574286818504333, 3.0194385051727295, -0.10047688335180283, 1.1424096822738647, 1.6274151802062988]\n",
      "Grand sum of 564 tensor sets is: [78.33808135986328, 665.4798583984375, -61.53729248046875, -474.952392578125, 778.5672607421875]\n",
      "\n",
      "Instance 655 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 16: [-0.37733423709869385, 0.5333801507949829, -0.6908556818962097, 0.9990147352218628, -2.116403579711914]\n",
      "Grand sum of 565 tensor sets is: [77.96074676513672, 666.0132446289062, -62.2281494140625, -473.953369140625, 776.4508666992188]\n",
      "\n",
      "Instance 656 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 10: [-0.3380887508392334, 0.8445478677749634, -0.46117353439331055, -1.1198434829711914, 0.3094198703765869]\n",
      "Grand sum of 566 tensor sets is: [77.6226577758789, 666.8577880859375, -62.68932342529297, -475.0732116699219, 776.7603149414062]\n",
      "\n",
      "Instance 657 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "jumping at index 41: [-0.4575067460536957, 1.7783516645431519, 1.3145811557769775, -0.27519047260284424, 0.664000928401947]\n",
      "Grand sum of 567 tensor sets is: [77.16515350341797, 668.6361694335938, -61.37474060058594, -475.348388671875, 777.42431640625]\n",
      "\n",
      "Instance 658 of jumping.\n",
      "Looking for vocab token: jumping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 659 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 17: [0.12344194948673248, 1.2636774778366089, 0.8765870332717896, -2.6923327445983887, 1.7145066261291504]\n",
      "Grand sum of 568 tensor sets is: [77.2885971069336, 669.8998413085938, -60.49815368652344, -478.04071044921875, 779.1387939453125]\n",
      "\n",
      "Instance 660 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 569 tensor sets is: [76.62294006347656, 672.135498046875, -61.66950607299805, -478.96197509765625, 778.2632446289062]\n",
      "\n",
      "Instance 661 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 14: [0.3288518786430359, 1.6602275371551514, -0.023261122405529022, -0.25790902972221375, -0.057217299938201904]\n",
      "Grand sum of 570 tensor sets is: [76.95178985595703, 673.7957153320312, -61.69276809692383, -479.2198791503906, 778.2060546875]\n",
      "\n",
      "Instance 662 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 9: [-0.24728195369243622, 2.795597553253174, -0.6286807656288147, 2.2367796897888184, 2.5313897132873535]\n",
      "Grand sum of 571 tensor sets is: [76.70450592041016, 676.59130859375, -62.321449279785156, -476.98309326171875, 780.7374267578125]\n",
      "\n",
      "Instance 663 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 18: [-0.2878198027610779, 2.6920323371887207, -1.3067439794540405, 0.8264749050140381, -0.6759641170501709]\n",
      "Grand sum of 572 tensor sets is: [76.41668701171875, 679.2833251953125, -63.62819290161133, -476.1566162109375, 780.0614624023438]\n",
      "\n",
      "Instance 664 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 31: [0.7308530807495117, 1.446509599685669, 0.9715222120285034, -0.756840705871582, 1.6107447147369385]\n",
      "Grand sum of 573 tensor sets is: [77.14753723144531, 680.7298583984375, -62.65666961669922, -476.9134521484375, 781.6721801757812]\n",
      "\n",
      "Instance 665 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 31: [1.430494785308838, 0.8120304346084595, -0.8986666202545166, 1.6041817665100098, 2.5023880004882812]\n",
      "Grand sum of 574 tensor sets is: [78.57803344726562, 681.5418701171875, -63.555335998535156, -475.30926513671875, 784.174560546875]\n",
      "\n",
      "Instance 666 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "jumping at index 29: [0.8516820669174194, 1.4935299158096313, -0.19400280714035034, -0.09721004962921143, 2.741417646408081]\n",
      "Grand sum of 575 tensor sets is: [79.42971801757812, 683.035400390625, -63.74934005737305, -475.4064636230469, 786.9159545898438]\n",
      "\n",
      "Instance 667 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "jumping at index 4: [-0.34915977716445923, 1.7563753128051758, -0.03190874680876732, -0.38795405626296997, 1.2782108783721924]\n",
      "Grand sum of 576 tensor sets is: [79.08055877685547, 684.791748046875, -63.78125, -475.7944030761719, 788.1941528320312]\n",
      "\n",
      "Instance 668 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "jumping at index 5: [0.7915236353874207, 1.6789355278015137, 1.0239193439483643, 0.7109813690185547, 0.9336308836936951]\n",
      "Grand sum of 577 tensor sets is: [79.87208557128906, 686.470703125, -62.75733184814453, -475.08343505859375, 789.1278076171875]\n",
      "\n",
      "Instance 669 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 578 tensor sets is: [79.20642852783203, 688.7063598632812, -63.92868423461914, -476.00469970703125, 788.2522583007812]\n",
      "\n",
      "Instance 670 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 579 tensor sets is: [80.31245422363281, 689.644775390625, -64.53215789794922, -476.3061828613281, 790.1722412109375]\n",
      "\n",
      "Instance 671 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 21: [-0.2422589808702469, 1.3204066753387451, 0.12648260593414307, -2.3849573135375977, 3.035633087158203]\n",
      "Grand sum of 580 tensor sets is: [80.07019805908203, 690.9652099609375, -64.40567779541016, -478.6911315917969, 793.2078857421875]\n",
      "\n",
      "Instance 672 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 37: [0.3997982144355774, 1.3414490222930908, 1.5068955421447754, 0.7156324982643127, 2.184640645980835]\n",
      "Grand sum of 581 tensor sets is: [80.4699935913086, 692.306640625, -62.898780822753906, -477.9754943847656, 795.3925170898438]\n",
      "\n",
      "Instance 673 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 18: [0.32166165113449097, 1.696913242340088, 0.23226530849933624, -2.0332815647125244, -0.050274163484573364]\n",
      "Grand sum of 582 tensor sets is: [80.79165649414062, 694.0035400390625, -62.6665153503418, -480.0087890625, 795.3422241210938]\n",
      "\n",
      "Instance 674 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "jumping at index 4: [0.14600473642349243, 0.35729503631591797, 1.1024868488311768, -4.778727054595947, 2.742537260055542]\n",
      "Grand sum of 583 tensor sets is: [80.93766021728516, 694.36083984375, -61.564029693603516, -484.7875061035156, 798.0847778320312]\n",
      "\n",
      "Instance 675 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 676 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 14: [0.2808583378791809, 2.771909713745117, -0.8323837518692017, -0.2240634262561798, -0.910395622253418]\n",
      "Grand sum of 584 tensor sets is: [81.21852111816406, 697.1327514648438, -62.39641189575195, -485.0115661621094, 797.1743774414062]\n",
      "\n",
      "Instance 677 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "jumping at index 43: [0.15862983465194702, 3.1660399436950684, 0.05191449448466301, -1.1215159893035889, -1.3951140642166138]\n",
      "Grand sum of 585 tensor sets is: [81.37715148925781, 700.2987670898438, -62.34449768066406, -486.1330871582031, 795.7792358398438]\n",
      "\n",
      "Instance 678 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "jumping at index 6: [0.3865165412425995, 2.634694814682007, 0.40490055084228516, -2.5356078147888184, 4.900007724761963]\n",
      "Grand sum of 586 tensor sets is: [81.76366424560547, 702.9334716796875, -61.939598083496094, -488.668701171875, 800.6792602539062]\n",
      "\n",
      "Instance 679 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 587 tensor sets is: [81.64570617675781, 703.3617553710938, -61.75664138793945, -490.86431884765625, 803.1810302734375]\n",
      "\n",
      "Instance 680 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 14: [1.342180609703064, 4.004761695861816, -1.184451699256897, -1.3626747131347656, 0.029245316982269287]\n",
      "Grand sum of 588 tensor sets is: [82.98788452148438, 707.3665161132812, -62.94109344482422, -492.22698974609375, 803.2102661132812]\n",
      "\n",
      "Instance 681 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 17: [-0.5658351182937622, 0.7403227090835571, 1.4259098768234253, -2.8691232204437256, 1.9500267505645752]\n",
      "Grand sum of 589 tensor sets is: [82.42205047607422, 708.1068115234375, -61.51518249511719, -495.0960998535156, 805.1602783203125]\n",
      "\n",
      "Instance 682 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 26: [-1.0301182270050049, 2.505089282989502, 0.3863261342048645, -0.5880890488624573, -1.0540591478347778]\n",
      "Grand sum of 590 tensor sets is: [81.39192962646484, 710.6118774414062, -61.12885665893555, -495.6842041015625, 804.106201171875]\n",
      "\n",
      "Instance 683 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 14: [-0.20360486209392548, 1.809217929840088, 0.23571869730949402, -3.5990326404571533, 1.656891107559204]\n",
      "Grand sum of 591 tensor sets is: [81.18832397460938, 712.4210815429688, -60.89313888549805, -499.2832336425781, 805.7631225585938]\n",
      "\n",
      "Instance 684 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "jumping at index 16: [-0.19716976583003998, 1.133171558380127, -0.1351303607225418, -0.5556939840316772, 1.324303150177002]\n",
      "Grand sum of 592 tensor sets is: [80.99115753173828, 713.5542602539062, -61.02827072143555, -499.83892822265625, 807.08740234375]\n",
      "\n",
      "Instance 685 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "jumping at index 46: [0.8350606560707092, -0.035800427198410034, -0.4806412160396576, -1.2478597164154053, 3.5835061073303223]\n",
      "Grand sum of 593 tensor sets is: [81.82621765136719, 713.5184326171875, -61.5089111328125, -501.0867919921875, 810.6708984375]\n",
      "\n",
      "Instance 686 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 687 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 688 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 32: [0.44631990790367126, -0.5024573802947998, -1.3448255062103271, -1.4877195358276367, 3.4152495861053467]\n",
      "Grand sum of 594 tensor sets is: [82.27253723144531, 713.0159912109375, -62.853736877441406, -502.57452392578125, 814.0861206054688]\n",
      "\n",
      "Instance 689 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [-0.45801663398742676, -0.9925666451454163, 0.12019787728786469, -2.552450656890869, 2.2982001304626465]\n",
      "Grand sum of 595 tensor sets is: [81.81452178955078, 712.0234375, -62.73353958129883, -505.1269836425781, 816.3843383789062]\n",
      "\n",
      "Instance 690 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.1620088368654251, 1.1503407955169678, 0.6928114295005798, -1.461946725845337, -0.6129564642906189]\n",
      "Grand sum of 596 tensor sets is: [81.97653198242188, 713.1737670898438, -62.04072952270508, -506.58892822265625, 815.7713623046875]\n",
      "\n",
      "Instance 691 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 597 tensor sets is: [81.85857391357422, 713.60205078125, -61.85777282714844, -508.7845458984375, 818.2731323242188]\n",
      "\n",
      "Instance 692 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 8: [1.6081643104553223, 1.7816197872161865, -0.13042369484901428, -1.6790143251419067, -0.3856751024723053]\n",
      "Grand sum of 598 tensor sets is: [83.46673583984375, 715.3836669921875, -61.988197326660156, -510.46356201171875, 817.887451171875]\n",
      "\n",
      "Instance 693 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 3: [-0.3364112377166748, 0.7250776886940002, 0.5786001086235046, -2.8928539752960205, 3.068901538848877]\n",
      "Grand sum of 599 tensor sets is: [83.13032531738281, 716.1087646484375, -61.40959548950195, -513.3564453125, 820.9563598632812]\n",
      "\n",
      "Instance 694 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 11: [-0.9404209852218628, 1.3332116603851318, -0.29651665687561035, -0.6964857578277588, 0.19971689581871033]\n",
      "Grand sum of 600 tensor sets is: [82.18990325927734, 717.4419555664062, -61.706111907958984, -514.0529174804688, 821.1560668945312]\n",
      "\n",
      "Instance 695 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 11: [-0.2800106704235077, 0.8600803017616272, -0.5554116368293762, 0.18709246814250946, 2.4371914863586426]\n",
      "Grand sum of 601 tensor sets is: [81.9098892211914, 718.3020629882812, -62.26152420043945, -513.8658447265625, 823.59326171875]\n",
      "\n",
      "Instance 696 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "jumping at index 50: [-0.32268255949020386, 0.8589609861373901, -0.5832332968711853, 0.40337073802948, 3.015742778778076]\n",
      "Grand sum of 602 tensor sets is: [81.58720397949219, 719.1610107421875, -62.844757080078125, -513.4624633789062, 826.6090087890625]\n",
      "\n",
      "Instance 697 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "jumping at index 34: [-1.0581508874893188, 0.807050883769989, 0.02778610587120056, -3.2855277061462402, 2.068340301513672]\n",
      "Grand sum of 603 tensor sets is: [80.529052734375, 719.9680786132812, -62.81697082519531, -516.7479858398438, 828.6773681640625]\n",
      "\n",
      "Instance 698 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 5: [0.7126748561859131, -1.1883958578109741, 0.6050469875335693, 0.6433212161064148, -1.7705343961715698]\n",
      "Grand sum of 604 tensor sets is: [81.24172973632812, 718.7796630859375, -62.2119255065918, -516.1046752929688, 826.9068603515625]\n",
      "\n",
      "Instance 699 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 10: [-0.5549705624580383, 0.9383563995361328, -1.0433672666549683, -1.9000530242919922, 3.4019625186920166]\n",
      "Grand sum of 605 tensor sets is: [80.68675994873047, 719.718017578125, -63.25529098510742, -518.0046997070312, 830.308837890625]\n",
      "\n",
      "Instance 700 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 10: [1.1628868579864502, 1.5503780841827393, -1.5853148698806763, 0.6310198307037354, 2.815361499786377]\n",
      "Grand sum of 606 tensor sets is: [81.84964752197266, 721.2683715820312, -64.84060668945312, -517.3736572265625, 833.1242065429688]\n",
      "\n",
      "Instance 701 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 22: [-0.5682231187820435, -0.27498918771743774, 0.8935573101043701, -0.5903955698013306, -0.4039151966571808]\n",
      "Grand sum of 607 tensor sets is: [81.28142547607422, 720.993408203125, -63.94704818725586, -517.9640502929688, 832.7202758789062]\n",
      "\n",
      "Instance 702 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 13: [-0.016562163829803467, 0.2762281894683838, 0.2770230770111084, 1.139621615409851, 0.6801573634147644]\n",
      "Grand sum of 608 tensor sets is: [81.26486206054688, 721.2696533203125, -63.67002487182617, -516.8244018554688, 833.4004516601562]\n",
      "\n",
      "Instance 703 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 15: [-0.20804712176322937, 1.2163101434707642, 0.8669770956039429, -1.0042997598648071, 1.4630087614059448]\n",
      "Grand sum of 609 tensor sets is: [81.05681610107422, 722.4859619140625, -62.80304718017578, -517.8286743164062, 834.8634643554688]\n",
      "\n",
      "Instance 704 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([120, 13, 768])\n",
      "Shape of summed layers is: 120 x 768\n",
      "jumping at index 39: [-0.5130093693733215, 0.551410973072052, -0.06441992521286011, -1.7155592441558838, -0.47331586480140686]\n",
      "Grand sum of 610 tensor sets is: [80.54380798339844, 723.037353515625, -62.86746597290039, -519.5442504882812, 834.39013671875]\n",
      "\n",
      "Instance 705 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 611 tensor sets is: [80.61473083496094, 725.3056030273438, -63.534637451171875, -519.3984985351562, 834.253662109375]\n",
      "\n",
      "Instance 706 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 23: [-0.21506333351135254, 1.7271305322647095, 1.4187977313995361, 1.0241318941116333, -0.24468010663986206]\n",
      "Grand sum of 612 tensor sets is: [80.39966583251953, 727.03271484375, -62.115840911865234, -518.3743896484375, 834.0089721679688]\n",
      "\n",
      "Instance 707 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "jumping at index 13: [-0.47159668803215027, -0.24363623559474945, 1.4926326274871826, -1.214100956916809, 2.7903449535369873]\n",
      "Grand sum of 613 tensor sets is: [79.92807006835938, 726.7890625, -60.623207092285156, -519.5885009765625, 836.79931640625]\n",
      "\n",
      "Instance 708 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 14: [0.5463207364082336, 0.46926140785217285, 0.6286690831184387, -1.3517343997955322, 0.25186434388160706]\n",
      "Grand sum of 614 tensor sets is: [80.4743881225586, 727.25830078125, -59.994537353515625, -520.9402465820312, 837.0512084960938]\n",
      "\n",
      "Instance 709 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "jumping at index 25: [0.48222294449806213, -1.0296201705932617, -0.9932917356491089, -2.765521764755249, -0.46050187945365906]\n",
      "Grand sum of 615 tensor sets is: [80.95661163330078, 726.2286987304688, -60.98782730102539, -523.7057495117188, 836.5906982421875]\n",
      "\n",
      "Instance 710 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 711 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 712 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 616 tensor sets is: [81.8990707397461, 726.2943115234375, -61.35810089111328, -524.1754760742188, 839.217041015625]\n",
      "\n",
      "Instance 713 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 18: [0.33556604385375977, 0.4340396821498871, -0.21892082691192627, -0.4964587092399597, 1.421252965927124]\n",
      "Grand sum of 617 tensor sets is: [82.23463439941406, 726.7283325195312, -61.577022552490234, -524.6719360351562, 840.6383056640625]\n",
      "\n",
      "Instance 714 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 13: [0.7004492878913879, 2.7540972232818604, -0.23578737676143646, -0.7075672149658203, 1.2136328220367432]\n",
      "Grand sum of 618 tensor sets is: [82.9350814819336, 729.482421875, -61.812808990478516, -525.3795166015625, 841.8519287109375]\n",
      "\n",
      "Instance 715 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 42: [0.8324199914932251, 1.2377440929412842, -0.18836480379104614, -1.9308220148086548, 2.0435562133789062]\n",
      "Grand sum of 619 tensor sets is: [83.76750183105469, 730.7201538085938, -62.00117492675781, -527.3103637695312, 843.8955078125]\n",
      "\n",
      "Instance 716 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 6: [-0.23268184065818787, 1.853500247001648, -0.04225912690162659, 1.0090223550796509, 0.8020153045654297]\n",
      "Grand sum of 620 tensor sets is: [83.53482055664062, 732.5736694335938, -62.043434143066406, -526.3013305664062, 844.697509765625]\n",
      "\n",
      "Instance 717 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 22: [0.5887835621833801, 1.8930625915527344, 1.6461272239685059, -0.7143222689628601, -1.1629728078842163]\n",
      "Grand sum of 621 tensor sets is: [84.12360382080078, 734.4667358398438, -60.397308349609375, -527.015625, 843.5345458984375]\n",
      "\n",
      "Instance 718 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 21: [-0.5548097491264343, 2.3528871536254883, -0.10347232967615128, -0.15484619140625, 1.054692029953003]\n",
      "Grand sum of 622 tensor sets is: [83.56879425048828, 736.8196411132812, -60.50078201293945, -527.1704711914062, 844.5892333984375]\n",
      "\n",
      "Instance 719 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 623 tensor sets is: [84.67481994628906, 737.758056640625, -61.10425567626953, -527.4719848632812, 846.5092163085938]\n",
      "\n",
      "Instance 720 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 33: [0.36902934312820435, 1.8270314931869507, 0.020975738763809204, 0.6596251130104065, 1.4041551351547241]\n",
      "Grand sum of 624 tensor sets is: [85.0438461303711, 739.5850830078125, -61.08327865600586, -526.8123779296875, 847.9133911132812]\n",
      "\n",
      "Instance 721 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 625 tensor sets is: [86.14987182617188, 740.5234985351562, -61.68675231933594, -527.1138916015625, 849.8333740234375]\n",
      "\n",
      "Instance 722 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 9: [0.9742454290390015, 0.4606822729110718, -0.8756764531135559, 2.3162121772766113, 2.319397449493408]\n",
      "Grand sum of 626 tensor sets is: [87.12411499023438, 740.9841918945312, -62.56242752075195, -524.7976684570312, 852.1527709960938]\n",
      "\n",
      "Instance 723 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 33: [0.8152782917022705, 1.4116679430007935, 0.134597510099411, -0.4210836589336395, 0.8835769891738892]\n",
      "Grand sum of 627 tensor sets is: [87.93939208984375, 742.3958740234375, -62.42782974243164, -525.21875, 853.036376953125]\n",
      "\n",
      "Instance 724 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 628 tensor sets is: [87.8214340209961, 742.8241577148438, -62.244873046875, -527.4143676757812, 855.5381469726562]\n",
      "\n",
      "Instance 725 of jumping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 726 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "jumping at index 2: [-0.4839186668395996, 1.9930144548416138, 0.4018217623233795, 0.8470385670661926, 2.9683635234832764]\n",
      "Grand sum of 629 tensor sets is: [87.33751678466797, 744.8171997070312, -61.84305191040039, -526.5673217773438, 858.5065307617188]\n",
      "\n",
      "Instance 727 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 35: [-0.5132607817649841, 1.3512204885482788, 0.46572747826576233, -2.7976067066192627, 1.6884405612945557]\n",
      "Grand sum of 630 tensor sets is: [86.82425689697266, 746.1683959960938, -61.377323150634766, -529.3649291992188, 860.1949462890625]\n",
      "\n",
      "Instance 728 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 4: [-0.11365900188684464, -0.04862982779741287, 0.9240747094154358, -1.6704210042953491, 4.756921768188477]\n",
      "Grand sum of 631 tensor sets is: [86.7105941772461, 746.1197509765625, -60.4532470703125, -531.0353393554688, 864.9518432617188]\n",
      "\n",
      "Instance 729 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 11: [-0.5857537388801575, 1.3595274686813354, 0.29637008905410767, -1.1186546087265015, 0.19201824069023132]\n",
      "Grand sum of 632 tensor sets is: [86.12483978271484, 747.479248046875, -60.15687561035156, -532.1539916992188, 865.1438598632812]\n",
      "\n",
      "Instance 730 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 5: [1.2401554584503174, 1.1670571565628052, -1.3274990320205688, 0.18887649476528168, 1.1148419380187988]\n",
      "Grand sum of 633 tensor sets is: [87.36499786376953, 748.6463012695312, -61.484375, -531.965087890625, 866.2587280273438]\n",
      "\n",
      "Instance 731 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 2: [-0.7709157466888428, 1.7791162729263306, -1.261553168296814, -1.6994409561157227, 0.7841392755508423]\n",
      "Grand sum of 634 tensor sets is: [86.59408569335938, 750.4254150390625, -62.74592971801758, -533.66455078125, 867.0428466796875]\n",
      "\n",
      "Instance 732 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "jumping at index 41: [0.5238171815872192, 0.6561354398727417, -0.23822088539600372, -1.9076517820358276, 3.9862101078033447]\n",
      "Grand sum of 635 tensor sets is: [87.11790466308594, 751.08154296875, -62.98414993286133, -535.5722045898438, 871.029052734375]\n",
      "\n",
      "Instance 733 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 2: [-0.04866538941860199, 3.2305614948272705, -0.5525559186935425, 0.30866649746894836, 0.7120120525360107]\n",
      "Grand sum of 636 tensor sets is: [87.0692367553711, 754.3121337890625, -63.536705017089844, -535.2635498046875, 871.7410888671875]\n",
      "\n",
      "Instance 734 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 19: [-0.6048576831817627, 1.2794169187545776, 0.48885801434516907, -1.6619263887405396, 0.25086280703544617]\n",
      "Grand sum of 637 tensor sets is: [86.4643783569336, 755.591552734375, -63.047847747802734, -536.9254760742188, 871.991943359375]\n",
      "\n",
      "Instance 735 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "jumping at index 23: [0.0651324912905693, 1.4379721879959106, -0.9390008449554443, -1.2241263389587402, 0.45867180824279785]\n",
      "Grand sum of 638 tensor sets is: [86.52951049804688, 757.029541015625, -63.986846923828125, -538.1495971679688, 872.4506225585938]\n",
      "\n",
      "Instance 736 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 14: [0.7392358779907227, 2.893446922302246, -0.569218635559082, -2.190586805343628, 0.7617772817611694]\n",
      "Grand sum of 639 tensor sets is: [87.26874542236328, 759.9229736328125, -64.55606842041016, -540.3402099609375, 873.21240234375]\n",
      "\n",
      "Instance 737 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [56]\n",
      "Size of token embeddings is torch.Size([113, 13, 768])\n",
      "Shape of summed layers is: 113 x 768\n",
      "jumping at index 56: [1.0273727178573608, 0.06734216213226318, 0.7550185918807983, -2.8567793369293213, 0.6297874450683594]\n",
      "Grand sum of 640 tensor sets is: [88.2961196899414, 759.9902954101562, -63.801048278808594, -543.1969604492188, 873.8421630859375]\n",
      "\n",
      "Instance 738 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 4: [-0.14195562899112701, -0.0828140377998352, 0.8077726364135742, -1.6037887334823608, 4.667163372039795]\n",
      "Grand sum of 641 tensor sets is: [88.15416717529297, 759.907470703125, -62.9932746887207, -544.8007202148438, 878.5093383789062]\n",
      "\n",
      "Instance 739 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 12: [-0.2296231985092163, 1.7531684637069702, -0.8488551378250122, -1.4074299335479736, 0.06776565313339233]\n",
      "Grand sum of 642 tensor sets is: [87.92454528808594, 761.66064453125, -63.84212875366211, -546.2081298828125, 878.5770874023438]\n",
      "\n",
      "Instance 740 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 15: [0.8146653175354004, 0.563853919506073, -1.3837621212005615, -1.8466439247131348, 0.0035219788551330566]\n",
      "Grand sum of 643 tensor sets is: [88.73921203613281, 762.2244873046875, -65.22589111328125, -548.0547485351562, 878.5806274414062]\n",
      "\n",
      "Instance 741 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 6: [0.9854766130447388, 1.7234883308410645, -0.8330509662628174, -1.1634881496429443, 1.3342032432556152]\n",
      "Grand sum of 644 tensor sets is: [89.72468566894531, 763.947998046875, -66.05894470214844, -549.21826171875, 879.9148559570312]\n",
      "\n",
      "Instance 742 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "jumping at index 44: [1.1507118940353394, 1.3979251384735107, -0.6424980759620667, 1.6594440937042236, 1.6973180770874023]\n",
      "Grand sum of 645 tensor sets is: [90.87539672851562, 765.345947265625, -66.70144653320312, -547.558837890625, 881.6121826171875]\n",
      "\n",
      "Instance 743 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 34: [0.1584690660238266, 0.2852349877357483, 0.36569327116012573, -1.1901384592056274, 1.8071717023849487]\n",
      "Grand sum of 646 tensor sets is: [91.03386688232422, 765.6311645507812, -66.33575439453125, -548.7489624023438, 883.4193725585938]\n",
      "\n",
      "Instance 744 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 6: [0.023333821445703506, -0.5385124683380127, 0.5684595704078674, -1.252334713935852, 3.4363441467285156]\n",
      "Grand sum of 647 tensor sets is: [91.05719757080078, 765.0926513671875, -65.76729583740234, -550.0012817382812, 886.855712890625]\n",
      "\n",
      "Instance 745 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 648 tensor sets is: [91.9996566772461, 765.1582641601562, -66.13756561279297, -550.4710083007812, 889.4820556640625]\n",
      "\n",
      "Instance 746 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 34: [0.4672603905200958, 3.4076428413391113, -0.18268625438213348, -1.3780057430267334, 0.9624878764152527]\n",
      "Grand sum of 649 tensor sets is: [92.4669189453125, 768.56591796875, -66.32025146484375, -551.8489990234375, 890.4445190429688]\n",
      "\n",
      "Instance 747 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 22: [0.32140153646469116, 0.3969002366065979, -0.614569902420044, -2.5347819328308105, 2.501255512237549]\n",
      "Grand sum of 650 tensor sets is: [92.78832244873047, 768.9628295898438, -66.93482208251953, -554.3837890625, 892.94580078125]\n",
      "\n",
      "Instance 748 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 34: [-0.19173111021518707, 1.7142558097839355, 0.3336302638053894, -1.8846291303634644, -1.5620291233062744]\n",
      "Grand sum of 651 tensor sets is: [92.59658813476562, 770.6770629882812, -66.60118865966797, -556.2684326171875, 891.3837890625]\n",
      "\n",
      "Instance 749 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 4: [0.09458118677139282, 0.4204852879047394, 0.8594053983688354, -2.071963310241699, 4.083411693572998]\n",
      "Grand sum of 652 tensor sets is: [92.69116973876953, 771.0975341796875, -65.74178314208984, -558.3403930664062, 895.4672241210938]\n",
      "\n",
      "Instance 750 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 4: [0.6034160256385803, 3.037170648574829, -0.4543449580669403, 0.5786311030387878, 3.482548713684082]\n",
      "Grand sum of 653 tensor sets is: [93.29458618164062, 774.1347045898438, -66.19612884521484, -557.7617797851562, 898.9497680664062]\n",
      "\n",
      "Instance 751 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [-0.45801663398742676, -0.9925666451454163, 0.12019787728786469, -2.552450656890869, 2.2982001304626465]\n",
      "Grand sum of 654 tensor sets is: [92.8365707397461, 773.1421508789062, -66.075927734375, -560.314208984375, 901.2479858398438]\n",
      "\n",
      "Instance 752 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 9: [0.367389440536499, 1.9501442909240723, -1.753766655921936, 1.5705338716506958, 2.7279410362243652]\n",
      "Grand sum of 655 tensor sets is: [93.2039566040039, 775.09228515625, -67.82969665527344, -558.74365234375, 903.9759521484375]\n",
      "\n",
      "Instance 753 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 26: [0.4207395911216736, 3.113448143005371, -0.3722262382507324, 0.7853184342384338, 0.2575523257255554]\n",
      "Grand sum of 656 tensor sets is: [93.62469482421875, 778.2057495117188, -68.20191955566406, -557.9583129882812, 904.2335205078125]\n",
      "\n",
      "Instance 754 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 47: [0.19753491878509521, -0.49670493602752686, 1.2710473537445068, -1.0343971252441406, 2.435713768005371]\n",
      "Grand sum of 657 tensor sets is: [93.82222747802734, 777.7090454101562, -66.93087005615234, -558.9927368164062, 906.6692504882812]\n",
      "\n",
      "Instance 755 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 9: [0.6253506541252136, 1.9694825410842896, -0.9544293880462646, 2.2001442909240723, 2.1940345764160156]\n",
      "Grand sum of 658 tensor sets is: [94.44757843017578, 779.6785278320312, -67.88529968261719, -556.7926025390625, 908.86328125]\n",
      "\n",
      "Instance 756 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 16: [-0.12060099840164185, 1.1788747310638428, -0.2228856235742569, -1.8320858478546143, 2.812713384628296]\n",
      "Grand sum of 659 tensor sets is: [94.32698059082031, 780.857421875, -68.10818481445312, -558.6246948242188, 911.6759643554688]\n",
      "\n",
      "Instance 757 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 13: [0.857542097568512, 2.189934253692627, 0.6419040560722351, -1.9424240589141846, 2.373100757598877]\n",
      "Grand sum of 660 tensor sets is: [95.18452453613281, 783.04736328125, -67.46627807617188, -560.567138671875, 914.049072265625]\n",
      "\n",
      "Instance 758 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 40: [-0.9915033578872681, 0.4319162368774414, -0.396843284368515, -1.4249237775802612, 2.6269309520721436]\n",
      "Grand sum of 661 tensor sets is: [94.19302368164062, 783.4793090820312, -67.86312103271484, -561.9920654296875, 916.676025390625]\n",
      "\n",
      "Instance 759 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 760 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 16: [-0.5514659881591797, 2.123021364212036, -0.8042794466018677, -0.6271752119064331, 0.2717481255531311]\n",
      "Grand sum of 662 tensor sets is: [93.64155578613281, 785.6023559570312, -68.66740417480469, -562.6192626953125, 916.94775390625]\n",
      "\n",
      "Instance 761 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 663 tensor sets is: [94.58401489257812, 785.66796875, -69.03767395019531, -563.0889892578125, 919.5740966796875]\n",
      "\n",
      "Instance 762 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 30: [1.866204857826233, 1.9160879850387573, 0.27221930027008057, -2.286775588989258, 2.977480411529541]\n",
      "Grand sum of 664 tensor sets is: [96.4502182006836, 787.5840454101562, -68.76545715332031, -565.3757934570312, 922.5515747070312]\n",
      "\n",
      "Instance 763 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 21: [0.8010463714599609, 2.2617106437683105, -0.7556520104408264, 0.6980852484703064, 0.05314922332763672]\n",
      "Grand sum of 665 tensor sets is: [97.25126647949219, 789.8457641601562, -69.52111053466797, -564.677734375, 922.604736328125]\n",
      "\n",
      "Instance 764 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 17: [-0.5847640633583069, 2.5004918575286865, -0.5207924246788025, 0.7108270525932312, -0.5779743194580078]\n",
      "Grand sum of 666 tensor sets is: [96.66650390625, 792.3462524414062, -70.04190063476562, -563.9669189453125, 922.0267333984375]\n",
      "\n",
      "Instance 765 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 5: [1.4823381900787354, -0.13535474240779877, -1.9158642292022705, -0.6993157863616943, 2.336350679397583]\n",
      "Grand sum of 667 tensor sets is: [98.14884185791016, 792.2108764648438, -71.957763671875, -564.666259765625, 924.3630981445312]\n",
      "\n",
      "Instance 766 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 9: [-0.12836232781410217, 1.1554715633392334, -0.9004456996917725, 1.2552746534347534, 1.2583143711090088]\n",
      "Grand sum of 668 tensor sets is: [98.02047729492188, 793.3663330078125, -72.85820770263672, -563.4110107421875, 925.6213989257812]\n",
      "\n",
      "Instance 767 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 34: [0.20885950326919556, 1.252306580543518, -1.0478776693344116, 0.6597935557365417, -0.0448276549577713]\n",
      "Grand sum of 669 tensor sets is: [98.22933959960938, 794.61865234375, -73.90608215332031, -562.751220703125, 925.5765991210938]\n",
      "\n",
      "Instance 768 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 3: [0.9590214490890503, 1.3055617809295654, -0.5994742512702942, 1.6914172172546387, 2.0659019947052]\n",
      "Grand sum of 670 tensor sets is: [99.18836212158203, 795.9241943359375, -74.50555419921875, -561.059814453125, 927.6425170898438]\n",
      "\n",
      "Instance 769 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 16: [-0.5698445439338684, 0.04927486181259155, 0.8161448240280151, -1.44945228099823, -0.7318428754806519]\n",
      "Grand sum of 671 tensor sets is: [98.61851501464844, 795.9734497070312, -73.68940734863281, -562.50927734375, 926.91064453125]\n",
      "\n",
      "Instance 770 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 771 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 11: [0.5511431097984314, 1.725875973701477, 0.4837196171283722, -0.08816338330507278, 0.14515084028244019]\n",
      "Grand sum of 672 tensor sets is: [99.1696548461914, 797.6993408203125, -73.2056884765625, -562.597412109375, 927.0557861328125]\n",
      "\n",
      "Instance 772 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 773 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 11: [0.18768379092216492, 1.0077871084213257, -0.05484902113676071, -1.4646780490875244, 1.8372764587402344]\n",
      "Grand sum of 673 tensor sets is: [99.35733795166016, 798.7071533203125, -73.26053619384766, -564.0620727539062, 928.89306640625]\n",
      "\n",
      "Instance 774 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 775 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 2: [0.9051524996757507, 0.2025757133960724, -1.8392506837844849, -0.9587124586105347, 6.4265522956848145]\n",
      "Grand sum of 674 tensor sets is: [100.26248931884766, 798.9097290039062, -75.09978485107422, -565.0208129882812, 935.3196411132812]\n",
      "\n",
      "Instance 776 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 675 tensor sets is: [100.14453125, 799.3380126953125, -74.91682434082031, -567.2164306640625, 937.8214111328125]\n",
      "\n",
      "Instance 777 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 6: [0.3408573269844055, 1.0531648397445679, 0.5774562358856201, -1.4270344972610474, 3.0989136695861816]\n",
      "Grand sum of 676 tensor sets is: [100.48538970947266, 800.3911743164062, -74.33937072753906, -568.6434936523438, 940.9203491210938]\n",
      "\n",
      "Instance 778 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 10: [-0.5549705624580383, 0.9383563995361328, -1.0433672666549683, -1.9000530242919922, 3.4019625186920166]\n",
      "Grand sum of 677 tensor sets is: [99.930419921875, 801.3295288085938, -75.38273620605469, -570.5435180664062, 944.3223266601562]\n",
      "\n",
      "Instance 779 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [54]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "jumping at index 54: [0.20575451850891113, 2.12734055519104, -0.5187445282936096, -0.023733802139759064, 6.031990051269531]\n",
      "Grand sum of 678 tensor sets is: [100.13617706298828, 803.4568481445312, -75.90148162841797, -570.5672607421875, 950.3543090820312]\n",
      "\n",
      "Instance 780 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 17: [-0.14308947324752808, 0.9984482526779175, 0.8949154019355774, -2.7205686569213867, 4.878147125244141]\n",
      "Grand sum of 679 tensor sets is: [99.99308776855469, 804.455322265625, -75.0065689086914, -573.287841796875, 955.2324829101562]\n",
      "\n",
      "Instance 781 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 21: [-0.3690270781517029, 0.6089944839477539, 0.37507274746894836, -1.3476811647415161, 0.4101061224937439]\n",
      "Grand sum of 680 tensor sets is: [99.62406158447266, 805.0643310546875, -74.6314926147461, -574.635498046875, 955.642578125]\n",
      "\n",
      "Instance 782 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 2: [0.8397401571273804, 2.67937970161438, -0.5033836960792542, -0.6710077524185181, 0.6935200095176697]\n",
      "Grand sum of 681 tensor sets is: [100.46379852294922, 807.7437133789062, -75.13488006591797, -575.3065185546875, 956.3361206054688]\n",
      "\n",
      "Instance 783 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 13: [-0.19341512024402618, 1.2205841541290283, -1.107857346534729, -2.2513277530670166, 4.798268795013428]\n",
      "Grand sum of 682 tensor sets is: [100.2703857421875, 808.9642944335938, -76.24273681640625, -577.557861328125, 961.1343994140625]\n",
      "\n",
      "Instance 784 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 683 tensor sets is: [101.21284484863281, 809.0299072265625, -76.61300659179688, -578.027587890625, 963.7607421875]\n",
      "\n",
      "Instance 785 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 9: [-0.5464600324630737, 0.9206307530403137, 0.7043673396110535, -1.2773005962371826, 4.94245719909668]\n",
      "Grand sum of 684 tensor sets is: [100.6663818359375, 809.9505615234375, -75.90863800048828, -579.3048706054688, 968.7031860351562]\n",
      "\n",
      "Instance 786 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 11: [-0.9404209852218628, 1.3332116603851318, -0.29651665687561035, -0.6964857578277588, 0.19971689581871033]\n",
      "Grand sum of 685 tensor sets is: [99.72595977783203, 811.2837524414062, -76.20515441894531, -580.0013427734375, 968.9028930664062]\n",
      "\n",
      "Instance 787 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 17: [-0.14114513993263245, 0.3504028618335724, -0.7664072513580322, -0.47179126739501953, 0.340099036693573]\n",
      "Grand sum of 686 tensor sets is: [99.5848159790039, 811.6341552734375, -76.97156524658203, -580.47314453125, 969.2429809570312]\n",
      "\n",
      "Instance 788 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 9: [-0.03600555658340454, 0.8831833600997925, 1.0259371995925903, -0.8603863716125488, 2.694690704345703]\n",
      "Grand sum of 687 tensor sets is: [99.54881286621094, 812.517333984375, -75.94562530517578, -581.3335571289062, 971.9376831054688]\n",
      "\n",
      "Instance 789 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 790 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 8: [0.16509486734867096, 1.5533570051193237, -1.6121785640716553, -2.276808023452759, 1.2977559566497803]\n",
      "Grand sum of 688 tensor sets is: [99.71390533447266, 814.0706787109375, -77.55780029296875, -583.6103515625, 973.2354125976562]\n",
      "\n",
      "Instance 791 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumping at index 21: [0.9240614175796509, 1.171043872833252, 0.7056305408477783, -1.4833251237869263, 3.5657074451446533]\n",
      "Grand sum of 689 tensor sets is: [100.63796997070312, 815.24169921875, -76.8521728515625, -585.0936889648438, 976.8011474609375]\n",
      "\n",
      "Instance 792 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 4: [-0.10616545379161835, 0.7205451130867004, -0.010105788707733154, 0.009276360273361206, 2.148364305496216]\n",
      "Grand sum of 690 tensor sets is: [100.53180694580078, 815.9622192382812, -76.8622817993164, -585.0844116210938, 978.9495239257812]\n",
      "\n",
      "Instance 793 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9, 31]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 9: [1.2506701946258545, 1.6626125574111938, -0.5078892111778259, -1.273102879524231, 2.005800724029541]\n",
      "jumping at index 31: [0.7732536792755127, 2.5619537830352783, 0.4718601703643799, -2.3979275226593018, 0.9653187990188599]\n",
      "Grand sum of 691 tensor sets is: [101.54376983642578, 818.0745239257812, -76.88029479980469, -586.919921875, 980.43505859375]\n",
      "\n",
      "Instance 794 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 10: [-0.02604977786540985, 0.3908761143684387, -0.15262669324874878, 0.8290106654167175, 1.0059099197387695]\n",
      "Grand sum of 692 tensor sets is: [101.5177230834961, 818.4653930664062, -77.03292083740234, -586.0908813476562, 981.4409790039062]\n",
      "\n",
      "Instance 795 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 15: [-0.6566887497901917, 0.9134571552276611, -1.4263105392456055, 1.034759283065796, 1.7500241994857788]\n",
      "Grand sum of 693 tensor sets is: [100.86103057861328, 819.3788452148438, -78.459228515625, -585.05615234375, 983.1909790039062]\n",
      "\n",
      "Instance 796 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 12: [-0.0377250611782074, 1.0415064096450806, 0.38520097732543945, -0.9763237237930298, 5.097087860107422]\n",
      "Grand sum of 694 tensor sets is: [100.82330322265625, 820.4203491210938, -78.07402801513672, -586.032470703125, 988.2880859375]\n",
      "\n",
      "Instance 797 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 11: [0.5726931095123291, 0.9217717051506042, 0.11356750130653381, -0.4933900237083435, 0.9245271682739258]\n",
      "Grand sum of 695 tensor sets is: [101.39599609375, 821.3421020507812, -77.96045684814453, -586.52587890625, 989.2125854492188]\n",
      "\n",
      "Instance 798 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 8: [-1.2639737129211426, 0.6090564727783203, 0.3014431595802307, -0.3816433250904083, 1.6203819513320923]\n",
      "Grand sum of 696 tensor sets is: [100.13201904296875, 821.951171875, -77.65901184082031, -586.9075317382812, 990.8329467773438]\n",
      "\n",
      "Instance 799 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 20: [-0.005018115043640137, 1.8508732318878174, 0.32849979400634766, -1.9787499904632568, 4.357486248016357]\n",
      "Grand sum of 697 tensor sets is: [100.12699890136719, 823.8020629882812, -77.33051300048828, -588.8862915039062, 995.1904296875]\n",
      "\n",
      "Instance 800 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5, 7]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 5: [0.08012139797210693, 1.4298999309539795, 0.6825599670410156, -1.999548316001892, 1.5458219051361084]\n",
      "jumping at index 7: [-0.40431758761405945, 1.075706124305725, 0.680483341217041, -1.9380565881729126, 0.5786451101303101]\n",
      "Grand sum of 698 tensor sets is: [99.96489715576172, 825.0548706054688, -76.64899444580078, -590.8551025390625, 996.252685546875]\n",
      "\n",
      "Instance 801 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 12: [0.014271581545472145, 0.7698538303375244, 0.670220673084259, -2.274761199951172, 2.379193067550659]\n",
      "Grand sum of 699 tensor sets is: [99.97917175292969, 825.82470703125, -75.97877502441406, -593.1298828125, 998.6318969726562]\n",
      "\n",
      "Instance 802 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 13: [1.152260661125183, 1.826936960220337, -1.8775004148483276, -1.2998021841049194, 0.9180302023887634]\n",
      "Grand sum of 700 tensor sets is: [101.13143157958984, 827.6516723632812, -77.85627746582031, -594.4296875, 999.5499267578125]\n",
      "\n",
      "Instance 803 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 701 tensor sets is: [102.23745727539062, 828.590087890625, -78.45974731445312, -594.731201171875, 1001.4699096679688]\n",
      "\n",
      "Instance 804 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "jumping at index 8: [-1.1505776643753052, 0.751672625541687, -0.25415873527526855, -2.3349034786224365, 1.1428751945495605]\n",
      "Grand sum of 702 tensor sets is: [101.08687591552734, 829.3417358398438, -78.71390533447266, -597.0661010742188, 1002.61279296875]\n",
      "\n",
      "Instance 805 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 13: [-0.41848456859588623, 3.7605581283569336, -0.19032703340053558, -2.9736311435699463, 1.8021800518035889]\n",
      "Grand sum of 703 tensor sets is: [100.66838836669922, 833.102294921875, -78.90423583984375, -600.0397338867188, 1004.4149780273438]\n",
      "\n",
      "Instance 806 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 20: [-0.5856131911277771, 0.7042771577835083, 1.3702073097229004, -1.433251142501831, 1.5097001791000366]\n",
      "Grand sum of 704 tensor sets is: [100.08277893066406, 833.8065795898438, -77.53402709960938, -601.4729614257812, 1005.9246826171875]\n",
      "\n",
      "Instance 807 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 808 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 17: [0.6409306526184082, 0.96159428358078, -0.9089269042015076, 0.3097139596939087, 3.9017627239227295]\n",
      "Grand sum of 705 tensor sets is: [100.72370910644531, 834.7681884765625, -78.44295501708984, -601.1632690429688, 1009.826416015625]\n",
      "\n",
      "Instance 809 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 2: [-0.38882124423980713, 1.0273430347442627, -0.9297577142715454, -0.5393945574760437, 4.565066337585449]\n",
      "Grand sum of 706 tensor sets is: [100.33488464355469, 835.7955322265625, -79.37271118164062, -601.70263671875, 1014.3914794921875]\n",
      "\n",
      "Instance 810 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 707 tensor sets is: [101.44091033935547, 836.7339477539062, -79.97618103027344, -602.004150390625, 1016.3114624023438]\n",
      "\n",
      "Instance 811 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 11: [-0.8400286436080933, 0.4650496244430542, 0.683211088180542, -1.9262374639511108, 2.903333902359009]\n",
      "Grand sum of 708 tensor sets is: [100.60088348388672, 837.198974609375, -79.29296875, -603.9303588867188, 1019.2147827148438]\n",
      "\n",
      "Instance 812 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 7: [-1.2702996730804443, 1.5190340280532837, -0.27041852474212646, -0.9055649638175964, 0.064581960439682]\n",
      "Grand sum of 709 tensor sets is: [99.33058166503906, 838.718017578125, -79.56338500976562, -604.8359375, 1019.2793579101562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 813 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 9: [-0.8559092879295349, 1.5527255535125732, -1.2524129152297974, 0.16071520745754242, -1.07575261592865]\n",
      "Grand sum of 710 tensor sets is: [98.47467041015625, 840.270751953125, -80.8157958984375, -604.6752319335938, 1018.20361328125]\n",
      "\n",
      "Instance 814 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 9: [1.694732427597046, 2.224386215209961, -0.8931211233139038, 2.262723445892334, -0.02128392457962036]\n",
      "Grand sum of 711 tensor sets is: [100.16940307617188, 842.4951171875, -81.70891571044922, -602.4125366210938, 1018.1823120117188]\n",
      "\n",
      "Instance 815 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 18: [0.18450555205345154, 1.3401581048965454, -0.56490159034729, -1.6718815565109253, 1.7920398712158203]\n",
      "Grand sum of 712 tensor sets is: [100.35391235351562, 843.8352661132812, -82.27381896972656, -604.0844116210938, 1019.974365234375]\n",
      "\n",
      "Instance 816 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 16: [-1.1679190397262573, 0.46667012572288513, -0.9200156927108765, 0.4555782079696655, -0.9702070951461792]\n",
      "Grand sum of 713 tensor sets is: [99.18599700927734, 844.3019409179688, -83.19383239746094, -603.6288452148438, 1019.004150390625]\n",
      "\n",
      "Instance 817 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 818 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 819 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "jumping at index 2: [0.4308440089225769, 1.7831170558929443, 0.8806191086769104, -1.8956959247589111, 3.3497681617736816]\n",
      "Grand sum of 714 tensor sets is: [99.6168441772461, 846.0850830078125, -82.3132095336914, -605.5245361328125, 1022.3539428710938]\n",
      "\n",
      "Instance 820 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 8: [-0.15180633962154388, 0.7557868361473083, -1.8513259887695312, -2.3993852138519287, 2.699646472930908]\n",
      "Grand sum of 715 tensor sets is: [99.46503448486328, 846.8408813476562, -84.16453552246094, -607.9239501953125, 1025.0535888671875]\n",
      "\n",
      "Instance 821 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "jumping at index 49: [-1.053907036781311, 1.3898166418075562, 0.8941856026649475, -1.230637788772583, 1.8660714626312256]\n",
      "Grand sum of 716 tensor sets is: [98.41112518310547, 848.230712890625, -83.27034759521484, -609.1546020507812, 1026.919677734375]\n",
      "\n",
      "Instance 822 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 717 tensor sets is: [98.48204803466797, 850.4989624023438, -83.93751525878906, -609.0088500976562, 1026.783203125]\n",
      "\n",
      "Instance 823 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 8: [0.4004212021827698, 1.1771268844604492, 0.27847903966903687, -2.2734997272491455, 0.40416184067726135]\n",
      "Grand sum of 718 tensor sets is: [98.8824691772461, 851.6760864257812, -83.6590347290039, -611.2823486328125, 1027.1873779296875]\n",
      "\n",
      "Instance 824 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "jumping at index 41: [-0.7638145685195923, 1.2618732452392578, -0.19394269585609436, 0.03740184009075165, 5.823589324951172]\n",
      "Grand sum of 719 tensor sets is: [98.11865234375, 852.93798828125, -83.85297393798828, -611.2449340820312, 1033.010986328125]\n",
      "\n",
      "Instance 825 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 25: [0.7941994071006775, 0.2640672028064728, -0.6523948311805725, -1.3811259269714355, 3.853062629699707]\n",
      "Grand sum of 720 tensor sets is: [98.91284942626953, 853.2020263671875, -84.50537109375, -612.6260375976562, 1036.864013671875]\n",
      "\n",
      "Instance 826 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 10: [0.02021751180291176, 1.7163705825805664, -1.033544659614563, -0.8300065994262695, -0.01966765522956848]\n",
      "Grand sum of 721 tensor sets is: [98.93306732177734, 854.9183959960938, -85.5389175415039, -613.4560546875, 1036.8443603515625]\n",
      "\n",
      "Instance 827 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "jumping at index 30: [-0.7525327205657959, 2.696751594543457, 0.7048580050468445, 0.9109683036804199, 3.1984121799468994]\n",
      "Grand sum of 722 tensor sets is: [98.18053436279297, 857.6151733398438, -84.83406066894531, -612.5451049804688, 1040.042724609375]\n",
      "\n",
      "Instance 828 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 3: [0.984954297542572, 1.5958571434020996, 0.0986940935254097, 2.000404119491577, 0.23942404985427856]\n",
      "Grand sum of 723 tensor sets is: [99.16548919677734, 859.2110595703125, -84.73536682128906, -610.544677734375, 1040.2821044921875]\n",
      "\n",
      "Instance 829 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 5: [0.7490861415863037, 1.4181156158447266, -1.2857462167739868, 1.245644450187683, 0.6330323815345764]\n",
      "Grand sum of 724 tensor sets is: [99.9145736694336, 860.629150390625, -86.02111053466797, -609.2990112304688, 1040.9151611328125]\n",
      "\n",
      "Instance 830 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 16: [-1.1110177040100098, 1.2909588813781738, 0.5047131180763245, -1.3646434545516968, 2.6240434646606445]\n",
      "Grand sum of 725 tensor sets is: [98.80355834960938, 861.9201049804688, -85.51639556884766, -610.6636352539062, 1043.5391845703125]\n",
      "\n",
      "Instance 831 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 26: [-0.6441004872322083, 0.8298875093460083, -0.09128323942422867, -2.3650214672088623, -0.11610707640647888]\n",
      "Grand sum of 726 tensor sets is: [98.15945434570312, 862.75, -85.60768127441406, -613.0286865234375, 1043.423095703125]\n",
      "\n",
      "Instance 832 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 26: [-1.0301182270050049, 2.505089282989502, 0.3863261342048645, -0.5880890488624573, -1.0540591478347778]\n",
      "Grand sum of 727 tensor sets is: [97.12933349609375, 865.2550659179688, -85.22135162353516, -613.6167602539062, 1042.3690185546875]\n",
      "\n",
      "Instance 833 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 28: [0.920145571231842, 0.4446057081222534, 0.5095522403717041, -1.7806884050369263, 4.602729320526123]\n",
      "Grand sum of 728 tensor sets is: [98.04947662353516, 865.6996459960938, -84.71179962158203, -615.3974609375, 1046.9718017578125]\n",
      "\n",
      "Instance 834 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 22: [-0.35819417238235474, 0.612126350402832, 0.13142533600330353, -0.7728006839752197, 4.352477073669434]\n",
      "Grand sum of 729 tensor sets is: [97.6912841796875, 866.311767578125, -84.58037567138672, -616.1702880859375, 1051.32421875]\n",
      "\n",
      "Instance 835 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 9: [1.583889126777649, 1.1684507131576538, 0.48292842507362366, 1.107537031173706, -0.09920558333396912]\n",
      "Grand sum of 730 tensor sets is: [99.27517700195312, 867.480224609375, -84.09745025634766, -615.062744140625, 1051.2249755859375]\n",
      "\n",
      "Instance 836 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 17: [0.8984235525131226, 1.5992035865783691, -0.36566871404647827, -0.38183799386024475, 1.581296443939209]\n",
      "Grand sum of 731 tensor sets is: [100.17359924316406, 869.0794067382812, -84.46311950683594, -615.444580078125, 1052.8062744140625]\n",
      "\n",
      "Instance 837 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 732 tensor sets is: [100.0556411743164, 869.5076904296875, -84.28015899658203, -617.6401977539062, 1055.3079833984375]\n",
      "\n",
      "Instance 838 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 23: [0.8059464693069458, 1.6431983709335327, -0.6913000345230103, -0.7946915626525879, 1.271850347518921]\n",
      "Grand sum of 733 tensor sets is: [100.86158752441406, 871.15087890625, -84.9714584350586, -618.4348754882812, 1056.579833984375]\n",
      "\n",
      "Instance 839 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "jumping at index 3: [-0.9901182055473328, 0.8260980844497681, -0.22760164737701416, -2.9054625034332275, 2.3074307441711426]\n",
      "Grand sum of 734 tensor sets is: [99.87146759033203, 871.9769897460938, -85.19905853271484, -621.34033203125, 1058.88720703125]\n",
      "\n",
      "Instance 840 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "jumping at index 10: [0.5755063891410828, 1.6639883518218994, 1.1763813495635986, -1.429166555404663, 1.9315612316131592]\n",
      "Grand sum of 735 tensor sets is: [100.44697570800781, 873.6409912109375, -84.02267456054688, -622.7694702148438, 1060.8187255859375]\n",
      "\n",
      "Instance 841 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 842 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 843 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 15: [0.6798548698425293, 0.5036550760269165, 0.18127749860286713, -1.8209627866744995, 2.2782840728759766]\n",
      "Grand sum of 736 tensor sets is: [101.1268310546875, 874.1446533203125, -83.84140014648438, -624.5904541015625, 1063.0970458984375]\n",
      "\n",
      "Instance 844 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 17: [0.4959365725517273, 1.5806009769439697, -0.7233333587646484, 0.9687038660049438, 0.21419686079025269]\n",
      "Grand sum of 737 tensor sets is: [101.62276458740234, 875.7252807617188, -84.56473541259766, -623.6217651367188, 1063.311279296875]\n",
      "\n",
      "Instance 845 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "jumping at index 38: [-0.24646562337875366, 0.5370606184005737, 0.5621848106384277, -2.63744854927063, 1.591066837310791]\n",
      "Grand sum of 738 tensor sets is: [101.37629699707031, 876.2623291015625, -84.00254821777344, -626.2592163085938, 1064.90234375]\n",
      "\n",
      "Instance 846 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 847 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 2: [0.22876472771167755, 1.6507195234298706, -0.3019067645072937, 0.42060819268226624, 1.4925576448440552]\n",
      "Grand sum of 739 tensor sets is: [101.60506439208984, 877.9130249023438, -84.30445861816406, -625.838623046875, 1066.3948974609375]\n",
      "\n",
      "Instance 848 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 21: [-0.18674913048744202, -0.17176184058189392, -0.16208890080451965, -0.9802609086036682, 1.8111200332641602]\n",
      "Grand sum of 740 tensor sets is: [101.4183120727539, 877.7412719726562, -84.46654510498047, -626.8189086914062, 1068.2060546875]\n",
      "\n",
      "Instance 849 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 19: [-0.591707170009613, 1.3312504291534424, 0.26506370306015015, -1.9584760665893555, -0.8858869075775146]\n",
      "Grand sum of 741 tensor sets is: [100.82660675048828, 879.072509765625, -84.20148468017578, -628.7774047851562, 1067.3201904296875]\n",
      "\n",
      "Instance 850 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 742 tensor sets is: [100.70864868164062, 879.5007934570312, -84.01852416992188, -630.9730224609375, 1069.8218994140625]\n",
      "\n",
      "Instance 851 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 14: [-0.12683804333209991, -0.01919478178024292, -0.30245888233184814, -1.4763822555541992, 0.9460887312889099]\n",
      "Grand sum of 743 tensor sets is: [100.5818099975586, 879.4816284179688, -84.32098388671875, -632.4494018554688, 1070.7679443359375]\n",
      "\n",
      "Instance 852 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 17: [-0.7939661741256714, 1.1697381734848022, 0.9148147702217102, -0.7581161260604858, 0.6520360708236694]\n",
      "Grand sum of 744 tensor sets is: [99.787841796875, 880.6513671875, -83.40616607666016, -633.20751953125, 1071.419921875]\n",
      "\n",
      "Instance 853 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 854 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 21: [-0.3690270781517029, 0.6089944839477539, 0.37507274746894836, -1.3476811647415161, 0.4101061224937439]\n",
      "Grand sum of 745 tensor sets is: [99.41881561279297, 881.2603759765625, -83.03108978271484, -634.55517578125, 1071.830078125]\n",
      "\n",
      "Instance 855 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 8: [0.6574286818504333, 3.0194385051727295, -0.10047688335180283, 1.1424096822738647, 1.6274151802062988]\n",
      "Grand sum of 746 tensor sets is: [100.07624053955078, 884.27978515625, -83.1315689086914, -633.4127807617188, 1073.45751953125]\n",
      "\n",
      "Instance 856 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 6: [0.784429669380188, 1.3313143253326416, -1.088119387626648, 0.26651549339294434, -0.4872676730155945]\n",
      "Grand sum of 747 tensor sets is: [100.86067199707031, 885.611083984375, -84.21968841552734, -633.146240234375, 1072.97021484375]\n",
      "\n",
      "Instance 857 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "jumping at index 22: [0.34527695178985596, 1.3844797611236572, 0.5317515730857849, -2.0635194778442383, -1.098248839378357]\n",
      "Grand sum of 748 tensor sets is: [101.20594787597656, 886.9955444335938, -83.68793487548828, -635.2097778320312, 1071.8719482421875]\n",
      "\n",
      "Instance 858 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "jumping at index 50: [0.2818916440010071, 0.929182767868042, 0.39655590057373047, -2.0364990234375, -0.7524497509002686]\n",
      "Grand sum of 749 tensor sets is: [101.48783874511719, 887.9247436523438, -83.2913818359375, -637.2462768554688, 1071.1195068359375]\n",
      "\n",
      "Instance 859 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [1.0845937728881836, 0.4794759452342987, 0.4092938303947449, -1.880601406097412, 2.256963014602661]\n",
      "Grand sum of 750 tensor sets is: [102.57243347167969, 888.4042358398438, -82.88208770751953, -639.1268920898438, 1073.37646484375]\n",
      "\n",
      "Instance 860 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([130, 13, 768])\n",
      "Shape of summed layers is: 130 x 768\n",
      "jumping at index 47: [-0.5222559571266174, 2.19464111328125, -0.4422873556613922, 0.2943294048309326, 0.8757312297821045]\n",
      "Grand sum of 751 tensor sets is: [102.05017852783203, 890.598876953125, -83.32437133789062, -638.8325805664062, 1074.252197265625]\n",
      "\n",
      "Instance 861 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 32: [-1.1722298860549927, 1.7666914463043213, -1.335942268371582, 0.3611297905445099, 3.274168014526367]\n",
      "Grand sum of 752 tensor sets is: [100.87794494628906, 892.3655395507812, -84.66031646728516, -638.471435546875, 1077.5263671875]\n",
      "\n",
      "Instance 862 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [86]\n",
      "Size of token embeddings is torch.Size([111, 13, 768])\n",
      "Shape of summed layers is: 111 x 768\n",
      "jumping at index 86: [0.21690212190151215, -0.2832282781600952, -0.14953386783599854, -1.5661039352416992, 3.3010635375976562]\n",
      "Grand sum of 753 tensor sets is: [101.0948486328125, 892.0823364257812, -84.80985260009766, -640.0375366210938, 1080.827392578125]\n",
      "\n",
      "Instance 863 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 24: [0.028985165059566498, 1.245742678642273, 0.7290881872177124, -1.8127834796905518, 3.5277485847473145]\n",
      "Grand sum of 754 tensor sets is: [101.12383270263672, 893.3280639648438, -84.08076477050781, -641.850341796875, 1084.3551025390625]\n",
      "\n",
      "Instance 864 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 16: [1.2687703371047974, 0.7922388315200806, -0.4979352056980133, -1.8917155265808105, 1.4844298362731934]\n",
      "Grand sum of 755 tensor sets is: [102.3926010131836, 894.1203002929688, -84.57869720458984, -643.7420654296875, 1085.8394775390625]\n",
      "\n",
      "Instance 865 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 19: [-0.03830130770802498, 0.9205437302589417, -1.3995954990386963, 0.07167693972587585, -0.016575992107391357]\n",
      "Grand sum of 756 tensor sets is: [102.35430145263672, 895.0408325195312, -85.9782943725586, -643.67041015625, 1085.8228759765625]\n",
      "\n",
      "Instance 866 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 867 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 4: [0.18298791348934174, 0.4811306893825531, 0.5736400485038757, -1.922179937362671, 0.3372689187526703]\n",
      "Grand sum of 757 tensor sets is: [102.53729248046875, 895.52197265625, -85.40465545654297, -645.5925903320312, 1086.16015625]\n",
      "\n",
      "Instance 868 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 14: [-0.3112790286540985, -0.39983534812927246, 0.38083982467651367, -1.2597301006317139, 0.4483269155025482]\n",
      "Grand sum of 758 tensor sets is: [102.22601318359375, 895.1221313476562, -85.02381896972656, -646.852294921875, 1086.6085205078125]\n",
      "\n",
      "Instance 869 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 12: [0.381477415561676, 0.9623116254806519, 1.7117655277252197, -1.0681242942810059, 1.8206806182861328]\n",
      "Grand sum of 759 tensor sets is: [102.60749053955078, 896.08447265625, -83.31204986572266, -647.92041015625, 1088.42919921875]\n",
      "\n",
      "Instance 870 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 26: [-1.0301182270050049, 2.505089282989502, 0.3863261342048645, -0.5880890488624573, -1.0540591478347778]\n",
      "Grand sum of 760 tensor sets is: [101.5773696899414, 898.5895385742188, -82.92572021484375, -648.5084838867188, 1087.3751220703125]\n",
      "\n",
      "Instance 871 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 6: [-0.3955339789390564, 1.969935655593872, 0.5249022841453552, -2.540829658508301, -0.11107313632965088]\n",
      "Grand sum of 761 tensor sets is: [101.18183898925781, 900.5594482421875, -82.40081787109375, -651.04931640625, 1087.2640380859375]\n",
      "\n",
      "Instance 872 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 2: [-1.5424282550811768, 2.091109037399292, -0.38825252652168274, -0.599108874797821, 3.9930367469787598]\n",
      "Grand sum of 762 tensor sets is: [99.63941192626953, 902.6505737304688, -82.78907012939453, -651.6484375, 1091.257080078125]\n",
      "\n",
      "Instance 873 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 9: [-0.7395944595336914, 1.0991744995117188, 0.9956377744674683, 1.3651528358459473, -0.1836184561252594]\n",
      "Grand sum of 763 tensor sets is: [98.89981842041016, 903.749755859375, -81.7934341430664, -650.2832641601562, 1091.073486328125]\n",
      "\n",
      "Instance 874 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "jumping at index 30: [1.2452915906906128, 1.8306735754013062, -0.29504483938217163, -3.9827990531921387, 1.4435348510742188]\n",
      "Grand sum of 764 tensor sets is: [100.14511108398438, 905.5804443359375, -82.0884780883789, -654.2660522460938, 1092.5169677734375]\n",
      "\n",
      "Instance 875 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([108, 13, 768])\n",
      "Shape of summed layers is: 108 x 768\n",
      "jumping at index 34: [-0.2938055694103241, 1.818571925163269, 1.230261206626892, -0.9005066752433777, 0.9953678846359253]\n",
      "Grand sum of 765 tensor sets is: [99.85130310058594, 907.3989868164062, -80.85821533203125, -655.1665649414062, 1093.5123291015625]\n",
      "\n",
      "Instance 876 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 877 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "jumping at index 2: [-0.4839186668395996, 1.9930144548416138, 0.4018217623233795, 0.8470385670661926, 2.9683635234832764]\n",
      "Grand sum of 766 tensor sets is: [99.36738586425781, 909.3920288085938, -80.45639038085938, -654.3195190429688, 1096.480712890625]\n",
      "\n",
      "Instance 878 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 10: [-0.7976963520050049, 1.3492118120193481, 0.7439851760864258, 1.4990915060043335, 0.1791973114013672]\n",
      "Grand sum of 767 tensor sets is: [98.56968688964844, 910.7412109375, -79.71240234375, -652.8204345703125, 1096.659912109375]\n",
      "\n",
      "Instance 879 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 6: [-0.675956666469574, 1.0234107971191406, -1.2278856039047241, 0.002262592315673828, -0.8565534353256226]\n",
      "Grand sum of 768 tensor sets is: [97.89373016357422, 911.7646484375, -80.9402847290039, -652.8181762695312, 1095.8033447265625]\n",
      "\n",
      "Instance 880 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 881 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 8: [-0.6002712249755859, 1.2585495710372925, 0.49632716178894043, -1.98685622215271, 1.2343323230743408]\n",
      "Grand sum of 769 tensor sets is: [97.29345703125, 913.023193359375, -80.44395446777344, -654.8050537109375, 1097.0377197265625]\n",
      "\n",
      "Instance 882 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 2: [0.7012702226638794, 1.18586003780365, -1.3702391386032104, 0.9347699880599976, 2.6620893478393555]\n",
      "Grand sum of 770 tensor sets is: [97.9947280883789, 914.2090454101562, -81.81419372558594, -653.8703002929688, 1099.6998291015625]\n",
      "\n",
      "Instance 883 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [70]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([92, 13, 768])\n",
      "Shape of summed layers is: 92 x 768\n",
      "jumping at index 70: [-0.7389969229698181, 0.9433258771896362, -1.2310514450073242, -0.751402735710144, 0.8571385145187378]\n",
      "Grand sum of 771 tensor sets is: [97.25572967529297, 915.15234375, -83.04524230957031, -654.6217041015625, 1100.5570068359375]\n",
      "\n",
      "Instance 884 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 885 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 22: [-0.5682231187820435, -0.27498918771743774, 0.8935573101043701, -0.5903955698013306, -0.4039151966571808]\n",
      "Grand sum of 772 tensor sets is: [96.68750762939453, 914.8773803710938, -82.15168762207031, -655.2120971679688, 1100.153076171875]\n",
      "\n",
      "Instance 886 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 887 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 12: [-0.06527134776115417, 1.2893173694610596, 1.4454591274261475, -1.2446222305297852, 2.029571533203125]\n",
      "Grand sum of 773 tensor sets is: [96.62223815917969, 916.1666870117188, -80.70623016357422, -656.4567260742188, 1102.1826171875]\n",
      "\n",
      "Instance 888 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "jumping at index 18: [0.17714416980743408, 2.458848476409912, -0.10793695598840714, -0.5182816982269287, -0.5756936073303223]\n",
      "Grand sum of 774 tensor sets is: [96.79938507080078, 918.6255493164062, -80.81417083740234, -656.9750366210938, 1101.60693359375]\n",
      "\n",
      "Instance 889 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 25: [-0.018173806369304657, 0.37627294659614563, 0.013344958424568176, 0.05746757984161377, 0.853621244430542]\n",
      "Grand sum of 775 tensor sets is: [96.78121185302734, 919.0018310546875, -80.80082702636719, -656.9175415039062, 1102.4605712890625]\n",
      "\n",
      "Instance 890 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 25: [0.16898077726364136, 0.6857565641403198, -0.18358618021011353, 1.217691421508789, -0.16568639874458313]\n",
      "Grand sum of 776 tensor sets is: [96.9501953125, 919.6875610351562, -80.98441314697266, -655.6998291015625, 1102.294921875]\n",
      "\n",
      "Instance 891 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 12: [0.3643711805343628, 1.9394991397857666, 0.645682692527771, -1.2747101783752441, 2.096116304397583]\n",
      "Grand sum of 777 tensor sets is: [97.31456756591797, 921.6270751953125, -80.33872985839844, -656.9745483398438, 1104.3909912109375]\n",
      "\n",
      "Instance 892 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 11: [-0.5948383808135986, 1.4568099975585938, 0.9357178211212158, -1.950492262840271, 2.2315139770507812]\n",
      "Grand sum of 778 tensor sets is: [96.7197265625, 923.0838623046875, -79.40301513671875, -658.925048828125, 1106.62255859375]\n",
      "\n",
      "Instance 893 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20, 24]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 20: [0.24099932610988617, 1.5807961225509644, -0.04314655065536499, -0.8375091552734375, 2.8140416145324707]\n",
      "jumping at index 24: [0.26869693398475647, 1.5173941850662231, 0.07097034901380539, -1.4110867977142334, 2.2081780433654785]\n",
      "Grand sum of 779 tensor sets is: [96.97457122802734, 924.6329345703125, -79.38910675048828, -660.04931640625, 1109.1336669921875]\n",
      "\n",
      "Instance 894 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.7390782237052917, 0.5315726399421692, -0.3892924189567566, -1.738318681716919, 1.0037479400634766]\n",
      "Grand sum of 780 tensor sets is: [96.2354965209961, 925.1644897460938, -79.77839660644531, -661.7876586914062, 1110.137451171875]\n",
      "\n",
      "Instance 895 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "jumping at index 5: [-0.6027621626853943, 2.4430699348449707, 0.4415314793586731, -1.8163374662399292, -1.2350785732269287]\n",
      "Grand sum of 781 tensor sets is: [95.63273620605469, 927.6075439453125, -79.33686828613281, -663.60400390625, 1108.90234375]\n",
      "\n",
      "Instance 896 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "jumping at index 19: [-0.038380227982997894, 2.103327751159668, -0.5084494352340698, -1.8246737718582153, 2.796337842941284]\n",
      "Grand sum of 782 tensor sets is: [95.59435272216797, 929.7108764648438, -79.8453140258789, -665.4286499023438, 1111.69873046875]\n",
      "\n",
      "Instance 897 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 10: [0.5895342230796814, 1.3772997856140137, -0.015455961227416992, 0.9501087665557861, 0.08424803614616394]\n",
      "Grand sum of 783 tensor sets is: [96.18388366699219, 931.0881958007812, -79.86077117919922, -664.478515625, 1111.782958984375]\n",
      "\n",
      "Instance 898 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 899 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 7: [-1.1834008693695068, 1.2997305393218994, -0.39373356103897095, 0.5017326474189758, -1.6832995414733887]\n",
      "Grand sum of 784 tensor sets is: [95.00048065185547, 932.387939453125, -80.25450134277344, -663.976806640625, 1110.099609375]\n",
      "\n",
      "Instance 900 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 25: [-0.5749988555908203, 0.96495521068573, -0.027700988575816154, -1.1943520307540894, 0.4515261948108673]\n",
      "Grand sum of 785 tensor sets is: [94.42548370361328, 933.3529052734375, -80.2822036743164, -665.171142578125, 1110.5511474609375]\n",
      "\n",
      "Instance 901 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 902 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 903 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "jumping at index 6: [0.6541302800178528, 0.15481078624725342, 1.0914006233215332, -2.519733190536499, -0.29033219814300537]\n",
      "Grand sum of 786 tensor sets is: [95.0796127319336, 933.5076904296875, -79.19080352783203, -667.6908569335938, 1110.2608642578125]\n",
      "\n",
      "Instance 904 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 30: [0.665226936340332, 2.8858554363250732, -0.37608230113983154, 0.8918271064758301, 0.3094441592693329]\n",
      "Grand sum of 787 tensor sets is: [95.74484252929688, 936.3935546875, -79.56688690185547, -666.7990112304688, 1110.5703125]\n",
      "\n",
      "Instance 905 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 16: [0.36019662022590637, 0.4728078246116638, -0.723487913608551, -0.7233627438545227, 0.04174599051475525]\n",
      "Grand sum of 788 tensor sets is: [96.10504150390625, 936.8663330078125, -80.29037475585938, -667.5223999023438, 1110.612060546875]\n",
      "\n",
      "Instance 906 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 907 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [-0.45801663398742676, -0.9925666451454163, 0.12019787728786469, -2.552450656890869, 2.2982001304626465]\n",
      "Grand sum of 789 tensor sets is: [95.64702606201172, 935.873779296875, -80.17017364501953, -670.0748291015625, 1112.9102783203125]\n",
      "\n",
      "Instance 908 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 32: [0.3894241452217102, -0.23365256190299988, 0.743483304977417, -1.699202537536621, 1.3800745010375977]\n",
      "Grand sum of 790 tensor sets is: [96.03645324707031, 935.64013671875, -79.42668914794922, -671.7740478515625, 1114.2904052734375]\n",
      "\n",
      "Instance 909 of jumping.\n",
      "Looking for vocab token: jumping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 910 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "jumping at index 45: [-0.5077518820762634, -0.47017359733581543, -0.8879621028900146, -1.0786341428756714, -2.0062975883483887]\n",
      "Grand sum of 791 tensor sets is: [95.52870178222656, 935.1699829101562, -80.31465148925781, -672.8526611328125, 1112.2840576171875]\n",
      "\n",
      "Instance 911 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 21: [0.5107452273368835, 0.8595004677772522, 1.3154258728027344, -3.059227466583252, 3.9781131744384766]\n",
      "Grand sum of 792 tensor sets is: [96.03944396972656, 936.0294799804688, -78.99922180175781, -675.911865234375, 1116.26220703125]\n",
      "\n",
      "Instance 912 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 13: [-0.844961941242218, 1.4445881843566895, -0.7783585786819458, -1.5764769315719604, -0.4836842119693756]\n",
      "Grand sum of 793 tensor sets is: [95.1944808959961, 937.4740600585938, -79.77758026123047, -677.4883422851562, 1115.778564453125]\n",
      "\n",
      "Instance 913 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "jumping at index 20: [-0.14826706051826477, 0.9805942177772522, 1.5338064432144165, 0.8334009051322937, 0.8594400882720947]\n",
      "Grand sum of 794 tensor sets is: [95.04621124267578, 938.4546508789062, -78.2437744140625, -676.6549682617188, 1116.6380615234375]\n",
      "\n",
      "Instance 914 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 20: [0.6381655931472778, 0.41027048230171204, 0.34525996446609497, 0.18554933369159698, 0.69054114818573]\n",
      "Grand sum of 795 tensor sets is: [95.68437957763672, 938.8649291992188, -77.89851379394531, -676.4694213867188, 1117.32861328125]\n",
      "\n",
      "Instance 915 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 796 tensor sets is: [95.56642150878906, 939.293212890625, -77.7155532836914, -678.6650390625, 1119.830322265625]\n",
      "\n",
      "Instance 916 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 37: [-0.799726665019989, 1.881285548210144, -0.7017902731895447, -1.364182949066162, 1.1919909715652466]\n",
      "Grand sum of 797 tensor sets is: [94.76669311523438, 941.1744995117188, -78.41734313964844, -680.0292358398438, 1121.0223388671875]\n",
      "\n",
      "Instance 917 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 27: [0.6563594341278076, 1.6138348579406738, 0.9474664330482483, -2.03334903717041, 0.1691673994064331]\n",
      "Grand sum of 798 tensor sets is: [95.42304992675781, 942.788330078125, -77.46987915039062, -682.0625610351562, 1121.1915283203125]\n",
      "\n",
      "Instance 918 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [88]\n",
      "Size of token embeddings is torch.Size([95, 13, 768])\n",
      "Shape of summed layers is: 95 x 768\n",
      "jumping at index 88: [-0.6062509417533875, 2.0793256759643555, -0.00047641992568969727, -1.5232068300247192, 3.459336280822754]\n",
      "Grand sum of 799 tensor sets is: [94.8167953491211, 944.86767578125, -77.47035217285156, -683.5857543945312, 1124.65087890625]\n",
      "\n",
      "Instance 919 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 15: [1.0526185035705566, 3.26951003074646, -1.5262115001678467, -0.5293910503387451, 2.476245880126953]\n",
      "Grand sum of 800 tensor sets is: [95.86941528320312, 948.13720703125, -78.99656677246094, -684.1151733398438, 1127.1270751953125]\n",
      "\n",
      "Instance 920 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "jumping at index 20: [1.05021071434021, 1.3335754871368408, -0.6596840023994446, -0.9448868036270142, 0.28869688510894775]\n",
      "Grand sum of 801 tensor sets is: [96.91962432861328, 949.4707641601562, -79.65625, -685.06005859375, 1127.415771484375]\n",
      "\n",
      "Instance 921 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 21: [-0.22561115026474, 1.199408769607544, -1.124435544013977, -2.2751874923706055, 2.992100477218628]\n",
      "Grand sum of 802 tensor sets is: [96.69401550292969, 950.670166015625, -80.78068542480469, -687.3352661132812, 1130.4078369140625]\n",
      "\n",
      "Instance 922 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 5: [0.41450801491737366, -0.10058033466339111, 0.039633311331272125, -1.3952900171279907, 0.10941013693809509]\n",
      "Grand sum of 803 tensor sets is: [97.1085205078125, 950.569580078125, -80.74105072021484, -688.7305297851562, 1130.5172119140625]\n",
      "\n",
      "Instance 923 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 924 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 25: [-0.5749988555908203, 0.96495521068573, -0.027700988575816154, -1.1943520307540894, 0.4515261948108673]\n",
      "Grand sum of 804 tensor sets is: [96.53352355957031, 951.5345458984375, -80.76875305175781, -689.9248657226562, 1130.96875]\n",
      "\n",
      "Instance 925 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 19: [-0.46527615189552307, 0.6065371036529541, 0.8289234042167664, -1.5898692607879639, 2.0311319828033447]\n",
      "Grand sum of 805 tensor sets is: [96.06824493408203, 952.14111328125, -79.93982696533203, -691.5147094726562, 1132.9998779296875]\n",
      "\n",
      "Instance 926 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 14: [0.6350842714309692, -0.8848556280136108, 0.06305359303951263, 0.08564385026693344, -0.2319330871105194]\n",
      "Grand sum of 806 tensor sets is: [96.70333099365234, 951.2562866210938, -79.87677001953125, -691.4290771484375, 1132.7679443359375]\n",
      "\n",
      "Instance 927 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 26: [0.6357445120811462, 0.8501477241516113, 1.455955982208252, -3.3374431133270264, 0.29623156785964966]\n",
      "Grand sum of 807 tensor sets is: [97.33907318115234, 952.1064453125, -78.42081451416016, -694.7665405273438, 1133.064208984375]\n",
      "\n",
      "Instance 928 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 42: [0.6320647597312927, 1.8340967893600464, 0.8246183395385742, -3.5746028423309326, 2.1152195930480957]\n",
      "Grand sum of 808 tensor sets is: [97.97113800048828, 953.9405517578125, -77.59619903564453, -698.3411254882812, 1135.179443359375]\n",
      "\n",
      "Instance 929 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 930 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "jumping at index 48: [0.044173747301101685, 1.6423026323318481, 0.30019715428352356, -1.2085715532302856, 1.4189908504486084]\n",
      "Grand sum of 809 tensor sets is: [98.01531219482422, 955.5828247070312, -77.29600524902344, -699.5496826171875, 1136.598388671875]\n",
      "\n",
      "Instance 931 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 7: [0.23426741361618042, 0.47147995233535767, 0.3954772651195526, -0.3021303713321686, 2.7157769203186035]\n",
      "Grand sum of 810 tensor sets is: [98.24958038330078, 956.0543212890625, -76.90052795410156, -699.851806640625, 1139.314208984375]\n",
      "\n",
      "Instance 932 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 20: [0.8306000232696533, 1.5905482769012451, 0.6902852058410645, -0.6776464581489563, 1.4527207612991333]\n",
      "Grand sum of 811 tensor sets is: [99.0801773071289, 957.6448974609375, -76.21024322509766, -700.5294799804688, 1140.7669677734375]\n",
      "\n",
      "Instance 933 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "jumping at index 2: [0.21545517444610596, 1.9777345657348633, 1.1763720512390137, -1.7225703001022339, 5.687475681304932]\n",
      "Grand sum of 812 tensor sets is: [99.2956314086914, 959.6226196289062, -75.03387451171875, -702.2520751953125, 1146.4544677734375]\n",
      "\n",
      "Instance 934 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([125, 13, 768])\n",
      "Shape of summed layers is: 125 x 768\n",
      "jumping at index 24: [0.9334944486618042, 1.355334758758545, -1.2010105848312378, 1.5413011312484741, 0.4472700357437134]\n",
      "Grand sum of 813 tensor sets is: [100.2291259765625, 960.9779663085938, -76.2348861694336, -700.7107543945312, 1146.9017333984375]\n",
      "\n",
      "Instance 935 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 42: [0.39235180616378784, 3.7074759006500244, 0.08398696035146713, -0.5397744178771973, 2.1599314212799072]\n",
      "Grand sum of 814 tensor sets is: [100.62147521972656, 964.6854248046875, -76.1509017944336, -701.2505493164062, 1149.0616455078125]\n",
      "\n",
      "Instance 936 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 26: [-0.2899157404899597, 0.49383658170700073, 1.232061505317688, -1.9491629600524902, 0.6583724021911621]\n",
      "Grand sum of 815 tensor sets is: [100.33155822753906, 965.1792602539062, -74.91883850097656, -703.19970703125, 1149.719970703125]\n",
      "\n",
      "Instance 937 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 938 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 5: [0.2647077143192291, -0.39070749282836914, -1.129181981086731, -1.5012232065200806, 2.091923952102661]\n",
      "Grand sum of 816 tensor sets is: [100.59626770019531, 964.78857421875, -76.04801940917969, -704.700927734375, 1151.8118896484375]\n",
      "\n",
      "Instance 939 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 29: [0.020569443702697754, 0.5086538791656494, -0.4221397936344147, -0.2950047552585602, 1.4370883703231812]\n",
      "Grand sum of 817 tensor sets is: [100.61683654785156, 965.2972412109375, -76.47016143798828, -704.9959106445312, 1153.2490234375]\n",
      "\n",
      "Instance 940 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 22: [-0.3862418830394745, 1.9509999752044678, -0.10368555784225464, -3.021129846572876, 2.882521629333496]\n",
      "Grand sum of 818 tensor sets is: [100.23059844970703, 967.2482299804688, -76.57384490966797, -708.0170288085938, 1156.131591796875]\n",
      "\n",
      "Instance 941 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 942 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 943 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 22: [0.023632988333702087, 0.2800135612487793, 1.3155953884124756, -1.8660866022109985, 1.1009938716888428]\n",
      "Grand sum of 819 tensor sets is: [100.25423431396484, 967.5282592773438, -75.25824737548828, -709.8831176757812, 1157.2325439453125]\n",
      "\n",
      "Instance 944 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([92, 13, 768])\n",
      "Shape of summed layers is: 92 x 768\n",
      "jumping at index 25: [0.5257376432418823, 1.0534231662750244, 1.5916619300842285, -1.8960223197937012, -0.5981990098953247]\n",
      "Grand sum of 820 tensor sets is: [100.77996826171875, 968.5816650390625, -73.66658782958984, -711.7791137695312, 1156.6343994140625]\n",
      "\n",
      "Instance 945 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 40: [1.410071611404419, 0.9761842489242554, 0.08363066613674164, -1.6310920715332031, -0.040644288063049316]\n",
      "Grand sum of 821 tensor sets is: [102.1900405883789, 969.557861328125, -73.58295440673828, -713.4102172851562, 1156.59375]\n",
      "\n",
      "Instance 946 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "jumping at index 3: [0.24352087080478668, 2.0203301906585693, -0.15989559888839722, 0.29230016469955444, 2.1885874271392822]\n",
      "Grand sum of 822 tensor sets is: [102.43356323242188, 971.5781860351562, -73.74285125732422, -713.117919921875, 1158.7823486328125]\n",
      "\n",
      "Instance 947 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 948 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 2: [-0.33327916264533997, 0.33524972200393677, 0.5195640921592712, -0.5098734498023987, 3.8661181926727295]\n",
      "Grand sum of 823 tensor sets is: [102.10028076171875, 971.9134521484375, -73.2232894897461, -713.6278076171875, 1162.6484375]\n",
      "\n",
      "Instance 949 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 21: [-1.0087319612503052, 0.706183135509491, -0.8091945648193359, -2.3802988529205322, 5.5318098068237305]\n",
      "Grand sum of 824 tensor sets is: [101.09154510498047, 972.61962890625, -74.03248596191406, -716.0081176757812, 1168.1802978515625]\n",
      "\n",
      "Instance 950 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 36: [-0.09767734259366989, -0.02024279534816742, -0.35831907391548157, -1.9791213274002075, 0.24621516466140747]\n",
      "Grand sum of 825 tensor sets is: [100.99386596679688, 972.599365234375, -74.39080810546875, -717.9872436523438, 1168.426513671875]\n",
      "\n",
      "Instance 951 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 6: [0.06544824689626694, 1.4440839290618896, 0.6749966740608215, -1.6313462257385254, 2.187748908996582]\n",
      "Grand sum of 826 tensor sets is: [101.05931091308594, 974.04345703125, -73.71581268310547, -719.6185913085938, 1170.6142578125]\n",
      "\n",
      "Instance 952 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22, 368]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 22: [-0.20570963621139526, 2.138561964035034, 1.0356311798095703, -1.6269596815109253, 3.897801399230957]\n",
      "jumping at index 368: [-0.39903581142425537, 2.2914578914642334, 0.5616462230682373, -0.8992323279380798, 5.037720203399658]\n",
      "Grand sum of 827 tensor sets is: [100.7569351196289, 976.2584838867188, -72.91717529296875, -720.8817138671875, 1175.08203125]\n",
      "\n",
      "Instance 953 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 4: [0.01563168130815029, -0.15827181935310364, -0.11908279359340668, -0.3365756571292877, 2.8346242904663086]\n",
      "Grand sum of 828 tensor sets is: [100.77256774902344, 976.1002197265625, -73.0362548828125, -721.21826171875, 1177.9166259765625]\n",
      "\n",
      "Instance 954 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 829 tensor sets is: [100.84349060058594, 978.3684692382812, -73.70342254638672, -721.072509765625, 1177.7801513671875]\n",
      "\n",
      "Instance 955 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 21: [-0.3690270781517029, 0.6089944839477539, 0.37507274746894836, -1.3476811647415161, 0.4101061224937439]\n",
      "Grand sum of 830 tensor sets is: [100.4744644165039, 978.9774780273438, -73.3283462524414, -722.420166015625, 1178.1903076171875]\n",
      "\n",
      "Instance 956 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 11: [-0.9404209852218628, 1.3332116603851318, -0.29651665687561035, -0.6964857578277588, 0.19971689581871033]\n",
      "Grand sum of 831 tensor sets is: [99.53404235839844, 980.3106689453125, -73.62486267089844, -723.1166381835938, 1178.3900146484375]\n",
      "\n",
      "Instance 957 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 832 tensor sets is: [100.64006805419922, 981.2490844726562, -74.22833251953125, -723.4181518554688, 1180.31005859375]\n",
      "\n",
      "Instance 958 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14, 36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "jumping at index 14: [1.3278692960739136, 0.7764426469802856, 0.009657297283411026, -0.21813002228736877, 4.564845085144043]\n",
      "jumping at index 36: [-0.02493293583393097, 1.335567831993103, -0.06396785378456116, 0.5061655044555664, 2.4638843536376953]\n",
      "Grand sum of 833 tensor sets is: [101.29153442382812, 982.3051147460938, -74.25548553466797, -723.2741088867188, 1183.824462890625]\n",
      "\n",
      "Instance 959 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 7: [0.33699244260787964, 1.2498588562011719, -0.30578190088272095, 0.7316683530807495, -2.560098648071289]\n",
      "Grand sum of 834 tensor sets is: [101.62852478027344, 983.5549926757812, -74.56126403808594, -722.5424194335938, 1181.264404296875]\n",
      "\n",
      "Instance 960 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "jumping at index 4: [-0.08603861927986145, 1.4526970386505127, 1.9108186960220337, -2.265028476715088, 4.058130264282227]\n",
      "Grand sum of 835 tensor sets is: [101.54248809814453, 985.0076904296875, -72.65044403076172, -724.8074340820312, 1185.322509765625]\n",
      "\n",
      "Instance 961 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 6: [0.06544824689626694, 1.4440839290618896, 0.6749966740608215, -1.6313462257385254, 2.187748908996582]\n",
      "Grand sum of 836 tensor sets is: [101.6079330444336, 986.4517822265625, -71.97544860839844, -726.4387817382812, 1187.51025390625]\n",
      "\n",
      "Instance 962 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 963 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 27: [-0.7794146537780762, 1.6792470216751099, 0.47129151225090027, -0.04031819477677345, 1.4984405040740967]\n",
      "Grand sum of 837 tensor sets is: [100.82852172851562, 988.1310424804688, -71.50415802001953, -726.4791259765625, 1189.0086669921875]\n",
      "\n",
      "Instance 964 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 965 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 3: [0.10714997351169586, -0.024638324975967407, -0.7032103538513184, 2.642021656036377, 1.1378231048583984]\n",
      "Grand sum of 838 tensor sets is: [100.9356689453125, 988.1063842773438, -72.20736694335938, -723.8370971679688, 1190.146484375]\n",
      "\n",
      "Instance 966 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3, 13, 22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 3: [-0.3589745759963989, 2.1456704139709473, 1.3634159564971924, -2.3418993949890137, 2.720672607421875]\n",
      "jumping at index 13: [-0.2980554699897766, 2.0440382957458496, 2.104552984237671, -2.896242141723633, 1.2379006147384644]\n",
      "jumping at index 22: [-0.0731336772441864, 1.8769848346710205, 2.1254348754882812, -3.284000873565674, 1.691258430480957]\n",
      "Grand sum of 839 tensor sets is: [100.6922836303711, 990.1286010742188, -70.3428955078125, -726.6777954101562, 1192.02978515625]\n",
      "\n",
      "Instance 967 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([80, 13, 768])\n",
      "Shape of summed layers is: 80 x 768\n",
      "jumping at index 48: [-0.15810933709144592, 2.035818338394165, 0.3949601352214813, -2.7076053619384766, -1.1065068244934082]\n",
      "Grand sum of 840 tensor sets is: [100.53417205810547, 992.1644287109375, -69.94793701171875, -729.3853759765625, 1190.9232177734375]\n",
      "\n",
      "Instance 968 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 14: [-0.18382030725479126, 2.475590944290161, 1.4305212497711182, -3.457306385040283, 1.7122845649719238]\n",
      "Grand sum of 841 tensor sets is: [100.35034942626953, 994.6400146484375, -68.51741790771484, -732.8427124023438, 1192.635498046875]\n",
      "\n",
      "Instance 969 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 6: [0.5185101628303528, 0.6884564161300659, 0.030459482222795486, -1.2817790508270264, 3.0716114044189453]\n",
      "Grand sum of 842 tensor sets is: [100.86885833740234, 995.3284912109375, -68.4869613647461, -734.12451171875, 1195.7071533203125]\n",
      "\n",
      "Instance 970 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "jumping at index 4: [0.45062994956970215, 0.8893719911575317, 0.5036641955375671, -1.1840415000915527, 2.73227596282959]\n",
      "Grand sum of 843 tensor sets is: [101.31948852539062, 996.2178344726562, -67.9832992553711, -735.3085327148438, 1198.439453125]\n",
      "\n",
      "Instance 971 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [252]\n",
      "Size of token embeddings is torch.Size([291, 13, 768])\n",
      "Shape of summed layers is: 291 x 768\n",
      "jumping at index 252: [0.34512102603912354, 0.04645686596632004, -1.191251516342163, -2.8409135341644287, 0.7538981437683105]\n",
      "Grand sum of 844 tensor sets is: [101.66461181640625, 996.2642822265625, -69.17455291748047, -738.1494750976562, 1199.193359375]\n",
      "\n",
      "Instance 972 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 845 tensor sets is: [101.73553466796875, 998.5325317382812, -69.84172058105469, -738.0037231445312, 1199.056884765625]\n",
      "\n",
      "Instance 973 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 10: [0.3634711802005768, 0.8578542470932007, 0.8234139680862427, 0.9631776809692383, -0.08739712834358215]\n",
      "Grand sum of 846 tensor sets is: [102.09900665283203, 999.390380859375, -69.01830291748047, -737.04052734375, 1198.969482421875]\n",
      "\n",
      "Instance 974 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "jumping at index 2: [0.020747467875480652, 1.3123571872711182, -0.09415161609649658, 0.6329725980758667, 5.124721050262451]\n",
      "Grand sum of 847 tensor sets is: [102.1197509765625, 1000.7027587890625, -69.11245727539062, -736.4075317382812, 1204.09423828125]\n",
      "\n",
      "Instance 975 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 976 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 24: [-1.168601632118225, 1.3049286603927612, 1.3325551748275757, -2.0475759506225586, 1.4538986682891846]\n",
      "Grand sum of 848 tensor sets is: [100.9511489868164, 1002.0076904296875, -67.77989959716797, -738.455078125, 1205.548095703125]\n",
      "\n",
      "Instance 977 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 10: [-0.4180607199668884, 0.38969236612319946, -0.2848304808139801, 0.7859995365142822, 0.5217413902282715]\n",
      "Grand sum of 849 tensor sets is: [100.53308868408203, 1002.3973999023438, -68.06472778320312, -737.6690673828125, 1206.06982421875]\n",
      "\n",
      "Instance 978 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 850 tensor sets is: [100.60401153564453, 1004.6656494140625, -68.73189544677734, -737.5233154296875, 1205.933349609375]\n",
      "\n",
      "Instance 979 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 12: [1.2323881387710571, 0.5740475058555603, 0.22570157051086426, -2.702256917953491, -0.793825626373291]\n",
      "Grand sum of 851 tensor sets is: [101.8364028930664, 1005.2396850585938, -68.50619506835938, -740.2255859375, 1205.1395263671875]\n",
      "\n",
      "Instance 980 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 852 tensor sets is: [101.71844482421875, 1005.66796875, -68.32323455810547, -742.4212036132812, 1207.6412353515625]\n",
      "\n",
      "Instance 981 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 982 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 7: [1.3207587003707886, 1.0586144924163818, -0.7446189522743225, -1.2534661293029785, -1.194016933441162]\n",
      "Grand sum of 853 tensor sets is: [103.03919982910156, 1006.7265625, -69.06785583496094, -743.6746826171875, 1206.447265625]\n",
      "\n",
      "Instance 983 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 9: [0.537025511264801, 0.015566099435091019, -1.7376983165740967, -0.5078351497650146, 1.7369410991668701]\n",
      "Grand sum of 854 tensor sets is: [103.57622528076172, 1006.7421264648438, -70.80555725097656, -744.1824951171875, 1208.1842041015625]\n",
      "\n",
      "Instance 984 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 10: [0.41222405433654785, 0.6334203481674194, -0.4091969132423401, -0.6861308813095093, 3.2849647998809814]\n",
      "Grand sum of 855 tensor sets is: [103.98844909667969, 1007.3755493164062, -71.21475219726562, -744.86865234375, 1211.4691162109375]\n",
      "\n",
      "Instance 985 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 28: [0.9800693988800049, 1.6800044775009155, -1.8486523628234863, -2.5592761039733887, 1.1764518022537231]\n",
      "Grand sum of 856 tensor sets is: [104.96852111816406, 1009.0555419921875, -73.06340789794922, -747.4279174804688, 1212.6455078125]\n",
      "\n",
      "Instance 986 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([84, 13, 768])\n",
      "Shape of summed layers is: 84 x 768\n",
      "jumping at index 34: [0.4217416048049927, 1.764238953590393, 0.1948128044605255, 0.013831369578838348, 2.026181936264038]\n",
      "Grand sum of 857 tensor sets is: [105.39026641845703, 1010.8197631835938, -72.86859130859375, -747.4140625, 1214.671630859375]\n",
      "\n",
      "Instance 987 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 988 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 6: [-0.6748464703559875, 1.1967065334320068, 1.0071302652359009, -1.537649393081665, -0.6865041255950928]\n",
      "Grand sum of 858 tensor sets is: [104.71542358398438, 1012.0164794921875, -71.86145782470703, -748.9517211914062, 1213.985107421875]\n",
      "\n",
      "Instance 989 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [-0.45801663398742676, -0.9925666451454163, 0.12019787728786469, -2.552450656890869, 2.2982001304626465]\n",
      "Grand sum of 859 tensor sets is: [104.25740814208984, 1011.02392578125, -71.74125671386719, -751.504150390625, 1216.2833251953125]\n",
      "\n",
      "Instance 990 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 20: [0.5693351030349731, 1.4609131813049316, 0.43588218092918396, -0.9282507300376892, -0.9807419776916504]\n",
      "Grand sum of 860 tensor sets is: [104.82674407958984, 1012.48486328125, -71.30537414550781, -752.432373046875, 1215.3026123046875]\n",
      "\n",
      "Instance 991 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 11: [0.05567881837487221, 2.888899326324463, -0.7419373989105225, 1.777370572090149, 0.36278727650642395]\n",
      "Grand sum of 861 tensor sets is: [104.8824234008789, 1015.373779296875, -72.04730987548828, -750.655029296875, 1215.6654052734375]\n",
      "\n",
      "Instance 992 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 42: [0.22862111032009125, 0.879109263420105, 0.04811791330575943, -1.093132734298706, 3.3559107780456543]\n",
      "Grand sum of 862 tensor sets is: [105.11104583740234, 1016.2528686523438, -71.99919128417969, -751.7481689453125, 1219.0213623046875]\n",
      "\n",
      "Instance 993 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 5: [0.226921945810318, 1.9414998292922974, -0.6454330086708069, -0.21431457996368408, 0.5733477473258972]\n",
      "Grand sum of 863 tensor sets is: [105.33796691894531, 1018.1943969726562, -72.64462280273438, -751.9624633789062, 1219.5947265625]\n",
      "\n",
      "Instance 994 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 7: [-0.34107404947280884, 1.4503344297409058, 1.1670690774917603, -0.451070100069046, -1.1333807706832886]\n",
      "Grand sum of 864 tensor sets is: [104.99689483642578, 1019.6447143554688, -71.47755432128906, -752.4135131835938, 1218.4613037109375]\n",
      "\n",
      "Instance 995 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 19: [0.3025940954685211, 1.4932992458343506, -0.18579375743865967, 1.4605445861816406, 2.95237398147583]\n",
      "Grand sum of 865 tensor sets is: [105.29949188232422, 1021.1380004882812, -71.66334533691406, -750.9529418945312, 1221.4136962890625]\n",
      "\n",
      "Instance 996 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22, 43]\n",
      "Size of token embeddings is torch.Size([66, 13, 768])\n",
      "Shape of summed layers is: 66 x 768\n",
      "jumping at index 22: [0.16200068593025208, 1.0900218486785889, -0.6114656925201416, -1.5332577228546143, 5.187577247619629]\n",
      "jumping at index 43: [0.4812912344932556, 0.7969609498977661, -0.9203105568885803, -0.114717036485672, 6.271055221557617]\n",
      "Grand sum of 866 tensor sets is: [105.62113952636719, 1022.0814819335938, -72.42922973632812, -751.7769165039062, 1227.14306640625]\n",
      "\n",
      "Instance 997 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "jumping at index 9: [0.042227089405059814, 0.7560409307479858, 1.8086810111999512, -1.1038177013397217, -1.0360687971115112]\n",
      "Grand sum of 867 tensor sets is: [105.66336822509766, 1022.8375244140625, -70.62055206298828, -752.8807373046875, 1226.1070556640625]\n",
      "\n",
      "Instance 998 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 28: [0.018564745783805847, 0.3941499590873718, 0.9187951683998108, -0.657402515411377, 0.09331995248794556]\n",
      "Grand sum of 868 tensor sets is: [105.68193054199219, 1023.231689453125, -69.7017593383789, -753.5381469726562, 1226.2003173828125]\n",
      "\n",
      "Instance 999 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1000 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 13: [-0.21071583032608032, 2.300943374633789, -1.1538610458374023, -0.8799316883087158, -0.13595278561115265]\n",
      "Grand sum of 869 tensor sets is: [105.4712142944336, 1025.5325927734375, -70.85562133789062, -754.4180908203125, 1226.0643310546875]\n",
      "\n",
      "Instance 1001 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 28: [-0.17981253564357758, 1.6350996494293213, 0.7735832333564758, -0.9063516855239868, 0.05021873116493225]\n",
      "Grand sum of 870 tensor sets is: [105.2914047241211, 1027.167724609375, -70.08203887939453, -755.324462890625, 1226.114501953125]\n",
      "\n",
      "Instance 1002 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "jumping at index 56: [0.3352566659450531, 0.051159366965293884, -0.08840422332286835, -0.388019859790802, -0.6741926670074463]\n",
      "Grand sum of 871 tensor sets is: [105.62666320800781, 1027.2188720703125, -70.17044067382812, -755.7124633789062, 1225.4403076171875]\n",
      "\n",
      "Instance 1003 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 2: [0.338803231716156, 0.8053044676780701, -0.4703495502471924, -0.03326955437660217, 0.30696380138397217]\n",
      "Grand sum of 872 tensor sets is: [105.96546936035156, 1028.024169921875, -70.64079284667969, -755.7457275390625, 1225.747314453125]\n",
      "\n",
      "Instance 1004 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "jumping at index 2: [-0.15057827532291412, 0.6021322011947632, 0.40292370319366455, 0.7843326926231384, -0.22489666938781738]\n",
      "Grand sum of 873 tensor sets is: [105.81488800048828, 1028.6263427734375, -70.23786926269531, -754.9613647460938, 1225.5224609375]\n",
      "\n",
      "Instance 1005 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1006 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "jumping at index 23: [-0.19772833585739136, 1.1875858306884766, 0.7777953147888184, -1.4517982006072998, 4.237594127655029]\n",
      "Grand sum of 874 tensor sets is: [105.61715698242188, 1029.81396484375, -69.46007537841797, -756.4131469726562, 1229.760009765625]\n",
      "\n",
      "Instance 1007 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 875 tensor sets is: [105.68807983398438, 1032.0821533203125, -70.12724304199219, -756.2673950195312, 1229.62353515625]\n",
      "\n",
      "Instance 1008 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 10: [1.0304419994354248, 0.5602362751960754, -0.16812650859355927, -0.7661343216896057, -0.12766501307487488]\n",
      "Grand sum of 876 tensor sets is: [106.71852111816406, 1032.642333984375, -70.29537200927734, -757.0335083007812, 1229.495849609375]\n",
      "\n",
      "Instance 1009 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5, 10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 5: [-0.07831103354692459, 0.9144704341888428, 1.0206133127212524, -0.3876001238822937, 2.88274884223938]\n",
      "jumping at index 10: [-0.2516126334667206, 1.5862977504730225, 1.3703515529632568, -1.2783427238464355, 3.511446714401245]\n",
      "Grand sum of 877 tensor sets is: [106.55355834960938, 1033.8927001953125, -69.09989166259766, -757.866455078125, 1232.6929931640625]\n",
      "\n",
      "Instance 1010 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 25: [0.9731660485267639, 0.31035810708999634, -0.19579502940177917, -1.3055816888809204, 5.179411888122559]\n",
      "Grand sum of 878 tensor sets is: [107.52672576904297, 1034.2030029296875, -69.29568481445312, -759.1720581054688, 1237.8724365234375]\n",
      "\n",
      "Instance 1011 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 6: [0.26484575867652893, 1.0200520753860474, 0.793125569820404, -1.742936372756958, -0.742552638053894]\n",
      "Grand sum of 879 tensor sets is: [107.79157257080078, 1035.2230224609375, -68.50255584716797, -760.9149780273438, 1237.1298828125]\n",
      "\n",
      "Instance 1012 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 3: [-0.15000753104686737, 1.5367943048477173, 0.7817214727401733, -1.4560481309890747, 2.888338088989258]\n",
      "Grand sum of 880 tensor sets is: [107.64156341552734, 1036.759765625, -67.72083282470703, -762.3710327148438, 1240.0181884765625]\n",
      "\n",
      "Instance 1013 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1014 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "jumping at index 23: [0.0024036988615989685, 1.388634204864502, -0.19299528002738953, 0.340522825717926, 1.0238932371139526]\n",
      "Grand sum of 881 tensor sets is: [107.64396667480469, 1038.1484375, -67.91382598876953, -762.030517578125, 1241.0421142578125]\n",
      "\n",
      "Instance 1015 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 7: [0.5645135641098022, 1.5169450044631958, -1.2913496494293213, -0.29798397421836853, 3.9028964042663574]\n",
      "Grand sum of 882 tensor sets is: [108.20848083496094, 1039.6654052734375, -69.2051773071289, -762.3284912109375, 1244.945068359375]\n",
      "\n",
      "Instance 1016 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1017 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 5: [-0.16487039625644684, 2.0809123516082764, -0.03982533514499664, 0.22064973413944244, 0.28740155696868896]\n",
      "Grand sum of 883 tensor sets is: [108.04360961914062, 1041.746337890625, -69.24500274658203, -762.1078491210938, 1245.232421875]\n",
      "\n",
      "Instance 1018 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 5: [-0.7130388021469116, 2.2272589206695557, 0.40641871094703674, -2.341632127761841, 3.3932266235351562]\n",
      "Grand sum of 884 tensor sets is: [107.33057403564453, 1043.9736328125, -68.83858489990234, -764.449462890625, 1248.6256103515625]\n",
      "\n",
      "Instance 1019 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 21: [-0.3690270781517029, 0.6089944839477539, 0.37507274746894836, -1.3476811647415161, 0.4101061224937439]\n",
      "Grand sum of 885 tensor sets is: [106.9615478515625, 1044.5826416015625, -68.46350860595703, -765.797119140625, 1249.0357666015625]\n",
      "\n",
      "Instance 1020 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 36: [0.18490207195281982, 1.6813851594924927, -0.6127765774726868, -1.4117670059204102, 1.349001407623291]\n",
      "Grand sum of 886 tensor sets is: [107.14644622802734, 1046.2640380859375, -69.07628631591797, -767.2088623046875, 1250.384765625]\n",
      "\n",
      "Instance 1021 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "jumping at index 50: [0.2818916440010071, 0.929182767868042, 0.39655590057373047, -2.0364990234375, -0.7524497509002686]\n",
      "Grand sum of 887 tensor sets is: [107.42833709716797, 1047.1932373046875, -68.67973327636719, -769.245361328125, 1249.63232421875]\n",
      "\n",
      "Instance 1022 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "jumping at index 46: [0.8296969532966614, 1.8233622312545776, -0.45063549280166626, -2.9883086681365967, 1.2240177392959595]\n",
      "Grand sum of 888 tensor sets is: [108.2580337524414, 1049.0166015625, -69.13037109375, -772.233642578125, 1250.8563232421875]\n",
      "\n",
      "Instance 1023 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 9: [0.7046055197715759, 0.07044050097465515, 0.20659664273262024, -2.801126718521118, 4.789655685424805]\n",
      "Grand sum of 889 tensor sets is: [108.96263885498047, 1049.0870361328125, -68.92377471923828, -775.0347900390625, 1255.64599609375]\n",
      "\n",
      "Instance 1024 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([5, 13, 768])\n",
      "Shape of summed layers is: 5 x 768\n",
      "jumping at index 3: [-0.9271969199180603, 1.5554709434509277, 1.1696677207946777, -1.2267011404037476, 3.065227508544922]\n",
      "Grand sum of 890 tensor sets is: [108.03543853759766, 1050.6424560546875, -67.75410461425781, -776.261474609375, 1258.711181640625]\n",
      "\n",
      "Instance 1025 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 18: [0.4689202904701233, 0.4614652395248413, -0.12097983062267303, -1.7901724576950073, 1.571666955947876]\n",
      "Grand sum of 891 tensor sets is: [108.50435638427734, 1051.1038818359375, -67.87508392333984, -778.0516357421875, 1260.2828369140625]\n",
      "\n",
      "Instance 1026 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "jumping at index 39: [0.6345549821853638, 0.7282143235206604, 0.9024720788002014, -2.2162187099456787, 3.074106454849243]\n",
      "Grand sum of 892 tensor sets is: [109.13890838623047, 1051.8321533203125, -66.97261047363281, -780.2678833007812, 1263.35693359375]\n",
      "\n",
      "Instance 1027 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 15: [-0.7051292657852173, 0.9931679964065552, 0.3311164975166321, 0.24273215234279633, -0.6256638765335083]\n",
      "Grand sum of 893 tensor sets is: [108.43377685546875, 1052.8253173828125, -66.64149475097656, -780.025146484375, 1262.7313232421875]\n",
      "\n",
      "Instance 1028 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 13: [-0.844961941242218, 1.4445881843566895, -0.7783585786819458, -1.5764769315719604, -0.4836842119693756]\n",
      "Grand sum of 894 tensor sets is: [107.58881378173828, 1054.2698974609375, -67.41985321044922, -781.6016235351562, 1262.2476806640625]\n",
      "\n",
      "Instance 1029 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 12: [0.7082458734512329, 0.1922776997089386, -0.043706633150577545, -2.8919763565063477, 5.0833210945129395]\n",
      "Grand sum of 895 tensor sets is: [108.29705810546875, 1054.462158203125, -67.46356201171875, -784.4935913085938, 1267.3310546875]\n",
      "\n",
      "Instance 1030 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "jumping at index 33: [-0.35379505157470703, 1.8618324995040894, -0.5174007415771484, -1.299837589263916, 0.8436850309371948]\n",
      "Grand sum of 896 tensor sets is: [107.9432601928711, 1056.323974609375, -67.98096466064453, -785.79345703125, 1268.1746826171875]\n",
      "\n",
      "Instance 1031 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 13: [0.12754327058792114, 0.3290135860443115, -0.45480072498321533, -0.393043577671051, 1.7677761316299438]\n",
      "Grand sum of 897 tensor sets is: [108.07080078125, 1056.6529541015625, -68.4357681274414, -786.1865234375, 1269.9425048828125]\n",
      "\n",
      "Instance 1032 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 33: [-1.1198893785476685, 0.5589663982391357, 1.5990439653396606, -1.1379026174545288, 1.9479146003723145]\n",
      "Grand sum of 898 tensor sets is: [106.95091247558594, 1057.2119140625, -66.83672332763672, -787.3244018554688, 1271.890380859375]\n",
      "\n",
      "Instance 1033 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 899 tensor sets is: [107.02183532714844, 1059.4801025390625, -67.50389099121094, -787.1786499023438, 1271.75390625]\n",
      "\n",
      "Instance 1034 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "jumping at index 12: [-0.31560391187667847, 0.7342891693115234, 0.7482239007949829, -1.6974607706069946, 1.994020700454712]\n",
      "Grand sum of 900 tensor sets is: [106.70623016357422, 1060.21435546875, -66.75566864013672, -788.8760986328125, 1273.7479248046875]\n",
      "\n",
      "Instance 1035 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1036 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 18: [0.5740740895271301, 1.1148533821105957, -0.4687334895133972, -0.06620006263256073, 2.066556215286255]\n",
      "Grand sum of 901 tensor sets is: [107.28030395507812, 1061.3292236328125, -67.22440338134766, -788.9423217773438, 1275.814453125]\n",
      "\n",
      "Instance 1037 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [56]\n",
      "Size of token embeddings is torch.Size([105, 13, 768])\n",
      "Shape of summed layers is: 105 x 768\n",
      "jumping at index 56: [0.10928376019001007, 0.6670560836791992, -0.6805736422538757, -1.044701337814331, 2.3987479209899902]\n",
      "Grand sum of 902 tensor sets is: [107.38958740234375, 1061.996337890625, -67.90497589111328, -789.9869995117188, 1278.2132568359375]\n",
      "\n",
      "Instance 1038 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [-0.45801663398742676, -0.9925666451454163, 0.12019787728786469, -2.552450656890869, 2.2982001304626465]\n",
      "Grand sum of 903 tensor sets is: [106.93157196044922, 1061.0037841796875, -67.78477478027344, -792.5394287109375, 1280.511474609375]\n",
      "\n",
      "Instance 1039 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1040 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 5: [0.37926843762397766, 0.07532516121864319, -0.6550198197364807, -3.1424057483673096, 1.2816658020019531]\n",
      "Grand sum of 904 tensor sets is: [107.31083679199219, 1061.0791015625, -68.4397964477539, -795.6818237304688, 1281.7930908203125]\n",
      "\n",
      "Instance 1041 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 13: [0.7244613170623779, 1.183945894241333, 0.1059522032737732, -1.4899629354476929, 6.480525970458984]\n",
      "Grand sum of 905 tensor sets is: [108.0353012084961, 1062.2630615234375, -68.33384704589844, -797.1718139648438, 1288.2735595703125]\n",
      "\n",
      "Instance 1042 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 12: [0.9788152575492859, 2.254517078399658, -0.27061381936073303, 1.0867741107940674, -2.18143367767334]\n",
      "Grand sum of 906 tensor sets is: [109.01411437988281, 1064.517578125, -68.60446166992188, -796.0850219726562, 1286.0921630859375]\n",
      "\n",
      "Instance 1043 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 24: [-1.168601632118225, 1.3049286603927612, 1.3325551748275757, -2.0475759506225586, 1.4538986682891846]\n",
      "Grand sum of 907 tensor sets is: [107.84551239013672, 1065.822509765625, -67.27190399169922, -798.132568359375, 1287.5460205078125]\n",
      "\n",
      "Instance 1044 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 33: [0.8152782917022705, 1.4116679430007935, 0.134597510099411, -0.4210836589336395, 0.8835769891738892]\n",
      "Grand sum of 908 tensor sets is: [108.6607894897461, 1067.234130859375, -67.1373062133789, -798.5536499023438, 1288.4295654296875]\n",
      "\n",
      "Instance 1045 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "jumping at index 24: [0.35247716307640076, 1.0700777769088745, 0.9371951222419739, -0.9119969606399536, -1.448403000831604]\n",
      "Grand sum of 909 tensor sets is: [109.01326751708984, 1068.30419921875, -66.20011138916016, -799.4656372070312, 1286.981201171875]\n",
      "\n",
      "Instance 1046 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 16: [-0.9494363069534302, 1.084290623664856, 1.3339028358459473, -0.07541213929653168, -1.7546687126159668]\n",
      "Grand sum of 910 tensor sets is: [108.06382751464844, 1069.3885498046875, -64.8662109375, -799.5410766601562, 1285.2265625]\n",
      "\n",
      "Instance 1047 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 10: [0.4854053854942322, 3.0854480266571045, -1.1404130458831787, 0.6498169898986816, 4.543484687805176]\n",
      "Grand sum of 911 tensor sets is: [108.54923248291016, 1072.4739990234375, -66.00662231445312, -798.8912353515625, 1289.77001953125]\n",
      "\n",
      "Instance 1048 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 912 tensor sets is: [108.4312744140625, 1072.90234375, -65.82366180419922, -801.0868530273438, 1292.271728515625]\n",
      "\n",
      "Instance 1049 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [59, 67, 86]\n",
      "Size of token embeddings is torch.Size([95, 13, 768])\n",
      "Shape of summed layers is: 95 x 768\n",
      "jumping at index 59: [-0.597472071647644, 1.9290164709091187, 0.39777156710624695, -0.3377723693847656, 4.88252592086792]\n",
      "jumping at index 67: [0.27216440439224243, 0.323249876499176, 0.8599326610565186, -1.0509147644042969, 4.0786848068237305]\n",
      "jumping at index 86: [0.6335155367851257, 1.716138243675232, -0.9949565529823303, -0.8053230047225952, 1.0204575061798096]\n",
      "Grand sum of 913 tensor sets is: [108.53401184082031, 1074.22509765625, -65.73607635498047, -801.8181762695312, 1295.5989990234375]\n",
      "\n",
      "Instance 1050 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([65, 13, 768])\n",
      "Shape of summed layers is: 65 x 768\n",
      "jumping at index 44: [-0.6094821691513062, 0.8042696714401245, 0.13904492557048798, -1.1232070922851562, 2.794036865234375]\n",
      "Grand sum of 914 tensor sets is: [107.92453002929688, 1075.0294189453125, -65.59703063964844, -802.94140625, 1298.39306640625]\n",
      "\n",
      "Instance 1051 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 915 tensor sets is: [108.86698913574219, 1075.0950927734375, -65.96730041503906, -803.4111328125, 1301.0194091796875]\n",
      "\n",
      "Instance 1052 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 30: [-0.8231475353240967, 0.20416110754013062, -0.8184887170791626, -1.1787834167480469, 4.866078853607178]\n",
      "Grand sum of 916 tensor sets is: [108.04383850097656, 1075.2991943359375, -66.7857894897461, -804.5899047851562, 1305.885498046875]\n",
      "\n",
      "Instance 1053 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 18: [-0.14663121104240417, 0.8647294044494629, 0.23719897866249084, -0.7359180450439453, -0.4649866223335266]\n",
      "Grand sum of 917 tensor sets is: [107.89720916748047, 1076.1639404296875, -66.54859161376953, -805.3258056640625, 1305.4205322265625]\n",
      "\n",
      "Instance 1054 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1055 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 18: [-0.09389281272888184, 2.381730556488037, -0.32549095153808594, -2.0120508670806885, 2.182973623275757]\n",
      "Grand sum of 918 tensor sets is: [107.80331420898438, 1078.545654296875, -66.87408447265625, -807.3378295898438, 1307.603515625]\n",
      "\n",
      "Instance 1056 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 13: [0.6902663707733154, 2.7183258533477783, -0.3350308835506439, 0.7006884813308716, 2.0618696212768555]\n",
      "Grand sum of 919 tensor sets is: [108.49358367919922, 1081.2640380859375, -67.20911407470703, -806.6371459960938, 1309.6654052734375]\n",
      "\n",
      "Instance 1057 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 26: [-1.0301182270050049, 2.505089282989502, 0.3863261342048645, -0.5880890488624573, -1.0540591478347778]\n",
      "Grand sum of 920 tensor sets is: [107.46346282958984, 1083.7691650390625, -66.82278442382812, -807.2252197265625, 1308.611328125]\n",
      "\n",
      "Instance 1058 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 7: [-0.23318755626678467, 0.09440761804580688, -0.22055111825466156, 0.2676260471343994, 1.1203038692474365]\n",
      "Grand sum of 921 tensor sets is: [107.23027801513672, 1083.863525390625, -67.0433349609375, -806.9575805664062, 1309.731689453125]\n",
      "\n",
      "Instance 1059 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "jumping at index 7: [0.9583044648170471, 0.09798276424407959, 0.5966603755950928, 1.2945300340652466, 0.15195876359939575]\n",
      "Grand sum of 922 tensor sets is: [108.18858337402344, 1083.9615478515625, -66.4466781616211, -805.6630249023438, 1309.8836669921875]\n",
      "\n",
      "Instance 1060 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1061 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 37: [0.21845246851444244, 0.7483001947402954, -0.7188933491706848, -0.6421590447425842, 0.7699949145317078]\n",
      "Grand sum of 923 tensor sets is: [108.40703582763672, 1084.7098388671875, -67.16557312011719, -806.30517578125, 1310.6536865234375]\n",
      "\n",
      "Instance 1062 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 6: [0.5038516521453857, 0.11363096535205841, -0.8157063722610474, -1.2760887145996094, 1.9054895639419556]\n",
      "Grand sum of 924 tensor sets is: [108.910888671875, 1084.823486328125, -67.98127746582031, -807.5812377929688, 1312.5592041015625]\n",
      "\n",
      "Instance 1063 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 5: [0.2191915512084961, 0.8629750609397888, -0.829214334487915, -1.1574831008911133, -0.20328721404075623]\n",
      "Grand sum of 925 tensor sets is: [109.13008117675781, 1085.6864013671875, -68.81049346923828, -808.7387084960938, 1312.35595703125]\n",
      "\n",
      "Instance 1064 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 23: [0.833997905254364, -0.11222034692764282, 0.5630210638046265, -2.992384433746338, 0.5996201634407043]\n",
      "Grand sum of 926 tensor sets is: [109.96408081054688, 1085.57421875, -68.24747467041016, -811.7310791015625, 1312.95556640625]\n",
      "\n",
      "Instance 1065 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 17: [-0.3757674992084503, 1.4410209655761719, 0.3388148248195648, -1.7374526262283325, 3.890359401702881]\n",
      "Grand sum of 927 tensor sets is: [109.58831024169922, 1087.0152587890625, -67.90866088867188, -813.468505859375, 1316.845947265625]\n",
      "\n",
      "Instance 1066 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 25: [-0.018173806369304657, 0.37627294659614563, 0.013344958424568176, 0.05746757984161377, 0.853621244430542]\n",
      "Grand sum of 928 tensor sets is: [109.57013702392578, 1087.3914794921875, -67.89531707763672, -813.4110107421875, 1317.6995849609375]\n",
      "\n",
      "Instance 1067 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 2: [1.248791217803955, 3.2339556217193604, 0.10171190649271011, 1.80276620388031, 3.040050983428955]\n",
      "Grand sum of 929 tensor sets is: [110.81893157958984, 1090.62548828125, -67.7936019897461, -811.6082153320312, 1320.7396240234375]\n",
      "\n",
      "Instance 1068 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 20: [0.1228397935628891, 3.406187057495117, 0.8704928755760193, -2.9921255111694336, 3.7005882263183594]\n",
      "Grand sum of 930 tensor sets is: [110.9417724609375, 1094.0316162109375, -66.92311096191406, -814.600341796875, 1324.440185546875]\n",
      "\n",
      "Instance 1069 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "jumping at index 6: [0.8558213710784912, 1.1078211069107056, 1.3175387382507324, -1.6295844316482544, 0.6914225220680237]\n",
      "Grand sum of 931 tensor sets is: [111.79759216308594, 1095.139404296875, -65.60557556152344, -816.2299194335938, 1325.131591796875]\n",
      "\n",
      "Instance 1070 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 10: [-1.1546436548233032, 1.9602737426757812, 0.6704723238945007, -0.5705102682113647, 0.36896178126335144]\n",
      "Grand sum of 932 tensor sets is: [110.64295196533203, 1097.0997314453125, -64.93510437011719, -816.8004150390625, 1325.5006103515625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 1071 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 9: [1.4313464164733887, 1.787060260772705, -0.847642719745636, -1.1422570943832397, 0.4533618688583374]\n",
      "Grand sum of 933 tensor sets is: [112.07429504394531, 1098.8868408203125, -65.78274536132812, -817.9426879882812, 1325.9539794921875]\n",
      "\n",
      "Instance 1072 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 13: [0.1951465904712677, 0.46814948320388794, 0.5263881683349609, -0.6699534058570862, 2.7529966831207275]\n",
      "Grand sum of 934 tensor sets is: [112.26943969726562, 1099.35498046875, -65.25635528564453, -818.6126708984375, 1328.70703125]\n",
      "\n",
      "Instance 1073 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 26: [-1.0301182270050049, 2.505089282989502, 0.3863261342048645, -0.5880890488624573, -1.0540591478347778]\n",
      "Grand sum of 935 tensor sets is: [111.23931884765625, 1101.860107421875, -64.87002563476562, -819.2007446289062, 1327.6529541015625]\n",
      "\n",
      "Instance 1074 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 936 tensor sets is: [111.31024169921875, 1104.1282958984375, -65.53719329833984, -819.0549926757812, 1327.5164794921875]\n",
      "\n",
      "Instance 1075 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 23: [1.2821922302246094, 0.6302703619003296, -1.0816880464553833, -2.179220199584961, 2.5441086292266846]\n",
      "Grand sum of 937 tensor sets is: [112.59243774414062, 1104.758544921875, -66.61888122558594, -821.2341918945312, 1330.060546875]\n",
      "\n",
      "Instance 1076 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([92, 13, 768])\n",
      "Shape of summed layers is: 92 x 768\n",
      "jumping at index 25: [0.5257376432418823, 1.0534231662750244, 1.5916619300842285, -1.8960223197937012, -0.5981990098953247]\n",
      "Grand sum of 938 tensor sets is: [113.11817169189453, 1105.81201171875, -65.0272216796875, -823.1301879882812, 1329.46240234375]\n",
      "\n",
      "Instance 1077 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [176]\n",
      "Size of token embeddings is torch.Size([187, 13, 768])\n",
      "Shape of summed layers is: 187 x 768\n",
      "jumping at index 176: [0.7482905387878418, -0.20469540357589722, -0.1262795478105545, -1.7333192825317383, 2.578571081161499]\n",
      "Grand sum of 939 tensor sets is: [113.86646270751953, 1105.6072998046875, -65.15350341796875, -824.863525390625, 1332.041015625]\n",
      "\n",
      "Instance 1078 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 12: [0.032714322209358215, 2.185397148132324, -0.45400524139404297, -2.340874671936035, -0.9337621927261353]\n",
      "Grand sum of 940 tensor sets is: [113.89917755126953, 1107.792724609375, -65.60750579833984, -827.2044067382812, 1331.1072998046875]\n",
      "\n",
      "Instance 1079 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([130, 13, 768])\n",
      "Shape of summed layers is: 130 x 768\n",
      "jumping at index 47: [-0.5222559571266174, 2.19464111328125, -0.4422873556613922, 0.2943294048309326, 0.8757312297821045]\n",
      "Grand sum of 941 tensor sets is: [113.37692260742188, 1109.9873046875, -66.04978942871094, -826.9100952148438, 1331.9830322265625]\n",
      "\n",
      "Instance 1080 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 3: [-0.4427221715450287, 1.2117867469787598, -0.042897745966911316, -2.2887015342712402, 2.8328518867492676]\n",
      "Grand sum of 942 tensor sets is: [112.9342041015625, 1111.1990966796875, -66.09268951416016, -829.1987915039062, 1334.81591796875]\n",
      "\n",
      "Instance 1081 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1082 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1083 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "jumping at index 38: [0.7570106983184814, 2.1565771102905273, -0.5715723037719727, -1.0405645370483398, 3.5149083137512207]\n",
      "Grand sum of 943 tensor sets is: [113.69121551513672, 1113.355712890625, -66.66426086425781, -830.2393798828125, 1338.330810546875]\n",
      "\n",
      "Instance 1084 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 16: [0.8291242122650146, 2.3447353839874268, -1.1893393993377686, -2.417102098464966, -1.074022889137268]\n",
      "Grand sum of 944 tensor sets is: [114.52033996582031, 1115.700439453125, -67.85359954833984, -832.656494140625, 1337.2568359375]\n",
      "\n",
      "Instance 1085 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9, 46]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "jumping at index 9: [-0.09278823435306549, 2.4403867721557617, 0.3435354232788086, -4.628625869750977, 3.1322989463806152]\n",
      "jumping at index 46: [-0.3700089454650879, 1.4809892177581787, 0.15244804322719574, -3.4056951999664307, 1.8647345304489136]\n",
      "Grand sum of 945 tensor sets is: [114.2889404296875, 1117.6611328125, -67.60560607910156, -836.6736450195312, 1339.75537109375]\n",
      "\n",
      "Instance 1086 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 6: [0.23914703726768494, 1.1546095609664917, -0.4048100411891937, -1.5991638898849487, -2.920808792114258]\n",
      "Grand sum of 946 tensor sets is: [114.52808380126953, 1118.8157958984375, -68.01041412353516, -838.2728271484375, 1336.8345947265625]\n",
      "\n",
      "Instance 1087 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 25: [-0.5749988555908203, 0.96495521068573, -0.027700988575816154, -1.1943520307540894, 0.4515261948108673]\n",
      "Grand sum of 947 tensor sets is: [113.95308685302734, 1119.78076171875, -68.03811645507812, -839.4671630859375, 1337.2861328125]\n",
      "\n",
      "Instance 1088 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 14: [-0.14225932955741882, 1.1928808689117432, 0.9920032024383545, -1.5583573579788208, 1.848941683769226]\n",
      "Grand sum of 948 tensor sets is: [113.81082916259766, 1120.9736328125, -67.04611206054688, -841.0255126953125, 1339.1351318359375]\n",
      "\n",
      "Instance 1089 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 20: [0.8306000232696533, 1.5905482769012451, 0.6902852058410645, -0.6776464581489563, 1.4527207612991333]\n",
      "Grand sum of 949 tensor sets is: [114.64142608642578, 1122.564208984375, -66.35582733154297, -841.7031860351562, 1340.587890625]\n",
      "\n",
      "Instance 1090 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 8: [-0.3246769905090332, 1.3023380041122437, 0.8424094915390015, 0.03773108124732971, 1.383526086807251]\n",
      "Grand sum of 950 tensor sets is: [114.3167495727539, 1123.8665771484375, -65.51342010498047, -841.6654663085938, 1341.971435546875]\n",
      "\n",
      "Instance 1091 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 28: [-0.3326568603515625, 0.6814192533493042, 0.3379512429237366, -1.6457868814468384, -0.9429492354393005]\n",
      "Grand sum of 951 tensor sets is: [113.98409271240234, 1124.5479736328125, -65.17546844482422, -843.311279296875, 1341.0284423828125]\n",
      "\n",
      "Instance 1092 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1093 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 3: [0.12300816178321838, 0.3466910123825073, 1.0876100063323975, -2.044049024581909, 3.431976795196533]\n",
      "Grand sum of 952 tensor sets is: [114.10710144042969, 1124.8946533203125, -64.08786010742188, -845.3553466796875, 1344.46044921875]\n",
      "\n",
      "Instance 1094 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 8: [-0.06283314526081085, 2.704713821411133, 0.5438830852508545, -1.0691442489624023, -0.2953910529613495]\n",
      "Grand sum of 953 tensor sets is: [114.04426574707031, 1127.599365234375, -63.543975830078125, -846.4244995117188, 1344.1650390625]\n",
      "\n",
      "Instance 1095 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1096 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 26: [-0.2899157404899597, 0.49383658170700073, 1.232061505317688, -1.9491629600524902, 0.6583724021911621]\n",
      "Grand sum of 954 tensor sets is: [113.75434875488281, 1128.09326171875, -62.311912536621094, -848.3736572265625, 1344.8233642578125]\n",
      "\n",
      "Instance 1097 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 14: [-0.14878439903259277, 1.009660243988037, 0.5033732652664185, -0.8320887088775635, -0.6811625957489014]\n",
      "Grand sum of 955 tensor sets is: [113.6055679321289, 1129.1029052734375, -61.80854034423828, -849.2057495117188, 1344.1422119140625]\n",
      "\n",
      "Instance 1098 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "jumping at index 30: [0.6916707754135132, 2.0465869903564453, 0.435209184885025, -0.7176985740661621, 3.0297441482543945]\n",
      "Grand sum of 956 tensor sets is: [114.2972412109375, 1131.1495361328125, -61.37333297729492, -849.9234619140625, 1347.1719970703125]\n",
      "\n",
      "Instance 1099 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 957 tensor sets is: [114.3681640625, 1133.417724609375, -62.040504455566406, -849.7777099609375, 1347.0355224609375]\n",
      "\n",
      "Instance 1100 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 13: [-0.830101728439331, 0.06645972281694412, 1.2185111045837402, -1.1364784240722656, 0.19858834147453308]\n",
      "Grand sum of 958 tensor sets is: [113.5380630493164, 1133.484130859375, -60.82199478149414, -850.9141845703125, 1347.234130859375]\n",
      "\n",
      "Instance 1101 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 2: [-0.26093313097953796, 1.4591214656829834, 0.8060914874076843, -3.500817060470581, 2.1021854877471924]\n",
      "Grand sum of 959 tensor sets is: [113.27713012695312, 1134.9432373046875, -60.01590347290039, -854.4149780273438, 1349.3363037109375]\n",
      "\n",
      "Instance 1102 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 26: [-0.2899157404899597, 0.49383658170700073, 1.232061505317688, -1.9491629600524902, 0.6583724021911621]\n",
      "Grand sum of 960 tensor sets is: [112.98721313476562, 1135.4371337890625, -58.78384017944336, -856.3641357421875, 1349.99462890625]\n",
      "\n",
      "Instance 1103 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "jumping at index 6: [0.6541302800178528, 0.15481078624725342, 1.0914006233215332, -2.519733190536499, -0.29033219814300537]\n",
      "Grand sum of 961 tensor sets is: [113.64134216308594, 1135.5919189453125, -57.692440032958984, -858.8838500976562, 1349.704345703125]\n",
      "\n",
      "Instance 1104 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 962 tensor sets is: [113.71226501464844, 1137.860107421875, -58.35961151123047, -858.7380981445312, 1349.56787109375]\n",
      "\n",
      "Instance 1105 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1106 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 22: [0.4399569630622864, 2.4980456829071045, -0.978396475315094, -0.46248626708984375, 1.9889761209487915]\n",
      "Grand sum of 963 tensor sets is: [114.1522216796875, 1140.358154296875, -59.338008880615234, -859.2005615234375, 1351.556884765625]\n",
      "\n",
      "Instance 1107 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1108 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "jumping at index 3: [0.7338294982910156, 1.4274051189422607, 1.3882348537445068, -1.3043895959854126, 1.8279898166656494]\n",
      "Grand sum of 964 tensor sets is: [114.88604736328125, 1141.7855224609375, -57.94977569580078, -860.5049438476562, 1353.3848876953125]\n",
      "\n",
      "Instance 1109 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 32: [0.11277887970209122, 1.151456594467163, -0.31885266304016113, -1.8204846382141113, 0.6192172765731812]\n",
      "Grand sum of 965 tensor sets is: [114.99882507324219, 1142.93701171875, -58.26862716674805, -862.325439453125, 1354.004150390625]\n",
      "\n",
      "Instance 1110 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1111 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "jumping at index 7: [-0.2442341446876526, 0.0010925233364105225, 0.7177641987800598, -1.4655706882476807, 2.2643465995788574]\n",
      "Grand sum of 966 tensor sets is: [114.75459289550781, 1142.9381103515625, -57.55086135864258, -863.791015625, 1356.2685546875]\n",
      "\n",
      "Instance 1112 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 14: [0.33788853883743286, 1.1905254125595093, -0.955978512763977, -0.23321285843849182, 0.1453719139099121]\n",
      "Grand sum of 967 tensor sets is: [115.09248352050781, 1144.128662109375, -58.506839752197266, -864.0242309570312, 1356.4139404296875]\n",
      "\n",
      "Instance 1113 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([65, 13, 768])\n",
      "Shape of summed layers is: 65 x 768\n",
      "jumping at index 7: [-0.6215159893035889, 1.767713189125061, -0.12466464936733246, -0.8954287767410278, 2.4737613201141357]\n",
      "Grand sum of 968 tensor sets is: [114.4709701538086, 1145.8963623046875, -58.63150405883789, -864.919677734375, 1358.8876953125]\n",
      "\n",
      "Instance 1114 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 23: [0.7185612916946411, 1.483185052871704, 0.8792330026626587, -1.5330853462219238, -0.32554519176483154]\n",
      "Grand sum of 969 tensor sets is: [115.18952941894531, 1147.3795166015625, -57.75226974487305, -866.4527587890625, 1358.5621337890625]\n",
      "\n",
      "Instance 1115 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1116 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 12: [0.014271581545472145, 0.7698538303375244, 0.670220673084259, -2.274761199951172, 2.379193067550659]\n",
      "Grand sum of 970 tensor sets is: [115.20380401611328, 1148.1494140625, -57.08205032348633, -868.7275390625, 1360.9412841796875]\n",
      "\n",
      "Instance 1117 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 15: [-0.6566887497901917, 0.9134571552276611, -1.4263105392456055, 1.034759283065796, 1.7500241994857788]\n",
      "Grand sum of 971 tensor sets is: [114.54711151123047, 1149.0628662109375, -58.50836181640625, -867.6928100585938, 1362.6912841796875]\n",
      "\n",
      "Instance 1118 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 17: [1.140397548675537, 0.11614613234996796, -0.2750130593776703, -0.8440282344818115, 2.7345893383026123]\n",
      "Grand sum of 972 tensor sets is: [115.68750762939453, 1149.178955078125, -58.78337478637695, -868.536865234375, 1365.4259033203125]\n",
      "\n",
      "Instance 1119 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 8: [-0.09640547633171082, 1.3825467824935913, 0.8743740320205688, -3.438652992248535, 4.043587684631348]\n",
      "Grand sum of 973 tensor sets is: [115.59110260009766, 1150.5615234375, -57.909000396728516, -871.9755249023438, 1369.469482421875]\n",
      "\n",
      "Instance 1120 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 14: [0.4401259422302246, 1.6799509525299072, -0.2945428788661957, 0.3238125443458557, 0.9823015332221985]\n",
      "Grand sum of 974 tensor sets is: [116.0312271118164, 1152.241455078125, -58.20354461669922, -871.6517333984375, 1370.4517822265625]\n",
      "\n",
      "Instance 1121 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 11: [0.037330158054828644, 0.9371075630187988, -0.2796443700790405, -3.1707541942596436, 6.032158851623535]\n",
      "Grand sum of 975 tensor sets is: [116.06855773925781, 1153.1785888671875, -58.48318862915039, -874.822509765625, 1376.48388671875]\n",
      "\n",
      "Instance 1122 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 9: [-0.684893012046814, 1.0835795402526855, 1.8157563209533691, -0.4738711714744568, 1.0044677257537842]\n",
      "Grand sum of 976 tensor sets is: [115.3836669921875, 1154.26220703125, -56.66743087768555, -875.29638671875, 1377.4884033203125]\n",
      "\n",
      "Instance 1123 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1124 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "jumping at index 25: [1.3203295469284058, 1.1954823732376099, -0.3904622793197632, -0.015122324228286743, 0.9115465879440308]\n",
      "Grand sum of 977 tensor sets is: [116.70399475097656, 1155.4576416015625, -57.057891845703125, -875.3115234375, 1378.39990234375]\n",
      "\n",
      "Instance 1125 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 13: [1.0037174224853516, 2.6934499740600586, 0.170904278755188, 0.5406844019889832, 0.5075503587722778]\n",
      "Grand sum of 978 tensor sets is: [117.70771026611328, 1158.151123046875, -56.886985778808594, -874.7708129882812, 1378.907470703125]\n",
      "\n",
      "Instance 1126 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 6: [0.9881160259246826, -0.17123669385910034, -0.04006968438625336, -0.3580751121044159, 4.617228031158447]\n",
      "Grand sum of 979 tensor sets is: [118.6958236694336, 1157.9798583984375, -56.92705535888672, -875.12890625, 1383.524658203125]\n",
      "\n",
      "Instance 1127 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 11: [-0.2800106704235077, 0.8600803017616272, -0.5554116368293762, 0.18709246814250946, 2.4371914863586426]\n",
      "Grand sum of 980 tensor sets is: [118.41580963134766, 1158.8399658203125, -57.48246765136719, -874.9418334960938, 1385.9617919921875]\n",
      "\n",
      "Instance 1128 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 24: [0.14966028928756714, 1.8847687244415283, -0.328097939491272, -0.9762862920761108, 3.8596367835998535]\n",
      "Grand sum of 981 tensor sets is: [118.56546783447266, 1160.7247314453125, -57.81056594848633, -875.9180908203125, 1389.8214111328125]\n",
      "\n",
      "Instance 1129 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [-0.45801663398742676, -0.9925666451454163, 0.12019787728786469, -2.552450656890869, 2.2982001304626465]\n",
      "Grand sum of 982 tensor sets is: [118.10745239257812, 1159.732177734375, -57.69036865234375, -878.4705200195312, 1392.11962890625]\n",
      "\n",
      "Instance 1130 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 983 tensor sets is: [119.2134780883789, 1160.670654296875, -58.29384231567383, -878.7720336914062, 1394.0396728515625]\n",
      "\n",
      "Instance 1131 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 9: [-0.7877026200294495, 1.8068042993545532, 0.4628143012523651, 0.7519055604934692, -1.3287990093231201]\n",
      "Grand sum of 984 tensor sets is: [118.42577362060547, 1162.4774169921875, -57.83102798461914, -878.0201416015625, 1392.7108154296875]\n",
      "\n",
      "Instance 1132 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "jumping at index 2: [-0.2269877940416336, 1.1266318559646606, 0.7818954586982727, -1.6964972019195557, 1.5515165328979492]\n",
      "Grand sum of 985 tensor sets is: [118.19878387451172, 1163.60400390625, -57.04913330078125, -879.7166137695312, 1394.2623291015625]\n",
      "\n",
      "Instance 1133 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1134 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [1]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 1: [0.32723551988601685, 0.05863449349999428, -0.0502190962433815, -1.3801488876342773, 6.092805862426758]\n",
      "Grand sum of 986 tensor sets is: [118.52601623535156, 1163.66259765625, -57.0993537902832, -881.0967407226562, 1400.3551025390625]\n",
      "\n",
      "Instance 1135 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 16: [-0.14963462948799133, 1.4631445407867432, -0.9033258557319641, -0.7213727235794067, 2.1618762016296387]\n",
      "Grand sum of 987 tensor sets is: [118.37638092041016, 1165.125732421875, -58.00267791748047, -881.818115234375, 1402.5169677734375]\n",
      "\n",
      "Instance 1136 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 25: [-0.5749988555908203, 0.96495521068573, -0.027700988575816154, -1.1943520307540894, 0.4515261948108673]\n",
      "Grand sum of 988 tensor sets is: [117.80138397216797, 1166.0906982421875, -58.03038024902344, -883.012451171875, 1402.968505859375]\n",
      "\n",
      "Instance 1137 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 20: [0.12154117971658707, 2.618830680847168, -1.5801160335540771, -1.6303468942642212, -0.24597233533859253]\n",
      "Grand sum of 989 tensor sets is: [117.92292785644531, 1168.70947265625, -59.610496520996094, -884.642822265625, 1402.7225341796875]\n",
      "\n",
      "Instance 1138 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 23: [0.7258334755897522, 1.5169594287872314, 0.6997877359390259, 0.36567187309265137, 1.3399882316589355]\n",
      "Grand sum of 990 tensor sets is: [118.64875793457031, 1170.2264404296875, -58.910709381103516, -884.2771606445312, 1404.0625]\n",
      "\n",
      "Instance 1139 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "jumping at index 2: [-0.04568064957857132, 2.143955945968628, -0.5306233167648315, -1.1445549726486206, 2.130248546600342]\n",
      "Grand sum of 991 tensor sets is: [118.60308074951172, 1172.370361328125, -59.44133377075195, -885.4216918945312, 1406.1927490234375]\n",
      "\n",
      "Instance 1140 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 12: [0.1998007595539093, 0.8632572889328003, -0.017344485968351364, 0.1803893744945526, 0.04403418302536011]\n",
      "Grand sum of 992 tensor sets is: [118.8028793334961, 1173.233642578125, -59.45867919921875, -885.2413330078125, 1406.23681640625]\n",
      "\n",
      "Instance 1141 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "jumping at index 13: [0.3605128526687622, 0.9686628580093384, 0.14072251319885254, -0.4315699338912964, -0.8107609748840332]\n",
      "Grand sum of 993 tensor sets is: [119.16339111328125, 1174.2022705078125, -59.317955017089844, -885.6729125976562, 1405.426025390625]\n",
      "\n",
      "Instance 1142 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15, 31]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 15: [0.08026880025863647, 0.7037127017974854, 0.3708629906177521, -1.6507105827331543, 2.9486613273620605]\n",
      "jumping at index 31: [1.9059010744094849, 1.1347347497940063, -1.3747845888137817, -0.05924868956208229, 5.593079566955566]\n",
      "Grand sum of 994 tensor sets is: [120.15647888183594, 1175.1214599609375, -59.819915771484375, -886.5278930664062, 1409.6968994140625]\n",
      "\n",
      "Instance 1143 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1144 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 5: [1.1697816848754883, 1.3629698753356934, -0.5974289774894714, 1.9180444478988647, 0.7186437845230103]\n",
      "Grand sum of 995 tensor sets is: [121.32626342773438, 1176.484375, -60.41734313964844, -884.60986328125, 1410.41552734375]\n",
      "\n",
      "Instance 1145 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 15: [0.6160544753074646, 0.1404692679643631, 0.47749853134155273, -1.2258670330047607, 1.7250100374221802]\n",
      "Grand sum of 996 tensor sets is: [121.94231414794922, 1176.6248779296875, -59.93984603881836, -885.8357543945312, 1412.1405029296875]\n",
      "\n",
      "Instance 1146 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "jumping at index 28: [0.9057254791259766, 4.065723896026611, -0.6582452058792114, 1.6810951232910156, 1.6211390495300293]\n",
      "Grand sum of 997 tensor sets is: [122.84803771972656, 1180.6905517578125, -60.59809112548828, -884.1546630859375, 1413.7615966796875]\n",
      "\n",
      "Instance 1147 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1148 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 7: [0.22961752116680145, 1.6327959299087524, 0.48533952236175537, -3.218366861343384, 3.285374164581299]\n",
      "Grand sum of 998 tensor sets is: [123.07765197753906, 1182.3233642578125, -60.11275100708008, -887.373046875, 1417.0469970703125]\n",
      "\n",
      "Instance 1149 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 999 tensor sets is: [122.41199493408203, 1184.5589599609375, -61.28410339355469, -888.2943115234375, 1416.1715087890625]\n",
      "\n",
      "Instance 1150 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1151 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5, 18]\n",
      "Size of token embeddings is torch.Size([82, 13, 768])\n",
      "Shape of summed layers is: 82 x 768\n",
      "jumping at index 5: [0.8417825698852539, 1.4342056512832642, 1.1467734575271606, -1.527406930923462, 3.9550724029541016]\n",
      "jumping at index 18: [0.8150472044944763, 0.6396292448043823, 1.2116628885269165, -0.4615841209888458, 3.2076048851013184]\n",
      "Grand sum of 1000 tensor sets is: [123.24040985107422, 1185.5958251953125, -60.10488510131836, -889.288818359375, 1419.7528076171875]\n",
      "\n",
      "Instance 1152 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "jumping at index 31: [0.2582052946090698, 2.082854747772217, -0.23387348651885986, 0.5653214454650879, -0.10724392533302307]\n",
      "Grand sum of 1001 tensor sets is: [123.49861145019531, 1187.6787109375, -60.33876037597656, -888.7235107421875, 1419.6455078125]\n",
      "\n",
      "Instance 1153 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 12: [-0.44202691316604614, 1.1793547868728638, 0.5913769602775574, 0.07940148562192917, 0.3556559681892395]\n",
      "Grand sum of 1002 tensor sets is: [123.05658721923828, 1188.8580322265625, -59.74738311767578, -888.6441040039062, 1420.001220703125]\n",
      "\n",
      "Instance 1154 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "jumping at index 45: [0.2587438225746155, 0.03212229907512665, 0.8522984981536865, -0.28927963972091675, -0.500694990158081]\n",
      "Grand sum of 1003 tensor sets is: [123.3153305053711, 1188.89013671875, -58.895084381103516, -888.9334106445312, 1419.50048828125]\n",
      "\n",
      "Instance 1155 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [59]\n",
      "Size of token embeddings is torch.Size([95, 13, 768])\n",
      "Shape of summed layers is: 95 x 768\n",
      "jumping at index 59: [0.19104720652103424, 0.5245542526245117, 0.6101614832878113, -2.317316770553589, 2.155891180038452]\n",
      "Grand sum of 1004 tensor sets is: [123.50637817382812, 1189.4146728515625, -58.2849235534668, -891.250732421875, 1421.6563720703125]\n",
      "\n",
      "Instance 1156 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 30: [-0.8231475353240967, 0.20416110754013062, -0.8184887170791626, -1.1787834167480469, 4.866078853607178]\n",
      "Grand sum of 1005 tensor sets is: [122.6832275390625, 1189.6187744140625, -59.10341262817383, -892.4295043945312, 1426.5224609375]\n",
      "\n",
      "Instance 1157 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 21: [-0.03564516082406044, 2.9609222412109375, -1.965681552886963, -1.4648174047470093, 1.2329652309417725]\n",
      "Grand sum of 1006 tensor sets is: [122.6475830078125, 1192.5797119140625, -61.069095611572266, -893.8943481445312, 1427.75537109375]\n",
      "\n",
      "Instance 1158 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 30: [0.34793397784233093, 1.9186053276062012, -2.1105310916900635, -0.12762320041656494, 0.3603436350822449]\n",
      "Grand sum of 1007 tensor sets is: [122.99551391601562, 1194.498291015625, -63.17962646484375, -894.02197265625, 1428.11572265625]\n",
      "\n",
      "Instance 1159 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 34: [1.0066293478012085, 1.427017092704773, 0.45475372672080994, -1.4354588985443115, 1.3843114376068115]\n",
      "Grand sum of 1008 tensor sets is: [124.00214385986328, 1195.92529296875, -62.72487258911133, -895.4574584960938, 1429.5]\n",
      "\n",
      "Instance 1160 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 32: [-0.3589252829551697, 0.8914625644683838, 1.3874173164367676, -1.9580202102661133, 1.8122650384902954]\n",
      "Grand sum of 1009 tensor sets is: [123.64321899414062, 1196.8167724609375, -61.33745574951172, -897.4154663085938, 1431.312255859375]\n",
      "\n",
      "Instance 1161 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 10: [0.9770615696907043, 3.1887905597686768, -0.42276495695114136, 0.8634552955627441, 2.097226619720459]\n",
      "Grand sum of 1010 tensor sets is: [124.62027740478516, 1200.005615234375, -61.76021957397461, -896.552001953125, 1433.409423828125]\n",
      "\n",
      "Instance 1162 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 5: [0.06507087498903275, 2.038386821746826, 0.004037138074636459, -1.522611141204834, 2.951043128967285]\n",
      "Grand sum of 1011 tensor sets is: [124.68534851074219, 1202.0439453125, -61.75618362426758, -898.0745849609375, 1436.3604736328125]\n",
      "\n",
      "Instance 1163 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 28: [-0.35210275650024414, 1.434852123260498, 0.6825548410415649, 0.8300846219062805, 0.6295202970504761]\n",
      "Grand sum of 1012 tensor sets is: [124.33324432373047, 1203.478759765625, -61.07362747192383, -897.2445068359375, 1436.989990234375]\n",
      "\n",
      "Instance 1164 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1165 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 3: [0.12813431024551392, 2.034485101699829, -0.1865609586238861, 0.4503786265850067, 1.2658368349075317]\n",
      "Grand sum of 1013 tensor sets is: [124.46138000488281, 1205.5133056640625, -61.260189056396484, -896.7941284179688, 1438.255859375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 1166 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 21: [-0.4487553536891937, 1.8973160982131958, 0.3433661162853241, -2.414736032485962, 4.088293552398682]\n",
      "Grand sum of 1014 tensor sets is: [124.01262664794922, 1207.41064453125, -60.91682434082031, -899.2088623046875, 1442.3441162109375]\n",
      "\n",
      "Instance 1167 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([65, 13, 768])\n",
      "Shape of summed layers is: 65 x 768\n",
      "jumping at index 53: [-0.21031302213668823, 0.8493586182594299, 0.46800678968429565, -1.2802234888076782, 1.7717103958129883]\n",
      "Grand sum of 1015 tensor sets is: [123.80231475830078, 1208.260009765625, -60.44881820678711, -900.4890747070312, 1444.1158447265625]\n",
      "\n",
      "Instance 1168 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 10: [-0.14724689722061157, 1.0688234567642212, 0.16377422213554382, 0.7648996710777283, 1.2127821445465088]\n",
      "Grand sum of 1016 tensor sets is: [123.65506744384766, 1209.328857421875, -60.2850456237793, -899.7241821289062, 1445.32861328125]\n",
      "\n",
      "Instance 1169 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 4: [-0.4136340320110321, 0.6692928075790405, -0.7181901931762695, -2.4487388134002686, 1.8473122119903564]\n",
      "Grand sum of 1017 tensor sets is: [123.2414321899414, 1209.9981689453125, -61.00323486328125, -902.1729125976562, 1447.1759033203125]\n",
      "\n",
      "Instance 1170 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1171 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 9: [-0.11845327913761139, 1.4147177934646606, 1.0932838916778564, -2.644174337387085, 2.8766400814056396]\n",
      "Grand sum of 1018 tensor sets is: [123.12297821044922, 1211.412841796875, -59.909950256347656, -904.8170776367188, 1450.052490234375]\n",
      "\n",
      "Instance 1172 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 15: [-0.6566887497901917, 0.9134571552276611, -1.4263105392456055, 1.034759283065796, 1.7500241994857788]\n",
      "Grand sum of 1019 tensor sets is: [122.4662857055664, 1212.3262939453125, -61.33626174926758, -903.7823486328125, 1451.802490234375]\n",
      "\n",
      "Instance 1173 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1174 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 7: [0.28161221742630005, 3.0076346397399902, -0.10581955313682556, -2.5508031845092773, 4.196926116943359]\n",
      "Grand sum of 1020 tensor sets is: [122.74789428710938, 1215.333984375, -61.442081451416016, -906.3331298828125, 1455.9993896484375]\n",
      "\n",
      "Instance 1175 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "jumping at index 5: [0.06436984241008759, 0.4686775505542755, 0.022655285894870758, 0.48830029368400574, -1.0753027200698853]\n",
      "Grand sum of 1021 tensor sets is: [122.81226348876953, 1215.8026123046875, -61.41942596435547, -905.8448486328125, 1454.924072265625]\n",
      "\n",
      "Instance 1176 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1177 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "jumping at index 7: [1.1763309240341187, 1.8286149501800537, -0.19783616065979004, -1.5603753328323364, 0.04268553853034973]\n",
      "Grand sum of 1022 tensor sets is: [123.98859405517578, 1217.6312255859375, -61.61726379394531, -907.4052124023438, 1454.966796875]\n",
      "\n",
      "Instance 1178 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1179 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 5: [-0.12943467497825623, -0.03332150727510452, -0.6506512761116028, -1.1671987771987915, 0.7541149258613586]\n",
      "Grand sum of 1023 tensor sets is: [123.85916137695312, 1217.597900390625, -62.267913818359375, -908.5723876953125, 1455.720947265625]\n",
      "\n",
      "Instance 1180 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1181 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 3: [-0.42160505056381226, -0.5365645885467529, 1.8085284233093262, -0.9823049902915955, 2.8464407920837402]\n",
      "Grand sum of 1024 tensor sets is: [123.43755340576172, 1217.061279296875, -60.45938491821289, -909.5546875, 1458.5673828125]\n",
      "\n",
      "Instance 1182 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 2: [0.44988948106765747, 1.9763628244400024, -0.3153543770313263, -2.293877363204956, 6.5860795974731445]\n",
      "Grand sum of 1025 tensor sets is: [123.88744354248047, 1219.03759765625, -60.77473831176758, -911.8485717773438, 1465.1534423828125]\n",
      "\n",
      "Instance 1183 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 15: [0.6160544753074646, 0.1404692679643631, 0.47749853134155273, -1.2258670330047607, 1.7250100374221802]\n",
      "Grand sum of 1026 tensor sets is: [124.50349426269531, 1219.1781005859375, -60.2972412109375, -913.074462890625, 1466.87841796875]\n",
      "\n",
      "Instance 1184 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 29: [0.020569443702697754, 0.5086538791656494, -0.4221397936344147, -0.2950047552585602, 1.4370883703231812]\n",
      "Grand sum of 1027 tensor sets is: [124.52406311035156, 1219.686767578125, -60.71937942504883, -913.3694458007812, 1468.3155517578125]\n",
      "\n",
      "Instance 1185 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 7: [0.2865865230560303, 2.2952051162719727, -0.4844112694263458, -0.2023710459470749, 3.2676215171813965]\n",
      "Grand sum of 1028 tensor sets is: [124.8106460571289, 1221.98193359375, -61.203792572021484, -913.5718383789062, 1471.5831298828125]\n",
      "\n",
      "Instance 1186 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 9: [0.5399065613746643, 3.0352165699005127, -0.16569405794143677, 0.4379880428314209, -1.3063398599624634]\n",
      "Grand sum of 1029 tensor sets is: [125.35055541992188, 1225.01708984375, -61.36948776245117, -913.1338500976562, 1470.2767333984375]\n",
      "\n",
      "Instance 1187 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1188 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 10: [0.0861014574766159, 1.826906681060791, 0.7810129523277283, -1.9100122451782227, 3.42948055267334]\n",
      "Grand sum of 1030 tensor sets is: [125.43665313720703, 1226.843994140625, -60.58847427368164, -915.0438842773438, 1473.7061767578125]\n",
      "\n",
      "Instance 1189 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 15: [0.6160544753074646, 0.1404692679643631, 0.47749853134155273, -1.2258670330047607, 1.7250100374221802]\n",
      "Grand sum of 1031 tensor sets is: [126.05270385742188, 1226.9844970703125, -60.11097717285156, -916.269775390625, 1475.43115234375]\n",
      "\n",
      "Instance 1190 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 1032 tensor sets is: [126.12362670898438, 1229.252685546875, -60.77814865112305, -916.1240234375, 1475.294677734375]\n",
      "\n",
      "Instance 1191 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 1033 tensor sets is: [126.19454956054688, 1231.5208740234375, -61.44532012939453, -915.978271484375, 1475.158203125]\n",
      "\n",
      "Instance 1192 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1193 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 21: [0.4866870939731598, 0.7099449634552002, -2.458674669265747, 2.8655948638916016, 3.893641710281372]\n",
      "Grand sum of 1034 tensor sets is: [126.68123626708984, 1232.2308349609375, -63.903995513916016, -913.1126708984375, 1479.0518798828125]\n",
      "\n",
      "Instance 1194 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 10: [0.3934102952480316, 2.57570481300354, -0.29296016693115234, -0.40247711539268494, 2.305676221847534]\n",
      "Grand sum of 1035 tensor sets is: [127.07464599609375, 1234.8065185546875, -64.19695281982422, -913.51513671875, 1481.3575439453125]\n",
      "\n",
      "Instance 1195 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 8: [0.6574286818504333, 3.0194385051727295, -0.10047688335180283, 1.1424096822738647, 1.6274151802062988]\n",
      "Grand sum of 1036 tensor sets is: [127.73207092285156, 1237.825927734375, -64.29743194580078, -912.3727416992188, 1482.9849853515625]\n",
      "\n",
      "Instance 1196 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 25: [0.5190436840057373, 0.8700224757194519, -0.8603976964950562, -0.6370492577552795, 3.5333447456359863]\n",
      "Grand sum of 1037 tensor sets is: [128.25111389160156, 1238.6959228515625, -65.15782928466797, -913.009765625, 1486.518310546875]\n",
      "\n",
      "Instance 1197 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 15: [-0.6566887497901917, 0.9134571552276611, -1.4263105392456055, 1.034759283065796, 1.7500241994857788]\n",
      "Grand sum of 1038 tensor sets is: [127.59442138671875, 1239.609375, -66.58413696289062, -911.9750366210938, 1488.268310546875]\n",
      "\n",
      "Instance 1198 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([96, 13, 768])\n",
      "Shape of summed layers is: 96 x 768\n",
      "jumping at index 35: [0.5365764498710632, 1.0145232677459717, 1.4655184745788574, -0.875105082988739, -2.9615864753723145]\n",
      "Grand sum of 1039 tensor sets is: [128.13099670410156, 1240.6239013671875, -65.11862182617188, -912.8501586914062, 1485.3067626953125]\n",
      "\n",
      "Instance 1199 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "jumping at index 5: [1.1414028406143188, 1.0477741956710815, 0.30047595500946045, 1.3525488376617432, 2.6725101470947266]\n",
      "Grand sum of 1040 tensor sets is: [129.27239990234375, 1241.671630859375, -64.81814575195312, -911.4976196289062, 1487.979248046875]\n",
      "\n",
      "Instance 1200 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 6: [-0.06272770464420319, 0.25544214248657227, -0.11385463923215866, -1.3724913597106934, 1.4398703575134277]\n",
      "Grand sum of 1041 tensor sets is: [129.2096710205078, 1241.9271240234375, -64.93199920654297, -912.8701171875, 1489.4190673828125]\n",
      "\n",
      "Instance 1201 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "jumping at index 29: [0.5149160623550415, 1.8001139163970947, 0.07961207628250122, 0.5757966041564941, -1.2400494813919067]\n",
      "Grand sum of 1042 tensor sets is: [129.72459411621094, 1243.727294921875, -64.85238647460938, -912.2943115234375, 1488.1790771484375]\n",
      "\n",
      "Instance 1202 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 18: [0.09362941980361938, 1.8545362949371338, -0.2669375538825989, -0.869598388671875, -2.3431084156036377]\n",
      "Grand sum of 1043 tensor sets is: [129.81822204589844, 1245.581787109375, -65.11932373046875, -913.1639404296875, 1485.8359375]\n",
      "\n",
      "Instance 1203 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 22: [-0.5339784026145935, 1.1113027334213257, -0.010615558363497257, -1.3126296997070312, 1.4165459871292114]\n",
      "Grand sum of 1044 tensor sets is: [129.28424072265625, 1246.693115234375, -65.12993621826172, -914.4765625, 1487.25244140625]\n",
      "\n",
      "Instance 1204 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 21: [-0.24502511322498322, -0.17150768637657166, -0.32967185974121094, -0.39416149258613586, 0.7207329273223877]\n",
      "Grand sum of 1045 tensor sets is: [129.03921508789062, 1246.5216064453125, -65.45960998535156, -914.8707275390625, 1487.97314453125]\n",
      "\n",
      "Instance 1205 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([65, 13, 768])\n",
      "Shape of summed layers is: 65 x 768\n",
      "jumping at index 28: [-0.4354334771633148, 0.5395017266273499, 1.4408977031707764, -1.8802568912506104, 3.124866485595703]\n",
      "Grand sum of 1046 tensor sets is: [128.60377502441406, 1247.0611572265625, -64.01871490478516, -916.7509765625, 1491.0980224609375]\n",
      "\n",
      "Instance 1206 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 8: [0.450242817401886, 1.1013394594192505, -0.5497515201568604, -0.53336501121521, -0.2226167917251587]\n",
      "Grand sum of 1047 tensor sets is: [129.05401611328125, 1248.1624755859375, -64.56846618652344, -917.2843627929688, 1490.8753662109375]\n",
      "\n",
      "Instance 1207 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 1048 tensor sets is: [128.93606567382812, 1248.5908203125, -64.38550567626953, -919.47998046875, 1493.3770751953125]\n",
      "\n",
      "Instance 1208 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 26: [0.7026260495185852, 0.3814255893230438, 0.3939426839351654, -1.5107369422912598, -1.3141968250274658]\n",
      "Grand sum of 1049 tensor sets is: [129.63868713378906, 1248.9722900390625, -63.99156188964844, -920.99072265625, 1492.0628662109375]\n",
      "\n",
      "Instance 1209 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "jumping at index 2: [0.6420036554336548, 1.3083809614181519, -0.7563582062721252, -0.163419708609581, 1.6147119998931885]\n",
      "Grand sum of 1050 tensor sets is: [130.2806854248047, 1250.2806396484375, -64.74791717529297, -921.1541137695312, 1493.6776123046875]\n",
      "\n",
      "Instance 1210 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 34: [0.4832979142665863, 4.117745399475098, 0.4552106261253357, 1.704148530960083, -0.9056045413017273]\n",
      "Grand sum of 1051 tensor sets is: [130.76397705078125, 1254.3984375, -64.29270935058594, -919.449951171875, 1492.77197265625]\n",
      "\n",
      "Instance 1211 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 7: [1.8798917531967163, 0.754073977470398, -1.0040310621261597, 1.4105520248413086, 4.478516101837158]\n",
      "Grand sum of 1052 tensor sets is: [132.6438751220703, 1255.1524658203125, -65.29673767089844, -918.0394287109375, 1497.25048828125]\n",
      "\n",
      "Instance 1212 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 15: [-0.39191022515296936, 1.9845350980758667, 0.21233582496643066, 0.03290702402591705, -1.630664587020874]\n",
      "Grand sum of 1053 tensor sets is: [132.25196838378906, 1257.136962890625, -65.08440399169922, -918.0065307617188, 1495.619873046875]\n",
      "\n",
      "Instance 1213 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 1054 tensor sets is: [132.13401794433594, 1257.5653076171875, -64.90144348144531, -920.2021484375, 1498.12158203125]\n",
      "\n",
      "Instance 1214 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 10: [0.12948980927467346, 2.052687406539917, -0.6886069178581238, 0.20600590109825134, -0.5850812792778015]\n",
      "Grand sum of 1055 tensor sets is: [132.2635040283203, 1259.6180419921875, -65.59004974365234, -919.9961547851562, 1497.5364990234375]\n",
      "\n",
      "Instance 1215 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 1056 tensor sets is: [132.1455535888672, 1260.04638671875, -65.40708923339844, -922.1917724609375, 1500.0382080078125]\n",
      "\n",
      "Instance 1216 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "jumping at index 3: [0.35589200258255005, 1.2698299884796143, -0.2807168960571289, -2.838674306869507, 2.624447822570801]\n",
      "Grand sum of 1057 tensor sets is: [132.50144958496094, 1261.316162109375, -65.68780517578125, -925.0304565429688, 1502.66259765625]\n",
      "\n",
      "Instance 1217 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1218 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 1058 tensor sets is: [132.57237243652344, 1263.5843505859375, -66.35497283935547, -924.8847045898438, 1502.526123046875]\n",
      "\n",
      "Instance 1219 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 12: [0.14864864945411682, 2.8004796504974365, -0.2929815649986267, -0.8400757312774658, 1.4223129749298096]\n",
      "Grand sum of 1059 tensor sets is: [132.7210235595703, 1266.3848876953125, -66.64795684814453, -925.7247924804688, 1503.948486328125]\n",
      "\n",
      "Instance 1220 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 11: [0.5511431097984314, 1.725875973701477, 0.4837196171283722, -0.08816338330507278, 0.14515084028244019]\n",
      "Grand sum of 1060 tensor sets is: [133.2721710205078, 1268.1107177734375, -66.16423797607422, -925.8129272460938, 1504.0936279296875]\n",
      "\n",
      "Instance 1221 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 2: [0.21382635831832886, 1.0804924964904785, 1.0731837749481201, -2.1894335746765137, 1.423061490058899]\n",
      "Grand sum of 1061 tensor sets is: [133.48599243164062, 1269.191162109375, -65.09105682373047, -928.0023803710938, 1505.5167236328125]\n",
      "\n",
      "Instance 1222 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([130, 13, 768])\n",
      "Shape of summed layers is: 130 x 768\n",
      "jumping at index 47: [-0.5222559571266174, 2.19464111328125, -0.4422873556613922, 0.2943294048309326, 0.8757312297821045]\n",
      "Grand sum of 1062 tensor sets is: [132.96372985839844, 1271.3857421875, -65.53334045410156, -927.7080688476562, 1506.3924560546875]\n",
      "\n",
      "Instance 1223 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 3: [-0.6300530433654785, 1.0184173583984375, 0.1606988161802292, 0.10598099231719971, -0.40959861874580383]\n",
      "Grand sum of 1063 tensor sets is: [132.33367919921875, 1272.4041748046875, -65.37264251708984, -927.6021118164062, 1505.98291015625]\n",
      "\n",
      "Instance 1224 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 8: [0.5400097966194153, 1.3095262050628662, -0.7401522397994995, 0.24796269834041595, 3.085163116455078]\n",
      "Grand sum of 1064 tensor sets is: [132.87368774414062, 1273.7137451171875, -66.11279296875, -927.3541259765625, 1509.068115234375]\n",
      "\n",
      "Instance 1225 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 1065 tensor sets is: [132.20802307128906, 1275.9493408203125, -67.28414154052734, -928.275390625, 1508.192626953125]\n",
      "\n",
      "Instance 1226 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1227 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 25: [1.4225000143051147, 2.701568603515625, -0.0404810905456543, -2.430591106414795, -1.0758450031280518]\n",
      "Grand sum of 1066 tensor sets is: [133.63052368164062, 1278.65087890625, -67.32462310791016, -930.7059936523438, 1507.1168212890625]\n",
      "\n",
      "Instance 1228 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 12: [-0.024918444454669952, 0.15157155692577362, 0.20858025550842285, -0.8701708316802979, 0.5554196834564209]\n",
      "Grand sum of 1067 tensor sets is: [133.60560607910156, 1278.802490234375, -67.11604309082031, -931.576171875, 1507.6722412109375]\n",
      "\n",
      "Instance 1229 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "jumping at index 24: [-0.29616740345954895, 0.7504483461380005, -0.8280429840087891, -1.467978596687317, -0.650956928730011]\n",
      "Grand sum of 1068 tensor sets is: [133.30943298339844, 1279.552978515625, -67.94408416748047, -933.0441284179688, 1507.021240234375]\n",
      "\n",
      "Instance 1230 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 13: [-1.072502613067627, 1.4499375820159912, 0.700900137424469, 0.1764742136001587, 0.34802210330963135]\n",
      "Grand sum of 1069 tensor sets is: [132.23692321777344, 1281.0029296875, -67.2431869506836, -932.86767578125, 1507.3692626953125]\n",
      "\n",
      "Instance 1231 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 5: [-0.04820326715707779, 0.6203490495681763, 0.6340709328651428, -1.3026694059371948, 1.0303808450698853]\n",
      "Grand sum of 1070 tensor sets is: [132.188720703125, 1281.623291015625, -66.60911560058594, -934.1703491210938, 1508.399658203125]\n",
      "\n",
      "Instance 1232 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 16: [1.1555243730545044, 3.6165807247161865, -1.4209545850753784, -0.649924099445343, 1.077185869216919]\n",
      "Grand sum of 1071 tensor sets is: [133.34423828125, 1285.2398681640625, -68.03006744384766, -934.8202514648438, 1509.476806640625]\n",
      "\n",
      "Instance 1233 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 1072 tensor sets is: [134.45025634765625, 1286.1783447265625, -68.63353729248047, -935.1217651367188, 1511.3968505859375]\n",
      "\n",
      "Instance 1234 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1235 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3, 21]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 3: [0.7418760657310486, -0.8132890462875366, 1.2566837072372437, -1.381488561630249, 4.411623954772949]\n",
      "jumping at index 21: [-0.4512147605419159, 0.3013485074043274, 1.7343271970748901, -2.692249298095703, 3.3880884647369385]\n",
      "Grand sum of 1073 tensor sets is: [134.5955810546875, 1285.92236328125, -67.13803100585938, -937.1586303710938, 1515.2967529296875]\n",
      "\n",
      "Instance 1236 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 3: [-0.49774909019470215, 1.5053365230560303, -0.3491864502429962, -1.1604585647583008, 2.993807792663574]\n",
      "Grand sum of 1074 tensor sets is: [134.09783935546875, 1287.427734375, -67.48722076416016, -938.319091796875, 1518.29052734375]\n",
      "\n",
      "Instance 1237 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 5: [-0.678438127040863, 0.26044324040412903, 0.35167133808135986, -1.3650439977645874, 1.6772310733795166]\n",
      "Grand sum of 1075 tensor sets is: [133.41940307617188, 1287.688232421875, -67.13555145263672, -939.6841430664062, 1519.9677734375]\n",
      "\n",
      "Instance 1238 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 11: [0.023355305194854736, 0.9790647625923157, 0.910525918006897, -1.2109103202819824, 1.143540382385254]\n",
      "Grand sum of 1076 tensor sets is: [133.44276428222656, 1288.667236328125, -66.22502899169922, -940.8950805664062, 1521.111328125]\n",
      "\n",
      "Instance 1239 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 10: [1.4805079698562622, 1.434775471687317, 0.10996745526790619, -0.5283557176589966, 2.5943996906280518]\n",
      "Grand sum of 1077 tensor sets is: [134.92327880859375, 1290.10205078125, -66.11505889892578, -941.4234619140625, 1523.7056884765625]\n",
      "\n",
      "Instance 1240 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([138, 13, 768])\n",
      "Shape of summed layers is: 138 x 768\n",
      "jumping at index 11: [0.8881516456604004, 0.5340417623519897, -0.7874788045883179, -1.7507705688476562, 6.038873672485352]\n",
      "Grand sum of 1078 tensor sets is: [135.81143188476562, 1290.6361083984375, -66.90253448486328, -943.1742553710938, 1529.7445068359375]\n",
      "\n",
      "Instance 1241 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "jumping at index 48: [0.8041865825653076, 1.3624011278152466, -0.7524381875991821, -1.263533115386963, 1.8690009117126465]\n",
      "Grand sum of 1079 tensor sets is: [136.61561584472656, 1291.99853515625, -67.65497589111328, -944.4378051757812, 1531.613525390625]\n",
      "\n",
      "Instance 1242 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 22: [1.0373203754425049, 0.3330126404762268, -0.6624323129653931, -0.8866279721260071, 3.320871353149414]\n",
      "Grand sum of 1080 tensor sets is: [137.65293884277344, 1292.33154296875, -68.3174057006836, -945.324462890625, 1534.9344482421875]\n",
      "\n",
      "Instance 1243 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 19: [-0.6447800993919373, 1.8102850914001465, -0.8399112820625305, 0.09740263223648071, 3.1582157611846924]\n",
      "Grand sum of 1081 tensor sets is: [137.00816345214844, 1294.141845703125, -69.15731811523438, -945.22705078125, 1538.0926513671875]\n",
      "\n",
      "Instance 1244 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "jumping at index 37: [-0.06132751703262329, -0.1436651647090912, 1.246607780456543, -2.2464282512664795, 1.482811689376831]\n",
      "Grand sum of 1082 tensor sets is: [136.94683837890625, 1293.9981689453125, -67.91071319580078, -947.4734497070312, 1539.575439453125]\n",
      "\n",
      "Instance 1245 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [51]\n",
      "Size of token embeddings is torch.Size([99, 13, 768])\n",
      "Shape of summed layers is: 99 x 768\n",
      "jumping at index 51: [0.5679312348365784, 1.149552583694458, -0.28454387187957764, -1.808149814605713, 0.17608776688575745]\n",
      "Grand sum of 1083 tensor sets is: [137.5147705078125, 1295.147705078125, -68.19525909423828, -949.2816162109375, 1539.7515869140625]\n",
      "\n",
      "Instance 1246 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 5: [0.8096891641616821, 1.4217511415481567, -0.2398504912853241, -0.8757073879241943, -1.759968638420105]\n",
      "Grand sum of 1084 tensor sets is: [138.324462890625, 1296.5694580078125, -68.43511199951172, -950.1573486328125, 1537.9915771484375]\n",
      "\n",
      "Instance 1247 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "jumping at index 15: [-0.36341115832328796, 0.6014723777770996, 0.3761873245239258, 0.44589775800704956, -0.3990001976490021]\n",
      "Grand sum of 1085 tensor sets is: [137.96104431152344, 1297.1708984375, -68.05892181396484, -949.71142578125, 1537.592529296875]\n",
      "\n",
      "Instance 1248 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1249 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 14: [-0.01897997409105301, 0.3358309864997864, -0.5269247889518738, 0.27531886100769043, -0.207275390625]\n",
      "Grand sum of 1086 tensor sets is: [137.9420623779297, 1297.5067138671875, -68.58584594726562, -949.4360961914062, 1537.38525390625]\n",
      "\n",
      "Instance 1250 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "jumping at index 5: [0.9646300673484802, 0.8839856386184692, 1.0686763525009155, -0.025054246187210083, 2.276841402053833]\n",
      "Grand sum of 1087 tensor sets is: [138.9066925048828, 1298.3907470703125, -67.51716613769531, -949.4611206054688, 1539.662109375]\n",
      "\n",
      "Instance 1251 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 16: [0.6730164885520935, 2.0816540718078613, -0.560610294342041, -2.5114526748657227, 1.7626235485076904]\n",
      "Grand sum of 1088 tensor sets is: [139.5797119140625, 1300.472412109375, -68.07777404785156, -951.9725952148438, 1541.4246826171875]\n",
      "\n",
      "Instance 1252 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 16: [-0.13043251633644104, 0.6354612112045288, -0.2181362509727478, -2.454261302947998, -1.0239371061325073]\n",
      "Grand sum of 1089 tensor sets is: [139.44927978515625, 1301.10791015625, -68.29591369628906, -954.4268798828125, 1540.4007568359375]\n",
      "\n",
      "Instance 1253 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "jumping at index 50: [0.2818916440010071, 0.929182767868042, 0.39655590057373047, -2.0364990234375, -0.7524497509002686]\n",
      "Grand sum of 1090 tensor sets is: [139.73117065429688, 1302.037109375, -67.89936065673828, -956.46337890625, 1539.6483154296875]\n",
      "\n",
      "Instance 1254 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1255 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 26: [-0.2899157404899597, 0.49383658170700073, 1.232061505317688, -1.9491629600524902, 0.6583724021911621]\n",
      "Grand sum of 1091 tensor sets is: [139.44125366210938, 1302.531005859375, -66.66729736328125, -958.4125366210938, 1540.306640625]\n",
      "\n",
      "Instance 1256 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 17: [0.2785645127296448, 1.3229854106903076, -1.2049981355667114, -1.5688056945800781, -0.9548807740211487]\n",
      "Grand sum of 1092 tensor sets is: [139.71981811523438, 1303.85400390625, -67.87229919433594, -959.9813232421875, 1539.351806640625]\n",
      "\n",
      "Instance 1257 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "jumping at index 28: [-0.2539221942424774, 1.912196397781372, 0.05327734351158142, -0.4208964705467224, 0.5560922622680664]\n",
      "Grand sum of 1093 tensor sets is: [139.4658966064453, 1305.7662353515625, -67.81902313232422, -960.4022216796875, 1539.907958984375]\n",
      "\n",
      "Instance 1258 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 28: [-0.22825853526592255, 3.212845802307129, 0.4004361033439636, -0.405691921710968, 0.5869470238685608]\n",
      "Grand sum of 1094 tensor sets is: [139.23764038085938, 1308.9791259765625, -67.41858673095703, -960.8079223632812, 1540.494873046875]\n",
      "\n",
      "Instance 1259 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7, 16]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 7: [-0.3549448251724243, 0.15473392605781555, 0.5976057648658752, -1.5962176322937012, 2.901315450668335]\n",
      "jumping at index 16: [-0.34029868245124817, 0.884162187576294, 1.113792061805725, 0.05002245306968689, 1.718746304512024]\n",
      "Grand sum of 1095 tensor sets is: [138.8900146484375, 1309.49853515625, -66.5628890991211, -961.5809936523438, 1542.804931640625]\n",
      "\n",
      "Instance 1260 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 19: [-0.29517862200737, -0.7704188823699951, 0.274096816778183, -0.5151862502098083, 0.12308374047279358]\n",
      "Grand sum of 1096 tensor sets is: [138.59483337402344, 1308.7281494140625, -66.2887954711914, -962.09619140625, 1542.927978515625]\n",
      "\n",
      "Instance 1261 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 30: [-0.8231475353240967, 0.20416110754013062, -0.8184887170791626, -1.1787834167480469, 4.866078853607178]\n",
      "Grand sum of 1097 tensor sets is: [137.7716827392578, 1308.9322509765625, -67.10728454589844, -963.2749633789062, 1547.7940673828125]\n",
      "\n",
      "Instance 1262 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 9: [0.5219102501869202, 1.092707872390747, 0.39612889289855957, -1.993424654006958, 2.3247475624084473]\n",
      "Grand sum of 1098 tensor sets is: [138.29359436035156, 1310.02490234375, -66.7111587524414, -965.2683715820312, 1550.1187744140625]\n",
      "\n",
      "Instance 1263 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1264 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 1099 tensor sets is: [137.6279296875, 1312.260498046875, -67.88250732421875, -966.1896362304688, 1549.2432861328125]\n",
      "\n",
      "Instance 1265 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 8: [0.13416653871536255, 1.9738261699676514, -0.06679611653089523, -0.05616046488285065, 1.9287375211715698]\n",
      "Grand sum of 1100 tensor sets is: [137.76210021972656, 1314.234375, -67.94930267333984, -966.2457885742188, 1551.1719970703125]\n",
      "\n",
      "Instance 1266 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 31: [1.430494785308838, 0.8120304346084595, -0.8986666202545166, 1.6041817665100098, 2.5023880004882812]\n",
      "Grand sum of 1101 tensor sets is: [139.19259643554688, 1315.04638671875, -68.84796905517578, -964.6416015625, 1553.6744384765625]\n",
      "\n",
      "Instance 1267 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 15: [0.7227939963340759, 1.1484272480010986, -0.3671342432498932, -0.6243650913238525, 0.436604768037796]\n",
      "Grand sum of 1102 tensor sets is: [139.91539001464844, 1316.19482421875, -69.21510314941406, -965.2659912109375, 1554.111083984375]\n",
      "\n",
      "Instance 1268 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1269 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "jumping at index 11: [-0.14224442839622498, 1.3110692501068115, -0.47414863109588623, -2.3209140300750732, 0.9969590306282043]\n",
      "Grand sum of 1103 tensor sets is: [139.7731475830078, 1317.505859375, -69.68925476074219, -967.5869140625, 1555.1080322265625]\n",
      "\n",
      "Instance 1270 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "jumping at index 50: [0.7171430587768555, 1.8716447353363037, 0.7503807544708252, -0.2024250477552414, 1.0483607053756714]\n",
      "Grand sum of 1104 tensor sets is: [140.49029541015625, 1319.3775634765625, -68.93887329101562, -967.7893676757812, 1556.1563720703125]\n",
      "\n",
      "Instance 1271 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1272 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1273 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([130, 13, 768])\n",
      "Shape of summed layers is: 130 x 768\n",
      "jumping at index 47: [-0.5222559571266174, 2.19464111328125, -0.4422873556613922, 0.2943294048309326, 0.8757312297821045]\n",
      "Grand sum of 1105 tensor sets is: [139.96803283691406, 1321.572265625, -69.38115692138672, -967.4950561523438, 1557.0321044921875]\n",
      "\n",
      "Instance 1274 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 5: [0.4844236969947815, 1.511811375617981, 0.03365409001708031, 0.522022008895874, 0.23035115003585815]\n",
      "Grand sum of 1106 tensor sets is: [140.45245361328125, 1323.0841064453125, -69.34750366210938, -966.9730224609375, 1557.262451171875]\n",
      "\n",
      "Instance 1275 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1276 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 19: [-0.03830130770802498, 0.9205437302589417, -1.3995954990386963, 0.07167693972587585, -0.016575992107391357]\n",
      "Grand sum of 1107 tensor sets is: [140.41415405273438, 1324.004638671875, -70.74710083007812, -966.9013671875, 1557.245849609375]\n",
      "\n",
      "Instance 1277 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 1108 tensor sets is: [141.52017211914062, 1324.943115234375, -71.35057067871094, -967.202880859375, 1559.1658935546875]\n",
      "\n",
      "Instance 1278 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 2: [0.24410226941108704, 1.5174403190612793, 0.2435320019721985, -0.19182461500167847, 2.690512180328369]\n",
      "Grand sum of 1109 tensor sets is: [141.76426696777344, 1326.4605712890625, -71.10704040527344, -967.3947143554688, 1561.8564453125]\n",
      "\n",
      "Instance 1279 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14, 23]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 14: [-0.5712658166885376, -0.07588112354278564, -0.6544249653816223, -0.9897400736808777, -0.7336229085922241]\n",
      "jumping at index 23: [0.22150598466396332, 2.1849703788757324, -0.5186516046524048, 0.760105550289154, 1.561086893081665]\n",
      "Grand sum of 1110 tensor sets is: [141.58938598632812, 1327.51513671875, -71.6935806274414, -967.509521484375, 1562.2701416015625]\n",
      "\n",
      "Instance 1280 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 1111 tensor sets is: [140.92372131347656, 1329.750732421875, -72.86492919921875, -968.4307861328125, 1561.3946533203125]\n",
      "\n",
      "Instance 1281 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 26: [-0.2899157404899597, 0.49383658170700073, 1.232061505317688, -1.9491629600524902, 0.6583724021911621]\n",
      "Grand sum of 1112 tensor sets is: [140.63380432128906, 1330.24462890625, -71.63286590576172, -970.3799438476562, 1562.052978515625]\n",
      "\n",
      "Instance 1282 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 4: [0.15128733217716217, 1.0873640775680542, -0.7039695382118225, -0.7395501136779785, 1.2007737159729004]\n",
      "Grand sum of 1113 tensor sets is: [140.78509521484375, 1331.33203125, -72.33683776855469, -971.1195068359375, 1563.2537841796875]\n",
      "\n",
      "Instance 1283 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 8: [0.10658231377601624, 0.4081861972808838, -0.3132886588573456, -2.7949869632720947, 0.9311003684997559]\n",
      "Grand sum of 1114 tensor sets is: [140.8916778564453, 1331.740234375, -72.6501235961914, -973.9144897460938, 1564.1849365234375]\n",
      "\n",
      "Instance 1284 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 17: [-0.5670022368431091, 1.3378418684005737, 0.694636344909668, -0.8053896427154541, 1.6702719926834106]\n",
      "Grand sum of 1115 tensor sets is: [140.32467651367188, 1333.078125, -71.95549011230469, -974.7199096679688, 1565.855224609375]\n",
      "\n",
      "Instance 1285 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "jumping at index 26: [-0.24332551658153534, 1.7284153699874878, 0.4067128002643585, -1.227067232131958, -0.21288655698299408]\n",
      "Grand sum of 1116 tensor sets is: [140.0813446044922, 1334.8065185546875, -71.54877471923828, -975.9469604492188, 1565.642333984375]\n",
      "\n",
      "Instance 1286 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 31: [0.7308530807495117, 1.446509599685669, 0.9715222120285034, -0.756840705871582, 1.6107447147369385]\n",
      "Grand sum of 1117 tensor sets is: [140.81219482421875, 1336.2530517578125, -70.57725524902344, -976.7037963867188, 1567.2530517578125]\n",
      "\n",
      "Instance 1287 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 28: [-0.35210275650024414, 1.434852123260498, 0.6825548410415649, 0.8300846219062805, 0.6295202970504761]\n",
      "Grand sum of 1118 tensor sets is: [140.46009826660156, 1337.6878662109375, -69.89469909667969, -975.8737182617188, 1567.882568359375]\n",
      "\n",
      "Instance 1288 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 11: [0.5001713037490845, 1.2123361825942993, 0.9352744817733765, -0.6581228375434875, 1.7622449398040771]\n",
      "Grand sum of 1119 tensor sets is: [140.96026611328125, 1338.900146484375, -68.95942687988281, -976.5318603515625, 1569.644775390625]\n",
      "\n",
      "Instance 1289 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 3: [0.9983445405960083, 1.702528476715088, 0.2699477970600128, 0.9849691987037659, 0.984784722328186]\n",
      "Grand sum of 1120 tensor sets is: [141.9586181640625, 1340.6026611328125, -68.6894760131836, -975.546875, 1570.6295166015625]\n",
      "\n",
      "Instance 1290 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 11: [0.11784416437149048, 0.7459338903427124, 1.767837405204773, -2.5264415740966797, 2.9150595664978027]\n",
      "Grand sum of 1121 tensor sets is: [142.0764617919922, 1341.3486328125, -66.92163848876953, -978.0733032226562, 1573.5445556640625]\n",
      "\n",
      "Instance 1291 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1292 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 13: [-0.49467048048973083, 1.1070008277893066, 2.0101165771484375, -0.5434321165084839, -0.654647946357727]\n",
      "Grand sum of 1122 tensor sets is: [141.581787109375, 1342.4556884765625, -64.9115219116211, -978.6167602539062, 1572.889892578125]\n",
      "\n",
      "Instance 1293 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 19: [-0.03830130770802498, 0.9205437302589417, -1.3995954990386963, 0.07167693972587585, -0.016575992107391357]\n",
      "Grand sum of 1123 tensor sets is: [141.54348754882812, 1343.376220703125, -66.31111907958984, -978.5451049804688, 1572.873291015625]\n",
      "\n",
      "Instance 1294 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 11: [-0.2643984854221344, 0.6758172512054443, -0.13161952793598175, 1.5489534139633179, -2.1573641300201416]\n",
      "Grand sum of 1124 tensor sets is: [141.27908325195312, 1344.052001953125, -66.44274139404297, -976.9961547851562, 1570.7159423828125]\n",
      "\n",
      "Instance 1295 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 8: [0.6933277249336243, 1.8212454319000244, -0.8570820093154907, -0.1672651171684265, -1.140601634979248]\n",
      "Grand sum of 1125 tensor sets is: [141.972412109375, 1345.873291015625, -67.29981994628906, -977.1633911132812, 1569.5753173828125]\n",
      "\n",
      "Instance 1296 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 22: [0.2784970700740814, -1.2637193202972412, 1.1038281917572021, 0.12876532971858978, 0.08561824262142181]\n",
      "Grand sum of 1126 tensor sets is: [142.25091552734375, 1344.609619140625, -66.19599151611328, -977.0346069335938, 1569.660888671875]\n",
      "\n",
      "Instance 1297 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 8: [-0.19321928918361664, 0.31931406259536743, 0.6118962168693542, -0.5738357305526733, 2.123758316040039]\n",
      "Grand sum of 1127 tensor sets is: [142.0576934814453, 1344.928955078125, -65.58409881591797, -977.6084594726562, 1571.78466796875]\n",
      "\n",
      "Instance 1298 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 13: [0.5703390836715698, 0.985329270362854, -1.2494150400161743, -1.2365994453430176, 0.06861445307731628]\n",
      "Grand sum of 1128 tensor sets is: [142.62803649902344, 1345.914306640625, -66.83351135253906, -978.8450317382812, 1571.853271484375]\n",
      "\n",
      "Instance 1299 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 3: [-0.49774909019470215, 1.5053365230560303, -0.3491864502429962, -1.1604585647583008, 2.993807792663574]\n",
      "Grand sum of 1129 tensor sets is: [142.1302947998047, 1347.419677734375, -67.18270111083984, -980.0054931640625, 1574.8470458984375]\n",
      "\n",
      "Instance 1300 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 28: [1.182084083557129, 0.10233286023139954, -0.679546594619751, -1.387450933456421, 2.35876727104187]\n",
      "Grand sum of 1130 tensor sets is: [143.3123779296875, 1347.52197265625, -67.86225128173828, -981.3929443359375, 1577.205810546875]\n",
      "\n",
      "Instance 1301 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 17: [1.4785823822021484, 1.1244779825210571, 0.10887717455625534, -2.2942113876342773, 3.2049319744110107]\n",
      "Grand sum of 1131 tensor sets is: [144.79095458984375, 1348.646484375, -67.75337219238281, -983.6871337890625, 1580.4107666015625]\n",
      "\n",
      "Instance 1302 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 28: [-0.35210275650024414, 1.434852123260498, 0.6825548410415649, 0.8300846219062805, 0.6295202970504761]\n",
      "Grand sum of 1132 tensor sets is: [144.43885803222656, 1350.081298828125, -67.07081604003906, -982.8570556640625, 1581.040283203125]\n",
      "\n",
      "Instance 1303 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand sum of 1133 tensor sets is: [144.50978088378906, 1352.3494873046875, -67.73798370361328, -982.7113037109375, 1580.90380859375]\n",
      "\n",
      "Instance 1304 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1305 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 9: [-0.3024223744869232, 0.8902010917663574, 0.015083743259310722, 0.7616061568260193, 0.7762861251831055]\n",
      "Grand sum of 1134 tensor sets is: [144.2073516845703, 1353.23974609375, -67.722900390625, -981.94970703125, 1581.6800537109375]\n",
      "\n",
      "Instance 1306 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 5: [0.9759706854820251, 0.507866382598877, -0.4072251319885254, 0.0380379855632782, -1.2849617004394531]\n",
      "Grand sum of 1135 tensor sets is: [145.18331909179688, 1353.74755859375, -68.130126953125, -981.9116821289062, 1580.3951416015625]\n",
      "\n",
      "Instance 1307 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1308 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "jumping at index 44: [-0.6368817090988159, 0.6037139296531677, 0.8334202766418457, -1.6647071838378906, 3.1260898113250732]\n",
      "Grand sum of 1136 tensor sets is: [144.5464324951172, 1354.351318359375, -67.29670715332031, -983.576416015625, 1583.521240234375]\n",
      "\n",
      "Instance 1309 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 26: [-0.42238935828208923, 0.5552464127540588, -0.7650701999664307, -2.3228096961975098, 2.3747477531433105]\n",
      "Grand sum of 1137 tensor sets is: [144.12403869628906, 1354.9066162109375, -68.06177520751953, -985.8992309570312, 1585.89599609375]\n",
      "\n",
      "Instance 1310 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 8: [0.11649608612060547, 0.543063759803772, -0.6276301145553589, -1.2466058731079102, 0.3732287883758545]\n",
      "Grand sum of 1138 tensor sets is: [144.24053955078125, 1355.44970703125, -68.68940734863281, -987.1458129882812, 1586.2691650390625]\n",
      "\n",
      "Instance 1311 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 12: [0.08596286177635193, 0.3565206229686737, -0.6839335560798645, 0.7519950866699219, 3.702965259552002]\n",
      "Grand sum of 1139 tensor sets is: [144.32650756835938, 1355.8062744140625, -69.37334442138672, -986.393798828125, 1589.97216796875]\n",
      "\n",
      "Instance 1312 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "jumping at index 38: [0.2717476189136505, 2.116726875305176, -0.48209813237190247, 0.5474063158035278, -0.6847974061965942]\n",
      "Grand sum of 1140 tensor sets is: [144.59825134277344, 1357.9229736328125, -69.8554458618164, -985.8463745117188, 1589.287353515625]\n",
      "\n",
      "Instance 1313 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 10: [0.0871509462594986, 1.076375961303711, 0.9952808022499084, -1.6336662769317627, 2.6344785690307617]\n",
      "Grand sum of 1141 tensor sets is: [144.68540954589844, 1358.9993896484375, -68.86016845703125, -987.4800415039062, 1591.921875]\n",
      "\n",
      "Instance 1314 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "jumping at index 13: [0.49266672134399414, 3.2094552516937256, 0.06130558252334595, 0.5826850533485413, 0.853138267993927]\n",
      "Grand sum of 1142 tensor sets is: [145.17807006835938, 1362.2088623046875, -68.79886627197266, -986.8973388671875, 1592.7750244140625]\n",
      "\n",
      "Instance 1315 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 12: [0.1267954409122467, 2.161839485168457, 1.015095829963684, -0.24178862571716309, 0.21259836852550507]\n",
      "Grand sum of 1143 tensor sets is: [145.30487060546875, 1364.3707275390625, -67.78376770019531, -987.1390991210938, 1592.9876708984375]\n",
      "\n",
      "Instance 1316 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1317 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 13: [-0.47091472148895264, 1.975493311882019, -0.13606517016887665, -0.9335098266601562, -0.337423712015152]\n",
      "Grand sum of 1144 tensor sets is: [144.83395385742188, 1366.34619140625, -67.91983032226562, -988.0726318359375, 1592.6502685546875]\n",
      "\n",
      "Instance 1318 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "jumping at index 28: [0.6530265808105469, 0.4485156834125519, -1.3532534837722778, -2.0567541122436523, 3.0218565464019775]\n",
      "Grand sum of 1145 tensor sets is: [145.4869842529297, 1366.794677734375, -69.27308654785156, -990.12939453125, 1595.672119140625]\n",
      "\n",
      "Instance 1319 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "jumping at index 6: [-0.10690924525260925, 2.012617826461792, 0.9265398979187012, -2.4842638969421387, 2.3060476779937744]\n",
      "Grand sum of 1146 tensor sets is: [145.3800811767578, 1368.8072509765625, -68.34654998779297, -992.6136474609375, 1597.9781494140625]\n",
      "\n",
      "Instance 1320 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 11: [-0.8400286436080933, 0.4650496244430542, 0.683211088180542, -1.9262374639511108, 2.903333902359009]\n",
      "Grand sum of 1147 tensor sets is: [144.54005432128906, 1369.2723388671875, -67.66333770751953, -994.5398559570312, 1600.8814697265625]\n",
      "\n",
      "Instance 1321 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 6: [0.6514647006988525, 2.658696413040161, -1.561407446861267, 2.0427706241607666, 0.2244156301021576]\n",
      "Grand sum of 1148 tensor sets is: [145.19151306152344, 1371.9310302734375, -69.22474670410156, -992.4970703125, 1601.1058349609375]\n",
      "\n",
      "Instance 1322 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 21: [0.46514084935188293, 0.44277423620224, 0.2876852750778198, -2.4315195083618164, 3.023970603942871]\n",
      "Grand sum of 1149 tensor sets is: [145.65664672851562, 1372.373779296875, -68.93706512451172, -994.9285888671875, 1604.1297607421875]\n",
      "\n",
      "Instance 1323 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "jumping at index 38: [-0.9542199373245239, 0.555483341217041, 0.10684607177972794, -0.6490185856819153, 1.5292553901672363]\n",
      "Grand sum of 1150 tensor sets is: [144.70242309570312, 1372.9293212890625, -68.83021545410156, -995.57763671875, 1605.6590576171875]\n",
      "\n",
      "Instance 1324 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 22: [-0.5339784026145935, 1.1113027334213257, -0.010615558363497257, -1.3126296997070312, 1.4165459871292114]\n",
      "Grand sum of 1151 tensor sets is: [144.16844177246094, 1374.0406494140625, -68.84082794189453, -996.8902587890625, 1607.0755615234375]\n",
      "\n",
      "Instance 1325 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 42: [0.39235180616378784, 3.7074759006500244, 0.08398696035146713, -0.5397744178771973, 2.1599314212799072]\n",
      "Grand sum of 1152 tensor sets is: [144.560791015625, 1377.7481689453125, -68.75684356689453, -997.4300537109375, 1609.2354736328125]\n",
      "\n",
      "Instance 1326 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 11: [-0.7200403809547424, 0.9202905893325806, 0.797938883304596, -1.6913402080535889, 1.4499619007110596]\n",
      "Grand sum of 1153 tensor sets is: [143.8407440185547, 1378.66845703125, -67.95890808105469, -999.1213989257812, 1610.6854248046875]\n",
      "\n",
      "Instance 1327 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "jumping at index 6: [-0.07688720524311066, 1.5826109647750854, -0.18721595406532288, -2.314385175704956, 2.5064117908477783]\n",
      "Grand sum of 1154 tensor sets is: [143.76385498046875, 1380.2510986328125, -68.14612579345703, -1001.435791015625, 1613.19189453125]\n",
      "\n",
      "Instance 1328 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 1155 tensor sets is: [144.869873046875, 1381.1895751953125, -68.74959564208984, -1001.7373046875, 1615.1119384765625]\n",
      "\n",
      "Instance 1329 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 1156 tensor sets is: [145.97589111328125, 1382.1280517578125, -69.35306549072266, -1002.038818359375, 1617.031982421875]\n",
      "\n",
      "Instance 1330 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "jumping at index 27: [0.33240240812301636, -0.38151437044143677, -0.9474729299545288, -0.9681206941604614, 1.8185551166534424]\n",
      "Grand sum of 1157 tensor sets is: [146.30828857421875, 1381.74658203125, -70.300537109375, -1003.0069580078125, 1618.8505859375]\n",
      "\n",
      "Instance 1331 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 12: [0.2922954559326172, 1.1521719694137573, 0.5552330613136292, -2.0968563556671143, 6.240932464599609]\n",
      "Grand sum of 1158 tensor sets is: [146.6005859375, 1382.8988037109375, -69.74530029296875, -1005.1038208007812, 1625.091552734375]\n",
      "\n",
      "Instance 1332 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "jumping at index 7: [-0.6251987814903259, 0.9720367193222046, -0.015318462625145912, -1.0636435747146606, -1.6231188774108887]\n",
      "Grand sum of 1159 tensor sets is: [145.9753875732422, 1383.870849609375, -69.7606201171875, -1006.16748046875, 1623.4683837890625]\n",
      "\n",
      "Instance 1333 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 19: [0.3025940954685211, 1.4932992458343506, -0.18579375743865967, 1.4605445861816406, 2.95237398147583]\n",
      "Grand sum of 1160 tensor sets is: [146.27798461914062, 1385.3641357421875, -69.9464111328125, -1004.7069091796875, 1626.4207763671875]\n",
      "\n",
      "Instance 1334 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1335 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "jumping at index 13: [0.204822838306427, 1.0394614934921265, 0.1838023066520691, 0.021946236491203308, 3.2177398204803467]\n",
      "Grand sum of 1161 tensor sets is: [146.48280334472656, 1386.403564453125, -69.76261138916016, -1004.6849365234375, 1629.6385498046875]\n",
      "\n",
      "Instance 1336 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 8: [0.6574286818504333, 3.0194385051727295, -0.10047688335180283, 1.1424096822738647, 1.6274151802062988]\n",
      "Grand sum of 1162 tensor sets is: [147.14022827148438, 1389.4229736328125, -69.86309051513672, -1003.5425415039062, 1631.2659912109375]\n",
      "\n",
      "Instance 1337 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 1163 tensor sets is: [147.02227783203125, 1389.851318359375, -69.68013000488281, -1005.7381591796875, 1633.7677001953125]\n",
      "\n",
      "Instance 1338 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 31: [0.7308530807495117, 1.446509599685669, 0.9715222120285034, -0.756840705871582, 1.6107447147369385]\n",
      "Grand sum of 1164 tensor sets is: [147.7531280517578, 1391.2978515625, -68.70861053466797, -1006.4949951171875, 1635.37841796875]\n",
      "\n",
      "Instance 1339 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 26: [-0.2899157404899597, 0.49383658170700073, 1.232061505317688, -1.9491629600524902, 0.6583724021911621]\n",
      "Grand sum of 1165 tensor sets is: [147.4632110595703, 1391.791748046875, -67.47654724121094, -1008.4441528320312, 1636.0367431640625]\n",
      "\n",
      "Instance 1340 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 12: [-0.961663007736206, 1.684625506401062, -0.5582045316696167, 1.232627511024475, 3.8001503944396973]\n",
      "Grand sum of 1166 tensor sets is: [146.5015411376953, 1393.476318359375, -68.03475189208984, -1007.2115478515625, 1639.8369140625]\n",
      "\n",
      "Instance 1341 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 2: [1.0791456699371338, 1.7008569240570068, 0.5252038836479187, 2.1739730834960938, 2.8281497955322266]\n",
      "Grand sum of 1167 tensor sets is: [147.5806884765625, 1395.1771240234375, -67.5095443725586, -1005.03759765625, 1642.6650390625]\n",
      "\n",
      "Instance 1342 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 29: [0.09273163229227066, 2.736220598220825, -0.46155858039855957, 1.9286001920700073, -0.46970024704933167]\n",
      "Grand sum of 1168 tensor sets is: [147.6734161376953, 1397.913330078125, -67.97109985351562, -1003.1090087890625, 1642.1953125]\n",
      "\n",
      "Instance 1343 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 4: [-0.4136340320110321, 0.6692928075790405, -0.7181901931762695, -2.4487388134002686, 1.8473122119903564]\n",
      "Grand sum of 1169 tensor sets is: [147.25978088378906, 1398.5826416015625, -68.68929290771484, -1005.5577392578125, 1644.0426025390625]\n",
      "\n",
      "Instance 1344 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "jumping at index 3: [0.24352087080478668, 2.0203301906585693, -0.15989559888839722, 0.29230016469955444, 2.1885874271392822]\n",
      "Grand sum of 1170 tensor sets is: [147.5032958984375, 1400.60302734375, -68.84918975830078, -1005.2654418945312, 1646.231201171875]\n",
      "\n",
      "Instance 1345 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 7: [-0.49931061267852783, -1.3099291324615479, 0.5012228488922119, -0.8854540586471558, 2.374804973602295]\n",
      "Grand sum of 1171 tensor sets is: [147.0039825439453, 1399.2930908203125, -68.34796905517578, -1006.15087890625, 1648.60595703125]\n",
      "\n",
      "Instance 1346 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 17: [1.2931166887283325, 1.9607224464416504, -0.01875368505716324, -0.3470510244369507, -1.3428982496261597]\n",
      "Grand sum of 1172 tensor sets is: [148.29710388183594, 1401.2537841796875, -68.3667221069336, -1006.4979248046875, 1647.2630615234375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 1347 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 11: [-0.040970463305711746, 0.3432273268699646, 0.23174548149108887, -2.2738027572631836, 4.995527744293213]\n",
      "Grand sum of 1173 tensor sets is: [148.25613403320312, 1401.5970458984375, -68.13497924804688, -1008.771728515625, 1652.258544921875]\n",
      "\n",
      "Instance 1348 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1349 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 22: [-0.5339784026145935, 1.1113027334213257, -0.010615558363497257, -1.3126296997070312, 1.4165459871292114]\n",
      "Grand sum of 1174 tensor sets is: [147.72215270996094, 1402.7083740234375, -68.14559173583984, -1010.0843505859375, 1653.675048828125]\n",
      "\n",
      "Instance 1350 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1351 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 19: [0.3922426998615265, 0.06087420880794525, 0.36733105778694153, -1.516219973564148, -0.2819420099258423]\n",
      "Grand sum of 1175 tensor sets is: [148.11439514160156, 1402.769287109375, -67.77825927734375, -1011.6005859375, 1653.39306640625]\n",
      "\n",
      "Instance 1352 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 7: [-0.047619935125112534, 1.4382808208465576, -0.8172945976257324, -1.7165710926055908, 1.1618987321853638]\n",
      "Grand sum of 1176 tensor sets is: [148.0667724609375, 1404.20751953125, -68.59555053710938, -1013.317138671875, 1654.554931640625]\n",
      "\n",
      "Instance 1353 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 7: [0.47289860248565674, -0.2473929524421692, 0.7989534139633179, 1.5153692960739136, -1.9184443950653076]\n",
      "Grand sum of 1177 tensor sets is: [148.5396728515625, 1403.9600830078125, -67.79660034179688, -1011.8017578125, 1652.636474609375]\n",
      "\n",
      "Instance 1354 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 29: [1.1631765365600586, 1.9847007989883423, -1.003488540649414, 0.565936803817749, 2.092439889907837]\n",
      "Grand sum of 1178 tensor sets is: [149.70285034179688, 1405.94482421875, -68.80008697509766, -1011.23583984375, 1654.7288818359375]\n",
      "\n",
      "Instance 1355 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([130, 13, 768])\n",
      "Shape of summed layers is: 130 x 768\n",
      "jumping at index 47: [-0.5222559571266174, 2.19464111328125, -0.4422873556613922, 0.2943294048309326, 0.8757312297821045]\n",
      "Grand sum of 1179 tensor sets is: [149.1805877685547, 1408.139404296875, -69.24237060546875, -1010.9415283203125, 1655.6046142578125]\n",
      "\n",
      "Instance 1356 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 13: [-0.45397675037384033, 0.1632172167301178, 0.945384681224823, -1.5936057567596436, 0.07818901538848877]\n",
      "Grand sum of 1180 tensor sets is: [148.7266082763672, 1408.3026123046875, -68.29698944091797, -1012.53515625, 1655.682861328125]\n",
      "\n",
      "Instance 1357 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 4: [0.15383142232894897, 1.3443162441253662, 0.5328470468521118, -1.8637255430221558, 2.4650590419769287]\n",
      "Grand sum of 1181 tensor sets is: [148.88043212890625, 1409.64697265625, -67.76414489746094, -1014.3988647460938, 1658.14794921875]\n",
      "\n",
      "Instance 1358 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 22: [0.05156209319829941, 1.5785856246948242, -0.27191072702407837, -3.6990625858306885, -0.8088696002960205]\n",
      "Grand sum of 1182 tensor sets is: [148.93199157714844, 1411.2255859375, -68.03605651855469, -1018.097900390625, 1657.339111328125]\n",
      "\n",
      "Instance 1359 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [68]\n",
      "Size of token embeddings is torch.Size([156, 13, 768])\n",
      "Shape of summed layers is: 156 x 768\n",
      "jumping at index 68: [-0.19629190862178802, 1.1275196075439453, 0.7770188450813293, -2.2653462886810303, 2.252507209777832]\n",
      "Grand sum of 1183 tensor sets is: [148.73570251464844, 1412.3531494140625, -67.25904083251953, -1020.3632202148438, 1659.5916748046875]\n",
      "\n",
      "Instance 1360 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 21: [-0.4765620827674866, -0.6388790607452393, 1.1897046566009521, -2.76592755317688, 4.393159866333008]\n",
      "Grand sum of 1184 tensor sets is: [148.25914001464844, 1411.7142333984375, -66.0693359375, -1023.129150390625, 1663.98486328125]\n",
      "\n",
      "Instance 1361 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "jumping at index 9: [-0.6587342619895935, 1.3192174434661865, -0.7866958379745483, 0.8879815936088562, 2.635479211807251]\n",
      "Grand sum of 1185 tensor sets is: [147.60040283203125, 1413.033447265625, -66.85603332519531, -1022.2411499023438, 1666.620361328125]\n",
      "\n",
      "Instance 1362 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1363 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 12: [0.9364367723464966, 0.5610723495483398, -0.795062243938446, 0.6189937591552734, 1.3209824562072754]\n",
      "Grand sum of 1186 tensor sets is: [148.53683471679688, 1413.594482421875, -67.65109252929688, -1021.6221313476562, 1667.9412841796875]\n",
      "\n",
      "Instance 1364 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 9: [-0.445920467376709, -0.6818546056747437, -0.19538208842277527, -1.3733646869659424, 3.98281192779541]\n",
      "Grand sum of 1187 tensor sets is: [148.09091186523438, 1412.91259765625, -67.84647369384766, -1022.9954833984375, 1671.924072265625]\n",
      "\n",
      "Instance 1365 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1366 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 26: [-0.2899157404899597, 0.49383658170700073, 1.232061505317688, -1.9491629600524902, 0.6583724021911621]\n",
      "Grand sum of 1188 tensor sets is: [147.80099487304688, 1413.406494140625, -66.61441040039062, -1024.9447021484375, 1672.5823974609375]\n",
      "\n",
      "Instance 1367 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "jumping at index 4: [0.13640853762626648, 0.6525253057479858, -0.0754527822136879, -2.7387750148773193, 2.527949810028076]\n",
      "Grand sum of 1189 tensor sets is: [147.93740844726562, 1414.0589599609375, -66.68986511230469, -1027.6834716796875, 1675.1103515625]\n",
      "\n",
      "Instance 1368 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "jumping at index 26: [0.6520350575447083, 2.5784506797790527, -0.16656944155693054, 0.7368629574775696, -0.7306061387062073]\n",
      "Grand sum of 1190 tensor sets is: [148.58944702148438, 1416.637451171875, -66.85643768310547, -1026.9466552734375, 1674.3797607421875]\n",
      "\n",
      "Instance 1369 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1370 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 1191 tensor sets is: [148.66036987304688, 1418.9056396484375, -67.52360534667969, -1026.8009033203125, 1674.2432861328125]\n",
      "\n",
      "Instance 1371 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 9: [-0.6566410064697266, 0.810236930847168, 1.021889328956604, -0.4259137809276581, 0.8553812503814697]\n",
      "Grand sum of 1192 tensor sets is: [148.00372314453125, 1419.7158203125, -66.50171661376953, -1027.226806640625, 1675.0986328125]\n",
      "\n",
      "Instance 1372 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 36: [-0.3423607349395752, 0.5841048359870911, 0.7428646683692932, -2.8558945655822754, -1.3183057308197021]\n",
      "Grand sum of 1193 tensor sets is: [147.66136169433594, 1420.2999267578125, -65.75885009765625, -1030.0826416015625, 1673.7802734375]\n",
      "\n",
      "Instance 1373 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 10: [1.7505693435668945, 0.39863041043281555, 0.9414114356040955, -2.8210177421569824, 4.02108097076416]\n",
      "Grand sum of 1194 tensor sets is: [149.41192626953125, 1420.6986083984375, -64.81743621826172, -1032.9036865234375, 1677.8013916015625]\n",
      "\n",
      "Instance 1374 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 7: [0.44268736243247986, 0.5771477222442627, 0.24859316647052765, 0.6916621327400208, 0.23440304398536682]\n",
      "Grand sum of 1195 tensor sets is: [149.8546142578125, 1421.2757568359375, -64.56884002685547, -1032.2120361328125, 1678.0357666015625]\n",
      "\n",
      "Instance 1375 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 10: [0.0861014574766159, 1.826906681060791, 0.7810129523277283, -1.9100122451782227, 3.42948055267334]\n",
      "Grand sum of 1196 tensor sets is: [149.9407196044922, 1423.1026611328125, -63.78782653808594, -1034.1220703125, 1681.4652099609375]\n",
      "\n",
      "Instance 1376 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 9: [0.37543195486068726, 1.407677412033081, 0.6603295803070068, -1.6734893321990967, 3.4490156173706055]\n",
      "Grand sum of 1197 tensor sets is: [150.31614685058594, 1424.5103759765625, -63.127498626708984, -1035.7955322265625, 1684.9141845703125]\n",
      "\n",
      "Instance 1377 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 13: [1.6676996946334839, 2.618494987487793, 0.8349012136459351, 0.4299490749835968, -0.08823055028915405]\n",
      "Grand sum of 1198 tensor sets is: [151.9838409423828, 1427.12890625, -62.292598724365234, -1035.3656005859375, 1684.825927734375]\n",
      "\n",
      "Instance 1378 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [84]\n",
      "Size of token embeddings is torch.Size([88, 13, 768])\n",
      "Shape of summed layers is: 88 x 768\n",
      "jumping at index 84: [0.027018696069717407, 1.5432895421981812, -1.008208990097046, -1.7889597415924072, 1.473524808883667]\n",
      "Grand sum of 1199 tensor sets is: [152.0108642578125, 1428.6722412109375, -63.30080795288086, -1037.154541015625, 1686.2994384765625]\n",
      "\n",
      "Instance 1379 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 31: [-0.5779282450675964, 0.26349666714668274, -1.220247507095337, -1.2293801307678223, -0.1118783950805664]\n",
      "Grand sum of 1200 tensor sets is: [151.4329376220703, 1428.935791015625, -64.52105712890625, -1038.3839111328125, 1686.1875]\n",
      "\n",
      "Instance 1380 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 24: [0.6840152740478516, 1.9302408695220947, -0.1704774796962738, -0.004680633544921875, 1.3423069715499878]\n",
      "Grand sum of 1201 tensor sets is: [152.11695861816406, 1430.8660888671875, -64.69153594970703, -1038.3885498046875, 1687.52978515625]\n",
      "\n",
      "Instance 1381 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 7: [0.4579438269138336, 1.311920166015625, -1.096378207206726, -1.3583685159683228, 3.559675693511963]\n",
      "Grand sum of 1202 tensor sets is: [152.5749053955078, 1432.177978515625, -65.78791046142578, -1039.7469482421875, 1691.0894775390625]\n",
      "\n",
      "Instance 1382 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1383 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 21: [-0.22561115026474, 1.199408769607544, -1.124435544013977, -2.2751874923706055, 2.992100477218628]\n",
      "Grand sum of 1203 tensor sets is: [152.3492889404297, 1433.37744140625, -66.91234588623047, -1042.0220947265625, 1694.08154296875]\n",
      "\n",
      "Instance 1384 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 22: [-0.5339784026145935, 1.1113027334213257, -0.010615558363497257, -1.3126296997070312, 1.4165459871292114]\n",
      "Grand sum of 1204 tensor sets is: [151.8153076171875, 1434.48876953125, -66.92295837402344, -1043.334716796875, 1695.498046875]\n",
      "\n",
      "Instance 1385 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 26: [0.6881157159805298, 1.208246111869812, -0.41319042444229126, 0.3908136487007141, 1.168442726135254]\n",
      "Grand sum of 1205 tensor sets is: [152.50341796875, 1435.697021484375, -67.33615112304688, -1042.94384765625, 1696.66650390625]\n",
      "\n",
      "Instance 1386 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "jumping at index 41: [-0.7579550743103027, 0.7489339709281921, 0.8583462238311768, -1.3837218284606934, 3.9166929721832275]\n",
      "Grand sum of 1206 tensor sets is: [151.74546813964844, 1436.4459228515625, -66.4778060913086, -1044.3275146484375, 1700.583251953125]\n",
      "\n",
      "Instance 1387 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "jumping at index 3: [1.2525086402893066, 1.289739727973938, -0.7177460789680481, 0.4740108549594879, 1.066016674041748]\n",
      "Grand sum of 1207 tensor sets is: [152.9979705810547, 1437.7357177734375, -67.19554901123047, -1043.853515625, 1701.6492919921875]\n",
      "\n",
      "Instance 1388 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 9: [0.16068606078624725, 1.885840892791748, -0.6133416891098022, -0.13941678404808044, 6.146084785461426]\n",
      "Grand sum of 1208 tensor sets is: [153.15866088867188, 1439.62158203125, -67.80889129638672, -1043.992919921875, 1707.79541015625]\n",
      "\n",
      "Instance 1389 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 11: [0.17262789607048035, 0.4425719976425171, -1.2976986169815063, -1.8239765167236328, 7.340121746063232]\n",
      "Grand sum of 1209 tensor sets is: [153.33128356933594, 1440.064208984375, -69.1065902709961, -1045.81689453125, 1715.135498046875]\n",
      "\n",
      "Instance 1390 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 17: [0.4204084277153015, 0.044763922691345215, 0.14958365261554718, -3.7319512367248535, 3.255631446838379]\n",
      "Grand sum of 1210 tensor sets is: [153.75169372558594, 1440.1090087890625, -68.9570083618164, -1049.548828125, 1718.39111328125]\n",
      "\n",
      "Instance 1391 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "jumping at index 3: [1.036368489265442, 0.7655541896820068, -0.4072747230529785, 0.6168743371963501, 4.921127796173096]\n",
      "Grand sum of 1211 tensor sets is: [154.78805541992188, 1440.87451171875, -69.3642807006836, -1048.9320068359375, 1723.312255859375]\n",
      "\n",
      "Instance 1392 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 15: [-0.1811130940914154, 0.008588127791881561, 0.4731466770172119, -2.3497228622436523, 1.032616138458252]\n",
      "Grand sum of 1212 tensor sets is: [154.60694885253906, 1440.883056640625, -68.8911361694336, -1051.28173828125, 1724.3448486328125]\n",
      "\n",
      "Instance 1393 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1394 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1395 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 27: [-0.053024083375930786, 0.005966559052467346, 0.722692608833313, -3.280547857284546, 0.34761637449264526]\n",
      "Grand sum of 1213 tensor sets is: [154.55392456054688, 1440.8890380859375, -68.16844177246094, -1054.562255859375, 1724.6925048828125]\n",
      "\n",
      "Instance 1396 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 22: [-0.5682231187820435, -0.27498918771743774, 0.8935573101043701, -0.5903955698013306, -0.4039151966571808]\n",
      "Grand sum of 1214 tensor sets is: [153.98570251464844, 1440.614013671875, -67.27488708496094, -1055.1527099609375, 1724.28857421875]\n",
      "\n",
      "Instance 1397 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 5: [-0.5658390522003174, 2.4790711402893066, 0.7584621906280518, -0.6448022127151489, -0.7330856919288635]\n",
      "Grand sum of 1215 tensor sets is: [153.41986083984375, 1443.0931396484375, -66.51642608642578, -1055.7974853515625, 1723.5555419921875]\n",
      "\n",
      "Instance 1398 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 36: [-0.11342096328735352, -0.20024588704109192, 0.9236140251159668, -2.862352132797241, 0.5080792903900146]\n",
      "Grand sum of 1216 tensor sets is: [153.3064422607422, 1442.8929443359375, -65.59281158447266, -1058.6597900390625, 1724.0635986328125]\n",
      "\n",
      "Instance 1399 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "jumping at index 43: [0.6702526807785034, 1.1282906532287598, -0.8756025433540344, -0.3209509253501892, -0.43803533911705017]\n",
      "Grand sum of 1217 tensor sets is: [153.97669982910156, 1444.021240234375, -66.46841430664062, -1058.980712890625, 1723.6256103515625]\n",
      "\n",
      "Instance 1400 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 12: [0.014271581545472145, 0.7698538303375244, 0.670220673084259, -2.274761199951172, 2.379193067550659]\n",
      "Grand sum of 1218 tensor sets is: [153.990966796875, 1444.7911376953125, -65.7981948852539, -1061.2554931640625, 1726.0047607421875]\n",
      "\n",
      "Instance 1401 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 20: [0.3869006633758545, -0.021019726991653442, 0.7291548252105713, -1.4851915836334229, 0.4334011673927307]\n",
      "Grand sum of 1219 tensor sets is: [154.37786865234375, 1444.7701416015625, -65.06903839111328, -1062.74072265625, 1726.4381103515625]\n",
      "\n",
      "Instance 1402 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 16: [0.1616198867559433, 1.4297727346420288, 0.09509739279747009, -0.4477881193161011, 1.285630702972412]\n",
      "Grand sum of 1220 tensor sets is: [154.53948974609375, 1446.199951171875, -64.97393798828125, -1063.1884765625, 1727.7237548828125]\n",
      "\n",
      "Instance 1403 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [-0.45801663398742676, -0.9925666451454163, 0.12019787728786469, -2.552450656890869, 2.2982001304626465]\n",
      "Grand sum of 1221 tensor sets is: [154.0814666748047, 1445.2073974609375, -64.8537368774414, -1065.740966796875, 1730.02197265625]\n",
      "\n",
      "Instance 1404 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1405 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [56]\n",
      "Size of token embeddings is torch.Size([113, 13, 768])\n",
      "Shape of summed layers is: 113 x 768\n",
      "jumping at index 56: [1.0273727178573608, 0.06734216213226318, 0.7550185918807983, -2.8567793369293213, 0.6297874450683594]\n",
      "Grand sum of 1222 tensor sets is: [155.1088409423828, 1445.2747802734375, -64.09871673583984, -1068.5977783203125, 1730.6517333984375]\n",
      "\n",
      "Instance 1406 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 1223 tensor sets is: [155.1797637939453, 1447.54296875, -64.76588439941406, -1068.4520263671875, 1730.5152587890625]\n",
      "\n",
      "Instance 1407 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 7: [0.767612874507904, 0.3806123733520508, 0.9841358065605164, 0.002268463373184204, -1.6141984462738037]\n",
      "Grand sum of 1224 tensor sets is: [155.94737243652344, 1447.923583984375, -63.7817497253418, -1068.44970703125, 1728.9010009765625]\n",
      "\n",
      "Instance 1408 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 2: [0.9584184885025024, 2.173999547958374, 0.3377169668674469, 0.6446294188499451, 0.9912510514259338]\n",
      "Grand sum of 1225 tensor sets is: [156.90579223632812, 1450.0975341796875, -63.444034576416016, -1067.8050537109375, 1729.8922119140625]\n",
      "\n",
      "Instance 1409 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 5: [0.6881737112998962, 0.10847042500972748, 1.0567643642425537, -2.1350724697113037, -1.0208295583724976]\n",
      "Grand sum of 1226 tensor sets is: [157.59396362304688, 1450.2060546875, -62.387271881103516, -1069.940185546875, 1728.871337890625]\n",
      "\n",
      "Instance 1410 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 1227 tensor sets is: [157.66488647460938, 1452.4742431640625, -63.054443359375, -1069.79443359375, 1728.73486328125]\n",
      "\n",
      "Instance 1411 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "jumping at index 40: [0.7859026193618774, 0.9125480651855469, -0.017595915123820305, -0.15485604107379913, 1.2452678680419922]\n",
      "Grand sum of 1228 tensor sets is: [158.45079040527344, 1453.3868408203125, -63.07204055786133, -1069.9493408203125, 1729.9801025390625]\n",
      "\n",
      "Instance 1412 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1413 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 18: [-0.6447811126708984, 1.4190617799758911, -0.15939617156982422, 1.4409452676773071, -0.39697813987731934]\n",
      "Grand sum of 1229 tensor sets is: [157.80601501464844, 1454.805908203125, -63.23143768310547, -1068.5084228515625, 1729.5831298828125]\n",
      "\n",
      "Instance 1414 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 5: [0.23235131800174713, -0.5252195000648499, -1.3044050931930542, -2.0490546226501465, 0.39547058939933777]\n",
      "Grand sum of 1230 tensor sets is: [158.03836059570312, 1454.2806396484375, -64.53584289550781, -1070.5574951171875, 1729.9786376953125]\n",
      "\n",
      "Instance 1415 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 7: [-1.1834008693695068, 1.2997305393218994, -0.39373356103897095, 0.5017326474189758, -1.6832995414733887]\n",
      "Grand sum of 1231 tensor sets is: [156.85496520996094, 1455.580322265625, -64.92957305908203, -1070.0557861328125, 1728.2952880859375]\n",
      "\n",
      "Instance 1416 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "jumping at index 8: [-0.23453012108802795, 1.2317235469818115, -0.1895308792591095, -0.30915001034736633, 0.2534104883670807]\n",
      "Grand sum of 1232 tensor sets is: [156.6204376220703, 1456.81201171875, -65.11910247802734, -1070.364990234375, 1728.5487060546875]\n",
      "\n",
      "Instance 1417 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 34: [1.0210165977478027, 1.3923653364181519, -0.5213966369628906, -1.785794973373413, -0.36463409662246704]\n",
      "Grand sum of 1233 tensor sets is: [157.64144897460938, 1458.204345703125, -65.6405029296875, -1072.1507568359375, 1728.18408203125]\n",
      "\n",
      "Instance 1418 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 31: [0.7308530807495117, 1.446509599685669, 0.9715222120285034, -0.756840705871582, 1.6107447147369385]\n",
      "Grand sum of 1234 tensor sets is: [158.37229919433594, 1459.65087890625, -64.66898345947266, -1072.9075927734375, 1729.7947998046875]\n",
      "\n",
      "Instance 1419 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 6: [-0.21661041676998138, 2.911177635192871, 0.4060923755168915, -2.7226715087890625, 2.121936321258545]\n",
      "Grand sum of 1235 tensor sets is: [158.1556854248047, 1462.56201171875, -64.26289367675781, -1075.6302490234375, 1731.916748046875]\n",
      "\n",
      "Instance 1420 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 22: [0.9905864596366882, 1.4447379112243652, -0.17332929372787476, -0.11323074996471405, -0.593859851360321]\n",
      "Grand sum of 1236 tensor sets is: [159.14627075195312, 1464.0067138671875, -64.43622589111328, -1075.7435302734375, 1731.3228759765625]\n",
      "\n",
      "Instance 1421 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 4: [0.08506937325000763, 0.3036542236804962, -0.17166432738304138, -2.374246120452881, 6.750240325927734]\n",
      "Grand sum of 1237 tensor sets is: [159.23133850097656, 1464.3104248046875, -64.6078872680664, -1078.1177978515625, 1738.0731201171875]\n",
      "\n",
      "Instance 1422 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 15: [-0.8481928110122681, 0.2219720482826233, -0.382502943277359, -1.196781873703003, 4.801535606384277]\n",
      "Grand sum of 1238 tensor sets is: [158.38314819335938, 1464.5323486328125, -64.99038696289062, -1079.3145751953125, 1742.8746337890625]\n",
      "\n",
      "Instance 1423 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 22: [-0.5339784026145935, 1.1113027334213257, -0.010615558363497257, -1.3126296997070312, 1.4165459871292114]\n",
      "Grand sum of 1239 tensor sets is: [157.8491668701172, 1465.6436767578125, -65.0009994506836, -1080.627197265625, 1744.2911376953125]\n",
      "\n",
      "Instance 1424 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "jumping at index 2: [0.1186460331082344, 1.737769365310669, -1.0715070962905884, 0.6905046701431274, -0.3601449131965637]\n",
      "Grand sum of 1240 tensor sets is: [157.9678192138672, 1467.3814697265625, -66.072509765625, -1079.9366455078125, 1743.9310302734375]\n",
      "\n",
      "Instance 1425 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1426 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1427 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 15: [-0.13520154356956482, 1.0451390743255615, -1.0693418979644775, 0.22091098129749298, -2.152014970779419]\n",
      "Grand sum of 1241 tensor sets is: [157.83261108398438, 1468.4266357421875, -67.14185333251953, -1079.7156982421875, 1741.779052734375]\n",
      "\n",
      "Instance 1428 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1429 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 30: [-0.2497079074382782, 3.450397491455078, -1.1943453550338745, -0.4486566185951233, 2.313051223754883]\n",
      "Grand sum of 1242 tensor sets is: [157.58290100097656, 1471.8770751953125, -68.33619689941406, -1080.164306640625, 1744.0921630859375]\n",
      "\n",
      "Instance 1430 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 8: [0.06438498944044113, 1.6004964113235474, 0.815522313117981, -1.834320068359375, 1.5324461460113525]\n",
      "Grand sum of 1243 tensor sets is: [157.6472930908203, 1473.4775390625, -67.52067565917969, -1081.9986572265625, 1745.6246337890625]\n",
      "\n",
      "Instance 1431 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 16: [-0.07891152799129486, 1.9695543050765991, -0.7210127115249634, -2.2792365550994873, -0.5809641480445862]\n",
      "Grand sum of 1244 tensor sets is: [157.56837463378906, 1475.4471435546875, -68.24169158935547, -1084.2779541015625, 1745.043701171875]\n",
      "\n",
      "Instance 1432 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 4: [-0.4705151319503784, 1.2267658710479736, 1.1173721551895142, -2.4692604541778564, 0.1466311365365982]\n",
      "Grand sum of 1245 tensor sets is: [157.0978546142578, 1476.6739501953125, -67.12432098388672, -1086.7471923828125, 1745.1903076171875]\n",
      "\n",
      "Instance 1433 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "jumping at index 45: [0.42737412452697754, 1.7977288961410522, 0.08131015300750732, -2.6250088214874268, 1.28715181350708]\n",
      "Grand sum of 1246 tensor sets is: [157.5252227783203, 1478.4716796875, -67.04301452636719, -1089.3721923828125, 1746.4774169921875]\n",
      "\n",
      "Instance 1434 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 13: [-0.8372983932495117, 1.4448473453521729, -0.1092948466539383, -2.1502437591552734, 3.720378875732422]\n",
      "Grand sum of 1247 tensor sets is: [156.68792724609375, 1479.91650390625, -67.15230560302734, -1091.5224609375, 1750.19775390625]\n",
      "\n",
      "Instance 1435 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 10: [-1.37912917137146, 1.65146803855896, 0.1772281378507614, -0.01627969741821289, -0.21894273161888123]\n",
      "Grand sum of 1248 tensor sets is: [155.3087921142578, 1481.5679931640625, -66.9750747680664, -1091.5386962890625, 1749.978759765625]\n",
      "\n",
      "Instance 1436 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 11: [-0.32531437277793884, 1.5780636072158813, -0.5252116918563843, -2.2605972290039062, 4.826300621032715]\n",
      "Grand sum of 1249 tensor sets is: [154.9834747314453, 1483.14599609375, -67.50028991699219, -1093.79931640625, 1754.8050537109375]\n",
      "\n",
      "Instance 1437 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 15: [-0.1829090416431427, 0.8954329490661621, 0.027719777077436447, -1.7968354225158691, 3.638551712036133]\n",
      "Grand sum of 1250 tensor sets is: [154.80056762695312, 1484.0413818359375, -67.47257232666016, -1095.59619140625, 1758.443603515625]\n",
      "\n",
      "Instance 1438 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 7: [0.2865865230560303, 2.2952051162719727, -0.4844112694263458, -0.2023710459470749, 3.2676215171813965]\n",
      "Grand sum of 1251 tensor sets is: [155.087158203125, 1486.3365478515625, -67.95698547363281, -1095.798583984375, 1761.711181640625]\n",
      "\n",
      "Instance 1439 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 17: [0.8275945782661438, 0.15486915409564972, -0.39898768067359924, -1.184410810470581, 1.344063401222229]\n",
      "Grand sum of 1252 tensor sets is: [155.9147491455078, 1486.491455078125, -68.35597229003906, -1096.9830322265625, 1763.0552978515625]\n",
      "\n",
      "Instance 1440 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 18: [0.06919235736131668, 0.46942147612571716, -0.16756728291511536, -1.0784820318222046, -0.821435272693634]\n",
      "Grand sum of 1253 tensor sets is: [155.98394775390625, 1486.9609375, -68.5235366821289, -1098.0615234375, 1762.23388671875]\n",
      "\n",
      "Instance 1441 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [480]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 480: [1.3795037269592285, 1.0536975860595703, 0.6297832727432251, -1.5170445442199707, 6.460476875305176]\n",
      "Grand sum of 1254 tensor sets is: [157.3634490966797, 1488.0146484375, -67.89375305175781, -1099.57861328125, 1768.6943359375]\n",
      "\n",
      "Instance 1442 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 15: [0.1385192722082138, 2.2828171253204346, 0.13089138269424438, 1.8485215902328491, -0.5103976726531982]\n",
      "Grand sum of 1255 tensor sets is: [157.50196838378906, 1490.2974853515625, -67.76286315917969, -1097.7301025390625, 1768.1839599609375]\n",
      "\n",
      "Instance 1443 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 5: [0.6662994623184204, 1.0786625146865845, 0.23071534931659698, 0.31318390369415283, 3.4704947471618652]\n",
      "Grand sum of 1256 tensor sets is: [158.16827392578125, 1491.3760986328125, -67.53215026855469, -1097.4168701171875, 1771.6544189453125]\n",
      "\n",
      "Instance 1444 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [-0.45801663398742676, -0.9925666451454163, 0.12019787728786469, -2.552450656890869, 2.2982001304626465]\n",
      "Grand sum of 1257 tensor sets is: [157.7102508544922, 1490.383544921875, -67.41194915771484, -1099.9693603515625, 1773.95263671875]\n",
      "\n",
      "Instance 1445 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 13: [-1.19417142868042, 0.9488720893859863, -0.6225058436393738, 0.38371747732162476, 0.8542305827140808]\n",
      "Grand sum of 1258 tensor sets is: [156.51608276367188, 1491.3323974609375, -68.03445434570312, -1099.585693359375, 1774.806884765625]\n",
      "\n",
      "Instance 1446 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "jumping at index 25: [1.5872840881347656, 0.44323617219924927, 0.14369328320026398, -0.7916043996810913, 3.100955009460449]\n",
      "Grand sum of 1259 tensor sets is: [158.10336303710938, 1491.775634765625, -67.89076232910156, -1100.3773193359375, 1777.9078369140625]\n",
      "\n",
      "Instance 1447 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 21: [0.893871545791626, 1.9775434732437134, 0.26958000659942627, -0.9580931067466736, -1.9460264444351196]\n",
      "Grand sum of 1260 tensor sets is: [158.9972381591797, 1493.753173828125, -67.62118530273438, -1101.33544921875, 1775.9617919921875]\n",
      "\n",
      "Instance 1448 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 13: [0.5605608820915222, 0.23155204951763153, -0.03323972597718239, -0.7923725843429565, 0.38265496492385864]\n",
      "Grand sum of 1261 tensor sets is: [159.55780029296875, 1493.9847412109375, -67.65442657470703, -1102.1278076171875, 1776.344482421875]\n",
      "\n",
      "Instance 1449 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 9: [-0.8839394450187683, 0.7243333458900452, 0.3244500160217285, 2.0273778438568115, 0.1572961062192917]\n",
      "Grand sum of 1262 tensor sets is: [158.67385864257812, 1494.7091064453125, -67.3299789428711, -1100.1004638671875, 1776.5018310546875]\n",
      "\n",
      "Instance 1450 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 21: [-0.3690270781517029, 0.6089944839477539, 0.37507274746894836, -1.3476811647415161, 0.4101061224937439]\n",
      "Grand sum of 1263 tensor sets is: [158.30482482910156, 1495.318115234375, -66.95490264892578, -1101.4481201171875, 1776.9119873046875]\n",
      "\n",
      "Instance 1451 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 1264 tensor sets is: [159.24728393554688, 1495.3837890625, -67.3251724243164, -1101.9178466796875, 1779.538330078125]\n",
      "\n",
      "Instance 1452 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 1265 tensor sets is: [160.1897430419922, 1495.449462890625, -67.69544219970703, -1102.3875732421875, 1782.1646728515625]\n",
      "\n",
      "Instance 1453 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1454 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 15: [0.4350178837776184, 0.6663440465927124, 0.45846110582351685, -2.0485458374023438, 4.493319034576416]\n",
      "Grand sum of 1266 tensor sets is: [160.624755859375, 1496.1158447265625, -67.23698425292969, -1104.4361572265625, 1786.657958984375]\n",
      "\n",
      "Instance 1455 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 23: [-0.45801663398742676, -0.9925666451454163, 0.12019787728786469, -2.552450656890869, 2.2982001304626465]\n",
      "Grand sum of 1267 tensor sets is: [160.16673278808594, 1495.123291015625, -67.11678314208984, -1106.9886474609375, 1788.9561767578125]\n",
      "\n",
      "Instance 1456 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([86, 13, 768])\n",
      "Shape of summed layers is: 86 x 768\n",
      "jumping at index 30: [0.3576498329639435, 2.947761058807373, 0.2510061264038086, -0.4141994118690491, 1.4550787210464478]\n",
      "Grand sum of 1268 tensor sets is: [160.52438354492188, 1498.071044921875, -66.86577606201172, -1107.40283203125, 1790.4112548828125]\n",
      "\n",
      "Instance 1457 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 33: [0.18339507281780243, -0.13803139328956604, -1.7510677576065063, -1.4120465517044067, 2.5709357261657715]\n",
      "Grand sum of 1269 tensor sets is: [160.70777893066406, 1497.9329833984375, -68.6168441772461, -1108.8148193359375, 1792.982177734375]\n",
      "\n",
      "Instance 1458 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 12: [1.0125645399093628, 2.7222745418548584, -0.14607128500938416, 1.3489975929260254, 0.9897739887237549]\n",
      "Grand sum of 1270 tensor sets is: [161.7203369140625, 1500.6552734375, -68.7629165649414, -1107.4658203125, 1793.971923828125]\n",
      "\n",
      "Instance 1459 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 26: [0.6881157159805298, 1.208246111869812, -0.41319042444229126, 0.3908136487007141, 1.168442726135254]\n",
      "Grand sum of 1271 tensor sets is: [162.408447265625, 1501.863525390625, -69.17610931396484, -1107.074951171875, 1795.140380859375]\n",
      "\n",
      "Instance 1460 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 7: [-0.14097753167152405, 2.354384660720825, -1.258196234703064, -0.6472878456115723, 2.9646823406219482]\n",
      "Grand sum of 1272 tensor sets is: [162.26747131347656, 1504.2178955078125, -70.4343032836914, -1107.7222900390625, 1798.1051025390625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 1461 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1462 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 5: [0.9424604773521423, 0.06562995910644531, -0.3702724874019623, -0.4697447419166565, 2.626331090927124]\n",
      "Grand sum of 1273 tensor sets is: [163.20993041992188, 1504.2835693359375, -70.80457305908203, -1108.1920166015625, 1800.7314453125]\n",
      "\n",
      "Instance 1463 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 7: [0.2865865230560303, 2.2952051162719727, -0.4844112694263458, -0.2023710459470749, 3.2676215171813965]\n",
      "Grand sum of 1274 tensor sets is: [163.49652099609375, 1506.5787353515625, -71.28898620605469, -1108.3944091796875, 1803.9990234375]\n",
      "\n",
      "Instance 1464 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 8: [0.12432590872049332, 0.2860192656517029, 0.4179125130176544, -0.9930177927017212, -0.18331724405288696]\n",
      "Grand sum of 1275 tensor sets is: [163.620849609375, 1506.86474609375, -70.8710708618164, -1109.387451171875, 1803.815673828125]\n",
      "\n",
      "Instance 1465 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 26: [0.6357445120811462, 0.8501477241516113, 1.455955982208252, -3.3374431133270264, 0.29623156785964966]\n",
      "Grand sum of 1276 tensor sets is: [164.256591796875, 1507.71484375, -69.41511535644531, -1112.724853515625, 1804.1119384765625]\n",
      "\n",
      "Instance 1466 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "jumping at index 15: [0.5271641612052917, 0.491386353969574, -0.26269927620887756, -1.4334142208099365, -0.8807269334793091]\n",
      "Grand sum of 1277 tensor sets is: [164.78375244140625, 1508.2061767578125, -69.67781829833984, -1114.1583251953125, 1803.231201171875]\n",
      "\n",
      "Instance 1467 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 15: [0.585093080997467, 2.338531732559204, -1.1882803440093994, 0.12521907687187195, -1.6679006814956665]\n",
      "Grand sum of 1278 tensor sets is: [165.3688507080078, 1510.544677734375, -70.86609649658203, -1114.0330810546875, 1801.5633544921875]\n",
      "\n",
      "Instance 1468 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1469 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 19: [0.6889556646347046, 1.8297615051269531, 0.33190131187438965, -0.442871630191803, 2.850905418395996]\n",
      "Grand sum of 1279 tensor sets is: [166.05780029296875, 1512.3743896484375, -70.53419494628906, -1114.4759521484375, 1804.414306640625]\n",
      "\n",
      "Instance 1470 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 5: [0.1447770893573761, 0.9554280042648315, -0.24631963670253754, -0.2431286722421646, 1.1920195817947388]\n",
      "Grand sum of 1280 tensor sets is: [166.20257568359375, 1513.329833984375, -70.780517578125, -1114.7191162109375, 1805.6063232421875]\n",
      "\n",
      "Instance 1471 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 7: [-0.1860617995262146, 0.539292573928833, -0.297499418258667, 2.801253318786621, -0.32238900661468506]\n",
      "Grand sum of 1281 tensor sets is: [166.01651000976562, 1513.869140625, -71.07801818847656, -1111.9178466796875, 1805.283935546875]\n",
      "\n",
      "Instance 1472 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "jumping at index 22: [-0.33710432052612305, 0.8179218769073486, 1.8398451805114746, -1.1269288063049316, 0.04772976040840149]\n",
      "Grand sum of 1282 tensor sets is: [165.67941284179688, 1514.68701171875, -69.23817443847656, -1113.0447998046875, 1805.3316650390625]\n",
      "\n",
      "Instance 1473 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 24: [0.1404268741607666, -0.12180042266845703, 1.324100375175476, -2.1417744159698486, 0.33511993288993835]\n",
      "Grand sum of 1283 tensor sets is: [165.81983947753906, 1514.565185546875, -67.91407775878906, -1115.1865234375, 1805.666748046875]\n",
      "\n",
      "Instance 1474 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 6: [0.6278878450393677, 1.3142324686050415, -0.25446027517318726, 0.05969366431236267, 4.905253887176514]\n",
      "Grand sum of 1284 tensor sets is: [166.44772338867188, 1515.87939453125, -68.16854095458984, -1115.1268310546875, 1810.572021484375]\n",
      "\n",
      "Instance 1475 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1476 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "jumping at index 30: [0.18775025010108948, 0.973435640335083, 1.3487889766693115, -1.1418253183364868, 0.4587731659412384]\n",
      "Grand sum of 1285 tensor sets is: [166.63546752929688, 1516.852783203125, -66.81975555419922, -1116.2686767578125, 1811.03076171875]\n",
      "\n",
      "Instance 1477 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1478 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "jumping at index 7: [-0.2623164653778076, 1.2419853210449219, 1.7726713418960571, -1.9849079847335815, -0.9886692762374878]\n",
      "Grand sum of 1286 tensor sets is: [166.37315368652344, 1518.0947265625, -65.04708099365234, -1118.2535400390625, 1810.0421142578125]\n",
      "\n",
      "Instance 1479 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 1287 tensor sets is: [166.44407653808594, 1520.3629150390625, -65.71424865722656, -1118.1077880859375, 1809.9056396484375]\n",
      "\n",
      "Instance 1480 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1481 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "jumping at index 9: [-0.6587342619895935, 1.3192174434661865, -0.7866958379745483, 0.8879815936088562, 2.635479211807251]\n",
      "Grand sum of 1288 tensor sets is: [165.78533935546875, 1521.68212890625, -66.50094604492188, -1117.2198486328125, 1812.5411376953125]\n",
      "\n",
      "Instance 1482 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 12: [0.009519977495074272, 1.745697259902954, 1.1846520900726318, -0.5852099657058716, 6.801050186157227]\n",
      "Grand sum of 1289 tensor sets is: [165.79486083984375, 1523.4278564453125, -65.31629180908203, -1117.8050537109375, 1819.3421630859375]\n",
      "\n",
      "Instance 1483 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 11: [0.40079188346862793, 0.5429642200469971, -1.1999715566635132, 1.10344398021698, 2.272383689880371]\n",
      "Grand sum of 1290 tensor sets is: [166.19564819335938, 1523.9708251953125, -66.51626586914062, -1116.70166015625, 1821.614501953125]\n",
      "\n",
      "Instance 1484 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 16: [-0.10585488379001617, 2.990490674972534, 0.8214718699455261, -2.770456314086914, 3.2605817317962646]\n",
      "Grand sum of 1291 tensor sets is: [166.0897979736328, 1526.9613037109375, -65.69479370117188, -1119.47216796875, 1824.8751220703125]\n",
      "\n",
      "Instance 1485 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 10: [1.0331413745880127, 1.830254077911377, -0.4573289752006531, -0.5395779013633728, 0.847012996673584]\n",
      "Grand sum of 1292 tensor sets is: [167.12294006347656, 1528.79150390625, -66.1521224975586, -1120.01171875, 1825.72216796875]\n",
      "\n",
      "Instance 1486 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 5: [-0.2856781482696533, 0.8299692869186401, -1.2213295698165894, -0.6824219226837158, -0.14937853813171387]\n",
      "Grand sum of 1293 tensor sets is: [166.83726501464844, 1529.6214599609375, -67.37345123291016, -1120.694091796875, 1825.57275390625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 1487 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "jumping at index 2: [-0.37768810987472534, 0.9264461398124695, 0.1586836278438568, -2.4111945629119873, 1.8778245449066162]\n",
      "Grand sum of 1294 tensor sets is: [166.45957946777344, 1530.5478515625, -67.21476745605469, -1123.1053466796875, 1827.4505615234375]\n",
      "\n",
      "Instance 1488 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1489 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1490 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1491 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 24: [-1.168601632118225, 1.3049286603927612, 1.3325551748275757, -2.0475759506225586, 1.4538986682891846]\n",
      "Grand sum of 1295 tensor sets is: [165.29098510742188, 1531.852783203125, -65.88220977783203, -1125.1529541015625, 1828.9044189453125]\n",
      "\n",
      "Instance 1492 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 26: [-1.0301182270050049, 2.505089282989502, 0.3863261342048645, -0.5880890488624573, -1.0540591478347778]\n",
      "Grand sum of 1296 tensor sets is: [164.2608642578125, 1534.35791015625, -65.49588012695312, -1125.7410888671875, 1827.850341796875]\n",
      "\n",
      "Instance 1493 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "jumping at index 17: [-1.0375795364379883, 0.5813586711883545, 0.8033523559570312, -3.364104747772217, 5.010665416717529]\n",
      "Grand sum of 1297 tensor sets is: [163.22328186035156, 1534.939208984375, -64.6925277709961, -1129.105224609375, 1832.8609619140625]\n",
      "\n",
      "Instance 1494 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1495 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 28: [-0.35210275650024414, 1.434852123260498, 0.6825548410415649, 0.8300846219062805, 0.6295202970504761]\n",
      "Grand sum of 1298 tensor sets is: [162.87118530273438, 1536.3740234375, -64.00997161865234, -1128.275146484375, 1833.490478515625]\n",
      "\n",
      "Instance 1496 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [52]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "jumping at index 52: [-0.3980860710144043, 2.8901939392089844, 0.2910328209400177, -1.2776741981506348, 5.131888389587402]\n",
      "Grand sum of 1299 tensor sets is: [162.4730987548828, 1539.26416015625, -63.718936920166016, -1129.5528564453125, 1838.622314453125]\n",
      "\n",
      "Instance 1497 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 1300 tensor sets is: [162.5440216064453, 1541.5323486328125, -64.3861083984375, -1129.4071044921875, 1838.48583984375]\n",
      "\n",
      "Instance 1498 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 5: [0.27589893341064453, 1.003183126449585, -0.010117528960108757, -1.0197641849517822, 3.733774185180664]\n",
      "Grand sum of 1301 tensor sets is: [162.81991577148438, 1542.5355224609375, -64.39622497558594, -1130.4268798828125, 1842.2196044921875]\n",
      "\n",
      "Instance 1499 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "jumping at index 41: [-0.7579550743103027, 0.7489339709281921, 0.8583462238311768, -1.3837218284606934, 3.9166929721832275]\n",
      "Grand sum of 1302 tensor sets is: [162.0619659423828, 1543.284423828125, -63.537879943847656, -1131.810546875, 1846.1363525390625]\n",
      "\n",
      "Instance 1500 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 11: [-0.2800106704235077, 0.8600803017616272, -0.5554116368293762, 0.18709246814250946, 2.4371914863586426]\n",
      "Grand sum of 1303 tensor sets is: [161.78195190429688, 1544.14453125, -64.09329223632812, -1131.6234130859375, 1848.573486328125]\n",
      "\n",
      "Instance 1501 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 26: [-1.0301182270050049, 2.505089282989502, 0.3863261342048645, -0.5880890488624573, -1.0540591478347778]\n",
      "Grand sum of 1304 tensor sets is: [160.7518310546875, 1546.649658203125, -63.706966400146484, -1132.2115478515625, 1847.5194091796875]\n",
      "\n",
      "Instance 1502 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "jumping at index 6: [0.4044957160949707, 3.3696985244750977, -0.626144528388977, 1.9819526672363281, 1.6693295240402222]\n",
      "Grand sum of 1305 tensor sets is: [161.1563262939453, 1550.0194091796875, -64.33311462402344, -1130.2296142578125, 1849.188720703125]\n",
      "\n",
      "Instance 1503 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 21: [1.0898218154907227, 1.5352927446365356, 0.7841476202011108, 0.32752400636672974, 3.4535014629364014]\n",
      "Grand sum of 1306 tensor sets is: [162.24615478515625, 1551.5546875, -63.54896545410156, -1129.902099609375, 1852.6422119140625]\n",
      "\n",
      "Instance 1504 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 13: [-0.19341512024402618, 1.2205841541290283, -1.107857346534729, -2.2513277530670166, 4.798268795013428]\n",
      "Grand sum of 1307 tensor sets is: [162.052734375, 1552.7752685546875, -64.65682220458984, -1132.1534423828125, 1857.4404296875]\n",
      "\n",
      "Instance 1505 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1506 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 26: [-1.0301182270050049, 2.505089282989502, 0.3863261342048645, -0.5880890488624573, -1.0540591478347778]\n",
      "Grand sum of 1308 tensor sets is: [161.02261352539062, 1555.2803955078125, -64.27049255371094, -1132.7415771484375, 1856.3863525390625]\n",
      "\n",
      "Instance 1507 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "jumping at index 5: [0.9098513722419739, 1.216894268989563, -0.6516406536102295, 1.402858853340149, 2.6486692428588867]\n",
      "Grand sum of 1309 tensor sets is: [161.93246459960938, 1556.497314453125, -64.92213439941406, -1131.3387451171875, 1859.0350341796875]\n",
      "\n",
      "Instance 1508 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1509 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1510 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 5: [0.5115736126899719, 0.9410767555236816, 2.0870041847229004, 0.39754265546798706, -0.265471875667572]\n",
      "Grand sum of 1310 tensor sets is: [162.44403076171875, 1557.4383544921875, -62.83512878417969, -1130.941162109375, 1858.76953125]\n",
      "\n",
      "Instance 1511 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22, 368]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 22: [-0.20570963621139526, 2.138561964035034, 1.0356311798095703, -1.6269596815109253, 3.897801399230957]\n",
      "jumping at index 368: [-0.39903581142425537, 2.2914578914642334, 0.5616462230682373, -0.8992323279380798, 5.037720203399658]\n",
      "Grand sum of 1311 tensor sets is: [162.14166259765625, 1559.6533203125, -62.03649139404297, -1132.2042236328125, 1863.2373046875]\n",
      "\n",
      "Instance 1512 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "jumping at index 34: [-0.6656587719917297, 2.2356295585632324, -1.1713508367538452, -0.9212629795074463, -0.8755220174789429]\n",
      "Grand sum of 1312 tensor sets is: [161.4759979248047, 1561.888916015625, -63.20784378051758, -1133.12548828125, 1862.36181640625]\n",
      "\n",
      "Instance 1513 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 9: [0.24165558815002441, 0.27822959423065186, 1.5805034637451172, -2.0498406887054443, 1.9665133953094482]\n",
      "Grand sum of 1313 tensor sets is: [161.7176513671875, 1562.1671142578125, -61.627342224121094, -1135.17529296875, 1864.328369140625]\n",
      "\n",
      "Instance 1514 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1515 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 20: [0.3482828736305237, 1.9598846435546875, -0.7266094088554382, -0.7965636253356934, 0.5197771787643433]\n",
      "Grand sum of 1314 tensor sets is: [162.06593322753906, 1564.126953125, -62.35395050048828, -1135.9718017578125, 1864.84814453125]\n",
      "\n",
      "Instance 1516 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 6: [0.1647840440273285, 1.5459198951721191, 0.41310280561447144, 1.738429069519043, -1.5991592407226562]\n",
      "Grand sum of 1315 tensor sets is: [162.230712890625, 1565.6728515625, -61.94084930419922, -1134.2333984375, 1863.2490234375]\n",
      "\n",
      "Instance 1517 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 19: [0.8166017532348633, 2.793222665786743, 0.41722947359085083, -0.9929916262626648, 4.080804824829102]\n",
      "Grand sum of 1316 tensor sets is: [163.0473175048828, 1568.466064453125, -61.52362060546875, -1135.2264404296875, 1867.329833984375]\n",
      "\n",
      "Instance 1518 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1519 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "jumping at index 16: [0.5923840403556824, 1.9189479351043701, -0.9508758187294006, 0.6931953430175781, 1.8691129684448242]\n",
      "Grand sum of 1317 tensor sets is: [163.6396942138672, 1570.385009765625, -62.47449493408203, -1134.533203125, 1869.198974609375]\n",
      "\n",
      "Instance 1520 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 11: [0.0020392443984746933, 1.328076720237732, 1.0905930995941162, -1.0023878812789917, 3.5253400802612305]\n",
      "Grand sum of 1318 tensor sets is: [163.64173889160156, 1571.713134765625, -61.38390350341797, -1135.53564453125, 1872.724365234375]\n",
      "\n",
      "Instance 1521 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1522 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1523 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "jumping at index 3: [0.74709552526474, 2.5260632038116455, 0.19531162083148956, 1.2198665142059326, -0.08368843793869019]\n",
      "Grand sum of 1319 tensor sets is: [164.3888397216797, 1574.2392578125, -61.18859100341797, -1134.3157958984375, 1872.640625]\n",
      "\n",
      "Instance 1524 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 35: [0.44055575132369995, 0.06015303358435631, 0.9789133071899414, -0.8177602291107178, 0.9000521898269653]\n",
      "Grand sum of 1320 tensor sets is: [164.8293914794922, 1574.2994384765625, -60.209678649902344, -1135.133544921875, 1873.5406494140625]\n",
      "\n",
      "Instance 1525 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 32: [-0.21301989257335663, 0.8072513341903687, -0.6950997710227966, 1.0553878545761108, 1.487073302268982]\n",
      "Grand sum of 1321 tensor sets is: [164.6163787841797, 1575.106689453125, -60.90477752685547, -1134.078125, 1875.0277099609375]\n",
      "\n",
      "Instance 1526 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 28: [-0.35210275650024414, 1.434852123260498, 0.6825548410415649, 0.8300846219062805, 0.6295202970504761]\n",
      "Grand sum of 1322 tensor sets is: [164.2642822265625, 1576.54150390625, -60.22222137451172, -1133.248046875, 1875.6572265625]\n",
      "\n",
      "Instance 1527 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "jumping at index 12: [-0.07666724920272827, 0.8468081951141357, -0.6269850730895996, 1.1470990180969238, 3.3826940059661865]\n",
      "Grand sum of 1323 tensor sets is: [164.1876220703125, 1577.3883056640625, -60.849205017089844, -1132.1009521484375, 1879.0399169921875]\n",
      "\n",
      "Instance 1528 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 21: [-0.5548097491264343, 2.3528871536254883, -0.10347232967615128, -0.15484619140625, 1.054692029953003]\n",
      "Grand sum of 1324 tensor sets is: [163.6328125, 1579.7412109375, -60.95267868041992, -1132.255859375, 1880.0946044921875]\n",
      "\n",
      "Instance 1529 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "jumping at index 24: [1.7286657094955444, 0.07780255377292633, -0.589777946472168, -1.1348960399627686, 3.5739173889160156]\n",
      "Grand sum of 1325 tensor sets is: [165.36148071289062, 1579.8189697265625, -61.542457580566406, -1133.3907470703125, 1883.6685791015625]\n",
      "\n",
      "Instance 1530 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 9: [-0.09307220578193665, -0.4088105857372284, 1.3446024656295776, -1.2511727809906006, 1.4011890888214111]\n",
      "Grand sum of 1326 tensor sets is: [165.26840209960938, 1579.41015625, -60.19785690307617, -1134.6419677734375, 1885.06982421875]\n",
      "\n",
      "Instance 1531 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [57]\n",
      "Size of token embeddings is torch.Size([87, 13, 768])\n",
      "Shape of summed layers is: 87 x 768\n",
      "jumping at index 57: [0.8865533471107483, 0.6078497171401978, 1.9060046672821045, -3.8562653064727783, 0.8934416174888611]\n",
      "Grand sum of 1327 tensor sets is: [166.1549530029297, 1580.01806640625, -58.29185104370117, -1138.498291015625, 1885.9632568359375]\n",
      "\n",
      "Instance 1532 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "jumping at index 13: [-0.2525177597999573, 0.6933229565620422, -0.3736156225204468, 0.11457794159650803, -0.9496636986732483]\n",
      "Grand sum of 1328 tensor sets is: [165.90243530273438, 1580.71142578125, -58.66546630859375, -1138.3836669921875, 1885.0135498046875]\n",
      "\n",
      "Instance 1533 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 16: [1.2236003875732422, 0.3081361651420593, -0.5519952178001404, -4.05463171005249, 1.215848445892334]\n",
      "Grand sum of 1329 tensor sets is: [167.12603759765625, 1581.01953125, -59.21746063232422, -1142.4383544921875, 1886.2293701171875]\n",
      "\n",
      "Instance 1534 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1535 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [86]\n",
      "Size of token embeddings is torch.Size([175, 13, 768])\n",
      "Shape of summed layers is: 175 x 768\n",
      "jumping at index 86: [-0.48940330743789673, 2.5392491817474365, 0.6103597283363342, -3.4575564861297607, 2.7216389179229736]\n",
      "Grand sum of 1330 tensor sets is: [166.63662719726562, 1583.558837890625, -58.60710144042969, -1145.8958740234375, 1888.9510498046875]\n",
      "\n",
      "Instance 1536 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 21: [1.032656192779541, 1.3890693187713623, 0.2124519646167755, 0.6584137082099915, 1.1518317461013794]\n",
      "Grand sum of 1331 tensor sets is: [167.66928100585938, 1584.9478759765625, -58.394649505615234, -1145.2374267578125, 1890.1029052734375]\n",
      "\n",
      "Instance 1537 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([88, 13, 768])\n",
      "Shape of summed layers is: 88 x 768\n",
      "jumping at index 82: [0.1321622133255005, 1.2622759342193604, 0.1689830720424652, -1.1317715644836426, 5.031150817871094]\n",
      "Grand sum of 1332 tensor sets is: [167.8014373779297, 1586.210205078125, -58.22566604614258, -1146.369140625, 1895.134033203125]\n",
      "\n",
      "Instance 1538 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1539 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [359]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 359: [0.7535192370414734, 0.6318660974502563, 0.8483155369758606, -0.3475501537322998, 3.589747667312622]\n",
      "Grand sum of 1333 tensor sets is: [168.55496215820312, 1586.842041015625, -57.377349853515625, -1146.7166748046875, 1898.7237548828125]\n",
      "\n",
      "Instance 1540 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 1334 tensor sets is: [169.66098022460938, 1587.780517578125, -57.9808235168457, -1147.0181884765625, 1900.643798828125]\n",
      "\n",
      "Instance 1541 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "jumping at index 31: [0.010411515831947327, 2.0933127403259277, -1.1421815156936646, -0.31778693199157715, -0.67295902967453]\n",
      "Grand sum of 1335 tensor sets is: [169.67138671875, 1589.873779296875, -59.12300491333008, -1147.3359375, 1899.9708251953125]\n",
      "\n",
      "Instance 1542 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1543 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 1336 tensor sets is: [169.55343627929688, 1590.3021240234375, -58.94004821777344, -1149.531494140625, 1902.4725341796875]\n",
      "\n",
      "Instance 1544 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "jumping at index 26: [-1.0301182270050049, 2.505089282989502, 0.3863261342048645, -0.5880890488624573, -1.0540591478347778]\n",
      "Grand sum of 1337 tensor sets is: [168.5233154296875, 1592.8072509765625, -58.5537223815918, -1150.11962890625, 1901.41845703125]\n",
      "\n",
      "Instance 1545 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 1338 tensor sets is: [168.40536499023438, 1593.235595703125, -58.370765686035156, -1152.315185546875, 1903.920166015625]\n",
      "\n",
      "Instance 1546 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 38: [-0.472095787525177, 0.9956002235412598, -0.061650052666664124, -1.0481332540512085, 2.2737977504730225]\n",
      "Grand sum of 1339 tensor sets is: [167.9332733154297, 1594.231201171875, -58.43241500854492, -1153.36328125, 1906.1939697265625]\n",
      "\n",
      "Instance 1547 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 4: [-0.26443570852279663, 0.07598097622394562, 0.614093542098999, -2.2054219245910645, 2.4608006477355957]\n",
      "Grand sum of 1340 tensor sets is: [167.66883850097656, 1594.30712890625, -57.818321228027344, -1155.5687255859375, 1908.65478515625]\n",
      "\n",
      "Instance 1548 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "jumping at index 11: [0.9587279558181763, 0.0902765542268753, 0.3971436619758606, 0.580862820148468, 0.44726794958114624]\n",
      "Grand sum of 1341 tensor sets is: [168.6275634765625, 1594.3974609375, -57.42117691040039, -1154.9879150390625, 1909.10205078125]\n",
      "\n",
      "Instance 1549 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 30: [0.17394620180130005, 1.4126150608062744, 0.0551389716565609, 0.1898638904094696, 0.21667326986789703]\n",
      "Grand sum of 1342 tensor sets is: [168.801513671875, 1595.81005859375, -57.36603927612305, -1154.798095703125, 1909.3187255859375]\n",
      "\n",
      "Instance 1550 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([114, 13, 768])\n",
      "Shape of summed layers is: 114 x 768\n",
      "jumping at index 20: [0.4807663559913635, 0.9489654302597046, -0.5720985531806946, -0.16463372111320496, 3.052708148956299]\n",
      "Grand sum of 1343 tensor sets is: [169.28228759765625, 1596.759033203125, -57.93813705444336, -1154.9627685546875, 1912.3714599609375]\n",
      "\n",
      "Instance 1551 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([84, 13, 768])\n",
      "Shape of summed layers is: 84 x 768\n",
      "jumping at index 34: [0.4217416048049927, 1.764238953590393, 0.1948128044605255, 0.013831369578838348, 2.026181936264038]\n",
      "Grand sum of 1344 tensor sets is: [169.7040252685547, 1598.5233154296875, -57.743324279785156, -1154.948974609375, 1914.3975830078125]\n",
      "\n",
      "Instance 1552 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1553 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 15: [-0.12123332917690277, 1.9904230833053589, -0.18795381486415863, -1.2361701726913452, 1.0164364576339722]\n",
      "Grand sum of 1345 tensor sets is: [169.58279418945312, 1600.5137939453125, -57.931278228759766, -1156.1851806640625, 1915.4140625]\n",
      "\n",
      "Instance 1554 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "jumping at index 18: [0.17471164464950562, 0.8682138919830322, -0.6757179498672485, -0.9109388589859009, 0.5397040247917175]\n",
      "Grand sum of 1346 tensor sets is: [169.75750732421875, 1601.3819580078125, -58.60699462890625, -1157.0960693359375, 1915.9537353515625]\n",
      "\n",
      "Instance 1555 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 4: [-0.22439944744110107, 1.3007655143737793, 0.515769362449646, -1.7076525688171387, 0.6429990530014038]\n",
      "Grand sum of 1347 tensor sets is: [169.53311157226562, 1602.6827392578125, -58.091224670410156, -1158.8037109375, 1916.5966796875]\n",
      "\n",
      "Instance 1556 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 7: [-0.19811305403709412, 1.8097498416900635, -0.013773694634437561, -3.0702290534973145, 5.212948322296143]\n",
      "Grand sum of 1348 tensor sets is: [169.33499145507812, 1604.492431640625, -58.10499954223633, -1161.8739013671875, 1921.8095703125]\n",
      "\n",
      "Instance 1557 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "jumping at index 9: [0.28989124298095703, 1.4078189134597778, -0.3812672793865204, 0.37246036529541016, 1.4992337226867676]\n",
      "Grand sum of 1349 tensor sets is: [169.6248779296875, 1605.9002685546875, -58.48626708984375, -1161.50146484375, 1923.308837890625]\n",
      "\n",
      "Instance 1558 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 15: [-0.7831825017929077, 0.57438063621521, 1.186976671218872, 0.68511563539505, 0.3889674246311188]\n",
      "Grand sum of 1350 tensor sets is: [168.84169006347656, 1606.474609375, -57.29928970336914, -1160.81640625, 1923.69775390625]\n",
      "\n",
      "Instance 1559 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "jumping at index 17: [-0.12495506554841995, 2.0015013217926025, -0.30803897976875305, -0.9233701229095459, -0.3004939556121826]\n",
      "Grand sum of 1351 tensor sets is: [168.71673583984375, 1608.47607421875, -57.607330322265625, -1161.73974609375, 1923.397216796875]\n",
      "\n",
      "Instance 1560 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 14: [0.9119187593460083, 0.5587763786315918, -1.0927715301513672, -0.3678048849105835, 0.3884700536727905]\n",
      "Grand sum of 1352 tensor sets is: [169.628662109375, 1609.0347900390625, -58.700103759765625, -1162.1075439453125, 1923.78564453125]\n",
      "\n",
      "Instance 1561 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "jumping at index 12: [0.3117132782936096, 1.3156592845916748, -0.2792051136493683, -1.2459180355072021, 4.774489402770996]\n",
      "Grand sum of 1353 tensor sets is: [169.94036865234375, 1610.3504638671875, -58.97930908203125, -1163.353515625, 1928.5601806640625]\n",
      "\n",
      "Instance 1562 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 3: [-0.10561396181583405, 0.06769555807113647, 0.34830573201179504, -3.112055778503418, 4.173076629638672]\n",
      "Grand sum of 1354 tensor sets is: [169.83474731445312, 1610.418212890625, -58.631004333496094, -1166.465576171875, 1932.7332763671875]\n",
      "\n",
      "Instance 1563 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "jumping at index 8: [0.18254496157169342, 2.055244207382202, 0.02957453578710556, -3.3003482818603516, 3.488027572631836]\n",
      "Grand sum of 1355 tensor sets is: [170.0172882080078, 1612.4735107421875, -58.6014289855957, -1169.765869140625, 1936.2213134765625]\n",
      "\n",
      "Instance 1564 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 26: [0.543067455291748, 1.813652515411377, 0.8963204622268677, -1.5250945091247559, -0.26645055413246155]\n",
      "Grand sum of 1356 tensor sets is: [170.5603485107422, 1614.287109375, -57.705108642578125, -1171.291015625, 1935.954833984375]\n",
      "\n",
      "Instance 1565 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [55]\n",
      "Size of token embeddings is torch.Size([96, 13, 768])\n",
      "Shape of summed layers is: 96 x 768\n",
      "jumping at index 55: [-0.5240907669067383, 1.389330267906189, -0.09384319931268692, -1.9772714376449585, 2.8417844772338867]\n",
      "Grand sum of 1357 tensor sets is: [170.0362548828125, 1615.6763916015625, -57.7989501953125, -1173.268310546875, 1938.796630859375]\n",
      "\n",
      "Instance 1566 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 11: [-0.2800106704235077, 0.8600803017616272, -0.5554116368293762, 0.18709246814250946, 2.4371914863586426]\n",
      "Grand sum of 1358 tensor sets is: [169.75624084472656, 1616.5364990234375, -58.35436248779297, -1173.0811767578125, 1941.2337646484375]\n",
      "\n",
      "Instance 1567 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "jumping at index 39: [0.05125763267278671, 1.8901770114898682, -0.44007018208503723, -2.328925371170044, 2.970991611480713]\n",
      "Grand sum of 1359 tensor sets is: [169.8074951171875, 1618.4266357421875, -58.79443359375, -1175.41015625, 1944.2047119140625]\n",
      "\n",
      "Instance 1568 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 24: [-0.11232049018144608, 0.935117244720459, 1.5263991355895996, -0.739344596862793, -1.9889785051345825]\n",
      "Grand sum of 1360 tensor sets is: [169.69517517089844, 1619.3616943359375, -57.268035888671875, -1176.1495361328125, 1942.2156982421875]\n",
      "\n",
      "Instance 1569 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "jumping at index 10: [0.4702067971229553, 1.6842637062072754, -0.06335670500993729, -2.2070891857147217, 1.816317081451416]\n",
      "Grand sum of 1361 tensor sets is: [170.16537475585938, 1621.0458984375, -57.33139419555664, -1178.3565673828125, 1944.031982421875]\n",
      "\n",
      "Instance 1570 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [204]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 204: [-0.11795713752508163, 0.42829927802085876, 0.18295854330062866, -2.195608139038086, 2.5017623901367188]\n",
      "Grand sum of 1362 tensor sets is: [170.04742431640625, 1621.4742431640625, -57.1484375, -1180.5521240234375, 1946.53369140625]\n",
      "\n",
      "Instance 1571 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 28: [0.25575998425483704, 0.7088467478752136, -0.03731977567076683, -0.6877821683883667, -2.3160855770111084]\n",
      "Grand sum of 1363 tensor sets is: [170.3031768798828, 1622.18310546875, -57.18575668334961, -1181.2398681640625, 1944.2176513671875]\n",
      "\n",
      "Instance 1572 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "jumping at index 30: [-0.8231475353240967, 0.20416110754013062, -0.8184887170791626, -1.1787834167480469, 4.866078853607178]\n",
      "Grand sum of 1364 tensor sets is: [169.4800262451172, 1622.38720703125, -58.00424575805664, -1182.418701171875, 1949.083740234375]\n",
      "\n",
      "Instance 1573 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "jumping at index 4: [-0.8541432023048401, 1.3724664449691772, 0.34170711040496826, -0.5703526139259338, -0.43003493547439575]\n",
      "Grand sum of 1365 tensor sets is: [168.62588500976562, 1623.7596435546875, -57.662540435791016, -1182.989013671875, 1948.6536865234375]\n",
      "\n",
      "Instance 1574 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1575 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 1366 tensor sets is: [169.73190307617188, 1624.6981201171875, -58.266014099121094, -1183.29052734375, 1950.57373046875]\n",
      "\n",
      "Instance 1576 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "jumping at index 18: [1.1060254573822021, 0.9384400248527527, -0.6034731864929199, -0.3014914393424988, 1.9200122356414795]\n",
      "Grand sum of 1367 tensor sets is: [170.83792114257812, 1625.6365966796875, -58.86948776245117, -1183.592041015625, 1952.4937744140625]\n",
      "\n",
      "Instance 1577 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "jumping at index 25: [0.996261715888977, 2.792245626449585, -0.13768014311790466, -0.9878970384597778, 0.981727659702301]\n",
      "Grand sum of 1368 tensor sets is: [171.8341827392578, 1628.4288330078125, -59.00716781616211, -1184.5799560546875, 1953.4754638671875]\n",
      "\n",
      "Instance 1578 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "jumping at index 18: [0.4348266124725342, 2.2591068744659424, 0.7274890542030334, -0.2774863839149475, 2.1991002559661865]\n",
      "Grand sum of 1369 tensor sets is: [172.26901245117188, 1630.68798828125, -58.27967834472656, -1184.857421875, 1955.674560546875]\n",
      "\n",
      "Instance 1579 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "jumping at index 6: [0.5185101628303528, 0.6884564161300659, 0.030459482222795486, -1.2817790508270264, 3.0716114044189453]\n",
      "Grand sum of 1370 tensor sets is: [172.7875213623047, 1631.37646484375, -58.24921798706055, -1186.13916015625, 1958.7462158203125]\n",
      "\n",
      "Instance 1580 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "jumping at index 2: [0.4827658534049988, 0.3745099902153015, 0.7454357147216797, 0.5203248262405396, 1.4627286195755005]\n",
      "Grand sum of 1371 tensor sets is: [173.27029418945312, 1631.7509765625, -57.5037841796875, -1185.6187744140625, 1960.208984375]\n",
      "\n",
      "Instance 1581 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 7: [0.2865865230560303, 2.2952051162719727, -0.4844112694263458, -0.2023710459470749, 3.2676215171813965]\n",
      "Grand sum of 1372 tensor sets is: [173.556884765625, 1634.046142578125, -57.988197326660156, -1185.8211669921875, 1963.4765625]\n",
      "\n",
      "Instance 1582 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "jumping at index 24: [0.015511110424995422, 0.48908570408821106, -0.07761222869157791, -2.3037636280059814, 0.47671324014663696]\n",
      "Grand sum of 1373 tensor sets is: [173.57240295410156, 1634.5352783203125, -58.06581115722656, -1188.1248779296875, 1963.9532470703125]\n",
      "\n",
      "Instance 1583 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "jumping at index 29: [0.5149160623550415, 1.8001139163970947, 0.07961207628250122, 0.5757966041564941, -1.2400494813919067]\n",
      "Grand sum of 1374 tensor sets is: [174.0873260498047, 1636.33544921875, -57.98619842529297, -1187.549072265625, 1962.7132568359375]\n",
      "\n",
      "Instance 1584 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [22, 368]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "jumping at index 22: [-0.20570963621139526, 2.138561964035034, 1.0356311798095703, -1.6269596815109253, 3.897801399230957]\n",
      "jumping at index 368: [-0.39903581142425537, 2.2914578914642334, 0.5616462230682373, -0.8992323279380798, 5.037720203399658]\n",
      "Grand sum of 1375 tensor sets is: [173.7849578857422, 1638.5504150390625, -57.18756103515625, -1188.8121337890625, 1967.1810302734375]\n",
      "\n",
      "Instance 1585 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 17: [0.7049703598022461, 0.11868781596422195, -0.10709413886070251, -1.2908024787902832, 1.9882268905639648]\n",
      "Grand sum of 1376 tensor sets is: [174.48992919921875, 1638.6690673828125, -57.294654846191406, -1190.1029052734375, 1969.1693115234375]\n",
      "\n",
      "Instance 1586 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1587 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1588 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 15: [0.24360936880111694, -0.08231945335865021, 1.9601669311523438, -0.7117013931274414, 1.2269816398620605]\n",
      "Grand sum of 1377 tensor sets is: [174.73353576660156, 1638.5867919921875, -55.33448791503906, -1190.8145751953125, 1970.396240234375]\n",
      "\n",
      "Instance 1589 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "jumping at index 7: [-0.16865335404872894, 1.5582128763198853, 0.6590616703033447, -0.11329391598701477, 0.6099684238433838]\n",
      "Grand sum of 1378 tensor sets is: [174.56488037109375, 1640.14501953125, -54.6754264831543, -1190.9278564453125, 1971.0062255859375]\n",
      "\n",
      "Instance 1590 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "jumping at index 15: [1.3447822332382202, -0.0930841714143753, -0.17796993255615234, -1.4226096868515015, -0.2552211284637451]\n",
      "Grand sum of 1379 tensor sets is: [175.90966796875, 1640.0518798828125, -54.853397369384766, -1192.3504638671875, 1970.7509765625]\n",
      "\n",
      "Instance 1591 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "jumping at index 13: [-1.3959053754806519, 1.4100013971328735, -0.10375277698040009, 0.014124870300292969, -0.5665733814239502]\n",
      "Grand sum of 1380 tensor sets is: [174.51376342773438, 1641.4619140625, -54.957149505615234, -1192.3363037109375, 1970.1844482421875]\n",
      "\n",
      "Instance 1592 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "jumping at index 23: [0.5443965792655945, 0.6708879470825195, 0.33039259910583496, -1.0710588693618774, -0.38315680623054504]\n",
      "Grand sum of 1381 tensor sets is: [175.05816650390625, 1642.1328125, -54.62675857543945, -1193.4073486328125, 1969.80126953125]\n",
      "\n",
      "Instance 1593 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "jumping at index 2: [0.4845794439315796, 0.9742089509963989, -0.4725404977798462, 0.712526798248291, -0.8149725198745728]\n",
      "Grand sum of 1382 tensor sets is: [175.54273986816406, 1643.1070556640625, -55.099300384521484, -1192.69482421875, 1968.986328125]\n",
      "\n",
      "Instance 1594 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "jumping at index 11: [-0.9404209852218628, 1.3332116603851318, -0.29651665687561035, -0.6964857578277588, 0.19971689581871033]\n",
      "Grand sum of 1383 tensor sets is: [174.60232543945312, 1644.4403076171875, -55.395816802978516, -1193.391357421875, 1969.18603515625]\n",
      "\n",
      "Instance 1595 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "jumping at index 29: [0.5149160623550415, 1.8001139163970947, 0.07961207628250122, 0.5757966041564941, -1.2400494813919067]\n",
      "Grand sum of 1384 tensor sets is: [175.11724853515625, 1646.240478515625, -55.31620407104492, -1192.8155517578125, 1967.946044921875]\n",
      "\n",
      "Instance 1596 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1597 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "jumping at index 6: [-0.8102516531944275, 0.9628722071647644, -0.4800909459590912, -0.6283703446388245, 2.013972520828247]\n",
      "Grand sum of 1385 tensor sets is: [174.30699157714844, 1647.203369140625, -55.796295166015625, -1193.4439697265625, 1969.9599609375]\n",
      "\n",
      "Instance 1598 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "jumping at index 24: [-0.509059727191925, 1.9850763082504272, 0.7527409195899963, -2.8219566345214844, 2.925808906555176]\n",
      "Grand sum of 1386 tensor sets is: [173.7979278564453, 1649.1884765625, -55.04355239868164, -1196.265869140625, 1972.8857421875]\n",
      "\n",
      "Instance 1599 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "jumping at index 28: [0.5625673532485962, 0.516460657119751, -0.7662215828895569, -1.611074447631836, 0.8949755430221558]\n",
      "Grand sum of 1387 tensor sets is: [174.36048889160156, 1649.7049560546875, -55.80977249145508, -1197.876953125, 1973.78076171875]\n",
      "\n",
      "Instance 1600 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([84, 13, 768])\n",
      "Shape of summed layers is: 84 x 768\n",
      "jumping at index 37: [0.8672967553138733, 0.8214049935340881, 0.26674914360046387, -1.0425506830215454, -1.3616009950637817]\n",
      "Grand sum of 1388 tensor sets is: [175.227783203125, 1650.5263671875, -55.54302215576172, -1198.9195556640625, 1972.419189453125]\n",
      "\n",
      "Instance 1601 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1602 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "\n",
      "Instance 1603 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "jumping at index 13: [-0.19341512024402618, 1.2205841541290283, -1.107857346534729, -2.2513277530670166, 4.798268795013428]\n",
      "Grand sum of 1389 tensor sets is: [175.03436279296875, 1651.7469482421875, -56.65087890625, -1201.1708984375, 1977.2174072265625]\n",
      "\n",
      "Instance 1604 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "jumping at index 28: [-0.102994903922081, 0.8686107993125916, 0.7609058618545532, -1.0381016731262207, -0.31174948811531067]\n",
      "Grand sum of 1390 tensor sets is: [174.93136596679688, 1652.6156005859375, -55.88997268676758, -1202.208984375, 1976.9056396484375]\n",
      "\n",
      "Instance 1605 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "jumping at index 29: [1.0793029069900513, 1.1062235832214355, 0.6292182803153992, -0.35578596591949463, 3.0110855102539062]\n",
      "Grand sum of 1391 tensor sets is: [176.0106658935547, 1653.7218017578125, -55.2607536315918, -1202.5648193359375, 1979.916748046875]\n",
      "\n",
      "Instance 1606 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "jumping at index 8: [0.07092136144638062, 2.26822829246521, -0.667169988155365, 0.14574219286441803, -0.13647538423538208]\n",
      "Grand sum of 1392 tensor sets is: [176.0815887451172, 1655.989990234375, -55.92792510986328, -1202.4190673828125, 1979.7802734375]\n",
      "\n",
      "Instance 1607 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "jumping at index 7: [0.2865865230560303, 2.2952051162719727, -0.4844112694263458, -0.2023710459470749, 3.2676215171813965]\n",
      "Grand sum of 1393 tensor sets is: [176.36817932128906, 1658.28515625, -56.41233825683594, -1202.6214599609375, 1983.0478515625]\n",
      "\n",
      "Instance 1608 of jumping.\n",
      "Looking for vocab token: jumping\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "jumping at index 6: [-0.11658041179180145, 0.6050342321395874, -0.3428540825843811, -0.3408825993537903, -0.48139137029647827]\n",
      "Grand sum of 1394 tensor sets is: [176.25160217285156, 1658.89013671875, -56.755191802978516, -1202.96240234375, 1982.56640625]\n",
      "Mean of tensors is: tensor([ 0.1264,  1.1900, -0.0407, -0.8630,  1.4222]) (768 features in tensor)\n",
      "Saved the embedding for jumping.\n",
      "Saved the count of sentences used to create jumping embedding\n",
      "Run time for jumping was 197.44030414428562 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "forever\n",
      "\n",
      "Instance 1 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 8: [0.7191540598869324, 2.597564220428467, -0.13535168766975403, -1.43770432472229, 1.2078138589859009]\n",
      "Grand sum of 1 tensor sets is: [0.7191540598869324, 2.597564220428467, -0.13535168766975403, -1.43770432472229, 1.2078138589859009]\n",
      "\n",
      "Instance 2 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 3: [-0.5356701612472534, -0.29583367705345154, -0.26752108335494995, -1.0099997520446777, 0.29283127188682556]\n",
      "Grand sum of 2 tensor sets is: [0.18348389863967896, 2.3017306327819824, -0.402872771024704, -2.4477040767669678, 1.5006451606750488]\n",
      "\n",
      "Instance 3 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 3: [1.411973476409912, -0.3463144600391388, 0.9556238055229187, 0.11789199709892273, 2.1909661293029785]\n",
      "Grand sum of 3 tensor sets is: [1.5954573154449463, 1.955416202545166, 0.5527510643005371, -2.3298120498657227, 3.6916112899780273]\n",
      "\n",
      "Instance 4 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 5: [-0.38718658685684204, -0.7276805639266968, -0.4920441210269928, -0.22040963172912598, 0.40566539764404297]\n",
      "Grand sum of 4 tensor sets is: [1.208270788192749, 1.2277356386184692, 0.06070694327354431, -2.5502216815948486, 4.09727668762207]\n",
      "\n",
      "Instance 5 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 5 tensor sets is: [0.4735336899757385, 1.6325863599777222, 0.45239177346229553, -2.9072816371917725, -0.4635601043701172]\n",
      "\n",
      "Instance 6 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 7 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 8: [-0.10179618000984192, 1.067324161529541, 0.7539536952972412, 1.039597511291504, -4.619247913360596]\n",
      "Grand sum of 6 tensor sets is: [0.3717375099658966, 2.6999106407165527, 1.2063454389572144, -1.8676841259002686, -5.082808017730713]\n",
      "\n",
      "Instance 8 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([66, 13, 768])\n",
      "Shape of summed layers is: 66 x 768\n",
      "forever at index 35: [-0.09498852491378784, 2.0100526809692383, 1.5552904605865479, -1.162734031677246, -1.3423813581466675]\n",
      "Grand sum of 7 tensor sets is: [0.27674898505210876, 4.709963321685791, 2.7616357803344727, -3.0304181575775146, -6.42518949508667]\n",
      "\n",
      "Instance 9 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 10 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 29: [-0.7432736754417419, 0.39003321528434753, -1.1465339660644531, -0.49671924114227295, -3.3246891498565674]\n",
      "Grand sum of 8 tensor sets is: [-0.4665246903896332, 5.099996566772461, 1.6151018142700195, -3.527137279510498, -9.749878883361816]\n",
      "\n",
      "Instance 11 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 18: [0.33662721514701843, 0.8413910269737244, 0.31720060110092163, 0.8050816059112549, -3.1957664489746094]\n",
      "Grand sum of 9 tensor sets is: [-0.12989747524261475, 5.94138765335083, 1.932302474975586, -2.722055673599243, -12.945645332336426]\n",
      "\n",
      "Instance 12 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 13 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [-0.9885274171829224, 1.8550326824188232, -0.5057744979858398, -1.1332966089248657, -3.0532331466674805]\n",
      "Grand sum of 10 tensor sets is: [-1.118424892425537, 7.796420097351074, 1.426527976989746, -3.8553524017333984, -15.998878479003906]\n",
      "\n",
      "Instance 14 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [0.36479946970939636, 0.24682025611400604, -0.4848631024360657, -0.5248885154724121, -1.1551042795181274]\n",
      "Grand sum of 11 tensor sets is: [-0.7536253929138184, 8.043240547180176, 0.9416648745536804, -4.3802409172058105, -17.153982162475586]\n",
      "\n",
      "Instance 15 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [54]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "forever at index 54: [-0.35940536856651306, -0.052329882979393005, -1.0900832414627075, 0.6745641231536865, -0.9731273651123047]\n",
      "Grand sum of 12 tensor sets is: [-1.1130307912826538, 7.990910530090332, -0.1484183669090271, -3.705676794052124, -18.12710952758789]\n",
      "\n",
      "Instance 16 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 17 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 9: [0.44299352169036865, 1.1495238542556763, -0.23692981898784637, 0.6116688251495361, -0.23087841272354126]\n",
      "Grand sum of 13 tensor sets is: [-0.6700372695922852, 9.140434265136719, -0.38534820079803467, -3.094007968902588, -18.357988357543945]\n",
      "\n",
      "Instance 18 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 15: [-0.33253639936447144, 1.1534584760665894, -0.5106672048568726, 3.263065814971924, -1.3168686628341675]\n",
      "Grand sum of 14 tensor sets is: [-1.0025737285614014, 10.293892860412598, -0.8960154056549072, 0.16905784606933594, -19.674856185913086]\n",
      "\n",
      "Instance 19 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 20 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 24: [-0.6068921685218811, 0.3552637994289398, -1.4801371097564697, 0.039081502705812454, -6.3634562492370605]\n",
      "Grand sum of 15 tensor sets is: [-1.6094658374786377, 10.64915657043457, -2.376152515411377, 0.2081393450498581, -26.038312911987305]\n",
      "\n",
      "Instance 21 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 7: [-0.3308984339237213, 2.336886405944824, -0.22333519160747528, -0.8164075613021851, -0.8149641752243042]\n",
      "Grand sum of 16 tensor sets is: [-1.9403642416000366, 12.986042976379395, -2.599487781524658, -0.6082682013511658, -26.8532772064209]\n",
      "\n",
      "Instance 22 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 17 tensor sets is: [-1.2455809116363525, 14.11935043334961, -1.3963435888290405, -1.8776309490203857, -27.743518829345703]\n",
      "\n",
      "Instance 23 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 18 tensor sets is: [0.5457602739334106, 12.564712524414062, -2.078580379486084, -2.7235546112060547, -27.290456771850586]\n",
      "\n",
      "Instance 24 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 5: [1.1306650638580322, -0.13642485439777374, -0.2384241372346878, -0.6130810976028442, 1.5580562353134155]\n",
      "Grand sum of 19 tensor sets is: [1.6764253377914429, 12.428287506103516, -2.317004442214966, -3.3366355895996094, -25.73240089416504]\n",
      "\n",
      "Instance 25 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 26 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 27 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 28 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 65: [-0.4072004556655884, -1.165315866470337, -1.116049885749817, -1.6974555253982544, -1.7651728391647339]\n",
      "Grand sum of 20 tensor sets is: [1.2692248821258545, 11.262971878051758, -3.4330544471740723, -5.034090995788574, -27.497573852539062]\n",
      "\n",
      "Instance 29 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 7: [0.3452432453632355, -0.0796213299036026, 0.9808353185653687, -0.5610254406929016, -1.7266734838485718]\n",
      "Grand sum of 21 tensor sets is: [1.6144680976867676, 11.183350563049316, -2.452219009399414, -5.59511661529541, -29.224246978759766]\n",
      "\n",
      "Instance 30 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 31 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 22 tensor sets is: [0.8797309994697571, 11.588201522827148, -2.0605342388153076, -5.952176570892334, -33.78508377075195]\n",
      "\n",
      "Instance 32 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 12: [1.989641547203064, -1.980460286140442, 0.17916816473007202, -0.2772393226623535, -0.3257007598876953]\n",
      "Grand sum of 23 tensor sets is: [2.869372606277466, 9.607741355895996, -1.8813660144805908, -6.2294158935546875, -34.11078643798828]\n",
      "\n",
      "Instance 33 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [1.0301811695098877, 0.09672722220420837, -1.850982904434204, 0.9610074162483215, -2.2480239868164062]\n",
      "Grand sum of 24 tensor sets is: [3.8995537757873535, 9.704468727111816, -3.732348918914795, -5.268408298492432, -36.35881042480469]\n",
      "\n",
      "Instance 34 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 11: [-0.31641367077827454, 0.7202603220939636, 0.591779887676239, -0.8554326295852661, -3.843867778778076]\n",
      "Grand sum of 25 tensor sets is: [3.5831401348114014, 10.424729347229004, -3.140568971633911, -6.123840808868408, -40.20267868041992]\n",
      "\n",
      "Instance 35 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 24: [0.25700488686561584, -0.03141859173774719, 0.20902836322784424, -1.3262306451797485, -2.632232904434204]\n",
      "Grand sum of 26 tensor sets is: [3.8401451110839844, 10.393310546875, -2.9315404891967773, -7.450071334838867, -42.83491134643555]\n",
      "\n",
      "Instance 36 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 16: [0.6109100580215454, 1.0407249927520752, -0.32103198766708374, 0.7832146286964417, 1.2259743213653564]\n",
      "Grand sum of 27 tensor sets is: [4.45105504989624, 11.434035301208496, -3.252572536468506, -6.66685676574707, -41.60893630981445]\n",
      "\n",
      "Instance 37 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 38 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 39 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 40 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 29: [-0.573162317276001, 1.9490963220596313, 0.110131174325943, -0.13859392702579498, -2.50960636138916]\n",
      "Grand sum of 28 tensor sets is: [3.8778927326202393, 13.383131980895996, -3.1424412727355957, -6.805450916290283, -44.1185417175293]\n",
      "\n",
      "Instance 41 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 7: [-0.15988792479038239, -1.1692171096801758, 0.8905664682388306, 0.5047677755355835, -3.205986976623535]\n",
      "Grand sum of 29 tensor sets is: [3.7180047035217285, 12.21391487121582, -2.2518749237060547, -6.30068302154541, -47.324527740478516]\n",
      "\n",
      "Instance 42 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 17: [-0.014870241284370422, 0.479280024766922, -0.4836595952510834, -1.910595178604126, -2.7687973976135254]\n",
      "Grand sum of 30 tensor sets is: [3.703134536743164, 12.693195343017578, -2.735534429550171, -8.211277961730957, -50.093326568603516]\n",
      "\n",
      "Instance 43 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 53: [-0.03904017060995102, 1.962186336517334, 0.3727072477340698, -0.09199962019920349, -0.5388772487640381]\n",
      "Grand sum of 31 tensor sets is: [3.6640944480895996, 14.65538215637207, -2.3628273010253906, -8.303277969360352, -50.6322021484375]\n",
      "\n",
      "Instance 44 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 11: [0.21534135937690735, 1.6465163230895996, -1.2941712141036987, -0.3985404074192047, -0.6556155681610107]\n",
      "Grand sum of 32 tensor sets is: [3.8794357776641846, 16.301898956298828, -3.656998634338379, -8.701818466186523, -51.287818908691406]\n",
      "\n",
      "Instance 45 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 9: [0.5868903398513794, 2.3995285034179688, 0.14782699942588806, 2.6582632064819336, -2.754424571990967]\n",
      "Grand sum of 33 tensor sets is: [4.4663262367248535, 18.701427459716797, -3.509171724319458, -6.04355525970459, -54.04224395751953]\n",
      "\n",
      "Instance 46 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 47 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [0.42795416712760925, 2.9090933799743652, -0.045446738600730896, 1.2995531558990479, -2.2687487602233887]\n",
      "Grand sum of 34 tensor sets is: [4.894280433654785, 21.61052131652832, -3.5546183586120605, -4.744002342224121, -56.31099319458008]\n",
      "\n",
      "Instance 48 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.05075901746749878, 2.21020770072937, 0.04688075929880142, -1.1270672082901, -1.3753117322921753]\n",
      "Grand sum of 35 tensor sets is: [4.94503927230835, 23.820728302001953, -3.507737636566162, -5.871069431304932, -57.68630599975586]\n",
      "\n",
      "Instance 49 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 50 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([81, 13, 768])\n",
      "Shape of summed layers is: 81 x 768\n",
      "forever at index 29: [-0.26254963874816895, 1.9245914220809937, 0.24385541677474976, 0.16314160823822021, -2.029381275177002]\n",
      "Grand sum of 36 tensor sets is: [4.682489395141602, 25.745319366455078, -3.2638821601867676, -5.707927703857422, -59.7156867980957]\n",
      "\n",
      "Instance 51 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 17: [0.9762483835220337, 1.6133629083633423, -0.16661041975021362, -0.5501046180725098, 0.11941170692443848]\n",
      "Grand sum of 37 tensor sets is: [5.658737659454346, 27.35868263244629, -3.430492639541626, -6.258032321929932, -59.596275329589844]\n",
      "\n",
      "Instance 52 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 7: [0.2627567946910858, 0.7804337739944458, -1.0250258445739746, 0.8688831925392151, -2.487804889678955]\n",
      "Grand sum of 38 tensor sets is: [5.921494483947754, 28.139116287231445, -4.45551872253418, -5.389149188995361, -62.08407974243164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 53 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 54 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 17: [0.4191344678401947, 1.385462760925293, -0.6033922433853149, -0.3351379632949829, -0.8355673551559448]\n",
      "Grand sum of 39 tensor sets is: [6.3406291007995605, 29.524578094482422, -5.058910846710205, -5.724287033081055, -62.919647216796875]\n",
      "\n",
      "Instance 55 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 28: [-0.5247272253036499, 2.0715272426605225, 0.30445826053619385, 0.9370015859603882, -1.1634771823883057]\n",
      "Grand sum of 40 tensor sets is: [5.815901756286621, 31.596105575561523, -4.754452705383301, -4.787285327911377, -64.08312225341797]\n",
      "\n",
      "Instance 56 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 57 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 21: [-0.5127941370010376, 2.2104854583740234, -0.3321448564529419, 0.5112597346305847, -3.007030487060547]\n",
      "Grand sum of 41 tensor sets is: [5.303107738494873, 33.80659103393555, -5.086597442626953, -4.276025772094727, -67.09014892578125]\n",
      "\n",
      "Instance 58 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 15: [-0.05648037791252136, -0.2661914825439453, -0.42432665824890137, 0.6210795044898987, 0.7070977687835693]\n",
      "Grand sum of 42 tensor sets is: [5.246627330780029, 33.54039764404297, -5.510924339294434, -3.6549463272094727, -66.38304901123047]\n",
      "\n",
      "Instance 59 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [503]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "forever at index 503: [-0.07353340089321136, 0.12525241076946259, 0.0574597530066967, -1.6288232803344727, -0.9800407290458679]\n",
      "Grand sum of 43 tensor sets is: [5.173093795776367, 33.6656494140625, -5.453464508056641, -5.283769607543945, -67.36309051513672]\n",
      "\n",
      "Instance 60 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 61 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 62 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 63 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 64 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 2: [-0.4943588376045227, -0.12172213941812515, 0.38302338123321533, -0.10362742096185684, 3.108708620071411]\n",
      "Grand sum of 44 tensor sets is: [4.67873477935791, 33.54392623901367, -5.070441246032715, -5.387396812438965, -64.25437927246094]\n",
      "\n",
      "Instance 65 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [63]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "forever at index 63: [-0.3769170641899109, -0.25676125288009644, -1.0251142978668213, 0.10496726632118225, -0.19741638004779816]\n",
      "Grand sum of 45 tensor sets is: [4.301817893981934, 33.287166595458984, -6.095555305480957, -5.2824296951293945, -64.45179748535156]\n",
      "\n",
      "Instance 66 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [365]\n",
      "Size of token embeddings is torch.Size([434, 13, 768])\n",
      "Shape of summed layers is: 434 x 768\n",
      "forever at index 365: [1.3798778057098389, -1.364367961883545, 0.19967679679393768, -0.0958937257528305, -0.3721483647823334]\n",
      "Grand sum of 46 tensor sets is: [5.681695938110352, 31.92279815673828, -5.895878314971924, -5.378323554992676, -64.82394409179688]\n",
      "\n",
      "Instance 67 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 17: [-0.1618449091911316, 2.803406238555908, -0.02783997356891632, 1.758818507194519, -2.607349395751953]\n",
      "Grand sum of 47 tensor sets is: [5.519851207733154, 34.72620391845703, -5.923718452453613, -3.619504928588867, -67.43128967285156]\n",
      "\n",
      "Instance 68 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 17: [-0.02005722001194954, 1.3152607679367065, -1.4290310144424438, 0.4147929251194, -4.315427780151367]\n",
      "Grand sum of 48 tensor sets is: [5.499794006347656, 36.041465759277344, -7.352749347686768, -3.2047119140625, -71.74671936035156]\n",
      "\n",
      "Instance 69 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 22: [0.9892910718917847, 2.055809259414673, -0.2223549634218216, 1.243882417678833, -2.250450372695923]\n",
      "Grand sum of 49 tensor sets is: [6.4890851974487305, 38.09727478027344, -7.575104236602783, -1.960829496383667, -73.9971694946289]\n",
      "\n",
      "Instance 70 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 14: [0.05883900076150894, 0.5989603400230408, -1.1569530963897705, 1.2875593900680542, 0.3597419261932373]\n",
      "Grand sum of 50 tensor sets is: [6.547924041748047, 38.69623565673828, -8.732057571411133, -0.6732701063156128, -73.6374282836914]\n",
      "\n",
      "Instance 71 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 72 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 21: [0.2570752501487732, 0.2269812524318695, -0.6242523193359375, 0.012088358402252197, 1.0336214303970337]\n",
      "Grand sum of 51 tensor sets is: [6.804999351501465, 38.9232177734375, -9.35630989074707, -0.6611817479133606, -72.60380554199219]\n",
      "\n",
      "Instance 73 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 23: [0.25613653659820557, 1.7356373071670532, -0.0017609372735023499, 1.8842601776123047, -1.2285208702087402]\n",
      "Grand sum of 52 tensor sets is: [7.061135768890381, 40.65885543823242, -9.358070373535156, 1.2230784893035889, -73.83232879638672]\n",
      "\n",
      "Instance 74 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 21: [0.7414695024490356, 1.9673219919204712, -1.5429937839508057, -0.42146068811416626, -5.9759135246276855]\n",
      "Grand sum of 53 tensor sets is: [7.802605152130127, 42.62617874145508, -10.901063919067383, 0.8016178011894226, -79.80824279785156]\n",
      "\n",
      "Instance 75 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 5: [-0.3784233629703522, 0.5679839849472046, -1.0764378309249878, 0.38509634137153625, -1.1912689208984375]\n",
      "Grand sum of 54 tensor sets is: [7.424181938171387, 43.19416427612305, -11.97750186920166, 1.1867141723632812, -80.99951171875]\n",
      "\n",
      "Instance 76 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 4: [0.09937350451946259, -0.053620338439941406, -0.41772353649139404, 0.2857401669025421, -1.8334165811538696]\n",
      "Grand sum of 55 tensor sets is: [7.523555278778076, 43.14054489135742, -12.395225524902344, 1.472454309463501, -82.83293151855469]\n",
      "\n",
      "Instance 77 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([65, 13, 768])\n",
      "Shape of summed layers is: 65 x 768\n",
      "forever at index 34: [0.5431744456291199, 1.5784573554992676, 0.7300860285758972, -2.049726963043213, -2.3934013843536377]\n",
      "Grand sum of 56 tensor sets is: [8.066729545593262, 44.71900177001953, -11.665139198303223, -0.5772726535797119, -85.22633361816406]\n",
      "\n",
      "Instance 78 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 11: [0.28527164459228516, 1.3585447072982788, 0.30927810072898865, -0.4316023886203766, -0.7506300210952759]\n",
      "Grand sum of 57 tensor sets is: [8.352001190185547, 46.077545166015625, -11.355860710144043, -1.0088750123977661, -85.97696685791016]\n",
      "\n",
      "Instance 79 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 8: [0.1277688890695572, 0.7562439441680908, -0.6671024560928345, 0.46750175952911377, -5.518497467041016]\n",
      "Grand sum of 58 tensor sets is: [8.479769706726074, 46.83378982543945, -12.022963523864746, -0.5413732528686523, -91.49546813964844]\n",
      "\n",
      "Instance 80 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [0.9832466840744019, 0.19009733200073242, -0.32321181893348694, 0.42473360896110535, 0.5483078360557556]\n",
      "Grand sum of 59 tensor sets is: [9.463016510009766, 47.023887634277344, -12.346175193786621, -0.116639643907547, -90.94715881347656]\n",
      "\n",
      "Instance 81 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 14: [0.6575273871421814, 0.9453909397125244, -0.04407605528831482, -1.3207346200942993, -0.8325586915016174]\n",
      "Grand sum of 60 tensor sets is: [10.120543479919434, 47.96928024291992, -12.390251159667969, -1.437374234199524, -91.77971649169922]\n",
      "\n",
      "Instance 82 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 7: [-0.594904899597168, 0.3778582811355591, -0.46285784244537354, 0.39732038974761963, 0.9933804273605347]\n",
      "Grand sum of 61 tensor sets is: [9.525638580322266, 48.347137451171875, -12.853109359741211, -1.0400538444519043, -90.78633880615234]\n",
      "\n",
      "Instance 83 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 62 tensor sets is: [9.66369915008545, 49.00132751464844, -13.293691635131836, -1.337373971939087, -93.63664245605469]\n",
      "\n",
      "Instance 84 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 13: [-0.28013452887535095, 0.046411097049713135, 0.36416682600975037, -1.1516282558441162, -1.5601403713226318]\n",
      "Grand sum of 63 tensor sets is: [9.383564949035645, 49.04773712158203, -12.929524421691895, -2.489002227783203, -95.19678497314453]\n",
      "\n",
      "Instance 85 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [0.12532326579093933, 0.830530047416687, 0.16850844025611877, 0.04220421984791756, -1.1899375915527344]\n",
      "Grand sum of 64 tensor sets is: [9.508888244628906, 49.878265380859375, -12.761015892028809, -2.446798086166382, -96.38671875]\n",
      "\n",
      "Instance 86 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 27: [-0.06466715037822723, 2.3624961376190186, 0.1449052393436432, 0.09367165714502335, -2.1665356159210205]\n",
      "Grand sum of 65 tensor sets is: [9.444221496582031, 52.240760803222656, -12.616110801696777, -2.3531265258789062, -98.55325317382812]\n",
      "\n",
      "Instance 87 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [56]\n",
      "Size of token embeddings is torch.Size([91, 13, 768])\n",
      "Shape of summed layers is: 91 x 768\n",
      "forever at index 56: [0.5856547951698303, 1.0075129270553589, 0.5100997090339661, 0.2874833643436432, -4.67238187789917]\n",
      "Grand sum of 66 tensor sets is: [10.029876708984375, 53.24827194213867, -12.106011390686035, -2.065643072128296, -103.22563171386719]\n",
      "\n",
      "Instance 88 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 25: [-0.7543423175811768, 2.082606554031372, 0.04874349385499954, 1.4542419910430908, 0.31117159128189087]\n",
      "Grand sum of 67 tensor sets is: [9.275534629821777, 55.33087921142578, -12.057268142700195, -0.6114010810852051, -102.91445922851562]\n",
      "\n",
      "Instance 89 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 12: [-0.11768621951341629, 0.7034579515457153, 0.2306409478187561, -0.6983585953712463, -3.785707473754883]\n",
      "Grand sum of 68 tensor sets is: [9.157848358154297, 56.03433609008789, -11.826626777648926, -1.3097596168518066, -106.70016479492188]\n",
      "\n",
      "Instance 90 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 91 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 6: [1.4840185642242432, 0.8056843876838684, 0.9628342986106873, -0.2007702887058258, -0.06669455766677856]\n",
      "Grand sum of 69 tensor sets is: [10.641866683959961, 56.84001922607422, -10.863792419433594, -1.51052987575531, -106.76686096191406]\n",
      "\n",
      "Instance 92 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 93 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 5: [0.0168750137090683, 0.8767507672309875, -0.9784762859344482, -0.5706648826599121, -4.253544330596924]\n",
      "Grand sum of 70 tensor sets is: [10.65874195098877, 57.71677017211914, -11.842268943786621, -2.0811948776245117, -111.0204086303711]\n",
      "\n",
      "Instance 94 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 10: [0.008213147521018982, 1.6079100370407104, -0.5396812558174133, -0.27783647179603577, -4.414255142211914]\n",
      "Grand sum of 71 tensor sets is: [10.66695499420166, 59.32468032836914, -12.381950378417969, -2.3590314388275146, -115.43466186523438]\n",
      "\n",
      "Instance 95 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 7: [0.5393176674842834, -0.21558964252471924, -0.7408559918403625, 0.8230403661727905, -2.6602120399475098]\n",
      "Grand sum of 72 tensor sets is: [11.206273078918457, 59.10908889770508, -13.122806549072266, -1.5359910726547241, -118.0948715209961]\n",
      "\n",
      "Instance 96 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 97 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 98 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 3: [0.7058653831481934, 0.06750103831291199, 0.2829538583755493, -1.225460410118103, 2.4409291744232178]\n",
      "Grand sum of 73 tensor sets is: [11.912137985229492, 59.17658996582031, -12.839852333068848, -2.761451482772827, -115.65394592285156]\n",
      "\n",
      "Instance 99 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 29: [-0.8614733815193176, 0.874955415725708, 0.033043548464775085, 0.10623806715011597, -3.646181583404541]\n",
      "Grand sum of 74 tensor sets is: [11.050664901733398, 60.051544189453125, -12.806808471679688, -2.6552133560180664, -119.30012512207031]\n",
      "\n",
      "Instance 100 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 75 tensor sets is: [11.745448112487793, 61.184852600097656, -11.60366439819336, -3.9245760440826416, -120.19036865234375]\n",
      "\n",
      "Instance 101 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 3: [0.9615004062652588, -0.22749534249305725, -0.48265284299850464, -0.9635290503501892, 1.6354749202728271]\n",
      "Grand sum of 76 tensor sets is: [12.706948280334473, 60.95735549926758, -12.08631706237793, -4.8881049156188965, -118.55489349365234]\n",
      "\n",
      "Instance 102 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [81, 491]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "forever at index 81: [2.0577754974365234, -1.4608169794082642, -0.06573183834552765, -0.09661252796649933, 2.6260874271392822]\n",
      "forever at index 491: [0.7589441537857056, -0.3511114716529846, 0.4005799889564514, 2.0103604793548584, 0.8169938325881958]\n",
      "Grand sum of 77 tensor sets is: [14.115307807922363, 60.0513916015625, -11.918892860412598, -3.9312310218811035, -116.8333511352539]\n",
      "\n",
      "Instance 103 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 104 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 5: [0.4746123254299164, 0.7179093360900879, -1.6061632633209229, -0.20752042531967163, -0.48122382164001465]\n",
      "Grand sum of 78 tensor sets is: [14.589920043945312, 60.76930236816406, -13.525055885314941, -4.13875150680542, -117.3145751953125]\n",
      "\n",
      "Instance 105 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 15: [0.8613370656967163, -0.27026429772377014, 0.0656481385231018, 1.389837384223938, -1.9003642797470093]\n",
      "Grand sum of 79 tensor sets is: [15.45125675201416, 60.49903869628906, -13.459407806396484, -2.7489142417907715, -119.2149429321289]\n",
      "\n",
      "Instance 106 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 8: [0.8547707200050354, 0.9867058992385864, -0.43966206908226013, 0.21912992000579834, 0.4208742678165436]\n",
      "Grand sum of 80 tensor sets is: [16.306028366088867, 61.48574447631836, -13.899069786071777, -2.5297842025756836, -118.7940673828125]\n",
      "\n",
      "Instance 107 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 6: [0.9760435223579407, 0.21801522374153137, -0.09863260388374329, 0.6997371912002563, 2.3921279907226562]\n",
      "Grand sum of 81 tensor sets is: [17.282072067260742, 61.703758239746094, -13.997702598571777, -1.8300470113754272, -116.40193939208984]\n",
      "\n",
      "Instance 108 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 6: [0.9992403388023376, 2.57080078125, -0.10989263653755188, 0.12240174412727356, -4.035762310028076]\n",
      "Grand sum of 82 tensor sets is: [18.281312942504883, 64.2745590209961, -14.107595443725586, -1.707645297050476, -120.43769836425781]\n",
      "\n",
      "Instance 109 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 17: [0.4191344678401947, 1.385462760925293, -0.6033922433853149, -0.3351379632949829, -0.8355673551559448]\n",
      "Grand sum of 83 tensor sets is: [18.70044708251953, 65.66001892089844, -14.71098804473877, -2.042783260345459, -121.27326202392578]\n",
      "\n",
      "Instance 110 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 14: [0.6785960793495178, 1.6711984872817993, -0.15661373734474182, 0.6176260113716125, -4.228784084320068]\n",
      "Grand sum of 84 tensor sets is: [19.379043579101562, 67.33121490478516, -14.86760139465332, -1.4251573085784912, -125.50204467773438]\n",
      "\n",
      "Instance 111 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 112 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 113 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 3: [1.9462522268295288, -1.504098653793335, -0.9143209457397461, -0.38927921652793884, -0.034910738468170166]\n",
      "Grand sum of 85 tensor sets is: [21.32529640197754, 65.82711791992188, -15.781922340393066, -1.8144365549087524, -125.53695678710938]\n",
      "\n",
      "Instance 114 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 12: [0.7992830872535706, 1.2811670303344727, -0.25046080350875854, 1.8254444599151611, 2.410386562347412]\n",
      "Grand sum of 86 tensor sets is: [22.12458038330078, 67.10828399658203, -16.03238296508789, 0.011007905006408691, -123.12657165527344]\n",
      "\n",
      "Instance 115 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 116 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 22: [-0.19705107808113098, 1.1939126253128052, -0.2782352566719055, 0.03318857401609421, -5.209828853607178]\n",
      "Grand sum of 87 tensor sets is: [21.927528381347656, 68.30220031738281, -16.310617446899414, 0.0441964790225029, -128.33639526367188]\n",
      "\n",
      "Instance 117 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [52]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "forever at index 52: [0.6576010584831238, -0.5561482906341553, -1.461075782775879, 1.6579723358154297, -2.0563032627105713]\n",
      "Grand sum of 88 tensor sets is: [22.585128784179688, 67.74605560302734, -17.77169418334961, 1.7021688222885132, -130.3927001953125]\n",
      "\n",
      "Instance 118 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 26: [1.4113409519195557, 1.3431322574615479, -0.4094802439212799, -1.8300254344940186, -2.5290236473083496]\n",
      "Grand sum of 89 tensor sets is: [23.996469497680664, 69.08918762207031, -18.181175231933594, -0.12785661220550537, -132.92172241210938]\n",
      "\n",
      "Instance 119 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 13: [0.22568179666996002, 3.278318405151367, -0.3036077618598938, 2.1108458042144775, -2.526524066925049]\n",
      "Grand sum of 90 tensor sets is: [24.222150802612305, 72.36750793457031, -18.484783172607422, 1.9829891920089722, -135.4482421875]\n",
      "\n",
      "Instance 120 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 15: [0.9701793193817139, 1.1399104595184326, -0.07688301801681519, 0.38093411922454834, 0.9171814322471619]\n",
      "Grand sum of 91 tensor sets is: [25.19232940673828, 73.50741577148438, -18.56166648864746, 2.3639233112335205, -134.53106689453125]\n",
      "\n",
      "Instance 121 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 14: [0.40280306339263916, 0.2883625328540802, 0.5767067670822144, 1.9776690006256104, -3.085732936859131]\n",
      "Grand sum of 92 tensor sets is: [25.59513282775879, 73.7957763671875, -17.984960556030273, 4.341592311859131, -137.61680603027344]\n",
      "\n",
      "Instance 122 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 6: [0.44430556893348694, 2.078617811203003, -1.4057191610336304, 0.9681902527809143, -1.6914286613464355]\n",
      "Grand sum of 93 tensor sets is: [26.039438247680664, 75.87439727783203, -19.39068031311035, 5.3097825050354, -139.3082275390625]\n",
      "\n",
      "Instance 123 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 10: [-0.5831184983253479, 0.08082115650177002, -0.19026507437229156, 0.36846640706062317, -2.2450621128082275]\n",
      "Grand sum of 94 tensor sets is: [25.45631980895996, 75.95521545410156, -19.58094596862793, 5.678248882293701, -141.55328369140625]\n",
      "\n",
      "Instance 124 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 29: [-0.4119342863559723, 0.7438492774963379, 0.3242405652999878, 0.09723444283008575, -2.402214288711548]\n",
      "Grand sum of 95 tensor sets is: [25.04438591003418, 76.69906616210938, -19.25670623779297, 5.775483131408691, -143.95550537109375]\n",
      "\n",
      "Instance 125 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 10: [-0.03407025337219238, 1.6427831649780273, 0.05340385437011719, -0.9897501468658447, -2.1024770736694336]\n",
      "Grand sum of 96 tensor sets is: [25.01031494140625, 78.34185028076172, -19.20330238342285, 4.785733222961426, -146.0579833984375]\n",
      "\n",
      "Instance 126 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 16: [0.46422263979911804, -1.2326985597610474, 0.3509155809879303, 0.6110036969184875, -3.719074010848999]\n",
      "Grand sum of 97 tensor sets is: [25.474536895751953, 77.1091537475586, -18.852386474609375, 5.396737098693848, -149.7770538330078]\n",
      "\n",
      "Instance 127 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [56]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "forever at index 56: [1.2838568687438965, 0.2517474591732025, -0.37045618891716003, -0.8204279541969299, -1.189112901687622]\n",
      "Grand sum of 98 tensor sets is: [26.758394241333008, 77.36090087890625, -19.222843170166016, 4.5763092041015625, -150.96617126464844]\n",
      "\n",
      "Instance 128 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 129 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 130 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 21: [0.3762366771697998, 1.3540141582489014, 2.1655044555664062, 1.083864688873291, -1.9997957944869995]\n",
      "Grand sum of 99 tensor sets is: [27.13463020324707, 78.71491241455078, -17.05733871459961, 5.6601738929748535, -152.96597290039062]\n",
      "\n",
      "Instance 131 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 5: [0.6022050380706787, 2.4245588779449463, -0.6253792643547058, 1.0847046375274658, 0.46901166439056396]\n",
      "Grand sum of 100 tensor sets is: [27.736835479736328, 81.13947296142578, -17.68271827697754, 6.744878768920898, -152.49696350097656]\n",
      "\n",
      "Instance 132 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [0.9832466840744019, 0.19009733200073242, -0.32321181893348694, 0.42473360896110535, 0.5483078360557556]\n",
      "Grand sum of 101 tensor sets is: [28.720081329345703, 81.3295669555664, -18.005929946899414, 7.169612407684326, -151.9486541748047]\n",
      "\n",
      "Instance 133 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 3: [0.6386334896087646, -0.8177248239517212, 0.26064610481262207, 0.37270277738571167, 1.3229646682739258]\n",
      "Grand sum of 102 tensor sets is: [29.358715057373047, 80.5118408203125, -17.745283126831055, 7.5423150062561035, -150.6256866455078]\n",
      "\n",
      "Instance 134 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 135 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [0.13398882746696472, 0.39960426092147827, 0.6229570508003235, -1.1561012268066406, -0.6192765831947327]\n",
      "Grand sum of 103 tensor sets is: [29.492704391479492, 80.91144561767578, -17.122325897216797, 6.386213779449463, -151.24496459960938]\n",
      "\n",
      "Instance 136 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 5: [0.6065273284912109, 0.5816041231155396, -1.2424448728561401, 1.5382124185562134, -3.003850221633911]\n",
      "Grand sum of 104 tensor sets is: [30.099231719970703, 81.49304962158203, -18.364770889282227, 7.924426078796387, -154.24880981445312]\n",
      "\n",
      "Instance 137 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 138 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 105 tensor sets is: [30.237293243408203, 82.1472396850586, -18.80535316467285, 7.627106189727783, -157.09912109375]\n",
      "\n",
      "Instance 139 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 140 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 141 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 17: [0.8741735219955444, -0.2898051142692566, -0.5300635695457458, -0.36382320523262024, -2.9599013328552246]\n",
      "Grand sum of 106 tensor sets is: [31.111467361450195, 81.85743713378906, -19.335416793823242, 7.263282775878906, -160.05902099609375]\n",
      "\n",
      "Instance 142 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 7: [0.45563140511512756, 0.567535936832428, -1.1367441415786743, -2.0238988399505615, 0.07874453067779541]\n",
      "Grand sum of 107 tensor sets is: [31.56709861755371, 82.42497253417969, -20.47216033935547, 5.239383697509766, -159.9802703857422]\n",
      "\n",
      "Instance 143 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 19: [0.4870794415473938, 2.2466444969177246, 0.8410836458206177, 2.5061094760894775, -2.471078634262085]\n",
      "Grand sum of 108 tensor sets is: [32.054176330566406, 84.67161560058594, -19.63107681274414, 7.745492935180664, -162.45135498046875]\n",
      "\n",
      "Instance 144 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 145 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 146 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 10: [0.29222095012664795, 0.447043240070343, 0.09806007146835327, 2.096837282180786, -2.4652345180511475]\n",
      "Grand sum of 109 tensor sets is: [32.346397399902344, 85.11865997314453, -19.533016204833984, 9.842329978942871, -164.91659545898438]\n",
      "\n",
      "Instance 147 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 110 tensor sets is: [33.04117965698242, 86.25196838378906, -18.329872131347656, 8.572967529296875, -165.8068389892578]\n",
      "\n",
      "Instance 148 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 149 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 150 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 20: [0.17792409658432007, 0.0083761066198349, 0.28094732761383057, 0.05693155527114868, -0.18789738416671753]\n",
      "Grand sum of 111 tensor sets is: [33.2191047668457, 86.26034545898438, -18.048925399780273, 8.629899024963379, -165.99473571777344]\n",
      "\n",
      "Instance 151 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [1.0301811695098877, 0.09672722220420837, -1.850982904434204, 0.9610074162483215, -2.2480239868164062]\n",
      "Grand sum of 112 tensor sets is: [34.24928665161133, 86.35707092285156, -19.8999080657959, 9.590906143188477, -168.24276733398438]\n",
      "\n",
      "Instance 152 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [52]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "forever at index 52: [0.7173795104026794, -0.5452228784561157, -1.4434033632278442, 1.621253490447998, -1.997763752937317]\n",
      "Grand sum of 113 tensor sets is: [34.96666717529297, 85.81185150146484, -21.343311309814453, 11.212160110473633, -170.2405242919922]\n",
      "\n",
      "Instance 153 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([92, 13, 768])\n",
      "Shape of summed layers is: 92 x 768\n",
      "forever at index 74: [-0.008361276239156723, 1.7100136280059814, 1.036911964416504, -0.6515210866928101, -1.2560378313064575]\n",
      "Grand sum of 114 tensor sets is: [34.95830535888672, 87.52186584472656, -20.306400299072266, 10.560639381408691, -171.49656677246094]\n",
      "\n",
      "Instance 154 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 115 tensor sets is: [35.796932220458984, 87.87785339355469, -20.70806312561035, 11.074264526367188, -168.95748901367188]\n",
      "\n",
      "Instance 155 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 13: [1.0306488275527954, -0.006747938692569733, 0.1850205510854721, 0.4763759672641754, -1.3853257894515991]\n",
      "Grand sum of 116 tensor sets is: [36.827579498291016, 87.87110900878906, -20.523042678833008, 11.550640106201172, -170.3428192138672]\n",
      "\n",
      "Instance 156 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 9: [0.5218549370765686, 0.9484233856201172, 0.1863338053226471, 0.2602981925010681, -3.169445037841797]\n",
      "Grand sum of 117 tensor sets is: [37.34943389892578, 88.81953430175781, -20.336708068847656, 11.810937881469727, -173.51226806640625]\n",
      "\n",
      "Instance 157 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [56]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "forever at index 56: [-0.28290149569511414, 1.0157082080841064, -0.8006374835968018, -0.29228419065475464, -0.8355653285980225]\n",
      "Grand sum of 118 tensor sets is: [37.066532135009766, 89.83524322509766, -21.137346267700195, 11.518653869628906, -174.34783935546875]\n",
      "\n",
      "Instance 158 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 17: [0.07852816581726074, 0.13338014483451843, -0.3383643627166748, -0.2799450755119324, -4.226493835449219]\n",
      "Grand sum of 119 tensor sets is: [37.14506149291992, 89.96862030029297, -21.475709915161133, 11.23870849609375, -178.5743408203125]\n",
      "\n",
      "Instance 159 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 160 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "forever at index 41: [0.6806089282035828, 3.425706386566162, -0.24140112102031708, 2.948392868041992, -0.35718870162963867]\n",
      "Grand sum of 120 tensor sets is: [37.8256721496582, 93.39432525634766, -21.717111587524414, 14.187101364135742, -178.93153381347656]\n",
      "\n",
      "Instance 161 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 5: [0.38915425539016724, 1.5866619348526, -0.5450962781906128, 0.27150338888168335, -4.241290092468262]\n",
      "Grand sum of 121 tensor sets is: [38.21482467651367, 94.98098754882812, -22.26220703125, 14.45860481262207, -183.17282104492188]\n",
      "\n",
      "Instance 162 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [0.5422123670578003, -0.37714433670043945, -1.0815092325210571, 0.34070926904678345, -2.6716675758361816]\n",
      "Grand sum of 122 tensor sets is: [38.75703811645508, 94.60384368896484, -23.34371566772461, 14.799314498901367, -185.844482421875]\n",
      "\n",
      "Instance 163 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [200]\n",
      "Size of token embeddings is torch.Size([213, 13, 768])\n",
      "Shape of summed layers is: 213 x 768\n",
      "forever at index 200: [0.1345147043466568, -0.02110421657562256, -0.7276875376701355, -1.616793155670166, -0.752670168876648]\n",
      "Grand sum of 123 tensor sets is: [38.89155197143555, 94.5827407836914, -24.07140350341797, 13.18252182006836, -186.59715270996094]\n",
      "\n",
      "Instance 164 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 14: [0.601614773273468, 0.8601912260055542, -0.9699770212173462, -0.7300403714179993, -5.347193717956543]\n",
      "Grand sum of 124 tensor sets is: [39.493167877197266, 95.44293212890625, -25.041379928588867, 12.452481269836426, -191.94435119628906]\n",
      "\n",
      "Instance 165 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 13: [2.090362548828125, 0.31122276186943054, -0.4990423321723938, 1.3881272077560425, 0.070076584815979]\n",
      "Grand sum of 125 tensor sets is: [41.58353042602539, 95.75415802001953, -25.540422439575195, 13.840608596801758, -191.874267578125]\n",
      "\n",
      "Instance 166 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 2: [0.9920616149902344, 2.5151543617248535, -0.4167712330818176, -0.8444349765777588, 1.1123470067977905]\n",
      "Grand sum of 126 tensor sets is: [42.575592041015625, 98.2693099975586, -25.95719337463379, 12.996173858642578, -190.7619171142578]\n",
      "\n",
      "Instance 167 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 168 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 28: [0.5585262179374695, 0.08883644640445709, -0.15679088234901428, -0.3885485529899597, -1.183551549911499]\n",
      "Grand sum of 127 tensor sets is: [43.134117126464844, 98.35814666748047, -26.113985061645508, 12.607625007629395, -191.94546508789062]\n",
      "\n",
      "Instance 169 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 14: [-0.2895497977733612, 1.408115267753601, -1.4511997699737549, 0.0012910068035125732, -1.4930280447006226]\n",
      "Grand sum of 128 tensor sets is: [42.844566345214844, 99.7662582397461, -27.565185546875, 12.608916282653809, -193.43849182128906]\n",
      "\n",
      "Instance 170 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 26: [-0.013362910598516464, 0.50223708152771, 0.12009991705417633, 0.3801461160182953, -1.141701102256775]\n",
      "Grand sum of 129 tensor sets is: [42.83120346069336, 100.26849365234375, -27.445085525512695, 12.989062309265137, -194.5802001953125]\n",
      "\n",
      "Instance 171 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 6: [-0.19217664003372192, 0.27017340064048767, 0.03749892860651016, 0.5650085210800171, -6.663970947265625]\n",
      "Grand sum of 130 tensor sets is: [42.6390266418457, 100.53866577148438, -27.4075870513916, 13.554070472717285, -201.24417114257812]\n",
      "\n",
      "Instance 172 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 131 tensor sets is: [43.47765350341797, 100.8946533203125, -27.809249877929688, 14.067695617675781, -198.70509338378906]\n",
      "\n",
      "Instance 173 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [54]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "forever at index 54: [-0.35940536856651306, -0.052329882979393005, -1.0900832414627075, 0.6745641231536865, -0.9731273651123047]\n",
      "Grand sum of 132 tensor sets is: [43.118247985839844, 100.84232330322266, -28.899333953857422, 14.742259979248047, -199.67822265625]\n",
      "\n",
      "Instance 174 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 10: [0.6033225655555725, 1.6781742572784424, -1.2305907011032104, 2.282402753829956, -4.172649383544922]\n",
      "Grand sum of 133 tensor sets is: [43.7215690612793, 102.52050018310547, -30.129924774169922, 17.024662017822266, -203.8508758544922]\n",
      "\n",
      "Instance 175 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 15: [0.0014458000659942627, 0.6970324516296387, 0.35286039113998413, 0.5236789584159851, 1.2414965629577637]\n",
      "Grand sum of 134 tensor sets is: [43.72301483154297, 103.217529296875, -29.77706527709961, 17.548341751098633, -202.609375]\n",
      "\n",
      "Instance 176 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 9: [-0.3493066430091858, 2.6084718704223633, -0.4450094997882843, 0.8467295169830322, -1.6018298864364624]\n",
      "Grand sum of 135 tensor sets is: [43.37370681762695, 105.82600402832031, -30.222074508666992, 18.395071029663086, -204.21121215820312]\n",
      "\n",
      "Instance 177 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 13: [0.3141039311885834, 2.2977612018585205, 0.258650541305542, -0.33748751878738403, 0.2746395766735077]\n",
      "Grand sum of 136 tensor sets is: [43.687808990478516, 108.12376403808594, -29.963424682617188, 18.05758285522461, -203.9365692138672]\n",
      "\n",
      "Instance 178 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 13: [0.8454108238220215, 3.3999743461608887, 0.05868095904588699, 2.7731893062591553, -0.22204774618148804]\n",
      "Grand sum of 137 tensor sets is: [44.53321838378906, 111.52373504638672, -29.904743194580078, 20.830772399902344, -204.1586151123047]\n",
      "\n",
      "Instance 179 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [1.6994589567184448, -0.21792569756507874, 0.2326381653547287, -0.911447286605835, -4.0184526443481445]\n",
      "Grand sum of 138 tensor sets is: [46.2326774597168, 111.3058090209961, -29.67210578918457, 19.91932487487793, -208.17706298828125]\n",
      "\n",
      "Instance 180 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 8: [-0.6547608375549316, 1.8353558778762817, 0.40638697147369385, 0.7826361060142517, -2.7556517124176025]\n",
      "Grand sum of 139 tensor sets is: [45.57791519165039, 113.14116668701172, -29.265718460083008, 20.701961517333984, -210.93270874023438]\n",
      "\n",
      "Instance 181 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 33: [1.2642449140548706, 0.3875952661037445, 0.24833351373672485, 1.7202287912368774, -0.9544807076454163]\n",
      "Grand sum of 140 tensor sets is: [46.842159271240234, 113.52876281738281, -29.017385482788086, 22.422189712524414, -211.88719177246094]\n",
      "\n",
      "Instance 182 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 21: [0.1626555621623993, 1.6399589776992798, 0.9241192936897278, 1.4136115312576294, -2.7688186168670654]\n",
      "Grand sum of 141 tensor sets is: [47.00481414794922, 115.1687240600586, -28.093265533447266, 23.83580207824707, -214.656005859375]\n",
      "\n",
      "Instance 183 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 4: [0.8083117008209229, 0.3370521664619446, -0.39090535044670105, 0.0756462961435318, -0.6295108199119568]\n",
      "Grand sum of 142 tensor sets is: [47.81312561035156, 115.50577545166016, -28.48417091369629, 23.911447525024414, -215.2855224609375]\n",
      "\n",
      "Instance 184 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 21: [1.179478406906128, 0.6045626997947693, 0.8377909064292908, 0.3390519320964813, 0.28187432885169983]\n",
      "Grand sum of 143 tensor sets is: [48.99260330200195, 116.11033630371094, -27.646379470825195, 24.250499725341797, -215.00364685058594]\n",
      "\n",
      "Instance 185 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [58]\n",
      "Size of token embeddings is torch.Size([78, 13, 768])\n",
      "Shape of summed layers is: 78 x 768\n",
      "forever at index 58: [0.3089781403541565, 1.0574235916137695, -0.18703095614910126, 2.958552837371826, 0.4714229106903076]\n",
      "Grand sum of 144 tensor sets is: [49.30158233642578, 117.16776275634766, -27.833410263061523, 27.20905303955078, -214.5322265625]\n",
      "\n",
      "Instance 186 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 187 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 30: [0.5969967246055603, 2.250645160675049, -0.4836656153202057, -0.4443668723106384, -1.1396757364273071]\n",
      "Grand sum of 145 tensor sets is: [49.89857864379883, 119.41841125488281, -28.317075729370117, 26.764686584472656, -215.67190551757812]\n",
      "\n",
      "Instance 188 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 22: [0.05243326723575592, 1.7044944763183594, 0.8332476019859314, 0.7517306208610535, -1.115674376487732]\n",
      "Grand sum of 146 tensor sets is: [49.951011657714844, 121.12290954589844, -27.483827590942383, 27.516416549682617, -216.78758239746094]\n",
      "\n",
      "Instance 189 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 8: [0.05583301931619644, 1.4126535654067993, 0.3030741810798645, -1.8454694747924805, -2.143073320388794]\n",
      "Grand sum of 147 tensor sets is: [50.00684356689453, 122.53556060791016, -27.180753707885742, 25.670948028564453, -218.93064880371094]\n",
      "\n",
      "Instance 190 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 34: [0.1936466544866562, 3.3452351093292236, -1.0748815536499023, -0.5554482340812683, 1.9235846996307373]\n",
      "Grand sum of 148 tensor sets is: [50.20048904418945, 125.88079833984375, -28.255634307861328, 25.11549949645996, -217.00706481933594]\n",
      "\n",
      "Instance 191 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 192 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 193 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 194 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 26: [1.0388456583023071, 0.3898816704750061, 0.8309566974639893, -2.0560872554779053, 1.4297800064086914]\n",
      "Grand sum of 149 tensor sets is: [51.23933410644531, 126.27068328857422, -27.4246768951416, 23.059412002563477, -215.57728576660156]\n",
      "\n",
      "Instance 195 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 13: [1.0306488275527954, -0.006747938692569733, 0.1850205510854721, 0.4763759672641754, -1.3853257894515991]\n",
      "Grand sum of 150 tensor sets is: [52.269981384277344, 126.2639389038086, -27.239656448364258, 23.53578758239746, -216.96261596679688]\n",
      "\n",
      "Instance 196 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 2: [0.41277408599853516, 1.6970624923706055, -1.3459261655807495, 1.7138675451278687, -0.9608566164970398]\n",
      "Grand sum of 151 tensor sets is: [52.68275451660156, 127.96099853515625, -28.585582733154297, 25.24965476989746, -217.92347717285156]\n",
      "\n",
      "Instance 197 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 198 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [73]\n",
      "Size of token embeddings is torch.Size([502, 13, 768])\n",
      "Shape of summed layers is: 502 x 768\n",
      "forever at index 73: [-0.4114202857017517, 1.2488502264022827, 0.9239303469657898, 0.5437033772468567, -0.7385613322257996]\n",
      "Grand sum of 152 tensor sets is: [52.27133560180664, 129.20985412597656, -27.661651611328125, 25.793357849121094, -218.6620330810547]\n",
      "\n",
      "Instance 199 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 10: [0.286906361579895, 0.011619791388511658, -1.0992350578308105, 0.32898324728012085, -4.6502685546875]\n",
      "Grand sum of 153 tensor sets is: [52.55824279785156, 129.2214813232422, -28.760887145996094, 26.12234115600586, -223.3123016357422]\n",
      "\n",
      "Instance 200 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 201 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 30: [-0.4268273115158081, 2.0103864669799805, -0.24800533056259155, 1.1462608575820923, -3.5957446098327637]\n",
      "Grand sum of 154 tensor sets is: [52.13141632080078, 131.23187255859375, -29.008892059326172, 27.26860237121582, -226.90805053710938]\n",
      "\n",
      "Instance 202 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 203 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 204 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "forever at index 3: [1.0051188468933105, -0.13949325680732727, -0.5028639435768127, -0.42343443632125854, 1.4101810455322266]\n",
      "Grand sum of 155 tensor sets is: [53.13653564453125, 131.09237670898438, -29.511756896972656, 26.84516716003418, -225.49786376953125]\n",
      "\n",
      "Instance 205 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 206 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [170]\n",
      "Size of token embeddings is torch.Size([172, 13, 768])\n",
      "Shape of summed layers is: 172 x 768\n",
      "forever at index 170: [-0.25945013761520386, 0.7329016923904419, -0.1008215844631195, 1.5714210271835327, 1.6315354108810425]\n",
      "Grand sum of 156 tensor sets is: [52.8770866394043, 131.8252716064453, -29.612579345703125, 28.416587829589844, -223.8663330078125]\n",
      "\n",
      "Instance 207 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 15: [-0.4766021966934204, 1.74265718460083, 0.30620285868644714, 0.2167445719242096, -3.2190253734588623]\n",
      "Grand sum of 157 tensor sets is: [52.40048599243164, 133.56793212890625, -29.306377410888672, 28.633333206176758, -227.08535766601562]\n",
      "\n",
      "Instance 208 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4, 12]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "forever at index 4: [0.19769474864006042, 0.9239522218704224, 0.0995880514383316, -0.08179770410060883, 0.22700658440589905]\n",
      "forever at index 12: [0.6060097217559814, 0.5739008784294128, 0.5269936323165894, -1.1561846733093262, -0.11992436647415161]\n",
      "Grand sum of 158 tensor sets is: [52.802337646484375, 134.31686401367188, -28.993085861206055, 28.014341354370117, -227.0318145751953]\n",
      "\n",
      "Instance 209 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 15: [1.6396983861923218, 1.6421008110046387, -0.7140579223632812, 0.338726282119751, -2.332294225692749]\n",
      "Grand sum of 159 tensor sets is: [54.44203567504883, 135.95896911621094, -29.707143783569336, 28.35306739807129, -229.36410522460938]\n",
      "\n",
      "Instance 210 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 7: [0.13429990410804749, -0.49234986305236816, -0.32579588890075684, -0.31497836112976074, -3.119708299636841]\n",
      "Grand sum of 160 tensor sets is: [54.57633590698242, 135.46661376953125, -30.032939910888672, 28.038089752197266, -232.4838104248047]\n",
      "\n",
      "Instance 211 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([113, 13, 768])\n",
      "Shape of summed layers is: 113 x 768\n",
      "forever at index 110: [0.8210349082946777, -1.6566178798675537, -0.13650965690612793, 0.058199912309646606, -1.187120795249939]\n",
      "Grand sum of 161 tensor sets is: [55.397369384765625, 133.80999755859375, -30.169448852539062, 28.096290588378906, -233.67092895507812]\n",
      "\n",
      "Instance 212 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 213 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 162 tensor sets is: [54.66263198852539, 134.21484375, -29.77776336669922, 27.73923110961914, -238.2317657470703]\n",
      "\n",
      "Instance 214 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 215 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "forever at index 29: [0.41043511033058167, 1.438659906387329, 0.01475121546536684, -0.48798590898513794, -2.9926602840423584]\n",
      "Grand sum of 163 tensor sets is: [55.07306671142578, 135.65350341796875, -29.763011932373047, 27.251245498657227, -241.22442626953125]\n",
      "\n",
      "Instance 216 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 2: [-0.5513662099838257, 1.679462194442749, -0.07373695075511932, 1.2606719732284546, -3.93057918548584]\n",
      "Grand sum of 164 tensor sets is: [54.52170181274414, 137.3329620361328, -29.836748123168945, 28.511917114257812, -245.15499877929688]\n",
      "\n",
      "Instance 217 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 218 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 219 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 165 tensor sets is: [53.786964416503906, 137.73780822753906, -29.4450626373291, 28.154857635498047, -249.71583557128906]\n",
      "\n",
      "Instance 220 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 4: [0.10218548774719238, -0.33040904998779297, 0.7050803899765015, 1.0729711055755615, -1.0458729267120361]\n",
      "Grand sum of 166 tensor sets is: [53.8891487121582, 137.4073944091797, -28.73998260498047, 29.227828979492188, -250.76170349121094]\n",
      "\n",
      "Instance 221 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 167 tensor sets is: [54.0272102355957, 138.06158447265625, -29.180564880371094, 28.930509567260742, -253.6120147705078]\n",
      "\n",
      "Instance 222 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 223 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [77]\n",
      "Size of token embeddings is torch.Size([84, 13, 768])\n",
      "Shape of summed layers is: 84 x 768\n",
      "forever at index 77: [-0.13770227134227753, -0.4383464753627777, -0.839227020740509, 0.08529302477836609, -4.008317470550537]\n",
      "Grand sum of 168 tensor sets is: [53.88950729370117, 137.6232452392578, -30.019792556762695, 29.01580238342285, -257.6203308105469]\n",
      "\n",
      "Instance 224 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 4: [-0.5910714864730835, 0.5710277557373047, 0.045852310955524445, 2.6403183937072754, -0.13836893439292908]\n",
      "Grand sum of 169 tensor sets is: [53.29843521118164, 138.19427490234375, -29.973939895629883, 31.65612030029297, -257.7586975097656]\n",
      "\n",
      "Instance 225 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 7: [-0.12077398598194122, 0.26812586188316345, -0.2225586622953415, 0.10341227054595947, 2.6717119216918945]\n",
      "Grand sum of 170 tensor sets is: [53.17766189575195, 138.46240234375, -30.19649887084961, 31.759532928466797, -255.0869903564453]\n",
      "\n",
      "Instance 226 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 5: [0.4852544069290161, 1.6914072036743164, -0.23199820518493652, 1.2680755853652954, -3.5141422748565674]\n",
      "Grand sum of 171 tensor sets is: [53.66291809082031, 140.15380859375, -30.428497314453125, 33.02760696411133, -258.60113525390625]\n",
      "\n",
      "Instance 227 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 25: [0.7653751373291016, -0.32473504543304443, -1.170771837234497, 0.14972741901874542, -0.5124717950820923]\n",
      "Grand sum of 172 tensor sets is: [54.42829132080078, 139.82907104492188, -31.59926986694336, 33.17733383178711, -259.1136169433594]\n",
      "\n",
      "Instance 228 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([93, 13, 768])\n",
      "Shape of summed layers is: 93 x 768\n",
      "forever at index 38: [-1.1777037382125854, -0.2888803780078888, -0.2941676676273346, -0.5361020565032959, -3.4033100605010986]\n",
      "Grand sum of 173 tensor sets is: [53.250587463378906, 139.54019165039062, -31.8934383392334, 32.641231536865234, -262.5169372558594]\n",
      "\n",
      "Instance 229 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 230 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 16: [-0.0660596415400505, -0.03140056133270264, 0.36321645975112915, -0.6344414949417114, -0.8160111308097839]\n",
      "Grand sum of 174 tensor sets is: [53.18452835083008, 139.5087890625, -31.530221939086914, 32.00679016113281, -263.33294677734375]\n",
      "\n",
      "Instance 231 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 232 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 15: [0.9696832895278931, 0.3698159456253052, -0.568975567817688, 0.39616239070892334, -0.6662353277206421]\n",
      "Grand sum of 175 tensor sets is: [54.154212951660156, 139.87860107421875, -32.09919738769531, 32.4029541015625, -263.9991760253906]\n",
      "\n",
      "Instance 233 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 234 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 12: [1.1000237464904785, -0.0845547765493393, -0.6411045789718628, -0.05242128670215607, 0.6026915907859802]\n",
      "Grand sum of 176 tensor sets is: [55.25423812866211, 139.79405212402344, -32.74030303955078, 32.35053253173828, -263.396484375]\n",
      "\n",
      "Instance 235 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([93, 13, 768])\n",
      "Shape of summed layers is: 93 x 768\n",
      "forever at index 38: [-1.1777037382125854, -0.2888803780078888, -0.2941676676273346, -0.5361020565032959, -3.4033100605010986]\n",
      "Grand sum of 177 tensor sets is: [54.076534271240234, 139.5051727294922, -33.03446960449219, 31.814430236816406, -266.7998046875]\n",
      "\n",
      "Instance 236 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [0.7618420124053955, 0.8225862979888916, 0.4283001124858856, -0.34421825408935547, -0.9890788197517395]\n",
      "Grand sum of 178 tensor sets is: [54.838375091552734, 140.3277587890625, -32.606170654296875, 31.470211029052734, -267.78887939453125]\n",
      "\n",
      "Instance 237 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 30: [0.4442386329174042, 0.8462997078895569, -0.29612797498703003, 1.579073190689087, -3.6650590896606445]\n",
      "Grand sum of 179 tensor sets is: [55.28261184692383, 141.17405700683594, -32.90229797363281, 33.049285888671875, -271.4539489746094]\n",
      "\n",
      "Instance 238 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 3: [0.6562828421592712, 0.7376911640167236, -0.376034140586853, 0.00832703709602356, 0.2258373498916626]\n",
      "Grand sum of 180 tensor sets is: [55.93889617919922, 141.9117431640625, -33.2783317565918, 33.057613372802734, -271.2281188964844]\n",
      "\n",
      "Instance 239 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 11: [0.23037660121917725, -0.6087120175361633, -0.4014224410057068, -0.1932620257139206, -3.399423360824585]\n",
      "Grand sum of 181 tensor sets is: [56.169273376464844, 141.3030242919922, -33.679752349853516, 32.86435317993164, -274.6275329589844]\n",
      "\n",
      "Instance 240 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [1.0301811695098877, 0.09672722220420837, -1.850982904434204, 0.9610074162483215, -2.2480239868164062]\n",
      "Grand sum of 182 tensor sets is: [57.19945526123047, 141.39974975585938, -35.53073501586914, 33.82535934448242, -276.87554931640625]\n",
      "\n",
      "Instance 241 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 39: [0.2404685616493225, 1.8247365951538086, -0.22989042103290558, -1.4314640760421753, -0.01681867241859436]\n",
      "Grand sum of 183 tensor sets is: [57.43992233276367, 143.2244873046875, -35.760623931884766, 32.39389419555664, -276.8923645019531]\n",
      "\n",
      "Instance 242 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 13: [0.042440205812454224, 0.9103233814239502, 0.27958327531814575, -0.13246820867061615, -1.1993054151535034]\n",
      "Grand sum of 184 tensor sets is: [57.48236083984375, 144.1348114013672, -35.481040954589844, 32.26142501831055, -278.0916748046875]\n",
      "\n",
      "Instance 243 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 244 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 245 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 8: [0.5037220120429993, 1.4918696880340576, 0.9798704981803894, 0.04788631200790405, -1.3191083669662476]\n",
      "Grand sum of 185 tensor sets is: [57.986083984375, 145.62667846679688, -34.50117111206055, 32.30931091308594, -279.4107971191406]\n",
      "\n",
      "Instance 246 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 247 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 186 tensor sets is: [58.824710845947266, 145.982666015625, -34.90283203125, 32.82293701171875, -276.8717041015625]\n",
      "\n",
      "Instance 248 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [0.7610797882080078, 0.832751452922821, -0.4022613763809204, -0.6445860862731934, -1.7941073179244995]\n",
      "Grand sum of 187 tensor sets is: [59.585792541503906, 146.81541442871094, -35.305091857910156, 32.17835235595703, -278.6658020019531]\n",
      "\n",
      "Instance 249 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 26: [1.0388456583023071, 0.3898816704750061, 0.8309566974639893, -2.0560872554779053, 1.4297800064086914]\n",
      "Grand sum of 188 tensor sets is: [60.624637603759766, 147.20529174804688, -34.47413635253906, 30.122264862060547, -277.23602294921875]\n",
      "\n",
      "Instance 250 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 251 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 189 tensor sets is: [61.16854476928711, 146.57046508789062, -35.123687744140625, 28.980043411254883, -276.1146545410156]\n",
      "\n",
      "Instance 252 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 6: [1.5047965049743652, -1.8788789510726929, -1.6264352798461914, -1.642172932624817, -3.7543129920959473]\n",
      "Grand sum of 190 tensor sets is: [62.67333984375, 144.69158935546875, -36.7501220703125, 27.33786964416504, -279.86895751953125]\n",
      "\n",
      "Instance 253 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 6: [1.5093716382980347, 1.5261650085449219, -0.5223975777626038, 1.6939064264297485, 3.023444175720215]\n",
      "Grand sum of 191 tensor sets is: [64.18270874023438, 146.21775817871094, -37.272518157958984, 29.031776428222656, -276.84552001953125]\n",
      "\n",
      "Instance 254 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 13: [1.0439839363098145, 1.7772271633148193, 0.33017903566360474, 0.832247257232666, 1.334275245666504]\n",
      "Grand sum of 192 tensor sets is: [65.22669219970703, 147.99497985839844, -36.94234085083008, 29.864023208618164, -275.51123046875]\n",
      "\n",
      "Instance 255 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 256 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 257 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 258 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 259 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 53: [0.666554868221283, -0.7873085737228394, -1.431145191192627, 1.6047497987747192, -2.4387478828430176]\n",
      "Grand sum of 193 tensor sets is: [65.89324951171875, 147.20767211914062, -38.37348556518555, 31.468772888183594, -277.9499816894531]\n",
      "\n",
      "Instance 260 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 261 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 35: [1.7148849964141846, -0.6139543652534485, 0.5836049914360046, 0.4271795153617859, 0.9418890476226807]\n",
      "Grand sum of 194 tensor sets is: [67.6081314086914, 146.59371948242188, -37.789878845214844, 31.895952224731445, -277.0080871582031]\n",
      "\n",
      "Instance 262 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 195 tensor sets is: [67.99886322021484, 147.66754150390625, -38.013519287109375, 29.255002975463867, -271.6353454589844]\n",
      "\n",
      "Instance 263 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 13: [-0.07518090307712555, 2.7036352157592773, 0.014414936304092407, -0.5119169354438782, -1.3156647682189941]\n",
      "Grand sum of 196 tensor sets is: [67.9236831665039, 150.3711700439453, -37.99910354614258, 28.743085861206055, -272.9510192871094]\n",
      "\n",
      "Instance 264 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 39: [0.19261828064918518, -0.3796582818031311, -0.33681121468544006, -0.2852908670902252, -3.456049680709839]\n",
      "Grand sum of 197 tensor sets is: [68.11630249023438, 149.99151611328125, -38.335914611816406, 28.457794189453125, -276.4070739746094]\n",
      "\n",
      "Instance 265 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 16: [-0.6143075227737427, 0.5229971408843994, -0.012007132172584534, -0.722896933555603, -5.846844673156738]\n",
      "Grand sum of 198 tensor sets is: [67.50199127197266, 150.51451110839844, -38.347923278808594, 27.73489761352539, -282.25390625]\n",
      "\n",
      "Instance 266 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 267 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 268 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 269 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "forever at index 63: [0.004957273602485657, 1.517289161682129, 0.8008391261100769, -0.5652757883071899, -1.0557628870010376]\n",
      "Grand sum of 199 tensor sets is: [67.50695037841797, 152.03179931640625, -37.54708480834961, 27.16962242126465, -283.3096618652344]\n",
      "\n",
      "Instance 270 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 20: [0.452277272939682, 2.9945480823516846, -0.43394535779953003, 3.233825206756592, -1.5559725761413574]\n",
      "Grand sum of 200 tensor sets is: [67.959228515625, 155.02635192871094, -37.98102951049805, 30.4034481048584, -284.8656311035156]\n",
      "\n",
      "Instance 271 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "forever at index 38: [-0.47215840220451355, 2.4314959049224854, -1.065940499305725, 0.49987292289733887, -5.877922058105469]\n",
      "Grand sum of 201 tensor sets is: [67.48706817626953, 157.45785522460938, -39.04697036743164, 30.9033203125, -290.7435607910156]\n",
      "\n",
      "Instance 272 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 273 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 4: [0.9337961077690125, 0.9589104652404785, -0.07816442847251892, -1.526174783706665, 2.1484110355377197]\n",
      "Grand sum of 202 tensor sets is: [68.42086791992188, 158.41676330566406, -39.1251335144043, 29.377145767211914, -288.59515380859375]\n",
      "\n",
      "Instance 274 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 275 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 276 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 203 tensor sets is: [68.81656646728516, 159.42543029785156, -39.194705963134766, 28.693584442138672, -287.2988586425781]\n",
      "\n",
      "Instance 277 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 278 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "forever at index 15: [1.3674991130828857, 0.608260452747345, 0.7811641097068787, 0.7304633259773254, -1.102084755897522]\n",
      "Grand sum of 204 tensor sets is: [70.18406677246094, 160.03369140625, -38.413543701171875, 29.424047470092773, -288.40093994140625]\n",
      "\n",
      "Instance 279 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 280 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 45: [0.5411355495452881, 2.4098877906799316, -0.6645513772964478, -0.6027287840843201, -5.048913955688477]\n",
      "Grand sum of 205 tensor sets is: [70.72520446777344, 162.44357299804688, -39.078094482421875, 28.821319580078125, -293.4498596191406]\n",
      "\n",
      "Instance 281 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 22: [-0.21669335663318634, 1.4982473850250244, 0.8425693511962891, 1.2238821983337402, -1.4882514476776123]\n",
      "Grand sum of 206 tensor sets is: [70.50851440429688, 163.9418182373047, -38.23552703857422, 30.045202255249023, -294.9381103515625]\n",
      "\n",
      "Instance 282 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 17: [0.5857963562011719, 0.0789162814617157, 0.5066025257110596, -0.37083789706230164, -2.2935097217559814]\n",
      "Grand sum of 207 tensor sets is: [71.09431457519531, 164.02073669433594, -37.72892379760742, 29.67436408996582, -297.23162841796875]\n",
      "\n",
      "Instance 283 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 208 tensor sets is: [70.4588851928711, 165.7425079345703, -36.99641036987305, 30.980144500732422, -297.4083557128906]\n",
      "\n",
      "Instance 284 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [59]\n",
      "Size of token embeddings is torch.Size([70, 13, 768])\n",
      "Shape of summed layers is: 70 x 768\n",
      "forever at index 59: [0.3281249403953552, 0.6565637588500977, 0.5129944086074829, -1.865070104598999, 1.0563477277755737]\n",
      "Grand sum of 209 tensor sets is: [70.7870101928711, 166.39907836914062, -36.48341751098633, 29.115074157714844, -296.3520202636719]\n",
      "\n",
      "Instance 285 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 4: [0.7348008155822754, 0.5549254417419434, -0.4228194057941437, -0.41446995735168457, 1.0226702690124512]\n",
      "Grand sum of 210 tensor sets is: [71.52181243896484, 166.95401000976562, -36.9062385559082, 28.700603485107422, -295.329345703125]\n",
      "\n",
      "Instance 286 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 287 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 34: [0.6486272215843201, -1.0370575189590454, -0.09705671668052673, -1.00471031665802, -4.569666862487793]\n",
      "Grand sum of 211 tensor sets is: [72.17044067382812, 165.9169464111328, -37.0032958984375, 27.695892333984375, -299.8990173339844]\n",
      "\n",
      "Instance 288 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 17: [-0.13607007265090942, 1.8986718654632568, -0.37130412459373474, -0.72118079662323, -3.5670278072357178]\n",
      "Grand sum of 212 tensor sets is: [72.03437042236328, 167.81561279296875, -37.37459945678711, 26.974712371826172, -303.4660339355469]\n",
      "\n",
      "Instance 289 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 18: [-0.14648619294166565, 2.278364658355713, -0.12697172164916992, -1.4611973762512207, -4.027456283569336]\n",
      "Grand sum of 213 tensor sets is: [71.88788604736328, 170.09397888183594, -37.50157165527344, 25.51351547241211, -307.4934997558594]\n",
      "\n",
      "Instance 290 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 5: [1.0211596488952637, 0.0001398324966430664, 0.6943266987800598, -0.8475960493087769, -2.0102028846740723]\n",
      "Grand sum of 214 tensor sets is: [72.90904235839844, 170.0941162109375, -36.80724334716797, 24.66592025756836, -309.5036926269531]\n",
      "\n",
      "Instance 291 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 22: [1.0447087287902832, 2.1122286319732666, -1.7294576168060303, 0.7505699396133423, -1.055406928062439]\n",
      "Grand sum of 215 tensor sets is: [73.95375061035156, 172.2063446044922, -38.53670120239258, 25.41649055480957, -310.5591125488281]\n",
      "\n",
      "Instance 292 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 20: [0.7176733016967773, 0.5806524753570557, -0.31177499890327454, -0.6119822263717651, 0.8959013223648071]\n",
      "Grand sum of 216 tensor sets is: [74.67142486572266, 172.78700256347656, -38.84847640991211, 24.804508209228516, -309.6632080078125]\n",
      "\n",
      "Instance 293 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 217 tensor sets is: [75.3662109375, 173.92030334472656, -37.64533233642578, 23.535144805908203, -310.5534362792969]\n",
      "\n",
      "Instance 294 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 218 tensor sets is: [75.91011810302734, 173.2854766845703, -38.294883728027344, 22.39292335510254, -309.43206787109375]\n",
      "\n",
      "Instance 295 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 24: [-0.8523253202438354, -0.6328328847885132, -0.9771106243133545, -0.2510988712310791, 3.493102788925171]\n",
      "Grand sum of 219 tensor sets is: [75.05779266357422, 172.65264892578125, -39.271995544433594, 22.14182472229004, -305.93896484375]\n",
      "\n",
      "Instance 296 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 31: [0.1327822506427765, 1.768356442451477, -0.5982832312583923, 1.0300589799880981, -1.1507861614227295]\n",
      "Grand sum of 220 tensor sets is: [75.1905746459961, 174.42100524902344, -39.870277404785156, 23.171884536743164, -307.0897521972656]\n",
      "\n",
      "Instance 297 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([132, 13, 768])\n",
      "Shape of summed layers is: 132 x 768\n",
      "forever at index 39: [1.6696369647979736, -0.3943413197994232, -0.24026665091514587, -0.7277573347091675, -2.270989418029785]\n",
      "Grand sum of 221 tensor sets is: [76.86021423339844, 174.0266571044922, -40.11054229736328, 22.444128036499023, -309.3607482910156]\n",
      "\n",
      "Instance 298 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 17: [0.33422285318374634, -0.8155922293663025, -0.537778913974762, -0.07142467796802521, -3.95515775680542]\n",
      "Grand sum of 222 tensor sets is: [77.1944351196289, 173.2110595703125, -40.64832305908203, 22.372703552246094, -313.31591796875]\n",
      "\n",
      "Instance 299 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 223 tensor sets is: [77.73834228515625, 172.57623291015625, -41.297874450683594, 21.23048210144043, -312.1945495605469]\n",
      "\n",
      "Instance 300 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 301 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 5: [0.6707496643066406, 2.3560831546783447, -0.462116003036499, 2.140932083129883, -2.8946869373321533]\n",
      "Grand sum of 224 tensor sets is: [78.40908813476562, 174.93231201171875, -41.75999069213867, 23.371414184570312, -315.0892333984375]\n",
      "\n",
      "Instance 302 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 303 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 304 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 13: [1.0306488275527954, -0.006747938692569733, 0.1850205510854721, 0.4763759672641754, -1.3853257894515991]\n",
      "Grand sum of 225 tensor sets is: [79.43973541259766, 174.92556762695312, -41.57497024536133, 23.847789764404297, -316.47454833984375]\n",
      "\n",
      "Instance 305 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 3: [0.7400802969932556, 0.339777410030365, -0.3671029508113861, -0.4082692563533783, 1.5671052932739258]\n",
      "Grand sum of 226 tensor sets is: [80.17981719970703, 175.26535034179688, -41.942073822021484, 23.43951988220215, -314.9074401855469]\n",
      "\n",
      "Instance 306 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 227 tensor sets is: [79.44508361816406, 175.67019653320312, -41.55038833618164, 23.082460403442383, -319.46826171875]\n",
      "\n",
      "Instance 307 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 6: [-0.4416569471359253, 1.2077667713165283, -1.660141110420227, 1.0642319917678833, -5.7456464767456055]\n",
      "Grand sum of 228 tensor sets is: [79.00342559814453, 176.87796020507812, -43.21052932739258, 24.146692276000977, -325.2138977050781]\n",
      "\n",
      "Instance 308 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 19: [0.8453393578529358, 0.5756961703300476, 0.3677171468734741, -0.46797752380371094, -2.994748592376709]\n",
      "Grand sum of 229 tensor sets is: [79.84876251220703, 177.4536590576172, -42.842811584472656, 23.678714752197266, -328.2086486816406]\n",
      "\n",
      "Instance 309 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 310 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 311 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 230 tensor sets is: [80.23949432373047, 178.52748107910156, -43.06645202636719, 21.037765502929688, -322.8359069824219]\n",
      "\n",
      "Instance 312 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 17: [0.15401679277420044, -0.38507068157196045, -0.2680610120296478, -1.1294636726379395, 0.3049810528755188]\n",
      "Grand sum of 231 tensor sets is: [80.39350891113281, 178.1424102783203, -43.33451461791992, 19.908302307128906, -322.5309143066406]\n",
      "\n",
      "Instance 313 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 3: [0.32117903232574463, 0.23560544848442078, -0.000872097909450531, -0.8400305509567261, -2.1225438117980957]\n",
      "Grand sum of 232 tensor sets is: [80.71469116210938, 178.37802124023438, -43.33538818359375, 19.06827163696289, -324.6534729003906]\n",
      "\n",
      "Instance 314 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 26: [0.48774534463882446, 0.9630345106124878, 1.6397507190704346, -0.557216465473175, -1.4603257179260254]\n",
      "Grand sum of 233 tensor sets is: [81.20243835449219, 179.34104919433594, -41.69563674926758, 18.51105499267578, -326.1138000488281]\n",
      "\n",
      "Instance 315 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 316 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 11: [-0.6461827754974365, 0.022598501294851303, -2.208751916885376, -2.117126941680908, -4.1703362464904785]\n",
      "Grand sum of 234 tensor sets is: [80.55625915527344, 179.3636474609375, -43.904388427734375, 16.39392852783203, -330.2841491699219]\n",
      "\n",
      "Instance 317 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 318 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 12: [-0.9848876595497131, 1.2672085762023926, 0.933860182762146, -0.028468072414398193, 2.0923991203308105]\n",
      "Grand sum of 235 tensor sets is: [79.57137298583984, 180.630859375, -42.97052764892578, 16.365461349487305, -328.1917419433594]\n",
      "\n",
      "Instance 319 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([65, 13, 768])\n",
      "Shape of summed layers is: 65 x 768\n",
      "forever at index 23: [0.7253994941711426, 1.3926748037338257, -0.6226719617843628, -0.17902952432632446, -3.4238855838775635]\n",
      "Grand sum of 236 tensor sets is: [80.2967758178711, 182.02352905273438, -43.59320068359375, 16.186431884765625, -331.6156311035156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 320 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 237 tensor sets is: [80.69247436523438, 183.03219604492188, -43.66277313232422, 15.502869606018066, -330.3193359375]\n",
      "\n",
      "Instance 321 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 26: [0.2902624011039734, 1.3440099954605103, -0.6953708529472351, -2.7803726196289062, -2.3086137771606445]\n",
      "Grand sum of 238 tensor sets is: [80.98273468017578, 184.37620544433594, -44.3581428527832, 12.72249698638916, -332.6279602050781]\n",
      "\n",
      "Instance 322 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "forever at index 6: [0.8644115924835205, 0.06994235515594482, 0.24300886690616608, -2.889446258544922, 0.4938024580478668]\n",
      "Grand sum of 239 tensor sets is: [81.8471450805664, 184.44615173339844, -44.115135192871094, 9.833050727844238, -332.1341552734375]\n",
      "\n",
      "Instance 323 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 324 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 325 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 326 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 327 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 328 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 240 tensor sets is: [81.21171569824219, 186.1679229736328, -43.38262176513672, 11.13883113861084, -332.3108825683594]\n",
      "\n",
      "Instance 329 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 40: [-0.1687455177307129, -0.301961213350296, -0.16345909237861633, -1.9584980010986328, -1.1029413938522339]\n",
      "Grand sum of 241 tensor sets is: [81.04296875, 185.865966796875, -43.54608154296875, 9.180333137512207, -333.413818359375]\n",
      "\n",
      "Instance 330 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 331 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 3: [1.79982328414917, 0.5944511890411377, -0.3416613042354584, 1.113884687423706, 2.8593668937683105]\n",
      "Grand sum of 242 tensor sets is: [82.84278869628906, 186.46041870117188, -43.88774108886719, 10.294218063354492, -330.554443359375]\n",
      "\n",
      "Instance 332 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 333 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 10: [0.09301678091287613, 0.9464248418807983, -0.8344090580940247, 1.1356253623962402, -3.407395362854004]\n",
      "Grand sum of 243 tensor sets is: [82.93580627441406, 187.40684509277344, -44.72214889526367, 11.42984390258789, -333.96185302734375]\n",
      "\n",
      "Instance 334 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 16: [-0.17206121981143951, -0.0790608674287796, 0.17650659382343292, 0.43559396266937256, -1.4846999645233154]\n",
      "Grand sum of 244 tensor sets is: [82.76374816894531, 187.32778930664062, -44.5456428527832, 11.865437507629395, -335.4465637207031]\n",
      "\n",
      "Instance 335 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 9: [0.9755155444145203, -0.198129341006279, -1.0949721336364746, 0.5386164784431458, -4.17471981048584]\n",
      "Grand sum of 245 tensor sets is: [83.73926544189453, 187.12965393066406, -45.6406135559082, 12.404053688049316, -339.62127685546875]\n",
      "\n",
      "Instance 336 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [51]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "forever at index 51: [0.13289400935173035, 1.7258834838867188, 0.5039525032043457, 0.5202962160110474, 1.5060228109359741]\n",
      "Grand sum of 246 tensor sets is: [83.87216186523438, 188.85552978515625, -45.136661529541016, 12.924349784851074, -338.1152648925781]\n",
      "\n",
      "Instance 337 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 338 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 17: [0.4853895604610443, 0.9599072933197021, -0.4185337722301483, 1.1268037557601929, -1.951826810836792]\n",
      "Grand sum of 247 tensor sets is: [84.35755157470703, 189.8154296875, -45.55519485473633, 14.051153182983398, -340.06707763671875]\n",
      "\n",
      "Instance 339 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 32: [1.7246917486190796, 0.00025657564401626587, 0.6370725631713867, 0.6032079458236694, 1.5304012298583984]\n",
      "Grand sum of 248 tensor sets is: [86.08224487304688, 189.81568908691406, -44.918121337890625, 14.6543607711792, -338.53668212890625]\n",
      "\n",
      "Instance 340 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 4: [0.5347165465354919, 2.3209846019744873, -0.3974906802177429, -0.34963518381118774, -3.4103846549987793]\n",
      "Grand sum of 249 tensor sets is: [86.61695861816406, 192.1366729736328, -45.31561279296875, 14.304725646972656, -341.9470520019531]\n",
      "\n",
      "Instance 341 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 3: [0.09649815410375595, 1.077986478805542, 0.35919415950775146, -0.3421075642108917, -0.20371535420417786]\n",
      "Grand sum of 250 tensor sets is: [86.71345520019531, 193.21466064453125, -44.956417083740234, 13.962617874145508, -342.1507568359375]\n",
      "\n",
      "Instance 342 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 343 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 344 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [0.10237450152635574, 0.6033823490142822, -0.2452552169561386, -0.7208962440490723, 1.1564643383026123]\n",
      "Grand sum of 251 tensor sets is: [86.81582641601562, 193.8180389404297, -45.2016716003418, 13.241722106933594, -340.9942932128906]\n",
      "\n",
      "Instance 345 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 346 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 29: [0.14236760139465332, 2.899231433868408, 1.0018742084503174, -0.9774129390716553, -0.25950098037719727]\n",
      "Grand sum of 252 tensor sets is: [86.95819091796875, 196.71726989746094, -44.199798583984375, 12.26430892944336, -341.2537841796875]\n",
      "\n",
      "Instance 347 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 12: [0.35865265130996704, -0.14785723388195038, -0.678583562374115, 1.6075515747070312, -4.57158899307251]\n",
      "Grand sum of 253 tensor sets is: [87.31684112548828, 196.5694122314453, -44.87838363647461, 13.87186050415039, -345.82537841796875]\n",
      "\n",
      "Instance 348 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 349 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 350 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 33: [0.6652013659477234, 1.5494863986968994, 0.10869789123535156, -0.2992635667324066, -4.0500359535217285]\n",
      "Grand sum of 254 tensor sets is: [87.98204040527344, 198.118896484375, -44.769683837890625, 13.572596549987793, -349.87542724609375]\n",
      "\n",
      "Instance 351 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 352 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 353 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "forever at index 61: [0.5597091317176819, 0.7535359859466553, -0.6519070863723755, 2.161344528198242, 3.454082489013672]\n",
      "Grand sum of 255 tensor sets is: [88.541748046875, 198.8724365234375, -45.421592712402344, 15.733941078186035, -346.4213562011719]\n",
      "\n",
      "Instance 354 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 355 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 356 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 13: [-0.24294905364513397, 2.3057775497436523, -0.21658235788345337, 0.5192920565605164, -2.3123741149902344]\n",
      "Grand sum of 256 tensor sets is: [88.29879760742188, 201.17820739746094, -45.63817596435547, 16.253232955932617, -348.7337341308594]\n",
      "\n",
      "Instance 357 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 257 tensor sets is: [87.5640640258789, 201.5830535888672, -45.246490478515625, 15.896173477172852, -353.2945556640625]\n",
      "\n",
      "Instance 358 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [59]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "forever at index 59: [-0.30018261075019836, 0.3342972695827484, -1.133064866065979, 1.7275855541229248, -1.5296382904052734]\n",
      "Grand sum of 258 tensor sets is: [87.26387786865234, 201.9173583984375, -46.379554748535156, 17.62375831604004, -354.8241882324219]\n",
      "\n",
      "Instance 359 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 39: [-0.3034477233886719, 1.8906903266906738, -0.7529327273368835, -0.05776147544384003, -1.6281145811080933]\n",
      "Grand sum of 259 tensor sets is: [86.96043395996094, 203.80804443359375, -47.13248825073242, 17.565996170043945, -356.4523010253906]\n",
      "\n",
      "Instance 360 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 10: [0.6368868947029114, 2.5278406143188477, -2.2968199253082275, 0.0682826042175293, -4.704740524291992]\n",
      "Grand sum of 260 tensor sets is: [87.59732055664062, 206.3358917236328, -49.4293098449707, 17.634279251098633, -361.15704345703125]\n",
      "\n",
      "Instance 361 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 5: [-0.17400239408016205, -1.246393084526062, -0.8821679353713989, -0.652216911315918, 1.2159860134124756]\n",
      "Grand sum of 261 tensor sets is: [87.4233169555664, 205.08949279785156, -50.31147766113281, 16.98206329345703, -359.9410705566406]\n",
      "\n",
      "Instance 362 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 12: [-1.1614768505096436, 1.9158172607421875, -1.240506887435913, 1.4610017538070679, -1.468461275100708]\n",
      "Grand sum of 262 tensor sets is: [86.2618408203125, 207.00531005859375, -51.55198287963867, 18.443065643310547, -361.4095458984375]\n",
      "\n",
      "Instance 363 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 364 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 6: [-0.29501569271087646, 2.1437501907348633, 0.12286506593227386, 0.0072782933712005615, -0.7887570261955261]\n",
      "Grand sum of 263 tensor sets is: [85.96682739257812, 209.14906311035156, -51.42911911010742, 18.45034408569336, -362.19830322265625]\n",
      "\n",
      "Instance 365 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "forever at index 3: [1.110938310623169, 0.8518267869949341, -0.07668910920619965, 1.7204973697662354, 1.955981731414795]\n",
      "Grand sum of 264 tensor sets is: [87.07776641845703, 210.00088500976562, -51.50580978393555, 20.170841217041016, -360.2423095703125]\n",
      "\n",
      "Instance 366 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 10: [0.346113383769989, 2.896516799926758, -0.11678769439458847, 0.7523548007011414, -1.3682738542556763]\n",
      "Grand sum of 265 tensor sets is: [87.42388153076172, 212.89739990234375, -51.622596740722656, 20.92319679260254, -361.610595703125]\n",
      "\n",
      "Instance 367 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 7: [0.459736704826355, 1.441063404083252, -0.8194361925125122, 0.2522125244140625, -1.0471241474151611]\n",
      "Grand sum of 266 tensor sets is: [87.88362121582031, 214.33847045898438, -52.44203186035156, 21.1754093170166, -362.65771484375]\n",
      "\n",
      "Instance 368 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 65: [-0.4072004556655884, -1.165315866470337, -1.116049885749817, -1.6974555253982544, -1.7651728391647339]\n",
      "Grand sum of 267 tensor sets is: [87.4764175415039, 213.17315673828125, -53.558082580566406, 19.47795295715332, -364.4228820800781]\n",
      "\n",
      "Instance 369 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 19: [0.49743449687957764, 1.0211389064788818, -1.0215686559677124, 1.0156869888305664, -4.440690994262695]\n",
      "Grand sum of 268 tensor sets is: [87.9738540649414, 214.1942901611328, -54.57965087890625, 20.493640899658203, -368.86358642578125]\n",
      "\n",
      "Instance 370 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "forever at index 46: [0.43829992413520813, 1.254381775856018, 0.19513501226902008, -0.6573927402496338, -3.3892648220062256]\n",
      "Grand sum of 269 tensor sets is: [88.41215515136719, 215.44866943359375, -54.384517669677734, 19.83624839782715, -372.2528381347656]\n",
      "\n",
      "Instance 371 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 6: [-0.41197139024734497, -0.029540807008743286, 0.559522807598114, -0.8065746426582336, -3.5426504611968994]\n",
      "Grand sum of 270 tensor sets is: [88.00018310546875, 215.41912841796875, -53.82499313354492, 19.029674530029297, -375.7955017089844]\n",
      "\n",
      "Instance 372 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 4: [-0.39796182513237, 1.3489981889724731, -0.14370864629745483, 2.2360434532165527, -4.321237564086914]\n",
      "Grand sum of 271 tensor sets is: [87.60221862792969, 216.76812744140625, -53.96870040893555, 21.265718460083008, -380.1167297363281]\n",
      "\n",
      "Instance 373 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 374 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 375 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 376 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 5: [0.8722568154335022, 0.07259298861026764, -0.14886267483234406, -1.432004690170288, 0.4874582290649414]\n",
      "Grand sum of 272 tensor sets is: [88.47447204589844, 216.84071350097656, -54.11756134033203, 19.83371353149414, -379.6292724609375]\n",
      "\n",
      "Instance 377 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 18: [-0.09735257178544998, 1.7312589883804321, 0.49888479709625244, -1.898927092552185, -2.0053656101226807]\n",
      "Grand sum of 273 tensor sets is: [88.37712097167969, 218.5719757080078, -53.618675231933594, 17.934785842895508, -381.6346435546875]\n",
      "\n",
      "Instance 378 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 274 tensor sets is: [87.74169158935547, 220.2937469482422, -52.88616180419922, 19.24056625366211, -381.8113708496094]\n",
      "\n",
      "Instance 379 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 25: [0.7687876224517822, 0.631436824798584, 0.1204494833946228, 0.07258163392543793, 0.06837809085845947]\n",
      "Grand sum of 275 tensor sets is: [88.51048278808594, 220.92518615722656, -52.76571273803711, 19.313148498535156, -381.74298095703125]\n",
      "\n",
      "Instance 380 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 276 tensor sets is: [87.77574920654297, 221.3300323486328, -52.374027252197266, 18.95608901977539, -386.3038330078125]\n",
      "\n",
      "Instance 381 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [51]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "forever at index 51: [0.7895391583442688, -0.5338797569274902, -1.497312307357788, 1.7836365699768066, -1.9226436614990234]\n",
      "Grand sum of 277 tensor sets is: [88.5652847290039, 220.79615783691406, -53.871337890625, 20.73972511291504, -388.2264709472656]\n",
      "\n",
      "Instance 382 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 19: [0.06077258288860321, 1.3247112035751343, 0.2659372091293335, -1.859513521194458, -2.9772391319274902]\n",
      "Grand sum of 278 tensor sets is: [88.62606048583984, 222.12086486816406, -53.60540008544922, 18.880210876464844, -391.2037048339844]\n",
      "\n",
      "Instance 383 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 22: [0.05243326723575592, 1.7044944763183594, 0.8332476019859314, 0.7517306208610535, -1.115674376487732]\n",
      "Grand sum of 279 tensor sets is: [88.67849731445312, 223.8253631591797, -52.772151947021484, 19.631940841674805, -392.3193664550781]\n",
      "\n",
      "Instance 384 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 385 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 17: [0.31576359272003174, -0.182842418551445, -0.9659100770950317, -0.500556230545044, -6.088157653808594]\n",
      "Grand sum of 280 tensor sets is: [88.9942626953125, 223.64251708984375, -53.73806381225586, 19.131383895874023, -398.40753173828125]\n",
      "\n",
      "Instance 386 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 5: [0.40229618549346924, 0.7618048191070557, 0.2764597535133362, 0.7959837317466736, -1.0034433603286743]\n",
      "Grand sum of 281 tensor sets is: [89.39656066894531, 224.40432739257812, -53.461605072021484, 19.9273681640625, -399.4109802246094]\n",
      "\n",
      "Instance 387 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 3: [0.32994553446769714, 0.18613892793655396, -0.5860008597373962, 0.7834742069244385, 1.6237897872924805]\n",
      "Grand sum of 282 tensor sets is: [89.72650909423828, 224.59046936035156, -54.047607421875, 20.71084213256836, -397.7872009277344]\n",
      "\n",
      "Instance 388 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 22: [-0.05806301534175873, 1.2006810903549194, -0.26880061626434326, -0.24316075444221497, -4.091954708099365]\n",
      "Grand sum of 283 tensor sets is: [89.66844940185547, 225.79115295410156, -54.31640625, 20.467681884765625, -401.879150390625]\n",
      "\n",
      "Instance 389 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 390 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 391 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 17: [0.2555120885372162, 1.6588761806488037, -0.7331655621528625, -0.5276827216148376, -1.1627925634384155]\n",
      "Grand sum of 284 tensor sets is: [89.92395782470703, 227.4500274658203, -55.0495719909668, 19.939998626708984, -403.04193115234375]\n",
      "\n",
      "Instance 392 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "forever at index 47: [0.37123990058898926, 0.09170491993427277, -1.2406305074691772, 0.4676143229007721, -2.3907172679901123]\n",
      "Grand sum of 285 tensor sets is: [90.29519653320312, 227.54173278808594, -56.29020309448242, 20.40761375427246, -405.4326477050781]\n",
      "\n",
      "Instance 393 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 17: [0.33422285318374634, -0.8155922293663025, -0.537778913974762, -0.07142467796802521, -3.95515775680542]\n",
      "Grand sum of 286 tensor sets is: [90.6294174194336, 226.72613525390625, -56.82798385620117, 20.33618927001953, -409.3878173828125]\n",
      "\n",
      "Instance 394 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "forever at index 50: [0.8470101356506348, 1.582916259765625, 0.6739855408668518, -1.8968818187713623, -1.598803997039795]\n",
      "Grand sum of 287 tensor sets is: [91.47642517089844, 228.30905151367188, -56.15399932861328, 18.439308166503906, -410.98663330078125]\n",
      "\n",
      "Instance 395 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 40: [0.4996374249458313, -0.28994956612586975, -0.46465548872947693, -2.637877941131592, -3.694197416305542]\n",
      "Grand sum of 288 tensor sets is: [91.97605895996094, 228.01910400390625, -56.618656158447266, 15.801429748535156, -414.6808166503906]\n",
      "\n",
      "Instance 396 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "forever at index 29: [0.41043511033058167, 1.438659906387329, 0.01475121546536684, -0.48798590898513794, -2.9926602840423584]\n",
      "Grand sum of 289 tensor sets is: [92.3864974975586, 229.457763671875, -56.603904724121094, 15.313444137573242, -417.6734619140625]\n",
      "\n",
      "Instance 397 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 32: [0.35104840993881226, 1.4698810577392578, 0.35259872674942017, 1.1687417030334473, 1.278978705406189]\n",
      "Grand sum of 290 tensor sets is: [92.737548828125, 230.92764282226562, -56.251304626464844, 16.48218536376953, -416.39447021484375]\n",
      "\n",
      "Instance 398 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 399 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 12: [0.6732869744300842, 1.2281546592712402, -1.0723520517349243, 0.27118903398513794, 0.3601223826408386]\n",
      "Grand sum of 291 tensor sets is: [93.41083526611328, 232.15579223632812, -57.32365798950195, 16.753374099731445, -416.03436279296875]\n",
      "\n",
      "Instance 400 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 401 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 402 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 403 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 404 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 14: [0.21717102825641632, 1.0272125005722046, 0.001334354281425476, 0.6784761548042297, 0.5428685545921326]\n",
      "Grand sum of 292 tensor sets is: [93.62800598144531, 233.18299865722656, -57.322322845458984, 17.43185043334961, -415.4914855957031]\n",
      "\n",
      "Instance 405 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "forever at index 43: [0.705483615398407, -1.8049049377441406, 0.18964418768882751, 1.611323356628418, -4.160096645355225]\n",
      "Grand sum of 293 tensor sets is: [94.33348846435547, 231.3780975341797, -57.1326789855957, 19.043174743652344, -419.6515808105469]\n",
      "\n",
      "Instance 406 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 294 tensor sets is: [94.72918701171875, 232.3867645263672, -57.20225143432617, 18.35961151123047, -418.35528564453125]\n",
      "\n",
      "Instance 407 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 21: [0.047228697687387466, 1.8372012376785278, -1.002234935760498, -0.38471895456314087, -2.0041511058807373]\n",
      "Grand sum of 295 tensor sets is: [94.77641296386719, 234.22396850585938, -58.20448684692383, 17.974891662597656, -420.35943603515625]\n",
      "\n",
      "Instance 408 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 3: [0.6526132225990295, -0.4471782445907593, -0.9911380410194397, 1.1414761543273926, -3.1561355590820312]\n",
      "Grand sum of 296 tensor sets is: [95.42902374267578, 233.77679443359375, -59.19562530517578, 19.11636734008789, -423.51556396484375]\n",
      "\n",
      "Instance 409 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 20: [0.49671924114227295, 1.5395169258117676, -0.9589378237724304, 0.5220919847488403, 0.6332820653915405]\n",
      "Grand sum of 297 tensor sets is: [95.92574310302734, 235.31631469726562, -60.154563903808594, 19.638460159301758, -422.8822937011719]\n",
      "\n",
      "Instance 410 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 7: [-0.4916101098060608, -0.6664096713066101, -0.12604784965515137, 0.6688665747642517, -2.397365093231201]\n",
      "Grand sum of 298 tensor sets is: [95.43413543701172, 234.64990234375, -60.28061294555664, 20.307327270507812, -425.2796630859375]\n",
      "\n",
      "Instance 411 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 10: [1.1132359504699707, -0.2558574676513672, -1.2353980541229248, -1.2620007991790771, -3.5682826042175293]\n",
      "Grand sum of 299 tensor sets is: [96.54737091064453, 234.39404296875, -61.51601028442383, 19.045326232910156, -428.8479309082031]\n",
      "\n",
      "Instance 412 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 413 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 21: [-0.535773515701294, 1.6395306587219238, -1.3818767070770264, 0.8221904039382935, -4.881744861602783]\n",
      "Grand sum of 300 tensor sets is: [96.0115966796875, 236.0335693359375, -62.89788818359375, 19.867517471313477, -433.72967529296875]\n",
      "\n",
      "Instance 414 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 415 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 416 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 5: [0.9449121356010437, 0.5821481943130493, 0.18746080994606018, 1.0183961391448975, 0.06548896431922913]\n",
      "Grand sum of 301 tensor sets is: [96.95651245117188, 236.61572265625, -62.710426330566406, 20.885913848876953, -433.6641845703125]\n",
      "\n",
      "Instance 417 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 418 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [-1.1698884963989258, 2.265122413635254, -0.9261206388473511, 1.9116612672805786, -1.170229196548462]\n",
      "Grand sum of 302 tensor sets is: [95.78662109375, 238.88084411621094, -63.63654708862305, 22.797574996948242, -434.83441162109375]\n",
      "\n",
      "Instance 419 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "forever at index 30: [0.8707264065742493, 1.8328454494476318, 0.7314738035202026, -1.8197298049926758, 0.25720399618148804]\n",
      "Grand sum of 303 tensor sets is: [96.6573486328125, 240.71368408203125, -62.90507507324219, 20.97784423828125, -434.57720947265625]\n",
      "\n",
      "Instance 420 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 11: [0.8101917505264282, 1.5214287042617798, -0.3531336486339569, 1.297610878944397, -0.2794998288154602]\n",
      "Grand sum of 304 tensor sets is: [97.46753692626953, 242.235107421875, -63.258209228515625, 22.275455474853516, -434.8567199707031]\n",
      "\n",
      "Instance 421 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 422 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 11: [-0.31641367077827454, 0.7202603220939636, 0.591779887676239, -0.8554326295852661, -3.843867778778076]\n",
      "Grand sum of 305 tensor sets is: [97.151123046875, 242.9553680419922, -62.66642761230469, 21.42002296447754, -438.7005920410156]\n",
      "\n",
      "Instance 423 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 3: [0.45239970088005066, 1.2160876989364624, 0.4523605704307556, -1.3318394422531128, -1.1186206340789795]\n",
      "Grand sum of 306 tensor sets is: [97.60352325439453, 244.1714630126953, -62.21406555175781, 20.088184356689453, -439.8192138671875]\n",
      "\n",
      "Instance 424 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 6: [1.645500898361206, 1.0330228805541992, -0.27170538902282715, -0.7236489057540894, -2.187361240386963]\n",
      "Grand sum of 307 tensor sets is: [99.2490234375, 245.20448303222656, -62.48577117919922, 19.36453628540039, -442.0065612792969]\n",
      "\n",
      "Instance 425 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 426 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 427 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 8: [-0.290149062871933, -0.02817675471305847, -0.15473893284797668, 0.9954612255096436, -2.76697039604187]\n",
      "Grand sum of 308 tensor sets is: [98.95887756347656, 245.17630004882812, -62.64051055908203, 20.359996795654297, -444.7735290527344]\n",
      "\n",
      "Instance 428 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 6: [-0.5442730188369751, 1.1229743957519531, -0.8436547517776489, -0.9251019954681396, -2.7754995822906494]\n",
      "Grand sum of 309 tensor sets is: [98.41460418701172, 246.2992706298828, -63.48416519165039, 19.434894561767578, -447.5490417480469]\n",
      "\n",
      "Instance 429 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [337]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "forever at index 337: [0.6112003326416016, -1.23207426071167, 2.7271790504455566, -1.054767370223999, 2.5805835723876953]\n",
      "Grand sum of 310 tensor sets is: [99.02580261230469, 245.06719970703125, -60.75698471069336, 18.380126953125, -444.96844482421875]\n",
      "\n",
      "Instance 430 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 4: [0.46934792399406433, 1.9336235523223877, -0.3317047953605652, 1.933487057685852, -6.400432109832764]\n",
      "Grand sum of 311 tensor sets is: [99.49514770507812, 247.00082397460938, -61.088687896728516, 20.313613891601562, -451.3688659667969]\n",
      "\n",
      "Instance 431 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 16: [-0.40905389189720154, 1.7797865867614746, -0.9305259585380554, 0.21225056052207947, -1.5078635215759277]\n",
      "Grand sum of 312 tensor sets is: [99.08609008789062, 248.78060913085938, -62.01921463012695, 20.525863647460938, -452.8767395019531]\n",
      "\n",
      "Instance 432 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [1.6994589567184448, -0.21792569756507874, 0.2326381653547287, -0.911447286605835, -4.0184526443481445]\n",
      "Grand sum of 313 tensor sets is: [100.7855453491211, 248.56268310546875, -61.78657531738281, 19.614416122436523, -456.89520263671875]\n",
      "\n",
      "Instance 433 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 15: [0.0566026046872139, 2.2309651374816895, 0.4022204577922821, -1.5148526430130005, -2.5770621299743652]\n",
      "Grand sum of 314 tensor sets is: [100.84214782714844, 250.7936553955078, -61.38435363769531, 18.099563598632812, -459.4722595214844]\n",
      "\n",
      "Instance 434 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 435 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 5: [0.3561139702796936, 1.5587464570999146, -0.2110653668642044, 0.756371796131134, 1.6190857887268066]\n",
      "Grand sum of 315 tensor sets is: [101.1982650756836, 252.35240173339844, -61.595420837402344, 18.85593605041504, -457.8531799316406]\n",
      "\n",
      "Instance 436 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 11: [0.7073391675949097, 1.5037727355957031, 0.47212398052215576, -0.5865213871002197, 0.24240249395370483]\n",
      "Grand sum of 316 tensor sets is: [101.90560150146484, 253.85617065429688, -61.12329864501953, 18.2694149017334, -457.61077880859375]\n",
      "\n",
      "Instance 437 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 15: [-0.21757999062538147, -0.6876397728919983, 0.05092805624008179, -0.5392184257507324, -1.8225153684616089]\n",
      "Grand sum of 317 tensor sets is: [101.68801879882812, 253.1685333251953, -61.07237243652344, 17.730195999145508, -459.43328857421875]\n",
      "\n",
      "Instance 438 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 439 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 7: [-0.04990451782941818, 2.2297239303588867, 0.5164173245429993, 0.04650464653968811, -3.5357115268707275]\n",
      "Grand sum of 318 tensor sets is: [101.63811492919922, 255.39825439453125, -60.55595397949219, 17.776700973510742, -462.968994140625]\n",
      "\n",
      "Instance 440 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 319 tensor sets is: [102.0338134765625, 256.40692138671875, -60.625526428222656, 17.0931396484375, -461.6726989746094]\n",
      "\n",
      "Instance 441 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 7: [0.03397621959447861, 1.4683958292007446, -1.0015861988067627, -0.4376225769519806, -3.384150505065918]\n",
      "Grand sum of 320 tensor sets is: [102.06778717041016, 257.87530517578125, -61.627113342285156, 16.655517578125, -465.0568542480469]\n",
      "\n",
      "Instance 442 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [156]\n",
      "Size of token embeddings is torch.Size([173, 13, 768])\n",
      "Shape of summed layers is: 173 x 768\n",
      "forever at index 156: [1.1014792919158936, 1.3268744945526123, -0.6160379648208618, -0.7639551162719727, -1.046416997909546]\n",
      "Grand sum of 321 tensor sets is: [103.16926574707031, 259.2021789550781, -62.2431526184082, 15.891562461853027, -466.103271484375]\n",
      "\n",
      "Instance 443 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 322 tensor sets is: [103.86405181884766, 260.3354797363281, -61.040008544921875, 14.622200012207031, -466.9934997558594]\n",
      "\n",
      "Instance 444 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 445 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 323 tensor sets is: [104.558837890625, 261.4687805175781, -59.83686447143555, 13.352837562561035, -467.88372802734375]\n",
      "\n",
      "Instance 446 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [124]\n",
      "Size of token embeddings is torch.Size([196, 13, 768])\n",
      "Shape of summed layers is: 196 x 768\n",
      "forever at index 124: [0.7925847768783569, 0.1330840140581131, 0.026155460625886917, 0.43898651003837585, -0.5485879182815552]\n",
      "Grand sum of 324 tensor sets is: [105.35142517089844, 261.60186767578125, -59.81071090698242, 13.791824340820312, -468.43231201171875]\n",
      "\n",
      "Instance 447 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 13: [1.0306488275527954, -0.006747938692569733, 0.1850205510854721, 0.4763759672641754, -1.3853257894515991]\n",
      "Grand sum of 325 tensor sets is: [106.38207244873047, 261.5951232910156, -59.62569046020508, 14.268199920654297, -469.817626953125]\n",
      "\n",
      "Instance 448 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 17: [0.7275167107582092, 0.7272467613220215, -0.8498868942260742, -0.18625588715076447, -4.630862712860107]\n",
      "Grand sum of 326 tensor sets is: [107.10958862304688, 262.3223571777344, -60.47557830810547, 14.081944465637207, -474.448486328125]\n",
      "\n",
      "Instance 449 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 450 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 451 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 452 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 26: [1.0388456583023071, 0.3898816704750061, 0.8309566974639893, -2.0560872554779053, 1.4297800064086914]\n",
      "Grand sum of 327 tensor sets is: [108.1484375, 262.7122497558594, -59.644622802734375, 12.025856971740723, -473.0187072753906]\n",
      "\n",
      "Instance 453 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [73, 96]\n",
      "Size of token embeddings is torch.Size([151, 13, 768])\n",
      "Shape of summed layers is: 151 x 768\n",
      "forever at index 73: [0.4314891993999481, 1.013456106185913, 0.17208433151245117, 1.7308357954025269, -2.7192368507385254]\n",
      "forever at index 96: [0.41487836837768555, 1.3198745250701904, 0.37373441457748413, 1.0696985721588135, -4.241367816925049]\n",
      "Grand sum of 328 tensor sets is: [108.57162475585938, 263.87890625, -59.37171173095703, 13.426124572753906, -476.4990234375]\n",
      "\n",
      "Instance 454 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 455 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([121, 13, 768])\n",
      "Shape of summed layers is: 121 x 768\n",
      "forever at index 46: [0.41324031352996826, 1.1886684894561768, -0.8633221387863159, -0.13346849381923676, -1.5512607097625732]\n",
      "Grand sum of 329 tensor sets is: [108.98486328125, 265.06756591796875, -60.23503494262695, 13.292655944824219, -478.05029296875]\n",
      "\n",
      "Instance 456 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 457 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 458 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 459 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([134, 13, 768])\n",
      "Shape of summed layers is: 134 x 768\n",
      "forever at index 3: [1.0759409666061401, -0.5734074115753174, -0.3321888744831085, -0.44273561239242554, 0.9448695182800293]\n",
      "Grand sum of 330 tensor sets is: [110.06080627441406, 264.4941711425781, -60.567222595214844, 12.849920272827148, -477.1054382324219]\n",
      "\n",
      "Instance 460 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 10: [0.6000168323516846, 0.6712994575500488, -1.0956941843032837, -0.9217553734779358, -2.0032646656036377]\n",
      "Grand sum of 331 tensor sets is: [110.66082000732422, 265.16546630859375, -61.66291809082031, 11.9281644821167, -479.10870361328125]\n",
      "\n",
      "Instance 461 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 462 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 463 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 13: [-0.053628891706466675, 0.571681022644043, -0.6198208928108215, 2.311246633529663, -0.8500427603721619]\n",
      "Grand sum of 332 tensor sets is: [110.60719299316406, 265.7371520996094, -62.282737731933594, 14.239411354064941, -479.958740234375]\n",
      "\n",
      "Instance 464 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 18: [1.171404242515564, 0.8619588613510132, -0.5421693325042725, 0.6484472751617432, -6.229658126831055]\n",
      "Grand sum of 333 tensor sets is: [111.77859497070312, 266.59912109375, -62.82490539550781, 14.887858390808105, -486.1883850097656]\n",
      "\n",
      "Instance 465 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 5: [0.645175039768219, 0.25797390937805176, -1.1716532707214355, -1.5706835985183716, -0.12360075116157532]\n",
      "Grand sum of 334 tensor sets is: [112.42376708984375, 266.8570861816406, -63.996559143066406, 13.317174911499023, -486.3119812011719]\n",
      "\n",
      "Instance 466 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 467 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 6: [-0.511867344379425, 0.8925207853317261, -0.3688235878944397, 3.0762693881988525, -1.0904221534729004]\n",
      "Grand sum of 335 tensor sets is: [111.91190338134766, 267.7496032714844, -64.3653793334961, 16.393444061279297, -487.40240478515625]\n",
      "\n",
      "Instance 468 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 469 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 470 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 336 tensor sets is: [112.3026351928711, 268.82342529296875, -64.58901977539062, 13.752494812011719, -482.0296630859375]\n",
      "\n",
      "Instance 471 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 472 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 6: [0.49353116750717163, 0.12777285277843475, 0.3086288571357727, -0.6846472024917603, -2.9783544540405273]\n",
      "Grand sum of 337 tensor sets is: [112.7961654663086, 268.9512023925781, -64.28038787841797, 13.06784725189209, -485.0080261230469]\n",
      "\n",
      "Instance 473 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 10: [0.0834772139787674, 0.7151965498924255, -0.5225424766540527, -0.6321967840194702, -2.6142666339874268]\n",
      "Grand sum of 338 tensor sets is: [112.87964630126953, 269.6664123535156, -64.80293273925781, 12.435650825500488, -487.6222839355469]\n",
      "\n",
      "Instance 474 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 339 tensor sets is: [112.98106384277344, 271.40576171875, -65.44095611572266, 11.320267677307129, -491.9189147949219]\n",
      "\n",
      "Instance 475 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 6: [0.3158329129219055, 1.2737531661987305, -0.2943144738674164, 1.7444801330566406, 0.7471778988838196]\n",
      "Grand sum of 340 tensor sets is: [113.2968978881836, 272.67950439453125, -65.73526763916016, 13.06474781036377, -491.1717224121094]\n",
      "\n",
      "Instance 476 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 11: [-0.31641367077827454, 0.7202603220939636, 0.591779887676239, -0.8554326295852661, -3.843867778778076]\n",
      "Grand sum of 341 tensor sets is: [112.98048400878906, 273.3997497558594, -65.14348602294922, 12.209315299987793, -495.0155944824219]\n",
      "\n",
      "Instance 477 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 478 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 10: [-0.12935632467269897, 0.8726223707199097, -0.9647822976112366, 0.7543474435806274, -1.246204137802124]\n",
      "Grand sum of 342 tensor sets is: [112.85112762451172, 274.2723693847656, -66.10826873779297, 12.963663101196289, -496.2618103027344]\n",
      "\n",
      "Instance 479 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 8: [0.5321953892707825, 0.43688249588012695, 0.27559006214141846, 0.5427164435386658, -2.0752856731414795]\n",
      "Grand sum of 343 tensor sets is: [113.3833236694336, 274.7092590332031, -65.83267974853516, 13.506379127502441, -498.33709716796875]\n",
      "\n",
      "Instance 480 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 4: [0.18669968843460083, 0.1423853635787964, 0.5554900765419006, 0.634471595287323, -1.5151008367538452]\n",
      "Grand sum of 344 tensor sets is: [113.57002258300781, 274.8516540527344, -65.27719116210938, 14.140851020812988, -499.8522033691406]\n",
      "\n",
      "Instance 481 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 9: [0.5218549370765686, 0.9484233856201172, 0.1863338053226471, 0.2602981925010681, -3.169445037841797]\n",
      "Grand sum of 345 tensor sets is: [114.09188079833984, 275.8000793457031, -65.09085845947266, 14.401148796081543, -503.0216369628906]\n",
      "\n",
      "Instance 482 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 11: [0.7115077972412109, 1.304237961769104, -0.18021690845489502, -0.7568598389625549, 0.20930039882659912]\n",
      "Grand sum of 346 tensor sets is: [114.80339050292969, 277.10430908203125, -65.27107238769531, 13.644289016723633, -502.8123474121094]\n",
      "\n",
      "Instance 483 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 347 tensor sets is: [114.9048080444336, 278.8436584472656, -65.90909576416016, 12.528905868530273, -507.1089782714844]\n",
      "\n",
      "Instance 484 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 485 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 486 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.18560853600502014, 0.8244665265083313, -0.6213061213493347, 0.04695853590965271, -2.4362571239471436]\n",
      "Grand sum of 348 tensor sets is: [115.09041595458984, 279.6681213378906, -66.53040313720703, 12.575864791870117, -509.54522705078125]\n",
      "\n",
      "Instance 487 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 488 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 489 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 21: [-0.1533670723438263, 2.642219305038452, -0.25237488746643066, 1.4804121255874634, -1.4724417924880981]\n",
      "Grand sum of 349 tensor sets is: [114.93704986572266, 282.3103332519531, -66.78277587890625, 14.05627727508545, -511.0176696777344]\n",
      "\n",
      "Instance 490 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 491 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 350 tensor sets is: [115.6318359375, 283.4436340332031, -65.57962799072266, 12.786914825439453, -511.90789794921875]\n",
      "\n",
      "Instance 492 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 493 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 15: [-0.7074764966964722, 0.8615590333938599, 0.5234519839286804, -2.982962131500244, -4.208767890930176]\n",
      "Grand sum of 351 tensor sets is: [114.92436218261719, 284.3052062988281, -65.0561752319336, 9.803953170776367, -516.1166381835938]\n",
      "\n",
      "Instance 494 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 495 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 496 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 352 tensor sets is: [115.06242370605469, 284.9593811035156, -65.49675750732422, 9.506632804870605, -518.9669189453125]\n",
      "\n",
      "Instance 497 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 17: [-0.37074679136276245, -1.300351619720459, 0.8394469618797302, 0.06875747442245483, -2.0543787479400635]\n",
      "Grand sum of 353 tensor sets is: [114.6916732788086, 283.6590270996094, -64.65731048583984, 9.575389862060547, -521.0213012695312]\n",
      "\n",
      "Instance 498 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 24: [0.0564906969666481, 1.5961358547210693, -0.21980935335159302, 0.4509684443473816, -2.417222023010254]\n",
      "Grand sum of 354 tensor sets is: [114.74816131591797, 285.2551574707031, -64.87712097167969, 10.026358604431152, -523.4385375976562]\n",
      "\n",
      "Instance 499 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 500 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 355 tensor sets is: [115.58678436279297, 285.61114501953125, -65.2787857055664, 10.539983749389648, -520.8994750976562]\n",
      "\n",
      "Instance 501 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 18: [-0.19945091009140015, 1.0008183717727661, -0.46167904138565063, -0.8494142293930054, -4.617574214935303]\n",
      "Grand sum of 356 tensor sets is: [115.38733673095703, 286.6119689941406, -65.74046325683594, 9.690569877624512, -525.5170288085938]\n",
      "\n",
      "Instance 502 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 2: [0.14480718970298767, 1.6527358293533325, 0.05642533302307129, -0.6490291357040405, -0.7518052458763123]\n",
      "Grand sum of 357 tensor sets is: [115.53214263916016, 288.26470947265625, -65.68403625488281, 9.04154109954834, -526.2688598632812]\n",
      "\n",
      "Instance 503 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [-0.22103795409202576, 1.1044056415557861, -0.8202288150787354, -0.8617520332336426, -4.349509239196777]\n",
      "Grand sum of 358 tensor sets is: [115.31110382080078, 289.3691101074219, -66.50426483154297, 8.179788589477539, -530.6183471679688]\n",
      "\n",
      "Instance 504 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 12: [-0.2913428843021393, 0.4884646236896515, 0.3508222699165344, 0.8079254627227783, -3.620659589767456]\n",
      "Grand sum of 359 tensor sets is: [115.01976013183594, 289.8575744628906, -66.1534423828125, 8.987713813781738, -534.239013671875]\n",
      "\n",
      "Instance 505 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 360 tensor sets is: [114.28502655029297, 290.2624206542969, -65.76175689697266, 8.630654335021973, -538.7998657226562]\n",
      "\n",
      "Instance 506 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 507 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 361 tensor sets is: [114.82893371582031, 289.6275939941406, -66.41130828857422, 7.48843240737915, -537.6785278320312]\n",
      "\n",
      "Instance 508 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 10: [0.4279741048812866, 3.218817710876465, -0.19054971635341644, 0.3981199860572815, 1.1175789833068848]\n",
      "Grand sum of 362 tensor sets is: [115.25690460205078, 292.8464050292969, -66.60186004638672, 7.886552333831787, -536.5609741210938]\n",
      "\n",
      "Instance 509 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 6: [0.08490005135536194, 0.9207368493080139, -0.09529376775026321, 0.682913601398468, -0.7201701402664185]\n",
      "Grand sum of 363 tensor sets is: [115.34180450439453, 293.76715087890625, -66.69715118408203, 8.569465637207031, -537.2811279296875]\n",
      "\n",
      "Instance 510 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 511 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 7: [0.18849363923072815, 1.6421709060668945, -0.3667990565299988, 0.5922925472259521, -2.5301356315612793]\n",
      "Grand sum of 364 tensor sets is: [115.5302963256836, 295.4093322753906, -67.06394958496094, 9.161758422851562, -539.811279296875]\n",
      "\n",
      "Instance 512 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 513 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 4: [0.13317221403121948, 1.021802544593811, 0.1451103240251541, -0.5074756741523743, -1.4381725788116455]\n",
      "Grand sum of 365 tensor sets is: [115.66346740722656, 296.4311218261719, -66.91883850097656, 8.654282569885254, -541.2494506835938]\n",
      "\n",
      "Instance 514 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 515 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([65, 13, 768])\n",
      "Shape of summed layers is: 65 x 768\n",
      "forever at index 23: [0.7253994941711426, 1.3926748037338257, -0.6226719617843628, -0.17902952432632446, -3.4238855838775635]\n",
      "Grand sum of 366 tensor sets is: [116.38887023925781, 297.82379150390625, -67.54151153564453, 8.475253105163574, -544.67333984375]\n",
      "\n",
      "Instance 516 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 31: [1.1495728492736816, 0.5197712182998657, -1.0889344215393066, -0.20763200521469116, -4.2072882652282715]\n",
      "Grand sum of 367 tensor sets is: [117.53844451904297, 298.34356689453125, -68.63044738769531, 8.267621040344238, -548.880615234375]\n",
      "\n",
      "Instance 517 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 518 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "forever at index 33: [0.8253801465034485, 1.2741347551345825, -1.0795395374298096, 0.11612096428871155, 0.5780421495437622]\n",
      "Grand sum of 368 tensor sets is: [118.36382293701172, 299.6177062988281, -69.7099838256836, 8.383742332458496, -548.3025512695312]\n",
      "\n",
      "Instance 519 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 520 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 521 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 522 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 523 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 7: [0.1761549413204193, 2.133403778076172, -1.7946076393127441, -0.7151784300804138, -3.4160776138305664]\n",
      "Grand sum of 369 tensor sets is: [118.53997802734375, 301.7510986328125, -71.50459289550781, 7.6685638427734375, -551.7186279296875]\n",
      "\n",
      "Instance 524 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 525 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 19: [0.6877472400665283, 1.7774991989135742, 0.17305833101272583, -0.3987823724746704, 1.013139247894287]\n",
      "Grand sum of 370 tensor sets is: [119.22772216796875, 303.5285949707031, -71.33153533935547, 7.269781589508057, -550.7055053710938]\n",
      "\n",
      "Instance 526 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 14: [-0.3518234193325043, 1.799688696861267, 0.23400358855724335, -0.34661412239074707, -2.302055597305298]\n",
      "Grand sum of 371 tensor sets is: [118.87590026855469, 305.3282775878906, -71.0975341796875, 6.9231672286987305, -553.007568359375]\n",
      "\n",
      "Instance 527 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 528 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 529 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 4: [0.26632004976272583, 0.20438414812088013, -0.1490306258201599, 0.2400285005569458, -0.0943983793258667]\n",
      "Grand sum of 372 tensor sets is: [119.14221954345703, 305.53265380859375, -71.24656677246094, 7.163195610046387, -553.1019897460938]\n",
      "\n",
      "Instance 530 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 21: [0.3565308749675751, 0.13220976293087006, -1.0269287824630737, 2.582505464553833, -5.253737449645996]\n",
      "Grand sum of 373 tensor sets is: [119.49874877929688, 305.66485595703125, -72.27349853515625, 9.74570083618164, -558.355712890625]\n",
      "\n",
      "Instance 531 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 9: [0.059196531772613525, 0.950590968132019, -1.2724194526672363, 0.4952840209007263, -0.23718753457069397]\n",
      "Grand sum of 374 tensor sets is: [119.55794525146484, 306.6154479980469, -73.5459213256836, 10.240984916687012, -558.5928955078125]\n",
      "\n",
      "Instance 532 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 533 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 12: [0.5227048397064209, -0.30003082752227783, 0.2295801341533661, -1.0547748804092407, -1.7778465747833252]\n",
      "Grand sum of 375 tensor sets is: [120.08065032958984, 306.3154296875, -73.31633758544922, 9.186209678649902, -560.3707275390625]\n",
      "\n",
      "Instance 534 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 535 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 13: [0.2796343266963959, 1.152061939239502, -1.3426101207733154, 1.7266714572906494, -5.739847183227539]\n",
      "Grand sum of 376 tensor sets is: [120.36028289794922, 307.4674987792969, -74.65895080566406, 10.912880897521973, -566.110595703125]\n",
      "\n",
      "Instance 536 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [2.0521414279937744, 0.9227868318557739, -0.573654294013977, 0.21907693147659302, 2.9479923248291016]\n",
      "Grand sum of 377 tensor sets is: [122.41242218017578, 308.3902893066406, -75.23260498046875, 11.1319580078125, -563.16259765625]\n",
      "\n",
      "Instance 537 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 538 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 539 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 28: [-0.40216609835624695, -1.1905864477157593, -1.2507615089416504, -1.4978840351104736, -3.3217270374298096]\n",
      "Grand sum of 378 tensor sets is: [122.01025390625, 307.19970703125, -76.48336791992188, 9.634074211120605, -566.4843139648438]\n",
      "\n",
      "Instance 540 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 541 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 6: [-0.0659170001745224, 0.18661877512931824, -0.8259607553482056, 2.5733258724212646, -2.0173346996307373]\n",
      "Grand sum of 379 tensor sets is: [121.9443359375, 307.3863220214844, -77.309326171875, 12.20740032196045, -568.5016479492188]\n",
      "\n",
      "Instance 542 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 3: [0.5682512521743774, -0.6835931539535522, -0.13367708027362823, 0.2041182518005371, -1.0552748441696167]\n",
      "Grand sum of 380 tensor sets is: [122.51258850097656, 306.7027282714844, -77.44300079345703, 12.411518096923828, -569.5569458007812]\n",
      "\n",
      "Instance 543 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 544 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 23: [0.6096040606498718, -1.2785965204238892, -0.8943328857421875, 0.38729673624038696, 3.0688958168029785]\n",
      "Grand sum of 381 tensor sets is: [123.1221923828125, 305.42413330078125, -78.33733367919922, 12.79881477355957, -566.488037109375]\n",
      "\n",
      "Instance 545 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 6: [0.7688043713569641, -0.2114047110080719, 0.09582681953907013, -0.5051606893539429, -5.327108383178711]\n",
      "Grand sum of 382 tensor sets is: [123.89099884033203, 305.2127380371094, -78.24150848388672, 12.293654441833496, -571.8151245117188]\n",
      "\n",
      "Instance 546 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 547 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "forever at index 10: [0.9110139608383179, 1.882311224937439, 0.14112526178359985, 0.7173891067504883, -5.373398780822754]\n",
      "Grand sum of 383 tensor sets is: [124.80200958251953, 307.0950622558594, -78.10037994384766, 13.011043548583984, -577.1885375976562]\n",
      "\n",
      "Instance 548 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([90, 13, 768])\n",
      "Shape of summed layers is: 90 x 768\n",
      "forever at index 28: [0.545238196849823, 1.6048502922058105, -0.1607973426580429, 0.9840340614318848, -5.0614824295043945]\n",
      "Grand sum of 384 tensor sets is: [125.34724426269531, 308.6999206542969, -78.26117706298828, 13.995077133178711, -582.25]\n",
      "\n",
      "Instance 549 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 10: [0.25259077548980713, 1.006052017211914, -1.5156728029251099, -0.2061108648777008, -4.138446807861328]\n",
      "Grand sum of 385 tensor sets is: [125.59983825683594, 309.7059631347656, -79.77684783935547, 13.788966178894043, -586.388427734375]\n",
      "\n",
      "Instance 550 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 14: [0.5614771842956543, -0.230618417263031, -1.2935359477996826, 1.7855550050735474, -4.419286251068115]\n",
      "Grand sum of 386 tensor sets is: [126.16131591796875, 309.475341796875, -81.07038116455078, 15.5745210647583, -590.8077392578125]\n",
      "\n",
      "Instance 551 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 3: [0.2887707054615021, -0.030288711190223694, -0.8970314264297485, 0.1832599937915802, -1.4650238752365112]\n",
      "Grand sum of 387 tensor sets is: [126.45008850097656, 309.4450378417969, -81.96741485595703, 15.757781028747559, -592.2727661132812]\n",
      "\n",
      "Instance 552 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 553 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 4: [0.21138480305671692, -0.42136165499687195, -0.7626535892486572, 0.5962284803390503, -4.957610130310059]\n",
      "Grand sum of 388 tensor sets is: [126.6614761352539, 309.023681640625, -82.73007202148438, 16.3540096282959, -597.2303466796875]\n",
      "\n",
      "Instance 554 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 53: [0.666554868221283, -0.7873085737228394, -1.431145191192627, 1.6047497987747192, -2.4387478828430176]\n",
      "Grand sum of 389 tensor sets is: [127.32803344726562, 308.2363586425781, -84.16121673583984, 17.958759307861328, -599.6690673828125]\n",
      "\n",
      "Instance 555 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 556 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 26: [1.0388456583023071, 0.3898816704750061, 0.8309566974639893, -2.0560872554779053, 1.4297800064086914]\n",
      "Grand sum of 390 tensor sets is: [128.36688232421875, 308.6262512207031, -83.33026123046875, 15.902671813964844, -598.2392578125]\n",
      "\n",
      "Instance 557 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 5: [0.36773934960365295, 0.19787469506263733, -1.5957812070846558, 0.1278052181005478, -4.654290199279785]\n",
      "Grand sum of 391 tensor sets is: [128.734619140625, 308.8241271972656, -84.92604064941406, 16.03047752380371, -602.8935546875]\n",
      "\n",
      "Instance 558 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 559 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 560 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 10: [0.3929899334907532, 1.4893229007720947, -0.670498251914978, -1.6504474878311157, -0.6013908982276917]\n",
      "Grand sum of 392 tensor sets is: [129.1276092529297, 310.3134460449219, -85.59654235839844, 14.380029678344727, -603.4949340820312]\n",
      "\n",
      "Instance 561 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 393 tensor sets is: [128.3928680419922, 310.7182922363281, -85.2048568725586, 14.022970199584961, -608.0557861328125]\n",
      "\n",
      "Instance 562 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 563 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 5: [0.32324928045272827, -1.3164985179901123, -1.4354426860809326, 0.22405016422271729, -2.51029634475708]\n",
      "Grand sum of 394 tensor sets is: [128.7161102294922, 309.40179443359375, -86.64029693603516, 14.247020721435547, -610.5661010742188]\n",
      "\n",
      "Instance 564 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 565 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 7: [-0.31519579887390137, 0.7701456546783447, 1.0046956539154053, -0.1287778615951538, -4.125186443328857]\n",
      "Grand sum of 395 tensor sets is: [128.40090942382812, 310.17193603515625, -85.63560485839844, 14.118243217468262, -614.6912841796875]\n",
      "\n",
      "Instance 566 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 5: [-0.18694037199020386, 2.042536497116089, -0.40986740589141846, 0.24722908437252045, -1.599402666091919]\n",
      "Grand sum of 396 tensor sets is: [128.21397399902344, 312.2144775390625, -86.04547119140625, 14.365471839904785, -616.2907104492188]\n",
      "\n",
      "Instance 567 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 568 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 569 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [-1.0725300312042236, -0.14752542972564697, 0.1699531376361847, 0.4078018069267273, -0.3286052942276001]\n",
      "Grand sum of 397 tensor sets is: [127.14144134521484, 312.06695556640625, -85.87551879882812, 14.773273468017578, -616.6193237304688]\n",
      "\n",
      "Instance 570 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 53: [0.666554868221283, -0.7873085737228394, -1.431145191192627, 1.6047497987747192, -2.4387478828430176]\n",
      "Grand sum of 398 tensor sets is: [127.80799865722656, 311.2796325683594, -87.3066635131836, 16.378023147583008, -619.0580444335938]\n",
      "\n",
      "Instance 571 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 15: [0.44318655133247375, 1.0076255798339844, 0.4191220700740814, 0.1981743574142456, -3.5277934074401855]\n",
      "Grand sum of 399 tensor sets is: [128.25119018554688, 312.2872619628906, -86.88754272460938, 16.576196670532227, -622.5858154296875]\n",
      "\n",
      "Instance 572 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "forever at index 7: [0.4068053364753723, 1.9675004482269287, 0.09159311652183533, -0.7349298596382141, -5.582054138183594]\n",
      "Grand sum of 400 tensor sets is: [128.65798950195312, 314.2547607421875, -86.79595184326172, 15.841266632080078, -628.1678466796875]\n",
      "\n",
      "Instance 573 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 13: [-0.8286336064338684, 0.703874409198761, -0.22103522717952728, 1.5039246082305908, 2.983517646789551]\n",
      "Grand sum of 401 tensor sets is: [127.82935333251953, 314.9586486816406, -87.0169906616211, 17.345191955566406, -625.184326171875]\n",
      "\n",
      "Instance 574 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 402 tensor sets is: [127.96741485595703, 315.6128234863281, -87.45757293701172, 17.04787254333496, -628.0346069335938]\n",
      "\n",
      "Instance 575 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 7: [0.2910078465938568, 2.6688804626464844, 0.1685662865638733, 1.0849796533584595, -3.9479236602783203]\n",
      "Grand sum of 403 tensor sets is: [128.2584228515625, 318.2817077636719, -87.28900909423828, 18.13285255432129, -631.9825439453125]\n",
      "\n",
      "Instance 576 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 15: [-0.5016686916351318, -3.993448495864868, 0.2891518771648407, -0.7801859974861145, -1.3372100591659546]\n",
      "Grand sum of 404 tensor sets is: [127.75675201416016, 314.28826904296875, -86.9998550415039, 17.3526668548584, -633.3197631835938]\n",
      "\n",
      "Instance 577 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4, 12]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "forever at index 4: [0.19769474864006042, 0.9239522218704224, 0.0995880514383316, -0.08179770410060883, 0.22700658440589905]\n",
      "forever at index 12: [0.6060097217559814, 0.5739008784294128, 0.5269936323165894, -1.1561846733093262, -0.11992436647415161]\n",
      "Grand sum of 405 tensor sets is: [128.15859985351562, 315.0372009277344, -86.68656158447266, 16.733675003051758, -633.2662353515625]\n",
      "\n",
      "Instance 578 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 27: [0.8007772564888, 0.5045040845870972, -1.525667667388916, 0.09943705797195435, 0.7031981945037842]\n",
      "Grand sum of 406 tensor sets is: [128.95938110351562, 315.5417175292969, -88.21222686767578, 16.833112716674805, -632.5630493164062]\n",
      "\n",
      "Instance 579 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([86, 13, 768])\n",
      "Shape of summed layers is: 86 x 768\n",
      "forever at index 12: [0.7082570195198059, -0.44460543990135193, -0.6492201089859009, 1.9245091676712036, -1.3332226276397705]\n",
      "Grand sum of 407 tensor sets is: [129.66763305664062, 315.09710693359375, -88.8614501953125, 18.75762176513672, -633.8963012695312]\n",
      "\n",
      "Instance 580 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 581 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "forever at index 19: [1.166549563407898, 1.6819132566452026, 1.1824480295181274, 0.4018321931362152, -7.0594258308410645]\n",
      "Grand sum of 408 tensor sets is: [130.8341827392578, 316.7790222167969, -87.67900085449219, 19.159454345703125, -640.9557495117188]\n",
      "\n",
      "Instance 582 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [68]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "forever at index 68: [0.18286184966564178, 1.0914742946624756, 0.21739304065704346, -1.1669584512710571, 0.19820745289325714]\n",
      "Grand sum of 409 tensor sets is: [131.0170440673828, 317.8704833984375, -87.46160888671875, 17.992496490478516, -640.757568359375]\n",
      "\n",
      "Instance 583 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 10: [-0.5473657846450806, 1.272416591644287, 0.14618876576423645, 1.2308789491653442, -3.115260601043701]\n",
      "Grand sum of 410 tensor sets is: [130.4696807861328, 319.1429138183594, -87.31542205810547, 19.22337532043457, -643.872802734375]\n",
      "\n",
      "Instance 584 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 585 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 586 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 4: [0.6019265055656433, 1.2453086376190186, 0.43631142377853394, -0.4933153986930847, -1.4065874814987183]\n",
      "Grand sum of 411 tensor sets is: [131.0716094970703, 320.3882141113281, -86.87911224365234, 18.730060577392578, -645.2794189453125]\n",
      "\n",
      "Instance 587 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [372]\n",
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "forever at index 372: [0.04517780616879463, -0.17573755979537964, -1.0147148370742798, 0.24183887243270874, 0.13740798830986023]\n",
      "Grand sum of 412 tensor sets is: [131.11679077148438, 320.21246337890625, -87.89382934570312, 18.971899032592773, -645.1420288085938]\n",
      "\n",
      "Instance 588 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 589 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 590 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [0.6247493624687195, -0.4171220660209656, -0.551531195640564, -0.8069962859153748, -2.7009406089782715]\n",
      "Grand sum of 413 tensor sets is: [131.74154663085938, 319.79534912109375, -88.44535827636719, 18.16490364074707, -647.8429565429688]\n",
      "\n",
      "Instance 591 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 29: [0.9889218211174011, 1.8801085948944092, 0.7867690920829773, 1.597792148590088, -0.12941482663154602]\n",
      "Grand sum of 414 tensor sets is: [132.73046875, 321.6754455566406, -87.6585922241211, 19.7626953125, -647.9723510742188]\n",
      "\n",
      "Instance 592 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 593 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 19: [0.8453393578529358, 0.5756961703300476, 0.3677171468734741, -0.46797752380371094, -2.994748592376709]\n",
      "Grand sum of 415 tensor sets is: [133.5758056640625, 322.2511291503906, -87.29087829589844, 19.29471778869629, -650.9671020507812]\n",
      "\n",
      "Instance 594 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 19: [0.48172301054000854, 2.2028567790985107, 0.2751997113227844, -0.45182687044143677, -0.9253084063529968]\n",
      "Grand sum of 416 tensor sets is: [134.05752563476562, 324.4539794921875, -87.01567840576172, 18.842891693115234, -651.8923950195312]\n",
      "\n",
      "Instance 595 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 20: [1.6395035982131958, -0.7050268650054932, 1.2002038955688477, 0.6726716756820679, -6.13603401184082]\n",
      "Grand sum of 417 tensor sets is: [135.69703674316406, 323.74896240234375, -85.81547546386719, 19.51556396484375, -658.0284423828125]\n",
      "\n",
      "Instance 596 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 12: [-0.09072460234165192, 0.49171802401542664, 0.908133327960968, 0.2426213175058365, 0.6215524673461914]\n",
      "Grand sum of 418 tensor sets is: [135.60630798339844, 324.2406921386719, -84.90734100341797, 19.7581844329834, -657.4068603515625]\n",
      "\n",
      "Instance 597 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 10: [0.7191253304481506, 0.9061852693557739, -0.12055879831314087, 0.22425054013729095, 2.0256736278533936]\n",
      "Grand sum of 419 tensor sets is: [136.325439453125, 325.1468811035156, -85.02790069580078, 19.98243522644043, -655.3811645507812]\n",
      "\n",
      "Instance 598 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 599 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 53: [-0.5120791792869568, 0.556977391242981, -0.015256568789482117, -0.4715348184108734, -1.0344117879867554]\n",
      "Grand sum of 420 tensor sets is: [135.8133544921875, 325.703857421875, -85.04315948486328, 19.510900497436523, -656.4155883789062]\n",
      "\n",
      "Instance 600 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 601 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 3: [0.5272927284240723, 2.6107068061828613, -0.8389358520507812, 1.0574493408203125, 1.1373155117034912]\n",
      "Grand sum of 421 tensor sets is: [136.3406524658203, 328.3145751953125, -85.88209533691406, 20.568349838256836, -655.2782592773438]\n",
      "\n",
      "Instance 602 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 603 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [70]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "forever at index 70: [0.12317660450935364, 1.9651168584823608, 0.741847574710846, 2.8082058429718018, 0.027752645313739777]\n",
      "Grand sum of 422 tensor sets is: [136.46383666992188, 330.2796936035156, -85.14025115966797, 23.376556396484375, -655.25048828125]\n",
      "\n",
      "Instance 604 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 14: [0.335677832365036, 0.0539957731962204, 0.03554096817970276, -0.5628941059112549, -1.4555543661117554]\n",
      "Grand sum of 423 tensor sets is: [136.7995147705078, 330.33367919921875, -85.1047134399414, 22.813661575317383, -656.7060546875]\n",
      "\n",
      "Instance 605 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 9: [1.098562479019165, -0.07149630784988403, -0.65312260389328, -1.7348510026931763, -1.0558137893676758]\n",
      "Grand sum of 424 tensor sets is: [137.8980712890625, 330.2621765136719, -85.7578353881836, 21.07880973815918, -657.7618408203125]\n",
      "\n",
      "Instance 606 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 607 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 608 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 20: [0.6300421953201294, 1.8560326099395752, 0.22185774147510529, 0.3324981927871704, -0.6448638439178467]\n",
      "Grand sum of 425 tensor sets is: [138.52810668945312, 332.1181945800781, -85.53598022460938, 21.41130828857422, -658.4066772460938]\n",
      "\n",
      "Instance 609 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 610 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 7: [-0.1684616357088089, 0.4859640598297119, 1.1414120197296143, -1.4853575229644775, -1.8900843858718872]\n",
      "Grand sum of 426 tensor sets is: [138.35964965820312, 332.6041564941406, -84.39456939697266, 19.92595100402832, -660.2967529296875]\n",
      "\n",
      "Instance 611 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 427 tensor sets is: [138.46107482910156, 334.343505859375, -85.0325927734375, 18.81056785583496, -664.5933837890625]\n",
      "\n",
      "Instance 612 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([108, 13, 768])\n",
      "Shape of summed layers is: 108 x 768\n",
      "forever at index 50: [1.2580262422561646, -1.6834547519683838, -1.1553174257278442, -0.5332778692245483, -1.1019668579101562]\n",
      "Grand sum of 428 tensor sets is: [139.71910095214844, 332.6600646972656, -86.18791198730469, 18.27729034423828, -665.6953735351562]\n",
      "\n",
      "Instance 613 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 39: [0.5514844655990601, 0.7930175065994263, -0.6861960291862488, -1.0249943733215332, -3.571471691131592]\n",
      "Grand sum of 429 tensor sets is: [140.2705841064453, 333.4530944824219, -86.87410736083984, 17.252296447753906, -669.266845703125]\n",
      "\n",
      "Instance 614 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 615 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 616 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 617 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 53: [-0.03904017060995102, 1.962186336517334, 0.3727072477340698, -0.09199962019920349, -0.5388772487640381]\n",
      "Grand sum of 430 tensor sets is: [140.23153686523438, 335.415283203125, -86.50140380859375, 17.160297393798828, -669.8057250976562]\n",
      "\n",
      "Instance 618 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 9: [-0.6275916695594788, 0.704816460609436, -0.0405668169260025, -0.25239288806915283, -2.6285011768341064]\n",
      "Grand sum of 431 tensor sets is: [139.60394287109375, 336.1200866699219, -86.5419692993164, 16.90790367126465, -672.4342041015625]\n",
      "\n",
      "Instance 619 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 620 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 621 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 11: [0.6968948245048523, -0.21681609749794006, -0.8161472082138062, 0.35689598321914673, 2.7541592121124268]\n",
      "Grand sum of 432 tensor sets is: [140.30084228515625, 335.90325927734375, -87.35811614990234, 17.264799118041992, -669.6800537109375]\n",
      "\n",
      "Instance 622 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 9: [0.6202882528305054, 2.399752140045166, -0.4000081717967987, 1.6026326417922974, -1.3666386604309082]\n",
      "Grand sum of 433 tensor sets is: [140.92112731933594, 338.3030090332031, -87.75812530517578, 18.867431640625, -671.0466918945312]\n",
      "\n",
      "Instance 623 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([147, 13, 768])\n",
      "Shape of summed layers is: 147 x 768\n",
      "forever at index 4: [0.6714192032814026, -0.18098288774490356, 1.1543644666671753, -1.246457815170288, 0.2689766585826874]\n",
      "Grand sum of 434 tensor sets is: [141.59254455566406, 338.1220397949219, -86.603759765625, 17.620973587036133, -670.7777099609375]\n",
      "\n",
      "Instance 624 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 435 tensor sets is: [140.85780334472656, 338.5268859863281, -86.21207427978516, 17.263914108276367, -675.3385620117188]\n",
      "\n",
      "Instance 625 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 6: [-0.25888749957084656, 0.9949325323104858, 0.34648558497428894, 1.0551494359970093, 0.20401115715503693]\n",
      "Grand sum of 436 tensor sets is: [140.5989227294922, 339.5218200683594, -85.86558532714844, 18.319063186645508, -675.134521484375]\n",
      "\n",
      "Instance 626 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 627 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 5: [-0.1732800006866455, -0.09653332829475403, -1.28530752658844, 0.26850152015686035, -2.2695202827453613]\n",
      "Grand sum of 437 tensor sets is: [140.42564392089844, 339.42529296875, -87.15089416503906, 18.58756446838379, -677.404052734375]\n",
      "\n",
      "Instance 628 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 16: [-0.267183393239975, 0.6888870596885681, -0.5782589912414551, 0.3956179618835449, -6.064557075500488]\n",
      "Grand sum of 438 tensor sets is: [140.15846252441406, 340.1141662597656, -87.72915649414062, 18.983182907104492, -683.4686279296875]\n",
      "\n",
      "Instance 629 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 630 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 3: [-0.44392314553260803, 1.022566556930542, 0.14802449941635132, 0.5844260454177856, 1.4668262004852295]\n",
      "Grand sum of 439 tensor sets is: [139.71453857421875, 341.13671875, -87.58113098144531, 19.567609786987305, -682.0018310546875]\n",
      "\n",
      "Instance 631 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 8: [0.1277688890695572, 0.7562439441680908, -0.6671024560928345, 0.46750175952911377, -5.518497467041016]\n",
      "Grand sum of 440 tensor sets is: [139.84230041503906, 341.8929748535156, -88.24822998046875, 20.035112380981445, -687.5203247070312]\n",
      "\n",
      "Instance 632 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 633 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 4: [1.4042912721633911, 1.1161370277404785, -0.517616868019104, 1.9693149328231812, 0.7144657969474792]\n",
      "Grand sum of 441 tensor sets is: [141.24659729003906, 343.0091247558594, -88.7658462524414, 22.004426956176758, -686.8058471679688]\n",
      "\n",
      "Instance 634 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 53: [0.8870499730110168, 0.9918266534805298, -0.08423125743865967, -0.8764976263046265, 0.020298480987548828]\n",
      "Grand sum of 442 tensor sets is: [142.13365173339844, 344.0009460449219, -88.8500747680664, 21.1279296875, -686.7855224609375]\n",
      "\n",
      "Instance 635 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 636 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 20: [0.4525699019432068, 2.173279047012329, -0.14967072010040283, -0.7128849029541016, -1.8676687479019165]\n",
      "Grand sum of 443 tensor sets is: [142.5862274169922, 346.1742248535156, -88.99974822998047, 20.4150447845459, -688.6531982421875]\n",
      "\n",
      "Instance 637 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 638 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 6: [0.3315807580947876, 1.0195425748825073, -1.2288215160369873, -0.6724015474319458, -5.118860721588135]\n",
      "Grand sum of 444 tensor sets is: [142.9178009033203, 347.1937561035156, -90.22856903076172, 19.742643356323242, -693.7720336914062]\n",
      "\n",
      "Instance 639 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 3: [1.377890706062317, -0.20912587642669678, -1.9493390321731567, -1.0446254014968872, -0.20353758335113525]\n",
      "Grand sum of 445 tensor sets is: [144.29568481445312, 346.984619140625, -92.17790985107422, 18.698017120361328, -693.9755859375]\n",
      "\n",
      "Instance 640 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 5: [0.5628085732460022, -1.3375930786132812, -0.07100807875394821, -0.00019157305359840393, -1.747585415840149]\n",
      "Grand sum of 446 tensor sets is: [144.85848999023438, 345.64703369140625, -92.24891662597656, 18.697826385498047, -695.72314453125]\n",
      "\n",
      "Instance 641 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 642 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 16: [-0.267183393239975, 0.6888870596885681, -0.5782589912414551, 0.3956179618835449, -6.064557075500488]\n",
      "Grand sum of 447 tensor sets is: [144.59130859375, 346.3359069824219, -92.82717895507812, 19.09344482421875, -701.7877197265625]\n",
      "\n",
      "Instance 643 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 644 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 20: [0.3660978078842163, 0.7815632820129395, -1.7551839351654053, 2.035003900527954, -4.049980163574219]\n",
      "Grand sum of 448 tensor sets is: [144.95741271972656, 347.1174621582031, -94.58235931396484, 21.128448486328125, -705.8377075195312]\n",
      "\n",
      "Instance 645 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 10: [0.715054452419281, 1.377943515777588, -1.8506040573120117, -0.09136570990085602, 0.8692955374717712]\n",
      "Grand sum of 449 tensor sets is: [145.67247009277344, 348.4953918457031, -96.4329605102539, 21.03708267211914, -704.9683837890625]\n",
      "\n",
      "Instance 646 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 14: [-0.25291216373443604, 0.5679522752761841, -1.2608360052108765, 0.08006757497787476, -5.625535488128662]\n",
      "Grand sum of 450 tensor sets is: [145.4195556640625, 349.0633544921875, -97.69379425048828, 21.117149353027344, -710.5939331054688]\n",
      "\n",
      "Instance 647 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 25: [0.5084376335144043, 0.8439568281173706, -0.3973279595375061, -0.6807446479797363, -3.7994015216827393]\n",
      "Grand sum of 451 tensor sets is: [145.92799377441406, 349.9073181152344, -98.09112548828125, 20.436405181884766, -714.393310546875]\n",
      "\n",
      "Instance 648 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 23: [0.9342373609542847, 0.4670065641403198, 0.2874516248703003, -0.11764544248580933, 2.3023059368133545]\n",
      "Grand sum of 452 tensor sets is: [146.8622283935547, 350.37432861328125, -97.80367279052734, 20.31875991821289, -712.0910034179688]\n",
      "\n",
      "Instance 649 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 13: [0.015373706817626953, 0.6889562606811523, -1.0286530256271362, 3.6180052757263184, -0.554063618183136]\n",
      "Grand sum of 453 tensor sets is: [146.8776092529297, 351.06329345703125, -98.83232879638672, 23.936765670776367, -712.6450805664062]\n",
      "\n",
      "Instance 650 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 651 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 15: [0.21592628955841064, 1.9212560653686523, 0.012331966310739517, 0.7936554551124573, -0.30705326795578003]\n",
      "Grand sum of 454 tensor sets is: [147.09353637695312, 352.98455810546875, -98.81999969482422, 24.73042106628418, -712.9521484375]\n",
      "\n",
      "Instance 652 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 653 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 654 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "forever at index 3: [-0.11812451481819153, 1.8332041501998901, -0.8912343382835388, 0.6509721279144287, -1.4581923484802246]\n",
      "Grand sum of 455 tensor sets is: [146.9754180908203, 354.8177490234375, -99.71123504638672, 25.381393432617188, -714.4103393554688]\n",
      "\n",
      "Instance 655 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([66, 13, 768])\n",
      "Shape of summed layers is: 66 x 768\n",
      "forever at index 42: [0.19979456067085266, -0.21828821301460266, 0.4088609218597412, -0.6401990652084351, 0.7286964654922485]\n",
      "Grand sum of 456 tensor sets is: [147.1752166748047, 354.5994567871094, -99.30237579345703, 24.741193771362305, -713.681640625]\n",
      "\n",
      "Instance 656 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([65, 13, 768])\n",
      "Shape of summed layers is: 65 x 768\n",
      "forever at index 25: [-0.35535818338394165, 1.188887596130371, 0.14595982432365417, -1.1843032836914062, -0.9608800411224365]\n",
      "Grand sum of 457 tensor sets is: [146.81985473632812, 355.788330078125, -99.15641784667969, 23.5568904876709, -714.6425170898438]\n",
      "\n",
      "Instance 657 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([105, 13, 768])\n",
      "Shape of summed layers is: 105 x 768\n",
      "forever at index 36: [0.902238667011261, 0.2844647765159607, 1.2697429656982422, -1.955522894859314, -0.21582108736038208]\n",
      "Grand sum of 458 tensor sets is: [147.7220916748047, 356.0727844238281, -97.88667297363281, 21.601367950439453, -714.8583374023438]\n",
      "\n",
      "Instance 658 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 659 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 9: [-0.41614043712615967, 0.1468457281589508, -1.668521523475647, -0.5570715665817261, -1.841092824935913]\n",
      "Grand sum of 459 tensor sets is: [147.3059539794922, 356.2196350097656, -99.55519104003906, 21.044296264648438, -716.6994018554688]\n",
      "\n",
      "Instance 660 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 661 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15, 18]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 15: [0.6482625007629395, 2.3948636054992676, -0.08020243048667908, 1.4238835573196411, -0.34534138441085815]\n",
      "forever at index 18: [0.7159790396690369, 1.0385421514511108, 1.2280919551849365, 1.2297247648239136, -0.9954403042793274]\n",
      "Grand sum of 460 tensor sets is: [147.98806762695312, 357.93634033203125, -98.98124694824219, 22.37110137939453, -717.3698120117188]\n",
      "\n",
      "Instance 662 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 16: [0.8588739633560181, -3.1241517066955566, -0.10900598019361496, 0.05868205428123474, -2.3122904300689697]\n",
      "Grand sum of 461 tensor sets is: [148.84693908691406, 354.81219482421875, -99.09025573730469, 22.42978286743164, -719.68212890625]\n",
      "\n",
      "Instance 663 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 664 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 65: [-0.4072004556655884, -1.165315866470337, -1.116049885749817, -1.6974555253982544, -1.7651728391647339]\n",
      "Grand sum of 462 tensor sets is: [148.4397430419922, 353.6468811035156, -100.20630645751953, 20.73232650756836, -721.4473266601562]\n",
      "\n",
      "Instance 665 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 4: [-0.6020364165306091, 1.2419496774673462, -0.10363834351301193, -0.06093408167362213, -1.892760992050171]\n",
      "Grand sum of 463 tensor sets is: [147.83770751953125, 354.8888244628906, -100.30994415283203, 20.6713924407959, -723.340087890625]\n",
      "\n",
      "Instance 666 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 667 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 668 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 15: [1.8035955429077148, 2.9838778972625732, -1.0702712535858154, 2.34074068069458, -1.20751953125]\n",
      "Grand sum of 464 tensor sets is: [149.64129638671875, 357.8727111816406, -101.38021850585938, 23.01213264465332, -724.547607421875]\n",
      "\n",
      "Instance 669 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 21: [-0.6783986687660217, 0.7139027118682861, 1.465477466583252, -0.7937880754470825, -1.3639744520187378]\n",
      "Grand sum of 465 tensor sets is: [148.962890625, 358.58660888671875, -99.91474151611328, 22.21834373474121, -725.9115600585938]\n",
      "\n",
      "Instance 670 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [425]\n",
      "Size of token embeddings is torch.Size([462, 13, 768])\n",
      "Shape of summed layers is: 462 x 768\n",
      "forever at index 425: [-0.2304137796163559, 0.7662403583526611, 0.6429543495178223, 0.2339191734790802, -0.1948109269142151]\n",
      "Grand sum of 466 tensor sets is: [148.73248291015625, 359.35284423828125, -99.27178955078125, 22.45226287841797, -726.1063842773438]\n",
      "\n",
      "Instance 671 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [83]\n",
      "Size of token embeddings is torch.Size([87, 13, 768])\n",
      "Shape of summed layers is: 87 x 768\n",
      "forever at index 83: [0.6862549781799316, 0.5677590370178223, -0.7275421023368835, 0.8790295720100403, -1.76715087890625]\n",
      "Grand sum of 467 tensor sets is: [149.41873168945312, 359.92059326171875, -99.99932861328125, 23.3312931060791, -727.87353515625]\n",
      "\n",
      "Instance 672 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 673 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 4: [0.202688068151474, 1.3180568218231201, 0.14068174362182617, -0.016447290778160095, 0.40156853199005127]\n",
      "Grand sum of 468 tensor sets is: [149.6214141845703, 361.2386474609375, -99.85865020751953, 23.31484603881836, -727.4719848632812]\n",
      "\n",
      "Instance 674 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 7: [0.2379908561706543, 0.4396919310092926, -0.2641163468360901, -0.8718094825744629, -1.147977352142334]\n",
      "Grand sum of 469 tensor sets is: [149.85940551757812, 361.6783447265625, -100.12276458740234, 22.443037033081055, -728.6199340820312]\n",
      "\n",
      "Instance 675 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 15: [0.25548359751701355, -0.34476178884506226, -1.3081345558166504, 1.475437879562378, 0.31191331148147583]\n",
      "Grand sum of 470 tensor sets is: [150.11488342285156, 361.3335876464844, -101.43090057373047, 23.918474197387695, -728.3080444335938]\n",
      "\n",
      "Instance 676 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 7: [0.5394299030303955, 0.6200529336929321, -0.5164741277694702, 1.6348017454147339, -4.022064208984375]\n",
      "Grand sum of 471 tensor sets is: [150.65431213378906, 361.9536437988281, -101.94737243652344, 25.55327606201172, -732.330078125]\n",
      "\n",
      "Instance 677 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [85]\n",
      "Size of token embeddings is torch.Size([98, 13, 768])\n",
      "Shape of summed layers is: 98 x 768\n",
      "forever at index 85: [0.7952976822853088, 1.8565329313278198, 0.6157029867172241, -0.2494874894618988, -1.632564663887024]\n",
      "Grand sum of 472 tensor sets is: [151.44961547851562, 363.8101806640625, -101.33167266845703, 25.303789138793945, -733.962646484375]\n",
      "\n",
      "Instance 678 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 5: [0.6022050380706787, 2.4245588779449463, -0.6253792643547058, 1.0847046375274658, 0.46901166439056396]\n",
      "Grand sum of 473 tensor sets is: [152.05181884765625, 366.2347412109375, -101.9570541381836, 26.38849449157715, -733.49365234375]\n",
      "\n",
      "Instance 679 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 6: [0.5560442805290222, 2.1048972606658936, -0.4208127558231354, 0.6382150650024414, -0.011201798915863037]\n",
      "Grand sum of 474 tensor sets is: [152.6078643798828, 368.3396301269531, -102.37786865234375, 27.026710510253906, -733.5048828125]\n",
      "\n",
      "Instance 680 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 681 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.46203434467315674, 3.6418118476867676, -0.6979748606681824, 2.4230599403381348, -2.220381021499634]\n",
      "Grand sum of 475 tensor sets is: [153.0699005126953, 371.9814453125, -103.07584381103516, 29.449769973754883, -735.7252807617188]\n",
      "\n",
      "Instance 682 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 8: [0.46853458881378174, 1.8515475988388062, -0.7562364339828491, -1.4473137855529785, -1.561449646949768]\n",
      "Grand sum of 476 tensor sets is: [153.53843688964844, 373.8330078125, -103.83207702636719, 28.002456665039062, -737.2867431640625]\n",
      "\n",
      "Instance 683 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [1.0301811695098877, 0.09672722220420837, -1.850982904434204, 0.9610074162483215, -2.2480239868164062]\n",
      "Grand sum of 477 tensor sets is: [154.56861877441406, 373.92974853515625, -105.68305969238281, 28.963464736938477, -739.5347900390625]\n",
      "\n",
      "Instance 684 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 28: [0.03889094293117523, 1.9266648292541504, 1.1458404064178467, 0.6725225448608398, -2.605778217315674]\n",
      "Grand sum of 478 tensor sets is: [154.60751342773438, 375.8564147949219, -104.53721618652344, 29.635986328125, -742.1405639648438]\n",
      "\n",
      "Instance 685 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 4: [0.9956583976745605, -0.2118312269449234, 0.04984015226364136, 1.4031128883361816, 0.7239155769348145]\n",
      "Grand sum of 479 tensor sets is: [155.60316467285156, 375.64459228515625, -104.48737335205078, 31.039098739624023, -741.4166259765625]\n",
      "\n",
      "Instance 686 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 6: [0.1468038409948349, 1.4153075218200684, -1.0000721216201782, -1.3591614961624146, -2.315084934234619]\n",
      "Grand sum of 480 tensor sets is: [155.74996948242188, 377.0599060058594, -105.48744201660156, 29.6799373626709, -743.731689453125]\n",
      "\n",
      "Instance 687 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [0.36479946970939636, 0.24682025611400604, -0.4848631024360657, -0.5248885154724121, -1.1551042795181274]\n",
      "Grand sum of 481 tensor sets is: [156.11476135253906, 377.3067321777344, -105.97230529785156, 29.155048370361328, -744.8867797851562]\n",
      "\n",
      "Instance 688 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 689 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 690 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 691 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 692 of forever.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 693 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 26: [0.35811206698417664, 1.6544544696807861, -0.28324049711227417, -1.093572735786438, -4.59495735168457]\n",
      "Grand sum of 482 tensor sets is: [156.47286987304688, 378.961181640625, -106.25554656982422, 28.06147575378418, -749.4817504882812]\n",
      "\n",
      "Instance 694 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 3: [0.32117903232574463, 0.23560544848442078, -0.000872097909450531, -0.8400305509567261, -2.1225438117980957]\n",
      "Grand sum of 483 tensor sets is: [156.79405212402344, 379.19677734375, -106.25641632080078, 27.221445083618164, -751.6043090820312]\n",
      "\n",
      "Instance 695 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 32: [-0.14751744270324707, 2.2636611461639404, 0.4540150761604309, -1.6123569011688232, -0.6037414073944092]\n",
      "Grand sum of 484 tensor sets is: [156.6465301513672, 381.46044921875, -105.80239868164062, 25.609088897705078, -752.2080688476562]\n",
      "\n",
      "Instance 696 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 12: [0.4206554591655731, 1.103718638420105, 0.4194526970386505, -0.4705325961112976, 1.022660732269287]\n",
      "Grand sum of 485 tensor sets is: [157.0671844482422, 382.5641784667969, -105.38294219970703, 25.1385555267334, -751.1854248046875]\n",
      "\n",
      "Instance 697 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 698 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 486 tensor sets is: [157.9058074951172, 382.920166015625, -105.78460693359375, 25.652179718017578, -748.6463623046875]\n",
      "\n",
      "Instance 699 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 3: [0.9916622638702393, -0.11634179949760437, -0.5053298473358154, -0.6702648401260376, 1.3807599544525146]\n",
      "Grand sum of 487 tensor sets is: [158.89747619628906, 382.8038330078125, -106.2899398803711, 24.981914520263672, -747.265625]\n",
      "\n",
      "Instance 700 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 488 tensor sets is: [159.2882080078125, 383.8776550292969, -106.51358032226562, 22.340965270996094, -741.8928833007812]\n",
      "\n",
      "Instance 701 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 702 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 703 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 704 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 705 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [1.0301811695098877, 0.09672722220420837, -1.850982904434204, 0.9610074162483215, -2.2480239868164062]\n",
      "Grand sum of 489 tensor sets is: [160.31838989257812, 383.9743957519531, -108.36456298828125, 23.301973342895508, -744.1409301757812]\n",
      "\n",
      "Instance 706 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 10: [-0.003194287419319153, 3.0020017623901367, -0.9934726357460022, -0.005951300263404846, -0.08032992482185364]\n",
      "Grand sum of 490 tensor sets is: [160.31520080566406, 386.9764099121094, -109.3580322265625, 23.296022415161133, -744.2212524414062]\n",
      "\n",
      "Instance 707 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [104]\n",
      "Size of token embeddings is torch.Size([119, 13, 768])\n",
      "Shape of summed layers is: 119 x 768\n",
      "forever at index 104: [1.0649974346160889, 1.4500083923339844, -0.7240767478942871, -0.24990825355052948, -0.14254280924797058]\n",
      "Grand sum of 491 tensor sets is: [161.3802032470703, 388.4264221191406, -110.08210754394531, 23.046113967895508, -744.36376953125]\n",
      "\n",
      "Instance 708 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 709 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 15: [0.4135320484638214, 0.4106937646865845, -0.955510675907135, 1.236358404159546, 0.2074296474456787]\n",
      "Grand sum of 492 tensor sets is: [161.79373168945312, 388.8371276855469, -111.0376205444336, 24.282472610473633, -744.1563110351562]\n",
      "\n",
      "Instance 710 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 711 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 712 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 713 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 21: [0.8978962898254395, 1.9986608028411865, -0.9513338804244995, -0.5774953365325928, -2.8556385040283203]\n",
      "Grand sum of 493 tensor sets is: [162.69163513183594, 390.8357849121094, -111.98895263671875, 23.70497703552246, -747.011962890625]\n",
      "\n",
      "Instance 714 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 715 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 14: [0.542905867099762, 0.9680182933807373, -0.5615878701210022, -0.46200141310691833, 0.20962166786193848]\n",
      "Grand sum of 494 tensor sets is: [163.2345428466797, 391.8038024902344, -112.550537109375, 23.24297523498535, -746.8023681640625]\n",
      "\n",
      "Instance 716 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 23: [0.542675793170929, 1.48388671875, -0.32777583599090576, -0.8321834802627563, 1.3511385917663574]\n",
      "Grand sum of 495 tensor sets is: [163.7772216796875, 393.2876892089844, -112.87831115722656, 22.410791397094727, -745.4512329101562]\n",
      "\n",
      "Instance 717 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 718 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 31: [0.20083023607730865, 1.3254729509353638, -1.1853127479553223, 1.558513879776001, -2.18442702293396]\n",
      "Grand sum of 496 tensor sets is: [163.97805786132812, 394.6131591796875, -114.0636215209961, 23.96930503845215, -747.6356811523438]\n",
      "\n",
      "Instance 719 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 497 tensor sets is: [164.52195739746094, 393.97833251953125, -114.71317291259766, 22.827083587646484, -746.5143432617188]\n",
      "\n",
      "Instance 720 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 26: [0.29277291893959045, -0.015971899032592773, -0.391075074672699, -0.9018450975418091, -3.2865498065948486]\n",
      "Grand sum of 498 tensor sets is: [164.81472778320312, 393.9623718261719, -115.104248046875, 21.92523765563965, -749.8009033203125]\n",
      "\n",
      "Instance 721 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [0.3498929738998413, 0.9748251438140869, -1.374105453491211, -0.0809759795665741, -5.644731044769287]\n",
      "Grand sum of 499 tensor sets is: [165.1646270751953, 394.93719482421875, -116.47835540771484, 21.844261169433594, -755.4456176757812]\n",
      "\n",
      "Instance 722 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 15: [0.4689650535583496, 0.5765770673751831, 0.6741189956665039, -1.2973747253417969, -2.372622489929199]\n",
      "Grand sum of 500 tensor sets is: [165.6335906982422, 395.5137634277344, -115.80423736572266, 20.546886444091797, -757.8182373046875]\n",
      "\n",
      "Instance 723 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 30: [0.32608771324157715, 1.1130447387695312, -0.36733582615852356, -1.4276463985443115, 0.8360584378242493]\n",
      "Grand sum of 501 tensor sets is: [165.9596710205078, 396.6268005371094, -116.17156982421875, 19.119239807128906, -756.982177734375]\n",
      "\n",
      "Instance 724 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 725 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 16: [-0.1756521314382553, 2.30515193939209, -0.15239877998828888, -2.0975935459136963, 0.8856222033500671]\n",
      "Grand sum of 502 tensor sets is: [165.7840118408203, 398.93194580078125, -116.32396697998047, 17.02164649963379, -756.0965576171875]\n",
      "\n",
      "Instance 726 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 727 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 15: [0.7871898412704468, 1.6686460971832275, 0.5070920586585999, 0.7595564723014832, 1.4655563831329346]\n",
      "Grand sum of 503 tensor sets is: [166.57119750976562, 400.6005859375, -115.8168716430664, 17.78120231628418, -754.6309814453125]\n",
      "\n",
      "Instance 728 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 20: [-0.9986687302589417, 1.788968563079834, -0.6016136407852173, 2.0888519287109375, -0.43501269817352295]\n",
      "Grand sum of 504 tensor sets is: [165.57252502441406, 402.3895568847656, -116.41848754882812, 19.870054244995117, -755.0659790039062]\n",
      "\n",
      "Instance 729 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 730 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 731 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 17: [0.10911138355731964, 0.6701171398162842, -0.2799544334411621, -1.4025073051452637, -1.8531322479248047]\n",
      "Grand sum of 505 tensor sets is: [165.681640625, 403.0596618652344, -116.69844055175781, 18.467546463012695, -756.9191284179688]\n",
      "\n",
      "Instance 732 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 5: [0.2830432057380676, 0.31603628396987915, -0.1746586561203003, -1.273441195487976, -1.4339905977249146]\n",
      "Grand sum of 506 tensor sets is: [165.96469116210938, 403.3757019042969, -116.87310028076172, 17.19410514831543, -758.3531494140625]\n",
      "\n",
      "Instance 733 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([107, 13, 768])\n",
      "Shape of summed layers is: 107 x 768\n",
      "forever at index 42: [-0.20929639041423798, 0.32000550627708435, 0.1383977085351944, 1.0640039443969727, -2.332630157470703]\n",
      "Grand sum of 507 tensor sets is: [165.75540161132812, 403.6957092285156, -116.73470306396484, 18.25811004638672, -760.685791015625]\n",
      "\n",
      "Instance 734 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 21: [0.7872353196144104, 0.7699689269065857, -0.49908435344696045, 0.12089738249778748, -2.458338499069214]\n",
      "Grand sum of 508 tensor sets is: [166.54263305664062, 404.4656677246094, -117.2337875366211, 18.37900733947754, -763.1441040039062]\n",
      "\n",
      "Instance 735 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 40: [0.6953015327453613, -1.0296984910964966, 0.04725205898284912, -1.5207749605178833, 0.05389869213104248]\n",
      "Grand sum of 509 tensor sets is: [167.23793029785156, 403.43597412109375, -117.18653869628906, 16.858232498168945, -763.0902099609375]\n",
      "\n",
      "Instance 736 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [52]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "forever at index 52: [0.471918523311615, -0.3618389368057251, -0.5413256287574768, -0.3991427421569824, 1.5480782985687256]\n",
      "Grand sum of 510 tensor sets is: [167.70985412597656, 403.0741271972656, -117.72786712646484, 16.459089279174805, -761.5421142578125]\n",
      "\n",
      "Instance 737 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 24: [0.48892319202423096, 0.8739367127418518, -0.35139983892440796, -0.37098991870880127, -1.8001636266708374]\n",
      "Grand sum of 511 tensor sets is: [168.1987762451172, 403.94805908203125, -118.07926940917969, 16.088098526000977, -763.34228515625]\n",
      "\n",
      "Instance 738 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 2: [1.1141968965530396, -0.23476240038871765, -0.9401029944419861, 1.9605684280395508, -1.1663790941238403]\n",
      "Grand sum of 512 tensor sets is: [169.31297302246094, 403.7132873535156, -119.01937103271484, 18.048667907714844, -764.5086669921875]\n",
      "\n",
      "Instance 739 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 5: [0.48542577028274536, 1.2541369199752808, -1.407567024230957, 1.5818785429000854, -4.121397018432617]\n",
      "Grand sum of 513 tensor sets is: [169.79840087890625, 404.9674377441406, -120.42694091796875, 19.63054656982422, -768.6300659179688]\n",
      "\n",
      "Instance 740 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [102, 167]\n",
      "Size of token embeddings is torch.Size([186, 13, 768])\n",
      "Shape of summed layers is: 186 x 768\n",
      "forever at index 102: [0.15189987421035767, 0.9312858581542969, -1.0129108428955078, 0.15915696322917938, -2.6133224964141846]\n",
      "forever at index 167: [0.8973044157028198, 2.0149521827697754, 0.6894101500511169, -1.1778972148895264, -2.75626277923584]\n",
      "Grand sum of 514 tensor sets is: [170.322998046875, 406.4405517578125, -120.58869171142578, 19.12117576599121, -771.3148803710938]\n",
      "\n",
      "Instance 741 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 36: [-0.050302959978580475, 0.42136991024017334, -0.15843471884727478, -0.33366659283638, -0.636167049407959]\n",
      "Grand sum of 515 tensor sets is: [170.27268981933594, 406.8619079589844, -120.74712371826172, 18.78750991821289, -771.9510498046875]\n",
      "\n",
      "Instance 742 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 39: [0.2404685616493225, 1.8247365951538086, -0.22989042103290558, -1.4314640760421753, -0.01681867241859436]\n",
      "Grand sum of 516 tensor sets is: [170.51315307617188, 408.6866455078125, -120.97701263427734, 17.356046676635742, -771.9678955078125]\n",
      "\n",
      "Instance 743 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 744 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [57]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "forever at index 57: [0.8879796266555786, -0.38249471783638, -1.466266393661499, 1.3997703790664673, -4.345282077789307]\n",
      "Grand sum of 517 tensor sets is: [171.40113830566406, 408.30413818359375, -122.44327545166016, 18.755817413330078, -776.3131713867188]\n",
      "\n",
      "Instance 745 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 28: [0.6821977496147156, 1.6481544971466064, 0.14911490678787231, -0.8097460269927979, 0.9285337924957275]\n",
      "Grand sum of 518 tensor sets is: [172.08334350585938, 409.9523010253906, -122.29415893554688, 17.94607162475586, -775.3846435546875]\n",
      "\n",
      "Instance 746 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 519 tensor sets is: [172.6272430419922, 409.3174743652344, -122.94371032714844, 16.803850173950195, -774.2633056640625]\n",
      "\n",
      "Instance 747 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 748 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 16: [0.40573662519454956, 2.8713672161102295, -0.23209188878536224, 1.5066328048706055, -2.5575501918792725]\n",
      "Grand sum of 520 tensor sets is: [173.03297424316406, 412.1888427734375, -123.1758041381836, 18.310482025146484, -776.8208618164062]\n",
      "\n",
      "Instance 749 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 750 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 751 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 521 tensor sets is: [173.42868041992188, 413.197509765625, -123.24537658691406, 17.62691879272461, -775.5245361328125]\n",
      "\n",
      "Instance 752 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 6: [1.080199122428894, 0.8872127532958984, -1.3612358570098877, -0.5470148921012878, -4.741143703460693]\n",
      "Grand sum of 522 tensor sets is: [174.50888061523438, 414.084716796875, -124.60661315917969, 17.079904556274414, -780.2656860351562]\n",
      "\n",
      "Instance 753 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 2: [0.6029667854309082, -1.0797550678253174, 0.05845792591571808, -2.207092761993408, 3.284536361694336]\n",
      "Grand sum of 523 tensor sets is: [175.11184692382812, 413.0049743652344, -124.54815673828125, 14.872812271118164, -776.9811401367188]\n",
      "\n",
      "Instance 754 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 755 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 756 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 12: [0.8349117040634155, 1.6807787418365479, 0.5802929997444153, -0.15079578757286072, 0.503434419631958]\n",
      "Grand sum of 524 tensor sets is: [175.94676208496094, 414.6857604980469, -123.96786499023438, 14.722016334533691, -776.4777221679688]\n",
      "\n",
      "Instance 757 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 5: [-0.28156229853630066, -1.2672981023788452, -0.738512396812439, 2.2147016525268555, -6.728375434875488]\n",
      "Grand sum of 525 tensor sets is: [175.6652069091797, 413.41845703125, -124.70637512207031, 16.936717987060547, -783.2061157226562]\n",
      "\n",
      "Instance 758 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 28: [0.12872688472270966, -0.006600737571716309, 0.6904869079589844, -1.3795907497406006, -0.3783126473426819]\n",
      "Grand sum of 526 tensor sets is: [175.79393005371094, 413.411865234375, -124.01588439941406, 15.557126998901367, -783.5844116210938]\n",
      "\n",
      "Instance 759 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 527 tensor sets is: [175.05918884277344, 413.81671142578125, -123.62419891357422, 15.200067520141602, -788.145263671875]\n",
      "\n",
      "Instance 760 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 30: [-0.9046006798744202, 0.12822897732257843, -1.0701956748962402, 0.2594553530216217, -1.84276282787323]\n",
      "Grand sum of 528 tensor sets is: [174.1545867919922, 413.9449462890625, -124.69439697265625, 15.45952320098877, -789.988037109375]\n",
      "\n",
      "Instance 761 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 2: [-0.011558197438716888, 1.6737442016601562, 0.24011270701885223, 0.5058112144470215, 2.0135600566864014]\n",
      "Grand sum of 529 tensor sets is: [174.14303588867188, 415.6186828613281, -124.45428466796875, 15.965333938598633, -787.9744873046875]\n",
      "\n",
      "Instance 762 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 763 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 24: [0.8894685506820679, 1.9212164878845215, 0.2299211174249649, 1.3045077323913574, 1.5463601350784302]\n",
      "Grand sum of 530 tensor sets is: [175.03250122070312, 417.5398864746094, -124.224365234375, 17.26984214782715, -786.4281005859375]\n",
      "\n",
      "Instance 764 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 11: [1.0738987922668457, -1.0313669443130493, 0.005097478628158569, -0.5705676674842834, 1.267755150794983]\n",
      "Grand sum of 531 tensor sets is: [176.1063995361328, 416.5085144042969, -124.21926879882812, 16.69927406311035, -785.1603393554688]\n",
      "\n",
      "Instance 765 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 766 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 767 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 30: [0.485734224319458, 1.011696457862854, -0.3912021815776825, 0.865526556968689, -1.632388710975647]\n",
      "Grand sum of 532 tensor sets is: [176.59213256835938, 417.52020263671875, -124.6104736328125, 17.564800262451172, -786.792724609375]\n",
      "\n",
      "Instance 768 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 12: [0.07574449479579926, 2.3981525897979736, -0.8939185738563538, 1.267026424407959, -0.7092434167861938]\n",
      "Grand sum of 533 tensor sets is: [176.66787719726562, 419.9183654785156, -125.50439453125, 18.83182716369629, -787.501953125]\n",
      "\n",
      "Instance 769 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 770 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 771 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 5: [0.2830432057380676, 0.31603628396987915, -0.1746586561203003, -1.273441195487976, -1.4339905977249146]\n",
      "Grand sum of 534 tensor sets is: [176.950927734375, 420.2344055175781, -125.6790542602539, 17.558385848999023, -788.9359741210938]\n",
      "\n",
      "Instance 772 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 9: [0.16780094802379608, 0.3673933148384094, -0.6137426495552063, 1.908109426498413, -4.417931079864502]\n",
      "Grand sum of 535 tensor sets is: [177.1187286376953, 420.601806640625, -126.29279327392578, 19.466495513916016, -793.3538818359375]\n",
      "\n",
      "Instance 773 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 774 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 775 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 776 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 4: [1.1211358308792114, 2.7824907302856445, 0.3500211834907532, -1.1538059711456299, 4.497281551361084]\n",
      "Grand sum of 536 tensor sets is: [178.2398681640625, 423.3843078613281, -125.9427719116211, 18.31268882751465, -788.8566284179688]\n",
      "\n",
      "Instance 777 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 5: [0.6022050380706787, 2.4245588779449463, -0.6253792643547058, 1.0847046375274658, 0.46901166439056396]\n",
      "Grand sum of 537 tensor sets is: [178.84207153320312, 425.8088684082031, -126.56815338134766, 19.39739418029785, -788.3876342773438]\n",
      "\n",
      "Instance 778 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 26: [1.3161708116531372, 0.7910457253456116, 0.03404364734888077, -0.2547284960746765, -6.300019264221191]\n",
      "Grand sum of 538 tensor sets is: [180.1582489013672, 426.59991455078125, -126.53411102294922, 19.14266586303711, -794.6876831054688]\n",
      "\n",
      "Instance 779 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "forever at index 21: [0.10694491118192673, 3.1127190589904785, -0.12282704561948776, 1.005893349647522, -1.089453935623169]\n",
      "Grand sum of 539 tensor sets is: [180.26519775390625, 429.712646484375, -126.65693664550781, 20.1485595703125, -795.7771606445312]\n",
      "\n",
      "Instance 780 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 781 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 9: [1.098562479019165, -0.07149630784988403, -0.65312260389328, -1.7348510026931763, -1.0558137893676758]\n",
      "Grand sum of 540 tensor sets is: [181.36375427246094, 429.6411437988281, -127.31005859375, 18.413707733154297, -796.8329467773438]\n",
      "\n",
      "Instance 782 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 6: [-0.022896677255630493, 1.6134440898895264, -0.02442409098148346, -1.2658820152282715, -2.53287672996521]\n",
      "Grand sum of 541 tensor sets is: [181.34085083007812, 431.25457763671875, -127.33448028564453, 17.147825241088867, -799.3658447265625]\n",
      "\n",
      "Instance 783 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 4: [0.4971199035644531, 0.943888783454895, -0.03586447238922119, -0.9153103828430176, 1.1081030368804932]\n",
      "Grand sum of 542 tensor sets is: [181.8379669189453, 432.1984558105469, -127.37034606933594, 16.232515335083008, -798.2577514648438]\n",
      "\n",
      "Instance 784 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 785 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 15: [0.10400746762752533, 1.6669204235076904, 0.007456004619598389, -0.9575647711753845, -1.8934859037399292]\n",
      "Grand sum of 543 tensor sets is: [181.9419708251953, 433.8653869628906, -127.3628921508789, 15.274950981140137, -800.1512451171875]\n",
      "\n",
      "Instance 786 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 4: [0.1390530914068222, 0.43139609694480896, -0.8283774256706238, 0.8885035514831543, -2.148158550262451]\n",
      "Grand sum of 544 tensor sets is: [182.08102416992188, 434.2967834472656, -128.19126892089844, 16.163454055786133, -802.2993774414062]\n",
      "\n",
      "Instance 787 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "forever at index 25: [0.3997748792171478, 2.2176637649536133, 0.7562549710273743, 0.766328752040863, -4.266775131225586]\n",
      "Grand sum of 545 tensor sets is: [182.48080444335938, 436.5144348144531, -127.43501281738281, 16.92978286743164, -806.566162109375]\n",
      "\n",
      "Instance 788 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 6: [0.0788905918598175, -0.8366189002990723, -0.26902714371681213, 0.5236204862594604, -2.59700608253479]\n",
      "Grand sum of 546 tensor sets is: [182.5596923828125, 435.6778259277344, -127.70404052734375, 17.45340347290039, -809.1631469726562]\n",
      "\n",
      "Instance 789 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 12: [0.6729142069816589, 0.48703616857528687, -0.3849000632762909, -0.8173264861106873, -4.151731967926025]\n",
      "Grand sum of 547 tensor sets is: [183.23260498046875, 436.16485595703125, -128.0889434814453, 16.636077880859375, -813.3148803710938]\n",
      "\n",
      "Instance 790 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 12: [0.56778484582901, 0.6088502407073975, 1.3794455528259277, 0.22103863954544067, -0.800449013710022]\n",
      "Grand sum of 548 tensor sets is: [183.80038452148438, 436.7737121582031, -126.7094955444336, 16.85711669921875, -814.1153564453125]\n",
      "\n",
      "Instance 791 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 792 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 11: [0.8203142285346985, -0.30881795287132263, 0.4144551753997803, -1.0083351135253906, -4.022216320037842]\n",
      "Grand sum of 549 tensor sets is: [184.62069702148438, 436.46490478515625, -126.2950439453125, 15.84878158569336, -818.1375732421875]\n",
      "\n",
      "Instance 793 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 550 tensor sets is: [186.4120330810547, 434.9102783203125, -126.97727966308594, 15.00285816192627, -817.6845092773438]\n",
      "\n",
      "Instance 794 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 795 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 796 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 797 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 798 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 5: [0.24495182931423187, 0.9786214232444763, 0.40293043851852417, 1.7031179666519165, -0.5502043962478638]\n",
      "Grand sum of 551 tensor sets is: [186.656982421875, 435.8888854980469, -126.57434844970703, 16.705976486206055, -818.2347412109375]\n",
      "\n",
      "Instance 799 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 31: [0.4022781252861023, 1.9779874086380005, -1.1320397853851318, 0.7771404385566711, 0.6989696025848389]\n",
      "Grand sum of 552 tensor sets is: [187.05926513671875, 437.86688232421875, -127.70639038085938, 17.483116149902344, -817.5357666015625]\n",
      "\n",
      "Instance 800 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 553 tensor sets is: [186.32452392578125, 438.271728515625, -127.31470489501953, 17.126056671142578, -822.0966186523438]\n",
      "\n",
      "Instance 801 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 802 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 803 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 554 tensor sets is: [187.16314697265625, 438.6277160644531, -127.71636962890625, 17.639680862426758, -819.5575561523438]\n",
      "\n",
      "Instance 804 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([86, 13, 768])\n",
      "Shape of summed layers is: 86 x 768\n",
      "forever at index 23: [0.3566783666610718, 1.400369644165039, 0.40251845121383667, 1.8771018981933594, -5.286637306213379]\n",
      "Grand sum of 555 tensor sets is: [187.5198211669922, 440.028076171875, -127.31385040283203, 19.516782760620117, -824.8441772460938]\n",
      "\n",
      "Instance 805 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 556 tensor sets is: [188.3584442138672, 440.3840637207031, -127.71551513671875, 20.030406951904297, -822.3051147460938]\n",
      "\n",
      "Instance 806 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 807 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 17: [-0.17817369103431702, -0.17182965576648712, 0.07394170761108398, 0.7782841920852661, -0.9759155511856079]\n",
      "Grand sum of 557 tensor sets is: [188.18026733398438, 440.21221923828125, -127.64157104492188, 20.808691024780273, -823.281005859375]\n",
      "\n",
      "Instance 808 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 22: [-0.21675148606300354, 2.6656079292297363, -0.34882310032844543, 0.5089168548583984, -1.3530490398406982]\n",
      "Grand sum of 558 tensor sets is: [187.96351623535156, 442.8778381347656, -127.99039459228516, 21.317607879638672, -824.634033203125]\n",
      "\n",
      "Instance 809 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 40: [-0.33672064542770386, 2.106896162033081, -0.5561774969100952, 1.4709511995315552, -1.9291839599609375]\n",
      "Grand sum of 559 tensor sets is: [187.62680053710938, 444.9847412109375, -128.54656982421875, 22.788558959960938, -826.563232421875]\n",
      "\n",
      "Instance 810 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 19: [0.524848222732544, 2.5300283432006836, -1.4209479093551636, -1.904233694076538, -5.3815531730651855]\n",
      "Grand sum of 560 tensor sets is: [188.15164184570312, 447.5147705078125, -129.96751403808594, 20.88432502746582, -831.9447631835938]\n",
      "\n",
      "Instance 811 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 561 tensor sets is: [188.54237365722656, 448.5885925292969, -130.191162109375, 18.243375778198242, -826.572021484375]\n",
      "\n",
      "Instance 812 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 12: [-0.23490022122859955, 0.6261957287788391, -0.38854852318763733, 0.7706413269042969, -5.734294891357422]\n",
      "Grand sum of 562 tensor sets is: [188.30747985839844, 449.21478271484375, -130.5797119140625, 19.01401710510254, -832.3063354492188]\n",
      "\n",
      "Instance 813 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 563 tensor sets is: [188.85137939453125, 448.5799560546875, -131.22926330566406, 17.871795654296875, -831.1849975585938]\n",
      "\n",
      "Instance 814 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "forever at index 45: [0.8595426082611084, -2.3191938400268555, -0.8518744707107544, -2.7890124320983887, -1.6738653182983398]\n",
      "Grand sum of 564 tensor sets is: [189.71092224121094, 446.2607727050781, -132.0811309814453, 15.082782745361328, -832.85888671875]\n",
      "\n",
      "Instance 815 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 5: [-1.268799066543579, 0.30860471725463867, 0.7062539458274841, 0.1108228862285614, -3.278355121612549]\n",
      "Grand sum of 565 tensor sets is: [188.44212341308594, 446.5693664550781, -131.3748779296875, 15.193605422973633, -836.1372680664062]\n",
      "\n",
      "Instance 816 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 14: [-0.7065223455429077, 2.075387954711914, -0.18739531934261322, -0.2707900106906891, -3.837118148803711]\n",
      "Grand sum of 566 tensor sets is: [187.735595703125, 448.6447448730469, -131.56227111816406, 14.922815322875977, -839.974365234375]\n",
      "\n",
      "Instance 817 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 22: [0.6497958898544312, 1.7888762950897217, -0.29857170581817627, 0.6517108678817749, 0.9494466781616211]\n",
      "Grand sum of 567 tensor sets is: [188.38539123535156, 450.4336242675781, -131.86083984375, 15.574525833129883, -839.02490234375]\n",
      "\n",
      "Instance 818 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 819 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 9: [0.17169766128063202, 2.0983643531799316, -0.18142928183078766, -0.5145377516746521, -0.6681958436965942]\n",
      "Grand sum of 568 tensor sets is: [188.5570831298828, 452.531982421875, -132.04226684570312, 15.059988021850586, -839.693115234375]\n",
      "\n",
      "Instance 820 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([93, 13, 768])\n",
      "Shape of summed layers is: 93 x 768\n",
      "forever at index 38: [-1.1777037382125854, -0.2888803780078888, -0.2941676676273346, -0.5361020565032959, -3.4033100605010986]\n",
      "Grand sum of 569 tensor sets is: [187.37937927246094, 452.24310302734375, -132.33644104003906, 14.523885726928711, -843.096435546875]\n",
      "\n",
      "Instance 821 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [-0.13183727860450745, 1.8965065479278564, -0.03226179629564285, -0.7949715852737427, -1.3746719360351562]\n",
      "Grand sum of 570 tensor sets is: [187.24754333496094, 454.1396179199219, -132.3686981201172, 13.728914260864258, -844.4711303710938]\n",
      "\n",
      "Instance 822 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 8: [0.7191540598869324, 2.597564220428467, -0.13535168766975403, -1.43770432472229, 1.2078138589859009]\n",
      "Grand sum of 571 tensor sets is: [187.96669006347656, 456.7371826171875, -132.50404357910156, 12.291210174560547, -843.2633056640625]\n",
      "\n",
      "Instance 823 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 824 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 6: [1.2983585596084595, 1.3153674602508545, -0.7824043035507202, 1.2971186637878418, -4.519035339355469]\n",
      "Grand sum of 572 tensor sets is: [189.26504516601562, 458.05255126953125, -133.2864532470703, 13.588329315185547, -847.7823486328125]\n",
      "\n",
      "Instance 825 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 7: [0.6088832020759583, -1.836504578590393, -0.3845173716545105, -1.4431743621826172, -4.129314422607422]\n",
      "Grand sum of 573 tensor sets is: [189.87393188476562, 456.2160339355469, -133.6709747314453, 12.14515495300293, -851.9116821289062]\n",
      "\n",
      "Instance 826 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [-0.1773413121700287, 0.8982719779014587, -1.4495153427124023, 1.387389898300171, -5.608426570892334]\n",
      "Grand sum of 574 tensor sets is: [189.69659423828125, 457.11431884765625, -135.1204833984375, 13.53254508972168, -857.5200805664062]\n",
      "\n",
      "Instance 827 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 4: [0.3729330897331238, 1.3867568969726562, -0.516496479511261, 1.3165209293365479, -0.04785456508398056]\n",
      "Grand sum of 575 tensor sets is: [190.0695343017578, 458.5010681152344, -135.63697814941406, 14.849065780639648, -857.5679321289062]\n",
      "\n",
      "Instance 828 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 8: [0.8820449113845825, 1.102081298828125, -0.2501317262649536, -0.04444080591201782, -1.2644773721694946]\n",
      "Grand sum of 576 tensor sets is: [190.9515838623047, 459.6031494140625, -135.88711547851562, 14.804624557495117, -858.8323974609375]\n",
      "\n",
      "Instance 829 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 11: [0.45434433221817017, 2.4394805431365967, 0.08070823550224304, -1.328647494316101, 2.8135361671447754]\n",
      "Grand sum of 577 tensor sets is: [191.4059295654297, 462.0426330566406, -135.80641174316406, 13.475976943969727, -856.0188598632812]\n",
      "\n",
      "Instance 830 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 36: [-0.050302959978580475, 0.42136991024017334, -0.15843471884727478, -0.33366659283638, -0.636167049407959]\n",
      "Grand sum of 578 tensor sets is: [191.35562133789062, 462.4639892578125, -135.96484375, 13.14231014251709, -856.655029296875]\n",
      "\n",
      "Instance 831 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 832 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 14: [0.21717102825641632, 1.0272125005722046, 0.001334354281425476, 0.6784761548042297, 0.5428685545921326]\n",
      "Grand sum of 579 tensor sets is: [191.5727996826172, 463.4912109375, -135.96351623535156, 13.820786476135254, -856.1121826171875]\n",
      "\n",
      "Instance 833 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 6: [-0.0659170001745224, 0.18661877512931824, -0.8259607553482056, 2.5733258724212646, -2.0173346996307373]\n",
      "Grand sum of 580 tensor sets is: [191.5068817138672, 463.6778259277344, -136.7894744873047, 16.39411163330078, -858.1295166015625]\n",
      "\n",
      "Instance 834 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 835 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [57]\n",
      "Size of token embeddings is torch.Size([66, 13, 768])\n",
      "Shape of summed layers is: 66 x 768\n",
      "forever at index 57: [0.25110548734664917, 2.006178855895996, 0.22097688913345337, -0.28275740146636963, -1.5236082077026367]\n",
      "Grand sum of 581 tensor sets is: [191.7579803466797, 465.6839904785156, -136.56849670410156, 16.11135482788086, -859.6531372070312]\n",
      "\n",
      "Instance 836 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [50, 93]\n",
      "Size of token embeddings is torch.Size([102, 13, 768])\n",
      "Shape of summed layers is: 102 x 768\n",
      "forever at index 50: [-0.015815559774637222, 0.9289683103561401, -0.01102282851934433, 1.9878857135772705, -0.16151751577854156]\n",
      "forever at index 93: [0.27307963371276855, 0.1441645622253418, -0.10957235097885132, 1.4483338594436646, -2.1080620288848877]\n",
      "Grand sum of 582 tensor sets is: [191.88661193847656, 466.2205505371094, -136.62879943847656, 17.829463958740234, -860.7879028320312]\n",
      "\n",
      "Instance 837 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 838 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 839 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24, 37]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "forever at index 24: [0.6010355949401855, 0.8542024493217468, -0.41211870312690735, 0.5616418123245239, -1.0704801082611084]\n",
      "forever at index 37: [0.10999836027622223, 1.6557202339172363, -0.49876177310943604, 0.5565474033355713, -2.6296205520629883]\n",
      "Grand sum of 583 tensor sets is: [192.24212646484375, 467.47552490234375, -137.08424377441406, 18.388559341430664, -862.637939453125]\n",
      "\n",
      "Instance 840 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 9: [0.7103983163833618, 1.0403048992156982, -0.19822001457214355, 0.36342310905456543, -1.2483857870101929]\n",
      "Grand sum of 584 tensor sets is: [192.95252990722656, 468.5158386230469, -137.282470703125, 18.751981735229492, -863.8863525390625]\n",
      "\n",
      "Instance 841 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [0.36479946970939636, 0.24682025611400604, -0.4848631024360657, -0.5248885154724121, -1.1551042795181274]\n",
      "Grand sum of 585 tensor sets is: [193.31732177734375, 468.7626647949219, -137.767333984375, 18.227092742919922, -865.0414428710938]\n",
      "\n",
      "Instance 842 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [62]\n",
      "Size of token embeddings is torch.Size([80, 13, 768])\n",
      "Shape of summed layers is: 80 x 768\n",
      "forever at index 62: [-0.08291309326887131, 0.5645653605461121, 0.21457427740097046, 0.14816883206367493, 0.1922958642244339]\n",
      "Grand sum of 586 tensor sets is: [193.23440551757812, 469.3272399902344, -137.55276489257812, 18.375261306762695, -864.84912109375]\n",
      "\n",
      "Instance 843 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 844 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 6: [0.8535654544830322, 0.19294527173042297, 0.5053471326828003, 0.7963662147521973, -5.678613185882568]\n",
      "Grand sum of 587 tensor sets is: [194.0879669189453, 469.5201721191406, -137.04742431640625, 19.171627044677734, -870.5277099609375]\n",
      "\n",
      "Instance 845 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 846 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 847 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 588 tensor sets is: [194.9265899658203, 469.87615966796875, -137.44908142089844, 19.685251235961914, -867.9886474609375]\n",
      "\n",
      "Instance 848 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 589 tensor sets is: [195.0646514892578, 470.53033447265625, -137.88966369628906, 19.38793182373047, -870.8389282226562]\n",
      "\n",
      "Instance 849 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "forever at index 36: [0.1474810242652893, -0.9888689517974854, 0.9634909629821777, -0.7677390575408936, -0.6801234483718872]\n",
      "Grand sum of 590 tensor sets is: [195.21212768554688, 469.5414733886719, -136.92617797851562, 18.620193481445312, -871.51904296875]\n",
      "\n",
      "Instance 850 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 591 tensor sets is: [195.3135528564453, 471.28082275390625, -137.56419372558594, 17.504810333251953, -875.815673828125]\n",
      "\n",
      "Instance 851 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 21: [1.179478406906128, 0.6045626997947693, 0.8377909064292908, 0.3390519320964813, 0.28187432885169983]\n",
      "Grand sum of 592 tensor sets is: [196.49302673339844, 471.8853759765625, -136.72640991210938, 17.843862533569336, -875.5338134765625]\n",
      "\n",
      "Instance 852 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 14: [0.7897530794143677, -0.21480855345726013, -0.7274443507194519, -0.28141969442367554, 1.2782230377197266]\n",
      "Grand sum of 593 tensor sets is: [197.28277587890625, 471.6705627441406, -137.453857421875, 17.562442779541016, -874.255615234375]\n",
      "\n",
      "Instance 853 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 37: [0.7150909304618835, 1.579113245010376, -0.037696871906518936, -1.1114193201065063, -2.251307964324951]\n",
      "Grand sum of 594 tensor sets is: [197.99786376953125, 473.2496643066406, -137.49156188964844, 16.45102310180664, -876.5068969726562]\n",
      "\n",
      "Instance 854 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 855 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 856 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 15: [0.1252358853816986, 1.3405134677886963, -0.7572575807571411, 0.4299303889274597, -4.593039512634277]\n",
      "Grand sum of 595 tensor sets is: [198.1230926513672, 474.5901794433594, -138.2488250732422, 16.880952835083008, -881.0999145507812]\n",
      "\n",
      "Instance 857 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 16: [-0.25349122285842896, 1.918128252029419, -0.14949151873588562, -0.9453276991844177, -4.856919765472412]\n",
      "Grand sum of 596 tensor sets is: [197.86959838867188, 476.50830078125, -138.3983154296875, 15.935625076293945, -885.9568481445312]\n",
      "\n",
      "Instance 858 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 859 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 19: [0.31514549255371094, 0.7620363235473633, -0.1485857218503952, 0.25972509384155273, -5.583872318267822]\n",
      "Grand sum of 597 tensor sets is: [198.1847381591797, 477.27032470703125, -138.54690551757812, 16.195350646972656, -891.5407104492188]\n",
      "\n",
      "Instance 860 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 11: [0.06160822510719299, -1.8760002851486206, -1.0028713941574097, 0.12344498932361603, -1.0493837594985962]\n",
      "Grand sum of 598 tensor sets is: [198.24635314941406, 475.3943176269531, -139.54977416992188, 16.318796157836914, -892.590087890625]\n",
      "\n",
      "Instance 861 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 862 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 32: [-0.6230658292770386, 0.5303539037704468, -1.0711599588394165, -0.12893027067184448, -3.2016351222991943]\n",
      "Grand sum of 599 tensor sets is: [197.623291015625, 475.9246826171875, -140.62094116210938, 16.189865112304688, -895.791748046875]\n",
      "\n",
      "Instance 863 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 864 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 865 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 12: [-0.20518234372138977, 1.4490993022918701, 0.055230796337127686, 1.4929603338241577, -2.249056339263916]\n",
      "Grand sum of 600 tensor sets is: [197.41810607910156, 477.373779296875, -140.56570434570312, 17.682825088500977, -898.0408325195312]\n",
      "\n",
      "Instance 866 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [0.3498929738998413, 0.9748251438140869, -1.374105453491211, -0.0809759795665741, -5.644731044769287]\n",
      "Grand sum of 601 tensor sets is: [197.76800537109375, 478.3486022949219, -141.93980407714844, 17.601848602294922, -903.685546875]\n",
      "\n",
      "Instance 867 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 868 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 6: [-0.1267181783914566, 1.645769476890564, 0.07778481394052505, 0.06490051746368408, 0.6585238575935364]\n",
      "Grand sum of 602 tensor sets is: [197.6412811279297, 479.994384765625, -141.8620147705078, 17.666749954223633, -903.0270385742188]\n",
      "\n",
      "Instance 869 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 603 tensor sets is: [198.3360595703125, 481.127685546875, -140.65887451171875, 16.39738655090332, -903.9172973632812]\n",
      "\n",
      "Instance 870 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 871 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 25: [0.10129414498806, 2.7727768421173096, 0.3695603013038635, -0.6778906583786011, -1.7446908950805664]\n",
      "Grand sum of 604 tensor sets is: [198.43734741210938, 483.90045166015625, -140.289306640625, 15.71949577331543, -905.6619873046875]\n",
      "\n",
      "Instance 872 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [64]\n",
      "Size of token embeddings is torch.Size([83, 13, 768])\n",
      "Shape of summed layers is: 83 x 768\n",
      "forever at index 64: [0.7530649304389954, -0.3725394606590271, -0.5493227243423462, -0.6755709052085876, 1.2109248638153076]\n",
      "Grand sum of 605 tensor sets is: [199.19041442871094, 483.5279235839844, -140.838623046875, 15.043925285339355, -904.4510498046875]\n",
      "\n",
      "Instance 873 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 24: [0.5837600827217102, -0.7728415727615356, 0.7455956935882568, -1.0803914070129395, -1.7842985391616821]\n",
      "Grand sum of 606 tensor sets is: [199.774169921875, 482.7550964355469, -140.09303283691406, 13.963533401489258, -906.2353515625]\n",
      "\n",
      "Instance 874 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 875 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 876 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 6: [-0.886422872543335, 0.12700533866882324, -0.7039200663566589, 0.3196592926979065, -4.211904048919678]\n",
      "Grand sum of 607 tensor sets is: [198.8877410888672, 482.8821105957031, -140.7969512939453, 14.28319263458252, -910.447265625]\n",
      "\n",
      "Instance 877 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 878 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 6: [-0.0659170001745224, 0.18661877512931824, -0.8259607553482056, 2.5733258724212646, -2.0173346996307373]\n",
      "Grand sum of 608 tensor sets is: [198.8218231201172, 483.0687255859375, -141.62290954589844, 16.856517791748047, -912.464599609375]\n",
      "\n",
      "Instance 879 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 12: [0.498801052570343, 0.8746846914291382, -1.0013374090194702, 0.7699152231216431, -3.6038031578063965]\n",
      "Grand sum of 609 tensor sets is: [199.32061767578125, 483.94342041015625, -142.62425231933594, 17.626432418823242, -916.0684204101562]\n",
      "\n",
      "Instance 880 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 881 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 6: [-0.2094719409942627, 1.295346736907959, -1.333199143409729, -1.4172272682189941, -2.9168927669525146]\n",
      "Grand sum of 610 tensor sets is: [199.11114501953125, 485.23876953125, -143.95745849609375, 16.209205627441406, -918.9852905273438]\n",
      "\n",
      "Instance 882 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 17: [-0.4026026129722595, 3.2080466747283936, 1.1966592073440552, 1.6081608533859253, -2.246037244796753]\n",
      "Grand sum of 611 tensor sets is: [198.7085418701172, 488.4468078613281, -142.76080322265625, 17.817365646362305, -921.2313232421875]\n",
      "\n",
      "Instance 883 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "forever at index 45: [0.7877117395401001, 1.3120968341827393, 0.6212652921676636, -0.8319603204727173, -4.985375881195068]\n",
      "Grand sum of 612 tensor sets is: [199.49624633789062, 489.7589111328125, -142.13954162597656, 16.98540496826172, -926.2166748046875]\n",
      "\n",
      "Instance 884 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 885 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 33: [0.8219789266586304, 0.49377965927124023, 0.5926049947738647, 2.502669095993042, 3.099024772644043]\n",
      "Grand sum of 613 tensor sets is: [200.31822204589844, 490.252685546875, -141.54693603515625, 19.488073348999023, -923.11767578125]\n",
      "\n",
      "Instance 886 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 6: [-0.6427145600318909, 1.0359883308410645, -0.2626999616622925, 1.0278141498565674, -2.2743732929229736]\n",
      "Grand sum of 614 tensor sets is: [199.67550659179688, 491.2886657714844, -141.80963134765625, 20.515888214111328, -925.3920288085938]\n",
      "\n",
      "Instance 887 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 888 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 889 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 3: [1.3427575826644897, -0.7265383005142212, 0.4534558057785034, -1.0354959964752197, 1.510983943939209]\n",
      "Grand sum of 615 tensor sets is: [201.0182647705078, 490.5621337890625, -141.35617065429688, 19.480392456054688, -923.8810424804688]\n",
      "\n",
      "Instance 890 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 12: [0.9957161545753479, -0.21900273859500885, 0.7824002504348755, 1.6118316650390625, -4.627930164337158]\n",
      "Grand sum of 616 tensor sets is: [202.01397705078125, 490.3431396484375, -140.5737762451172, 21.09222412109375, -928.5089721679688]\n",
      "\n",
      "Instance 891 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 3: [0.32117903232574463, 0.23560544848442078, -0.000872097909450531, -0.8400305509567261, -2.1225438117980957]\n",
      "Grand sum of 617 tensor sets is: [202.3351593017578, 490.5787353515625, -140.57464599609375, 20.252193450927734, -930.6315307617188]\n",
      "\n",
      "Instance 892 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 9: [-0.10572759807109833, 1.0198439359664917, -0.9626713395118713, -0.5295765995979309, -6.015517234802246]\n",
      "Grand sum of 618 tensor sets is: [202.22943115234375, 491.59857177734375, -141.53732299804688, 19.72261619567871, -936.6470336914062]\n",
      "\n",
      "Instance 893 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 27: [0.161866694688797, 1.3101677894592285, 0.11965629458427429, -0.6409188508987427, -3.112687349319458]\n",
      "Grand sum of 619 tensor sets is: [202.39129638671875, 492.90875244140625, -141.41766357421875, 19.081697463989258, -939.7597045898438]\n",
      "\n",
      "Instance 894 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [78]\n",
      "Size of token embeddings is torch.Size([269, 13, 768])\n",
      "Shape of summed layers is: 269 x 768\n",
      "forever at index 78: [-0.8362959623336792, 1.3877527713775635, -0.461038202047348, 1.362511396408081, 3.3866772651672363]\n",
      "Grand sum of 620 tensor sets is: [201.5550079345703, 494.2965087890625, -141.8787078857422, 20.4442081451416, -936.373046875]\n",
      "\n",
      "Instance 895 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 25: [0.7254108190536499, 1.2825610637664795, 0.6096808314323425, -0.6638433933258057, -0.06686681509017944]\n",
      "Grand sum of 621 tensor sets is: [202.28042602539062, 495.5790710449219, -141.26902770996094, 19.780364990234375, -936.43994140625]\n",
      "\n",
      "Instance 896 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6, 22]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 6: [0.2983093857765198, -1.3938289880752563, 1.3416272401809692, 0.3631948232650757, 0.45766520500183105]\n",
      "forever at index 22: [2.392847090959549e-05, -0.13203713297843933, 1.0214550495147705, 0.45712339878082275, 1.3922576904296875]\n",
      "Grand sum of 622 tensor sets is: [202.42959594726562, 494.8161315917969, -140.08749389648438, 20.19052505493164, -935.5149536132812]\n",
      "\n",
      "Instance 897 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 14: [0.21717102825641632, 1.0272125005722046, 0.001334354281425476, 0.6784761548042297, 0.5428685545921326]\n",
      "Grand sum of 623 tensor sets is: [202.6467742919922, 495.8433532714844, -140.08616638183594, 20.869001388549805, -934.9721069335938]\n",
      "\n",
      "Instance 898 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "forever at index 3: [2.1085829734802246, -0.22018083930015564, -0.41315287351608276, 0.9463722109794617, 2.9587104320526123]\n",
      "Grand sum of 624 tensor sets is: [204.75535583496094, 495.6231689453125, -140.4993133544922, 21.81537437438965, -932.0133666992188]\n",
      "\n",
      "Instance 899 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 900 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 13: [0.42454877495765686, 1.2648096084594727, -0.8870201706886292, 0.5397659540176392, -1.100439429283142]\n",
      "Grand sum of 625 tensor sets is: [205.17990112304688, 496.8879699707031, -141.38633728027344, 22.355140686035156, -933.1138305664062]\n",
      "\n",
      "Instance 901 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 8: [-0.6515558362007141, -1.8557324409484863, 1.3475651741027832, 0.5839675068855286, -3.438915729522705]\n",
      "Grand sum of 626 tensor sets is: [204.52835083007812, 495.0322265625, -140.0387725830078, 22.93910789489746, -936.552734375]\n",
      "\n",
      "Instance 902 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 903 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 17: [1.8084579706192017, 0.08259857445955276, -1.3978650569915771, -0.25340536236763, 1.3263893127441406]\n",
      "Grand sum of 627 tensor sets is: [206.33680725097656, 495.1148376464844, -141.43663024902344, 22.68570327758789, -935.226318359375]\n",
      "\n",
      "Instance 904 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 13: [-0.6407610774040222, -0.3413890302181244, -0.6233797073364258, -0.6735564470291138, -1.0540176630020142]\n",
      "Grand sum of 628 tensor sets is: [205.696044921875, 494.7734375, -142.0600128173828, 22.01214599609375, -936.2803344726562]\n",
      "\n",
      "Instance 905 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 906 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 4: [-0.532880961894989, 1.17573881149292, 0.5018175840377808, -1.0266380310058594, -1.1968932151794434]\n",
      "Grand sum of 629 tensor sets is: [205.1631622314453, 495.9491882324219, -141.55819702148438, 20.98550796508789, -937.4772338867188]\n",
      "\n",
      "Instance 907 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 908 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 13: [-0.7455206513404846, 2.019754648208618, 1.1563960313796997, 0.26860278844833374, 0.37923663854599]\n",
      "Grand sum of 630 tensor sets is: [204.4176483154297, 497.96893310546875, -140.40179443359375, 21.25411033630371, -937.0980224609375]\n",
      "\n",
      "Instance 909 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 910 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [81, 154, 255, 298]\n",
      "Size of token embeddings is torch.Size([349, 13, 768])\n",
      "Shape of summed layers is: 349 x 768\n",
      "forever at index 81: [0.7712836265563965, -0.58416748046875, 0.46315085887908936, -0.03638163208961487, -2.115854501724243]\n",
      "forever at index 154: [0.6865423917770386, -0.7097386121749878, 0.598432719707489, -0.17060551047325134, -3.5942940711975098]\n",
      "forever at index 255: [0.4310741424560547, -0.2135126292705536, 0.40495216846466064, 0.026279032230377197, -3.8636951446533203]\n",
      "forever at index 298: [0.4929978847503662, -0.512580156326294, 0.3862549662590027, -0.49734434485435486, -4.049206256866455]\n",
      "Grand sum of 631 tensor sets is: [205.01312255859375, 497.46392822265625, -139.9385986328125, 21.084596633911133, -940.5037841796875]\n",
      "\n",
      "Instance 911 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 912 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 22: [0.13064539432525635, 2.5441982746124268, 0.9098130464553833, 1.5171351432800293, 2.7001898288726807]\n",
      "Grand sum of 632 tensor sets is: [205.14376831054688, 500.00811767578125, -139.02877807617188, 22.60173225402832, -937.8035888671875]\n",
      "\n",
      "Instance 913 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 39: [0.19261828064918518, -0.3796582818031311, -0.33681121468544006, -0.2852908670902252, -3.456049680709839]\n",
      "Grand sum of 633 tensor sets is: [205.3363800048828, 499.6284484863281, -139.36558532714844, 22.31644058227539, -941.2596435546875]\n",
      "\n",
      "Instance 914 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [60]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "forever at index 60: [-0.24782000482082367, 1.124830961227417, 0.8421822190284729, -0.36910197138786316, -1.9056727886199951]\n",
      "Grand sum of 634 tensor sets is: [205.08856201171875, 500.7532653808594, -138.52340698242188, 21.947338104248047, -943.1653442382812]\n",
      "\n",
      "Instance 915 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 916 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 917 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 28: [0.46433740854263306, 3.675095558166504, -1.5387306213378906, 2.001286745071411, -1.7253692150115967]\n",
      "Grand sum of 635 tensor sets is: [205.5529022216797, 504.4283752441406, -140.0621337890625, 23.948625564575195, -944.8906860351562]\n",
      "\n",
      "Instance 918 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 919 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 920 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 921 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [50]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 50: [0.05228728801012039, 0.4952118396759033, 0.9535128474235535, 1.7425158023834229, -0.43452188372612]\n",
      "Grand sum of 636 tensor sets is: [205.60519409179688, 504.923583984375, -139.10862731933594, 25.69114112854004, -945.3251953125]\n",
      "\n",
      "Instance 922 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 923 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 924 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 47: [0.48658251762390137, 1.981176733970642, -0.0849275216460228, 1.2615631818771362, -0.14378494024276733]\n",
      "Grand sum of 637 tensor sets is: [206.09178161621094, 506.9047546386719, -139.1935577392578, 26.95270347595215, -945.468994140625]\n",
      "\n",
      "Instance 925 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 926 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 19: [0.5640385150909424, 0.6420689821243286, -0.36865317821502686, 0.15522584319114685, -1.6072447299957275]\n",
      "Grand sum of 638 tensor sets is: [206.65582275390625, 507.54681396484375, -139.5622100830078, 27.107929229736328, -947.0762329101562]\n",
      "\n",
      "Instance 927 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 53: [0.666554868221283, -0.7873085737228394, -1.431145191192627, 1.6047497987747192, -2.4387478828430176]\n",
      "Grand sum of 639 tensor sets is: [207.32237243652344, 506.7594909667969, -140.9933624267578, 28.712678909301758, -949.5149536132812]\n",
      "\n",
      "Instance 928 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 929 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 18: [0.23628199100494385, 0.234147846698761, 0.3020831346511841, 1.5849838256835938, -3.6293108463287354]\n",
      "Grand sum of 640 tensor sets is: [207.55865478515625, 506.99365234375, -140.6912841796875, 30.29766273498535, -953.144287109375]\n",
      "\n",
      "Instance 930 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 10: [0.09152780473232269, 0.44090303778648376, -0.7119274735450745, 1.209414005279541, -4.417181015014648]\n",
      "Grand sum of 641 tensor sets is: [207.65017700195312, 507.4345703125, -141.40321350097656, 31.507076263427734, -957.5614624023438]\n",
      "\n",
      "Instance 931 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 932 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 933 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 18: [0.26137349009513855, 1.450752854347229, 0.15038534998893738, -0.8947299718856812, -1.7591055631637573]\n",
      "Grand sum of 642 tensor sets is: [207.9115447998047, 508.88531494140625, -141.25282287597656, 30.612346649169922, -959.320556640625]\n",
      "\n",
      "Instance 934 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([87, 13, 768])\n",
      "Shape of summed layers is: 87 x 768\n",
      "forever at index 17: [0.6314178109169006, 0.595920741558075, -0.04904396831989288, -1.9062787294387817, -0.17208734154701233]\n",
      "Grand sum of 643 tensor sets is: [208.54296875, 509.4812316894531, -141.30186462402344, 28.70606803894043, -959.4926147460938]\n",
      "\n",
      "Instance 935 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [78]\n",
      "Size of token embeddings is torch.Size([90, 13, 768])\n",
      "Shape of summed layers is: 90 x 768\n",
      "forever at index 78: [0.8413739204406738, 3.277523994445801, -0.29552823305130005, 3.6899023056030273, 0.3256022334098816]\n",
      "Grand sum of 644 tensor sets is: [209.38433837890625, 512.7587280273438, -141.59739685058594, 32.39596939086914, -959.1669921875]\n",
      "\n",
      "Instance 936 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 937 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 6: [0.3335062265396118, 0.3925465941429138, -0.061049677431583405, 2.1731157302856445, 0.12482381612062454]\n",
      "Grand sum of 645 tensor sets is: [209.7178497314453, 513.1512451171875, -141.658447265625, 34.56908416748047, -959.0421752929688]\n",
      "\n",
      "Instance 938 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 16: [0.17890754342079163, 0.8132357597351074, 0.5112795829772949, 3.022967576980591, -1.7655515670776367]\n",
      "Grand sum of 646 tensor sets is: [209.89675903320312, 513.9644775390625, -141.1471710205078, 37.5920524597168, -960.8077392578125]\n",
      "\n",
      "Instance 939 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 50: [-0.33926016092300415, 0.8958812952041626, 0.5922505259513855, -2.1911110877990723, -2.576876401901245]\n",
      "Grand sum of 647 tensor sets is: [209.5574951171875, 514.8603515625, -140.55491638183594, 35.40093994140625, -963.3846435546875]\n",
      "\n",
      "Instance 940 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 53: [0.666554868221283, -0.7873085737228394, -1.431145191192627, 1.6047497987747192, -2.4387478828430176]\n",
      "Grand sum of 648 tensor sets is: [210.2240447998047, 514.0730590820312, -141.98606872558594, 37.00569152832031, -965.8233642578125]\n",
      "\n",
      "Instance 941 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 23: [0.139860600233078, 0.8729767799377441, -1.0277200937271118, 0.5902076363563538, -3.758173942565918]\n",
      "Grand sum of 649 tensor sets is: [210.36390686035156, 514.946044921875, -143.0137939453125, 37.59589767456055, -969.58154296875]\n",
      "\n",
      "Instance 942 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 650 tensor sets is: [210.90780639648438, 514.3112182617188, -143.66334533691406, 36.45367431640625, -968.460205078125]\n",
      "\n",
      "Instance 943 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 944 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([96, 13, 768])\n",
      "Shape of summed layers is: 96 x 768\n",
      "forever at index 11: [0.22260445356369019, 1.2347253561019897, 0.35957837104797363, 0.5578352212905884, -2.8624496459960938]\n",
      "Grand sum of 651 tensor sets is: [211.1304168701172, 515.5459594726562, -143.30377197265625, 37.01150894165039, -971.3226318359375]\n",
      "\n",
      "Instance 945 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 652 tensor sets is: [211.67431640625, 514.9111328125, -143.9533233642578, 35.869285583496094, -970.2012939453125]\n",
      "\n",
      "Instance 946 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 10: [-0.5831184983253479, 0.08082115650177002, -0.19026507437229156, 0.36846640706062317, -2.2450621128082275]\n",
      "Grand sum of 653 tensor sets is: [211.09120178222656, 514.991943359375, -144.14358520507812, 36.23775100708008, -972.4463500976562]\n",
      "\n",
      "Instance 947 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "forever at index 47: [0.21457499265670776, 0.6532703638076782, 0.11238284409046173, 0.45271992683410645, -2.2243497371673584]\n",
      "Grand sum of 654 tensor sets is: [211.30577087402344, 515.6452026367188, -144.0312042236328, 36.69047164916992, -974.6707153320312]\n",
      "\n",
      "Instance 948 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 28: [0.9444878697395325, -0.12415841966867447, 0.19712412357330322, 0.022887345403432846, -1.9902994632720947]\n",
      "Grand sum of 655 tensor sets is: [212.25025939941406, 515.5210571289062, -143.83407592773438, 36.71335983276367, -976.6610107421875]\n",
      "\n",
      "Instance 949 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 11: [0.18490977585315704, 2.725903034210205, 0.3038412928581238, 1.3165322542190552, -1.7090939283370972]\n",
      "Grand sum of 656 tensor sets is: [212.43516540527344, 518.2469482421875, -143.5302276611328, 38.02989196777344, -978.3701171875]\n",
      "\n",
      "Instance 950 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 657 tensor sets is: [211.79974365234375, 519.96875, -142.79771423339844, 39.335670471191406, -978.5468139648438]\n",
      "\n",
      "Instance 951 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 952 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 953 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 954 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 3: [0.32117903232574463, 0.23560544848442078, -0.000872097909450531, -0.8400305509567261, -2.1225438117980957]\n",
      "Grand sum of 658 tensor sets is: [212.1209259033203, 520.204345703125, -142.798583984375, 38.49563980102539, -980.6693725585938]\n",
      "\n",
      "Instance 955 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 27: [-0.5189328789710999, 1.7928359508514404, 0.3700985610485077, 0.5733770728111267, -2.233680486679077]\n",
      "Grand sum of 659 tensor sets is: [211.60198974609375, 521.9971923828125, -142.42848205566406, 39.06901550292969, -982.903076171875]\n",
      "\n",
      "Instance 956 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2, 26]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 2: [0.09778758138418198, -2.4058289527893066, 1.1624819040298462, -1.1082245111465454, -0.501395583152771]\n",
      "forever at index 26: [-0.6597148180007935, -2.2088937759399414, 0.5281460285186768, -2.600926399230957, -2.3813602924346924]\n",
      "Grand sum of 660 tensor sets is: [211.32102966308594, 519.6898193359375, -141.5831756591797, 37.214439392089844, -984.344482421875]\n",
      "\n",
      "Instance 957 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 27: [0.7286797165870667, 2.1721646785736084, -1.6473852396011353, 2.294894218444824, -3.5823240280151367]\n",
      "Grand sum of 661 tensor sets is: [212.04971313476562, 521.8619995117188, -143.23056030273438, 39.509334564208984, -987.9268188476562]\n",
      "\n",
      "Instance 958 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 9: [0.16780094802379608, 0.3673933148384094, -0.6137426495552063, 1.908109426498413, -4.417931079864502]\n",
      "Grand sum of 662 tensor sets is: [212.21751403808594, 522.2293701171875, -143.84429931640625, 41.417442321777344, -992.3447265625]\n",
      "\n",
      "Instance 959 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 3: [-0.7221001386642456, 0.6190845966339111, 0.48678433895111084, -1.3270319700241089, -0.6283646821975708]\n",
      "Grand sum of 663 tensor sets is: [211.4954071044922, 522.8484497070312, -143.35751342773438, 40.09041213989258, -992.9730834960938]\n",
      "\n",
      "Instance 960 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 27: [0.12000208348035812, 0.8244129419326782, -1.6659821271896362, -1.6001089811325073, -6.472155570983887]\n",
      "Grand sum of 664 tensor sets is: [211.6154022216797, 523.6728515625, -145.02349853515625, 38.49030303955078, -999.4452514648438]\n",
      "\n",
      "Instance 961 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 665 tensor sets is: [210.8806610107422, 524.0776977539062, -144.63182067871094, 38.133243560791016, -1004.006103515625]\n",
      "\n",
      "Instance 962 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 963 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14, 43]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "forever at index 14: [0.8046202659606934, -0.39251089096069336, 2.169816017150879, -0.04021545499563217, 1.2714555263519287]\n",
      "forever at index 43: [0.08527516573667526, -0.02320249378681183, 2.025545358657837, -0.17090700566768646, 1.03388249874115]\n",
      "Grand sum of 666 tensor sets is: [211.3256072998047, 523.8698120117188, -142.5341339111328, 38.02768325805664, -1002.8534545898438]\n",
      "\n",
      "Instance 964 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 12: [-0.03214439004659653, 2.175400733947754, 0.1128188893198967, -1.4242780208587646, -0.01424911618232727]\n",
      "Grand sum of 667 tensor sets is: [211.29345703125, 526.0452270507812, -142.4213104248047, 36.6034049987793, -1002.86767578125]\n",
      "\n",
      "Instance 965 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 668 tensor sets is: [210.5587158203125, 526.4500732421875, -142.02963256835938, 36.24634552001953, -1007.4285278320312]\n",
      "\n",
      "Instance 966 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 14: [0.3215697407722473, 2.4613430500030518, -1.1816314458847046, 3.231326103210449, -3.1638054847717285]\n",
      "Grand sum of 669 tensor sets is: [210.88027954101562, 528.9114379882812, -143.2112579345703, 39.4776725769043, -1010.5923461914062]\n",
      "\n",
      "Instance 967 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [-0.8170713782310486, -0.8822799921035767, -0.17178037762641907, -1.4751193523406982, -1.7810887098312378]\n",
      "Grand sum of 670 tensor sets is: [210.06320190429688, 528.0291748046875, -143.38304138183594, 38.0025520324707, -1012.3734130859375]\n",
      "\n",
      "Instance 968 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 969 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 5: [1.3509546518325806, 0.20243185758590698, 0.6785516738891602, 0.0292045995593071, 0.7717301845550537]\n",
      "Grand sum of 671 tensor sets is: [211.41415405273438, 528.2316284179688, -142.70448303222656, 38.03175735473633, -1011.6016845703125]\n",
      "\n",
      "Instance 970 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 971 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 2: [1.1141968965530396, -0.23476240038871765, -0.9401029944419861, 1.9605684280395508, -1.1663790941238403]\n",
      "Grand sum of 672 tensor sets is: [212.52835083007812, 527.9968872070312, -143.64459228515625, 39.99232482910156, -1012.76806640625]\n",
      "\n",
      "Instance 972 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 3: [1.4898580312728882, 0.922845184803009, -0.2545323967933655, 0.11926747858524323, 3.737919807434082]\n",
      "Grand sum of 673 tensor sets is: [214.01820373535156, 528.9197387695312, -143.8991241455078, 40.11159133911133, -1009.0301513671875]\n",
      "\n",
      "Instance 973 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 5: [0.6022050380706787, 2.4245588779449463, -0.6253792643547058, 1.0847046375274658, 0.46901166439056396]\n",
      "Grand sum of 674 tensor sets is: [214.6204071044922, 531.3442993164062, -144.52450561523438, 41.19629669189453, -1008.5611572265625]\n",
      "\n",
      "Instance 974 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 975 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 3: [-0.25560691952705383, 0.22481021285057068, 0.536686897277832, -0.6660206913948059, -4.2832207679748535]\n",
      "Grand sum of 675 tensor sets is: [214.36480712890625, 531.569091796875, -143.98782348632812, 40.530277252197266, -1012.8443603515625]\n",
      "\n",
      "Instance 976 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 7: [0.41262415051460266, 0.31489428877830505, -0.8894286751747131, -0.6596914529800415, 1.0049713850021362]\n",
      "Grand sum of 676 tensor sets is: [214.77743530273438, 531.8839721679688, -144.87725830078125, 39.87058639526367, -1011.8394165039062]\n",
      "\n",
      "Instance 977 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 677 tensor sets is: [215.1681671142578, 532.9578247070312, -145.1009063720703, 37.229637145996094, -1006.4666748046875]\n",
      "\n",
      "Instance 978 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 979 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 12: [-0.23704087734222412, 1.120847463607788, -0.41716426610946655, 0.12498195469379425, -1.0393435955047607]\n",
      "Grand sum of 678 tensor sets is: [214.93112182617188, 534.0786743164062, -145.51806640625, 37.354618072509766, -1007.5060424804688]\n",
      "\n",
      "Instance 980 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 981 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 22: [0.9892910718917847, 2.055809259414673, -0.2223549634218216, 1.243882417678833, -2.250450372695923]\n",
      "Grand sum of 679 tensor sets is: [215.92041015625, 536.1344604492188, -145.74041748046875, 38.5984992980957, -1009.7564697265625]\n",
      "\n",
      "Instance 982 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "forever at index 11: [-0.11545421928167343, 1.1809431314468384, 0.21833963692188263, -0.029730483889579773, -1.6489841938018799]\n",
      "Grand sum of 680 tensor sets is: [215.80496215820312, 537.3154296875, -145.52207946777344, 38.56876754760742, -1011.4054565429688]\n",
      "\n",
      "Instance 983 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 3: [1.0472584962844849, -0.061225369572639465, -0.5887565612792969, -1.0105656385421753, 1.8751033544540405]\n",
      "Grand sum of 681 tensor sets is: [216.8522186279297, 537.2542114257812, -146.11083984375, 37.55820083618164, -1009.5303344726562]\n",
      "\n",
      "Instance 984 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 985 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 986 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 682 tensor sets is: [216.1174774169922, 537.6590576171875, -145.7191619873047, 37.201141357421875, -1014.0911865234375]\n",
      "\n",
      "Instance 987 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 11: [-0.5674815773963928, 1.6017507314682007, -0.6321079134941101, 1.8035888671875, -0.8080515265464783]\n",
      "Grand sum of 683 tensor sets is: [215.5500030517578, 539.2608032226562, -146.3512725830078, 39.004730224609375, -1014.8992309570312]\n",
      "\n",
      "Instance 988 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 989 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 15: [0.05439005792140961, 2.4217529296875, -0.24046678841114044, 0.4205487072467804, -0.597629725933075]\n",
      "Grand sum of 684 tensor sets is: [215.60440063476562, 541.6825561523438, -146.59173583984375, 39.42527770996094, -1015.4968872070312]\n",
      "\n",
      "Instance 990 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 15: [0.1307583749294281, 0.9025817513465881, 0.5103603601455688, -1.4661997556686401, 1.0970535278320312]\n",
      "Grand sum of 685 tensor sets is: [215.7351531982422, 542.5851440429688, -146.0813751220703, 37.95907974243164, -1014.3998413085938]\n",
      "\n",
      "Instance 991 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 992 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 993 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 686 tensor sets is: [216.12588500976562, 543.6589965820312, -146.30502319335938, 35.31813049316406, -1009.027099609375]\n",
      "\n",
      "Instance 994 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 11: [-0.5674815773963928, 1.6017507314682007, -0.6321079134941101, 1.8035888671875, -0.8080515265464783]\n",
      "Grand sum of 687 tensor sets is: [215.55841064453125, 545.2607421875, -146.9371337890625, 37.12171936035156, -1009.8351440429688]\n",
      "\n",
      "Instance 995 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 688 tensor sets is: [216.39703369140625, 545.61669921875, -147.3387908935547, 37.635345458984375, -1007.2960815429688]\n",
      "\n",
      "Instance 996 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 13: [-0.604102373123169, 1.6049821376800537, -0.0004253983497619629, 0.13224533200263977, -3.6214866638183594]\n",
      "Grand sum of 689 tensor sets is: [215.79293823242188, 547.2216796875, -147.33921813964844, 37.7675895690918, -1010.9175415039062]\n",
      "\n",
      "Instance 997 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 22: [0.9892910718917847, 2.055809259414673, -0.2223549634218216, 1.243882417678833, -2.250450372695923]\n",
      "Grand sum of 690 tensor sets is: [216.7822265625, 549.2774658203125, -147.5615692138672, 39.011470794677734, -1013.16796875]\n",
      "\n",
      "Instance 998 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 999 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [1.6994589567184448, -0.21792569756507874, 0.2326381653547287, -0.911447286605835, -4.0184526443481445]\n",
      "Grand sum of 691 tensor sets is: [218.481689453125, 549.0595703125, -147.3289337158203, 38.10002517700195, -1017.1864013671875]\n",
      "\n",
      "Instance 1000 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1001 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1002 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1003 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 25: [0.5099854469299316, 2.128878116607666, -0.31610333919525146, -0.3696194291114807, 0.014716029167175293]\n",
      "Grand sum of 692 tensor sets is: [218.99166870117188, 551.1884765625, -147.64503479003906, 37.730403900146484, -1017.1716918945312]\n",
      "\n",
      "Instance 1004 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 18: [0.5168213248252869, 0.41990381479263306, 0.032717831432819366, -0.1111772209405899, 0.2530871629714966]\n",
      "Grand sum of 693 tensor sets is: [219.50848388671875, 551.6083984375, -147.61231994628906, 37.61922836303711, -1016.9185791015625]\n",
      "\n",
      "Instance 1005 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 14: [0.21105608344078064, 0.18813079595565796, -1.58627450466156, 1.9123402833938599, -0.5879236459732056]\n",
      "Grand sum of 694 tensor sets is: [219.71954345703125, 551.7965087890625, -149.19859313964844, 39.53157043457031, -1017.5065307617188]\n",
      "\n",
      "Instance 1006 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 15: [1.124272108078003, 1.4469236135482788, -0.6729863286018372, 0.5849395990371704, 2.2119181156158447]\n",
      "Grand sum of 695 tensor sets is: [220.84381103515625, 553.243408203125, -149.87158203125, 40.11650848388672, -1015.2946166992188]\n",
      "\n",
      "Instance 1007 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 12: [-0.5580449104309082, 0.7681972980499268, 0.25648412108421326, 0.33078843355178833, -0.9353766441345215]\n",
      "Grand sum of 696 tensor sets is: [220.2857666015625, 554.0115966796875, -149.61509704589844, 40.447296142578125, -1016.22998046875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 1008 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 22: [0.11140218377113342, 1.1916406154632568, 0.15353934466838837, 1.0046420097351074, -3.2607672214508057]\n",
      "Grand sum of 697 tensor sets is: [220.3971710205078, 555.2032470703125, -149.46156311035156, 41.45193862915039, -1019.49072265625]\n",
      "\n",
      "Instance 1009 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "forever at index 5: [-0.21105942130088806, 0.5350940227508545, 0.06783214211463928, 1.861238956451416, -2.9850101470947266]\n",
      "Grand sum of 698 tensor sets is: [220.1861114501953, 555.7383422851562, -149.39373779296875, 43.31317901611328, -1022.4757080078125]\n",
      "\n",
      "Instance 1010 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 6: [0.5420534014701843, -0.8461405038833618, 0.7453724145889282, 0.029333531856536865, -2.970989227294922]\n",
      "Grand sum of 699 tensor sets is: [220.72816467285156, 554.8922119140625, -148.6483612060547, 43.34251403808594, -1025.4466552734375]\n",
      "\n",
      "Instance 1011 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 31: [0.1898767650127411, 1.9215118885040283, -0.23067627847194672, -0.4244334101676941, 1.4234541654586792]\n",
      "Grand sum of 700 tensor sets is: [220.9180450439453, 556.813720703125, -148.87904357910156, 42.9180793762207, -1024.023193359375]\n",
      "\n",
      "Instance 1012 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 13: [0.004826916381716728, -1.6624531745910645, -0.8186843395233154, 0.521091103553772, -2.3986587524414062]\n",
      "Grand sum of 701 tensor sets is: [220.92286682128906, 555.1512451171875, -149.69772338867188, 43.439170837402344, -1026.421875]\n",
      "\n",
      "Instance 1013 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 702 tensor sets is: [220.18812561035156, 555.5560913085938, -149.30604553222656, 43.08211135864258, -1030.982666015625]\n",
      "\n",
      "Instance 1014 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1015 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 3: [-0.6080850958824158, -0.2653849124908447, -0.3608410954475403, -0.22817233204841614, -1.0893254280090332]\n",
      "Grand sum of 703 tensor sets is: [219.58004760742188, 555.2907104492188, -149.66688537597656, 42.853939056396484, -1032.072021484375]\n",
      "\n",
      "Instance 1016 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 10: [-0.673468828201294, 0.43764790892601013, 0.14720065891742706, -1.9015147686004639, -4.963374137878418]\n",
      "Grand sum of 704 tensor sets is: [218.90658569335938, 555.7283325195312, -149.51968383789062, 40.952423095703125, -1037.035400390625]\n",
      "\n",
      "Instance 1017 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 5: [0.8466788530349731, 1.4278494119644165, -0.4568178653717041, 0.8693965673446655, 1.8269867897033691]\n",
      "Grand sum of 705 tensor sets is: [219.75326538085938, 557.1561889648438, -149.97650146484375, 41.82181930541992, -1035.2083740234375]\n",
      "\n",
      "Instance 1018 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 25: [0.3977018892765045, 0.8428394794464111, 0.29637476801872253, 1.2666987180709839, 1.0663275718688965]\n",
      "Grand sum of 706 tensor sets is: [220.15097045898438, 557.9990234375, -149.6801300048828, 43.08851623535156, -1034.14208984375]\n",
      "\n",
      "Instance 1019 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 6: [-0.01778288185596466, 0.8714627027511597, -0.2927319407463074, -0.01691664755344391, -2.2472894191741943]\n",
      "Grand sum of 707 tensor sets is: [220.13319396972656, 558.8704833984375, -149.9728546142578, 43.071598052978516, -1036.389404296875]\n",
      "\n",
      "Instance 1020 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 14: [1.2601665258407593, 0.46573084592819214, -0.922072172164917, 1.1466914415359497, 0.6718304753303528]\n",
      "Grand sum of 708 tensor sets is: [221.3933563232422, 559.3362426757812, -150.89492797851562, 44.21828842163086, -1035.717529296875]\n",
      "\n",
      "Instance 1021 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 17: [-0.5617873668670654, 0.4221842586994171, -1.3765459060668945, 0.9743287563323975, 1.6986067295074463]\n",
      "Grand sum of 709 tensor sets is: [220.83157348632812, 559.7584228515625, -152.27146911621094, 45.1926155090332, -1034.0189208984375]\n",
      "\n",
      "Instance 1022 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 22: [-0.19705107808113098, 1.1939126253128052, -0.2782352566719055, 0.03318857401609421, -5.209828853607178]\n",
      "Grand sum of 710 tensor sets is: [220.634521484375, 560.9523315429688, -152.54969787597656, 45.22580337524414, -1039.228759765625]\n",
      "\n",
      "Instance 1023 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1024 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "forever at index 45: [-0.03050950914621353, 1.9428646564483643, 0.5014650821685791, 1.5456663370132446, -0.43702343106269836]\n",
      "Grand sum of 711 tensor sets is: [220.60401916503906, 562.8952026367188, -152.04823303222656, 46.77146911621094, -1039.665771484375]\n",
      "\n",
      "Instance 1025 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 4: [0.2367076724767685, 1.3047798871994019, 0.48435282707214355, -1.2190741300582886, -3.260852336883545]\n",
      "Grand sum of 712 tensor sets is: [220.84072875976562, 564.2000122070312, -151.56387329101562, 45.55239486694336, -1042.9266357421875]\n",
      "\n",
      "Instance 1026 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1027 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 3: [-0.7221001386642456, 0.6190845966339111, 0.48678433895111084, -1.3270319700241089, -0.6283646821975708]\n",
      "Grand sum of 713 tensor sets is: [220.11862182617188, 564.819091796875, -151.07708740234375, 44.225364685058594, -1043.5550537109375]\n",
      "\n",
      "Instance 1028 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 8: [0.173453226685524, 2.8473663330078125, -0.5223441123962402, 0.4197302460670471, -1.5900177955627441]\n",
      "Grand sum of 714 tensor sets is: [220.2920684814453, 567.6664428710938, -151.59942626953125, 44.64509582519531, -1045.14501953125]\n",
      "\n",
      "Instance 1029 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 715 tensor sets is: [219.65664672851562, 569.3882446289062, -150.86691284179688, 45.95087432861328, -1045.32177734375]\n",
      "\n",
      "Instance 1030 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1031 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 3: [1.2700467109680176, 0.37594783306121826, -0.47246140241622925, -0.2889596223831177, 1.5766273736953735]\n",
      "Grand sum of 716 tensor sets is: [220.92669677734375, 569.7642211914062, -151.33937072753906, 45.66191482543945, -1043.7451171875]\n",
      "\n",
      "Instance 1032 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 30: [0.6327057480812073, -0.3882387578487396, 1.2785422801971436, -0.6588475704193115, -0.7912854552268982]\n",
      "Grand sum of 717 tensor sets is: [221.5594024658203, 569.3759765625, -150.06082153320312, 45.00306701660156, -1044.536376953125]\n",
      "\n",
      "Instance 1033 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [1.2882497310638428, 0.9116852283477783, 0.06462778151035309, 1.80868661403656, 1.71641206741333]\n",
      "Grand sum of 718 tensor sets is: [222.84765625, 570.2876586914062, -149.99620056152344, 46.81175231933594, -1042.8199462890625]\n",
      "\n",
      "Instance 1034 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1035 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1036 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1037 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1038 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [59]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "forever at index 59: [0.06699778884649277, 0.17746688425540924, -0.3299524188041687, -0.9141986966133118, -0.28669771552085876]\n",
      "Grand sum of 719 tensor sets is: [222.91465759277344, 570.4651489257812, -150.32615661621094, 45.897552490234375, -1043.106689453125]\n",
      "\n",
      "Instance 1039 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 4: [0.35170862078666687, 0.9449154138565063, -0.04480019956827164, -1.258223295211792, 0.7472262382507324]\n",
      "Grand sum of 720 tensor sets is: [223.26637268066406, 571.4100341796875, -150.37095642089844, 44.63932800292969, -1042.3594970703125]\n",
      "\n",
      "Instance 1040 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1041 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1042 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 39: [0.19261828064918518, -0.3796582818031311, -0.33681121468544006, -0.2852908670902252, -3.456049680709839]\n",
      "Grand sum of 721 tensor sets is: [223.458984375, 571.0303955078125, -150.707763671875, 44.35403823852539, -1045.8155517578125]\n",
      "\n",
      "Instance 1043 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1044 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 12: [0.9291580319404602, -0.3393807113170624, 0.4780503511428833, 0.40903139114379883, -0.8514553904533386]\n",
      "Grand sum of 722 tensor sets is: [224.3881378173828, 570.6910400390625, -150.22970581054688, 44.76306915283203, -1046.6669921875]\n",
      "\n",
      "Instance 1045 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 4: [0.3562908470630646, 0.009750992059707642, -0.2890988290309906, 2.1051158905029297, 3.013333320617676]\n",
      "Grand sum of 723 tensor sets is: [224.7444305419922, 570.7008056640625, -150.518798828125, 46.868186950683594, -1043.6536865234375]\n",
      "\n",
      "Instance 1046 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 12: [-0.40850141644477844, 3.1121931076049805, -0.4035433828830719, -0.22838839888572693, 1.1846771240234375]\n",
      "Grand sum of 724 tensor sets is: [224.33592224121094, 573.81298828125, -150.92234802246094, 46.63979721069336, -1042.468994140625]\n",
      "\n",
      "Instance 1047 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 15: [0.9261385798454285, 1.4049715995788574, -0.25676143169403076, -1.232149362564087, 0.19312670826911926]\n",
      "Grand sum of 725 tensor sets is: [225.26205444335938, 575.2179565429688, -151.17910766601562, 45.40764617919922, -1042.27587890625]\n",
      "\n",
      "Instance 1048 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 38: [-0.28408360481262207, -1.0642828941345215, -0.48851510882377625, 0.09789957106113434, -2.4956204891204834]\n",
      "Grand sum of 726 tensor sets is: [224.97796630859375, 574.1536865234375, -151.66761779785156, 45.50554656982422, -1044.771484375]\n",
      "\n",
      "Instance 1049 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 16: [0.30324578285217285, 0.493397980928421, -0.4766228199005127, 0.17180414497852325, -4.456072807312012]\n",
      "Grand sum of 727 tensor sets is: [225.28121948242188, 574.6470947265625, -152.1442413330078, 45.67734909057617, -1049.2275390625]\n",
      "\n",
      "Instance 1050 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 18: [0.8934524655342102, 0.33417797088623047, 1.416666030883789, -1.058761477470398, -0.044785916805267334]\n",
      "Grand sum of 728 tensor sets is: [226.17466735839844, 574.9812622070312, -150.72756958007812, 44.618587493896484, -1049.2723388671875]\n",
      "\n",
      "Instance 1051 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [106]\n",
      "Size of token embeddings is torch.Size([163, 13, 768])\n",
      "Shape of summed layers is: 163 x 768\n",
      "forever at index 106: [0.021144505590200424, 0.6377125978469849, -0.17794647812843323, -1.0601458549499512, -0.3298482298851013]\n",
      "Grand sum of 729 tensor sets is: [226.19581604003906, 575.6189575195312, -150.905517578125, 43.558441162109375, -1049.6021728515625]\n",
      "\n",
      "Instance 1052 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 29: [0.38109520077705383, 1.1194099187850952, -0.06497020274400711, -0.21425600349903107, -0.1119680106639862]\n",
      "Grand sum of 730 tensor sets is: [226.576904296875, 576.7383422851562, -150.97048950195312, 43.34418487548828, -1049.714111328125]\n",
      "\n",
      "Instance 1053 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 13: [-0.3818025290966034, -1.8624720573425293, -0.8241699934005737, -0.13120873272418976, 1.376434326171875]\n",
      "Grand sum of 731 tensor sets is: [226.19509887695312, 574.8758544921875, -151.79466247558594, 43.212974548339844, -1048.337646484375]\n",
      "\n",
      "Instance 1054 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1055 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 732 tensor sets is: [226.33316040039062, 575.530029296875, -152.23524475097656, 42.915653228759766, -1051.18798828125]\n",
      "\n",
      "Instance 1056 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 23: [0.33503878116607666, 2.049495220184326, -0.598809003829956, -0.40061673521995544, -0.22894245386123657]\n",
      "Grand sum of 733 tensor sets is: [226.66819763183594, 577.5795288085938, -152.8340606689453, 42.515037536621094, -1051.4168701171875]\n",
      "\n",
      "Instance 1057 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [-0.3898774981498718, 0.9123032093048096, -0.16742947697639465, 0.8552608489990234, 0.06718266010284424]\n",
      "Grand sum of 734 tensor sets is: [226.2783203125, 578.4918212890625, -153.00149536132812, 43.37030029296875, -1051.3497314453125]\n",
      "\n",
      "Instance 1058 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 9: [0.00775802880525589, -0.4664250314235687, -1.2065333127975464, 0.47917211055755615, -4.387584686279297]\n",
      "Grand sum of 735 tensor sets is: [226.28607177734375, 578.025390625, -154.20802307128906, 43.84947204589844, -1055.7373046875]\n",
      "\n",
      "Instance 1059 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 20: [-0.15736931562423706, 1.7162656784057617, 0.48248913884162903, -0.7240594625473022, -3.0724663734436035]\n",
      "Grand sum of 736 tensor sets is: [226.1287078857422, 579.7416381835938, -153.7255401611328, 43.12541198730469, -1058.809814453125]\n",
      "\n",
      "Instance 1060 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1061 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 27: [-0.35635626316070557, 2.0575835704803467, -0.3960023522377014, -0.4091857671737671, -7.869691371917725]\n",
      "Grand sum of 737 tensor sets is: [225.77235412597656, 581.7991943359375, -154.1215362548828, 42.716224670410156, -1066.6795654296875]\n",
      "\n",
      "Instance 1062 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 5: [0.3338528573513031, -1.2330166101455688, -0.6983059644699097, -0.0277060866355896, -4.413846015930176]\n",
      "Grand sum of 738 tensor sets is: [226.106201171875, 580.566162109375, -154.81983947753906, 42.68851852416992, -1071.0933837890625]\n",
      "\n",
      "Instance 1063 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [0.7601019144058228, 1.7670985460281372, 0.6998885273933411, 1.0222638845443726, -0.9792816638946533]\n",
      "Grand sum of 739 tensor sets is: [226.86630249023438, 582.333251953125, -154.1199493408203, 43.71078109741211, -1072.0726318359375]\n",
      "\n",
      "Instance 1064 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 10: [0.017273113131523132, 1.1723227500915527, -0.8395887613296509, -0.20554271340370178, -5.4766845703125]\n",
      "Grand sum of 740 tensor sets is: [226.88357543945312, 583.5055541992188, -154.95953369140625, 43.5052375793457, -1077.54931640625]\n",
      "\n",
      "Instance 1065 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 7: [0.8735243678092957, 0.3204411268234253, 0.07081387937068939, -0.7019412517547607, -3.0947420597076416]\n",
      "Grand sum of 741 tensor sets is: [227.75709533691406, 583.8259887695312, -154.8887176513672, 42.80329513549805, -1080.64404296875]\n",
      "\n",
      "Instance 1066 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 40: [0.33491891622543335, 1.7919116020202637, 0.13084109127521515, 0.2979971170425415, -1.9822839498519897]\n",
      "Grand sum of 742 tensor sets is: [228.09201049804688, 585.617919921875, -154.75787353515625, 43.10129165649414, -1082.6263427734375]\n",
      "\n",
      "Instance 1067 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 9: [-0.3284541666507721, 1.6693605184555054, -1.4137835502624512, 0.2971886992454529, -0.48168522119522095]\n",
      "Grand sum of 743 tensor sets is: [227.7635498046875, 587.2872924804688, -156.17166137695312, 43.39847946166992, -1083.1080322265625]\n",
      "\n",
      "Instance 1068 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1069 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 3: [0.5939326882362366, 0.464658260345459, -0.26392802596092224, 1.075594425201416, -4.111481666564941]\n",
      "Grand sum of 744 tensor sets is: [228.35748291015625, 587.751953125, -156.4355926513672, 44.47407531738281, -1087.219482421875]\n",
      "\n",
      "Instance 1070 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 12: [0.5041528940200806, 0.23832197487354279, -0.3613663911819458, 0.9017897844314575, -2.626415729522705]\n",
      "Grand sum of 745 tensor sets is: [228.86163330078125, 587.9902954101562, -156.79696655273438, 45.3758659362793, -1089.845947265625]\n",
      "\n",
      "Instance 1071 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1072 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1073 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 746 tensor sets is: [230.65296936035156, 586.4356689453125, -157.4792022705078, 44.52994155883789, -1089.3929443359375]\n",
      "\n",
      "Instance 1074 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 11: [0.9333776235580444, 0.1445973813533783, -0.07370059192180634, -0.3390940725803375, -1.5737152099609375]\n",
      "Grand sum of 747 tensor sets is: [231.5863494873047, 586.5802612304688, -157.5529022216797, 44.19084930419922, -1090.9666748046875]\n",
      "\n",
      "Instance 1075 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1076 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 12: [0.5500937700271606, 1.0123604536056519, -0.6112232208251953, 1.8724040985107422, 0.6658028960227966]\n",
      "Grand sum of 748 tensor sets is: [232.13644409179688, 587.5926513671875, -158.16412353515625, 46.063255310058594, -1090.3009033203125]\n",
      "\n",
      "Instance 1077 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "forever at index 65: [0.27338430285453796, -1.4315751791000366, 0.04108026251196861, -0.0735112726688385, -1.3981804847717285]\n",
      "Grand sum of 749 tensor sets is: [232.4098358154297, 586.1610717773438, -158.123046875, 45.989742279052734, -1091.6990966796875]\n",
      "\n",
      "Instance 1078 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 5: [1.1412562131881714, -0.18592238426208496, -0.6386037468910217, 0.1457490622997284, 0.9073255062103271]\n",
      "Grand sum of 750 tensor sets is: [233.55108642578125, 585.9751586914062, -158.76165771484375, 46.13549041748047, -1090.791748046875]\n",
      "\n",
      "Instance 1079 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1080 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 26: [0.37860313057899475, -0.8008484840393066, -0.2810317873954773, 0.23155392706394196, -1.51703679561615]\n",
      "Grand sum of 751 tensor sets is: [233.9296875, 585.17431640625, -159.04269409179688, 46.367042541503906, -1092.308837890625]\n",
      "\n",
      "Instance 1081 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 10: [-0.5864143371582031, 1.5583298206329346, 0.24704165756702423, -0.5418176651000977, -2.1007561683654785]\n",
      "Grand sum of 752 tensor sets is: [233.34327697753906, 586.732666015625, -158.795654296875, 45.825225830078125, -1094.4095458984375]\n",
      "\n",
      "Instance 1082 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1083 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [116]\n",
      "Size of token embeddings is torch.Size([462, 13, 768])\n",
      "Shape of summed layers is: 462 x 768\n",
      "forever at index 116: [0.23765015602111816, -2.0466861724853516, -0.8134642243385315, -1.0985758304595947, -2.5121848583221436]\n",
      "Grand sum of 753 tensor sets is: [233.5809326171875, 584.6859741210938, -159.60911560058594, 44.72665023803711, -1096.9217529296875]\n",
      "\n",
      "Instance 1084 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 16: [-0.8589853048324585, 2.337599515914917, 0.5998232960700989, -0.8309499025344849, -2.5316362380981445]\n",
      "Grand sum of 754 tensor sets is: [232.72195434570312, 587.0235595703125, -159.00929260253906, 43.89569854736328, -1099.453369140625]\n",
      "\n",
      "Instance 1085 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1086 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1087 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [0.7610797882080078, 0.832751452922821, -0.4022613763809204, -0.6445860862731934, -1.7941073179244995]\n",
      "Grand sum of 755 tensor sets is: [233.4830322265625, 587.8563232421875, -159.41156005859375, 43.25111389160156, -1101.2474365234375]\n",
      "\n",
      "Instance 1088 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 22: [-0.10093848407268524, 1.7860114574432373, -0.0004442557692527771, 0.2716578543186188, -0.2915533185005188]\n",
      "Grand sum of 756 tensor sets is: [233.38209533691406, 589.642333984375, -159.41200256347656, 43.522769927978516, -1101.5389404296875]\n",
      "\n",
      "Instance 1089 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 757 tensor sets is: [233.4835205078125, 591.3816528320312, -160.05001831054688, 42.407386779785156, -1105.8355712890625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 1090 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1091 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 17: [0.33422285318374634, -0.8155922293663025, -0.537778913974762, -0.07142467796802521, -3.95515775680542]\n",
      "Grand sum of 758 tensor sets is: [233.8177490234375, 590.5660400390625, -160.58779907226562, 42.335960388183594, -1109.790771484375]\n",
      "\n",
      "Instance 1092 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1093 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 25: [0.28167080879211426, 2.2542130947113037, 0.47150206565856934, 1.5483787059783936, -0.557115912437439]\n",
      "Grand sum of 759 tensor sets is: [234.09942626953125, 592.8202514648438, -160.11630249023438, 43.88433837890625, -1110.347900390625]\n",
      "\n",
      "Instance 1094 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([70, 13, 768])\n",
      "Shape of summed layers is: 70 x 768\n",
      "forever at index 9: [1.6644511222839355, -0.13930441439151764, -0.2492387741804123, -1.6878472566604614, 1.3475441932678223]\n",
      "Grand sum of 760 tensor sets is: [235.7638702392578, 592.6809692382812, -160.36553955078125, 42.19649124145508, -1109.0003662109375]\n",
      "\n",
      "Instance 1095 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([93, 13, 768])\n",
      "Shape of summed layers is: 93 x 768\n",
      "forever at index 38: [-1.1777037382125854, -0.2888803780078888, -0.2941676676273346, -0.5361020565032959, -3.4033100605010986]\n",
      "Grand sum of 761 tensor sets is: [234.58616638183594, 592.39208984375, -160.6597137451172, 41.6603889465332, -1112.4036865234375]\n",
      "\n",
      "Instance 1096 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1097 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 3: [0.5601764917373657, 2.4101593494415283, -0.532785177230835, 0.29369205236434937, -1.2736234664916992]\n",
      "Grand sum of 762 tensor sets is: [235.14634704589844, 594.80224609375, -161.1925048828125, 41.95408248901367, -1113.6773681640625]\n",
      "\n",
      "Instance 1098 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19, 26]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 19: [0.4227013885974884, -1.5703544616699219, 0.7075956463813782, -0.062207043170928955, 1.2094228267669678]\n",
      "forever at index 26: [0.2825496792793274, 0.07104995101690292, 0.30602583289146423, -1.6216111183166504, -1.7337663173675537]\n",
      "Grand sum of 763 tensor sets is: [235.4989776611328, 594.0526123046875, -160.68569946289062, 41.11217498779297, -1113.9395751953125]\n",
      "\n",
      "Instance 1099 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([93, 13, 768])\n",
      "Shape of summed layers is: 93 x 768\n",
      "forever at index 38: [-1.1777037382125854, -0.2888803780078888, -0.2941676676273346, -0.5361020565032959, -3.4033100605010986]\n",
      "Grand sum of 764 tensor sets is: [234.32127380371094, 593.7637329101562, -160.97987365722656, 40.576072692871094, -1117.3428955078125]\n",
      "\n",
      "Instance 1100 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1101 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 30: [-0.10385038703680038, 2.636639356613159, -0.7258002758026123, 0.15143448114395142, -0.13338175415992737]\n",
      "Grand sum of 765 tensor sets is: [234.21742248535156, 596.400390625, -161.70567321777344, 40.727508544921875, -1117.476318359375]\n",
      "\n",
      "Instance 1102 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 3: [0.05890386179089546, 0.3130619525909424, 0.2134251892566681, -1.435978651046753, -2.4240705966949463]\n",
      "Grand sum of 766 tensor sets is: [234.2763214111328, 596.7134399414062, -161.49224853515625, 39.29153060913086, -1119.900390625]\n",
      "\n",
      "Instance 1103 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1104 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 8: [0.15089261531829834, -0.1330181062221527, 0.5545244216918945, -0.2366374135017395, -0.017552614212036133]\n",
      "Grand sum of 767 tensor sets is: [234.42721557617188, 596.5804443359375, -160.93772888183594, 39.054893493652344, -1119.91796875]\n",
      "\n",
      "Instance 1105 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1106 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 5: [0.6192998290061951, -0.4194578230381012, -0.7460154294967651, -0.22186371684074402, 1.7287235260009766]\n",
      "Grand sum of 768 tensor sets is: [235.0465087890625, 596.1610107421875, -161.68374633789062, 38.833030700683594, -1118.189208984375]\n",
      "\n",
      "Instance 1107 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 6: [0.013345018029212952, 0.42145097255706787, -1.3005874156951904, -0.586449921131134, 0.005586385726928711]\n",
      "Grand sum of 769 tensor sets is: [235.0598602294922, 596.5824584960938, -162.9843292236328, 38.24658203125, -1118.18359375]\n",
      "\n",
      "Instance 1108 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 14: [0.600108802318573, -0.6396060585975647, 0.7405812740325928, 0.7529125213623047, -7.439220428466797]\n",
      "Grand sum of 770 tensor sets is: [235.65997314453125, 595.94287109375, -162.24374389648438, 38.99949645996094, -1125.622802734375]\n",
      "\n",
      "Instance 1109 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8, 20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 8: [1.108292818069458, -0.8982845544815063, -0.03486398234963417, 1.0533432960510254, -0.1114509105682373]\n",
      "forever at index 20: [0.6671366095542908, 0.44678765535354614, 0.45562827587127686, 1.6862297058105469, 0.09918069839477539]\n",
      "Grand sum of 771 tensor sets is: [236.5476837158203, 595.7171020507812, -162.03335571289062, 40.36928176879883, -1125.62890625]\n",
      "\n",
      "Instance 1110 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 50: [-0.11947064101696014, 0.6304356455802917, -0.1565760374069214, 1.5401872396469116, -1.7556185722351074]\n",
      "Grand sum of 772 tensor sets is: [236.42820739746094, 596.3475341796875, -162.18992614746094, 41.90946960449219, -1127.384521484375]\n",
      "\n",
      "Instance 1111 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1112 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1113 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1114 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1115 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 24: [0.8234068751335144, 2.824092149734497, -0.6316527128219604, 0.7275674343109131, 1.9463088512420654]\n",
      "Grand sum of 773 tensor sets is: [237.25161743164062, 599.171630859375, -162.8215789794922, 42.63703536987305, -1125.438232421875]\n",
      "\n",
      "Instance 1116 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1117 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 26: [0.7530208826065063, -1.809462547302246, -1.6850566864013672, 0.6257853507995605, 0.10847741365432739]\n",
      "Grand sum of 774 tensor sets is: [238.004638671875, 597.3621826171875, -164.5066375732422, 43.262821197509766, -1125.3297119140625]\n",
      "\n",
      "Instance 1118 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 10: [-0.003194287419319153, 3.0020017623901367, -0.9934726357460022, -0.005951300263404846, -0.08032992482185364]\n",
      "Grand sum of 775 tensor sets is: [238.00144958496094, 600.3641967773438, -165.50010681152344, 43.25687026977539, -1125.4100341796875]\n",
      "\n",
      "Instance 1119 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1120 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 26: [0.6136788725852966, 0.950835645198822, -0.35505130887031555, -1.5896496772766113, -0.32106801867485046]\n",
      "Grand sum of 776 tensor sets is: [238.61512756347656, 601.3150024414062, -165.85516357421875, 41.66722106933594, -1125.7310791015625]\n",
      "\n",
      "Instance 1121 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 29: [-0.05433337762951851, 1.7457032203674316, 0.3413127660751343, -0.18681737780570984, -5.073459148406982]\n",
      "Grand sum of 777 tensor sets is: [238.560791015625, 603.0607299804688, -165.51385498046875, 41.480403900146484, -1130.8045654296875]\n",
      "\n",
      "Instance 1122 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1123 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1124 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [56]\n",
      "Size of token embeddings is torch.Size([90, 13, 768])\n",
      "Shape of summed layers is: 90 x 768\n",
      "forever at index 56: [0.2800257205963135, 0.3119567036628723, -0.009472876787185669, -1.1043049097061157, -0.3244854211807251]\n",
      "Grand sum of 778 tensor sets is: [238.8408203125, 603.3726806640625, -165.52333068847656, 40.3760986328125, -1131.1290283203125]\n",
      "\n",
      "Instance 1125 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 3: [1.314339518547058, -0.46997585892677307, -0.24610655009746552, -1.3101853132247925, -0.44219571352005005]\n",
      "Grand sum of 779 tensor sets is: [240.15516662597656, 602.9027099609375, -165.76943969726562, 39.065914154052734, -1131.5711669921875]\n",
      "\n",
      "Instance 1126 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 26: [0.28025341033935547, 1.5528875589370728, 0.21808718144893646, 0.12596984207630157, -1.8625587224960327]\n",
      "Grand sum of 780 tensor sets is: [240.4354248046875, 604.4556274414062, -165.5513458251953, 39.1918830871582, -1133.4337158203125]\n",
      "\n",
      "Instance 1127 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1128 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1129 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 5: [-0.014664754271507263, -1.2344988584518433, 0.22723159193992615, 2.0939176082611084, -2.6420178413391113]\n",
      "Grand sum of 781 tensor sets is: [240.42076110839844, 603.2211303710938, -165.32411193847656, 41.28580093383789, -1136.07568359375]\n",
      "\n",
      "Instance 1130 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1131 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [91]\n",
      "Size of token embeddings is torch.Size([190, 13, 768])\n",
      "Shape of summed layers is: 190 x 768\n",
      "forever at index 91: [0.29785364866256714, -0.541795015335083, -1.7222260236740112, 0.8259334564208984, 1.3608709573745728]\n",
      "Grand sum of 782 tensor sets is: [240.71861267089844, 602.6793212890625, -167.0463409423828, 42.111732482910156, -1134.71484375]\n",
      "\n",
      "Instance 1132 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 8: [0.173453226685524, 2.8473663330078125, -0.5223441123962402, 0.4197302460670471, -1.5900177955627441]\n",
      "Grand sum of 783 tensor sets is: [240.89205932617188, 605.5266723632812, -167.5686798095703, 42.531463623046875, -1136.3048095703125]\n",
      "\n",
      "Instance 1133 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 9: [-0.3373768925666809, 2.8877124786376953, -0.18541233241558075, 1.273181676864624, -1.198625087738037]\n",
      "Grand sum of 784 tensor sets is: [240.5546875, 608.4143676757812, -167.75408935546875, 43.80464553833008, -1137.50341796875]\n",
      "\n",
      "Instance 1134 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 785 tensor sets is: [241.0985870361328, 607.779541015625, -168.4036407470703, 42.66242218017578, -1136.382080078125]\n",
      "\n",
      "Instance 1135 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 31: [0.2986893653869629, 3.0456411838531494, 0.09486352652311325, 0.8314635157585144, 0.3126234710216522]\n",
      "Grand sum of 786 tensor sets is: [241.39727783203125, 610.8251953125, -168.30877685546875, 43.4938850402832, -1136.0694580078125]\n",
      "\n",
      "Instance 1136 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1137 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1138 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [111]\n",
      "Size of token embeddings is torch.Size([122, 13, 768])\n",
      "Shape of summed layers is: 122 x 768\n",
      "forever at index 111: [0.10411998629570007, 1.400869369506836, -0.9842519164085388, -1.6438045501708984, -2.135253429412842]\n",
      "Grand sum of 787 tensor sets is: [241.50140380859375, 612.22607421875, -169.29302978515625, 41.85008239746094, -1138.2047119140625]\n",
      "\n",
      "Instance 1139 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 11: [0.35741859674453735, 0.9952375292778015, -0.23764678835868835, -0.054786860942840576, -7.530919075012207]\n",
      "Grand sum of 788 tensor sets is: [241.85882568359375, 613.2213134765625, -169.53067016601562, 41.79529571533203, -1145.735595703125]\n",
      "\n",
      "Instance 1140 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1141 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 10: [0.7015156149864197, 1.6863141059875488, 0.4860362410545349, -1.0204066038131714, 3.1100289821624756]\n",
      "Grand sum of 789 tensor sets is: [242.5603485107422, 614.9076538085938, -169.0446319580078, 40.7748908996582, -1142.6256103515625]\n",
      "\n",
      "Instance 1142 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1143 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 10: [0.965645968914032, -1.5705327987670898, -0.5791462659835815, 0.5382994413375854, -2.021357536315918]\n",
      "Grand sum of 790 tensor sets is: [243.5260009765625, 613.3370971679688, -169.623779296875, 41.31319046020508, -1144.64697265625]\n",
      "\n",
      "Instance 1144 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [-0.15920022130012512, 2.5161070823669434, 0.06372873485088348, -0.12602898478507996, -2.1756668090820312]\n",
      "Grand sum of 791 tensor sets is: [243.36680603027344, 615.8532104492188, -169.56004333496094, 41.18716049194336, -1146.8226318359375]\n",
      "\n",
      "Instance 1145 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 3: [0.4799607992172241, -0.822348952293396, -0.6614682078361511, 0.43917691707611084, -4.608087062835693]\n",
      "Grand sum of 792 tensor sets is: [243.84677124023438, 615.0308837890625, -170.2215118408203, 41.626338958740234, -1151.4306640625]\n",
      "\n",
      "Instance 1146 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 18: [0.23628199100494385, 0.234147846698761, 0.3020831346511841, 1.5849838256835938, -3.6293108463287354]\n",
      "Grand sum of 793 tensor sets is: [244.0830535888672, 615.2650146484375, -169.91943359375, 43.21132278442383, -1155.0599365234375]\n",
      "\n",
      "Instance 1147 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 4: [0.0006432980298995972, 1.204034447669983, -0.21375451982021332, 0.9234002828598022, 0.619876503944397]\n",
      "Grand sum of 794 tensor sets is: [244.0836944580078, 616.4690551757812, -170.13319396972656, 44.13472366333008, -1154.4400634765625]\n",
      "\n",
      "Instance 1148 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 3: [-0.22351723909378052, 1.8713332414627075, -0.2686646580696106, 0.4703209102153778, -4.461219310760498]\n",
      "Grand sum of 795 tensor sets is: [243.8601837158203, 618.3403930664062, -170.40185546875, 44.605045318603516, -1158.9012451171875]\n",
      "\n",
      "Instance 1149 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([84, 13, 768])\n",
      "Shape of summed layers is: 84 x 768\n",
      "forever at index 53: [0.5326882600784302, 1.0144591331481934, 0.1098579615354538, -1.438484787940979, -0.16275063157081604]\n",
      "Grand sum of 796 tensor sets is: [244.3928680419922, 619.3548583984375, -170.2919921875, 43.166561126708984, -1159.06396484375]\n",
      "\n",
      "Instance 1150 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [1.0301811695098877, 0.09672722220420837, -1.850982904434204, 0.9610074162483215, -2.2480239868164062]\n",
      "Grand sum of 797 tensor sets is: [245.4230499267578, 619.4515991210938, -172.14297485351562, 44.127567291259766, -1161.31201171875]\n",
      "\n",
      "Instance 1151 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1152 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1153 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([93, 13, 768])\n",
      "Shape of summed layers is: 93 x 768\n",
      "forever at index 38: [-1.1777037382125854, -0.2888803780078888, -0.2941676676273346, -0.5361020565032959, -3.4033100605010986]\n",
      "Grand sum of 798 tensor sets is: [244.24534606933594, 619.1627197265625, -172.43714904785156, 43.59146499633789, -1164.71533203125]\n",
      "\n",
      "Instance 1154 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 6: [0.11573264002799988, 0.6206953525543213, 0.08805956691503525, -1.5067120790481567, -2.556149959564209]\n",
      "Grand sum of 799 tensor sets is: [244.361083984375, 619.7833862304688, -172.34909057617188, 42.08475112915039, -1167.271484375]\n",
      "\n",
      "Instance 1155 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 800 tensor sets is: [244.9049835205078, 619.1485595703125, -172.99864196777344, 40.942527770996094, -1166.150146484375]\n",
      "\n",
      "Instance 1156 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [0.7601019144058228, 1.7670985460281372, 0.6998885273933411, 1.0222638845443726, -0.9792816638946533]\n",
      "Grand sum of 801 tensor sets is: [245.6650848388672, 620.9156494140625, -172.2987518310547, 41.96479034423828, -1167.12939453125]\n",
      "\n",
      "Instance 1157 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1158 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 5: [0.1588454693555832, 0.19027554988861084, -1.560716152191162, 0.6493700742721558, 1.1241559982299805]\n",
      "Grand sum of 802 tensor sets is: [245.8239288330078, 621.1058959960938, -173.85946655273438, 42.614158630371094, -1166.0052490234375]\n",
      "\n",
      "Instance 1159 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 22: [-0.3670539855957031, 0.0011832118034362793, -1.2915315628051758, -0.5052425265312195, -0.6545729637145996]\n",
      "Grand sum of 803 tensor sets is: [245.45687866210938, 621.1070556640625, -175.1510009765625, 42.108917236328125, -1166.6597900390625]\n",
      "\n",
      "Instance 1160 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 6: [-0.0659170001745224, 0.18661877512931824, -0.8259607553482056, 2.5733258724212646, -2.0173346996307373]\n",
      "Grand sum of 804 tensor sets is: [245.39096069335938, 621.293701171875, -175.97695922851562, 44.68224334716797, -1168.6771240234375]\n",
      "\n",
      "Instance 1161 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1162 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 6: [-0.18271899223327637, 0.5778391361236572, -1.5148038864135742, -0.37976282835006714, -4.164584636688232]\n",
      "Grand sum of 805 tensor sets is: [245.20823669433594, 621.8715209960938, -177.49176025390625, 44.3024787902832, -1172.8416748046875]\n",
      "\n",
      "Instance 1163 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1164 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 40: [0.4996374249458313, -0.28994956612586975, -0.46465548872947693, -2.637877941131592, -3.694197416305542]\n",
      "Grand sum of 806 tensor sets is: [245.70787048339844, 621.58154296875, -177.9564208984375, 41.66460037231445, -1176.535888671875]\n",
      "\n",
      "Instance 1165 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [1]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 1: [-0.8020803928375244, 0.5029280185699463, 0.8888673782348633, -1.107459545135498, 1.7448714971542358]\n",
      "Grand sum of 807 tensor sets is: [244.90579223632812, 622.08447265625, -177.0675506591797, 40.5571403503418, -1174.791015625]\n",
      "\n",
      "Instance 1166 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 808 tensor sets is: [245.44969177246094, 621.4496459960938, -177.71710205078125, 39.4149169921875, -1173.669677734375]\n",
      "\n",
      "Instance 1167 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 13: [0.11582419276237488, 1.1704378128051758, 0.3097184896469116, -0.8037848472595215, -2.3743531703948975]\n",
      "Grand sum of 809 tensor sets is: [245.56552124023438, 622.6200561523438, -177.40737915039062, 38.61113357543945, -1176.0440673828125]\n",
      "\n",
      "Instance 1168 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 10: [-0.15022647380828857, 0.6428577899932861, 0.42895323038101196, 0.41826295852661133, -2.744920015335083]\n",
      "Grand sum of 810 tensor sets is: [245.41529846191406, 623.262939453125, -176.97842407226562, 39.029396057128906, -1178.7889404296875]\n",
      "\n",
      "Instance 1169 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 15: [1.0502023696899414, 2.9821829795837402, 0.5346555709838867, 3.0718231201171875, -0.5228773951530457]\n",
      "Grand sum of 811 tensor sets is: [246.4654998779297, 626.2451171875, -176.4437713623047, 42.101219177246094, -1179.311767578125]\n",
      "\n",
      "Instance 1170 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1171 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [44, 79]\n",
      "Size of token embeddings is torch.Size([133, 13, 768])\n",
      "Shape of summed layers is: 133 x 768\n",
      "forever at index 44: [0.016630105674266815, -2.06002140045166, -0.7008528709411621, -1.4638872146606445, 2.12259840965271]\n",
      "forever at index 79: [0.5449973344802856, 1.1515415906906128, -0.044403694570064545, 0.8498622179031372, -0.4592420756816864]\n",
      "Grand sum of 812 tensor sets is: [246.74630737304688, 625.7908935546875, -176.81640625, 41.79420852661133, -1178.4801025390625]\n",
      "\n",
      "Instance 1172 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1173 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 3: [0.3385359048843384, 0.22741970419883728, 0.402500182390213, -0.4802960753440857, -2.368980646133423]\n",
      "Grand sum of 813 tensor sets is: [247.0848388671875, 626.018310546875, -176.41390991210938, 41.31391143798828, -1180.84912109375]\n",
      "\n",
      "Instance 1174 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 20: [0.8062273263931274, 2.8112878799438477, 0.2870252728462219, -1.345106840133667, 1.796799898147583]\n",
      "Grand sum of 814 tensor sets is: [247.8910675048828, 628.82958984375, -176.12689208984375, 39.96880340576172, -1179.0523681640625]\n",
      "\n",
      "Instance 1175 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 27: [0.2560088336467743, -0.5837509632110596, -0.5129121541976929, 0.9563289284706116, -5.077194690704346]\n",
      "Grand sum of 815 tensor sets is: [248.14707946777344, 628.245849609375, -176.63980102539062, 40.925132751464844, -1184.1295166015625]\n",
      "\n",
      "Instance 1176 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1177 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1178 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1179 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 17: [0.10504913330078125, 0.5698050260543823, -0.33563393354415894, -0.858928918838501, -3.173602819442749]\n",
      "Grand sum of 816 tensor sets is: [248.25213623046875, 628.815673828125, -176.97543334960938, 40.06620407104492, -1187.3031005859375]\n",
      "\n",
      "Instance 1180 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 19: [-0.29690396785736084, 0.5254038572311401, -1.3335943222045898, -0.6180360317230225, -5.0448102951049805]\n",
      "Grand sum of 817 tensor sets is: [247.95523071289062, 629.341064453125, -178.30902099609375, 39.44816970825195, -1192.347900390625]\n",
      "\n",
      "Instance 1181 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1182 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 818 tensor sets is: [248.35093688964844, 630.3497314453125, -178.37860107421875, 38.76460647583008, -1191.0516357421875]\n",
      "\n",
      "Instance 1183 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 4: [0.3901885449886322, 2.3366050720214844, -0.7134852409362793, 0.7530984282493591, 1.2703804969787598]\n",
      "Grand sum of 819 tensor sets is: [248.74111938476562, 632.6863403320312, -179.0920867919922, 39.517704010009766, -1189.78125]\n",
      "\n",
      "Instance 1184 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "forever at index 34: [1.1498994827270508, -0.6347489356994629, -0.26229047775268555, -1.1489098072052002, -5.753459453582764]\n",
      "Grand sum of 820 tensor sets is: [249.89102172851562, 632.0515747070312, -179.3543701171875, 38.36879348754883, -1195.53466796875]\n",
      "\n",
      "Instance 1185 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 9: [1.6709872484207153, -0.040044695138931274, -2.4553208351135254, -1.1866576671600342, -2.026268482208252]\n",
      "Grand sum of 821 tensor sets is: [251.56201171875, 632.0115356445312, -181.8096923828125, 37.18213653564453, -1197.5609130859375]\n",
      "\n",
      "Instance 1186 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "forever at index 33: [-0.07609444856643677, -0.10075649619102478, -0.27123621106147766, -0.6588248014450073, -0.8558635115623474]\n",
      "Grand sum of 822 tensor sets is: [251.4859161376953, 631.9107666015625, -182.0809326171875, 36.523311614990234, -1198.416748046875]\n",
      "\n",
      "Instance 1187 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 16: [0.2767907381057739, 1.2694132328033447, -0.4388395845890045, -0.37906399369239807, -1.1787409782409668]\n",
      "Grand sum of 823 tensor sets is: [251.76271057128906, 633.18017578125, -182.519775390625, 36.144248962402344, -1199.595458984375]\n",
      "\n",
      "Instance 1188 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1189 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 824 tensor sets is: [252.45748901367188, 634.3134765625, -181.31663513183594, 34.87488555908203, -1200.4857177734375]\n",
      "\n",
      "Instance 1190 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 7: [-0.07563738524913788, 0.4690820574760437, -0.9565166234970093, -0.7230218052864075, -2.634429454803467]\n",
      "Grand sum of 825 tensor sets is: [252.38185119628906, 634.7825317382812, -182.2731475830078, 34.15186309814453, -1203.1201171875]\n",
      "\n",
      "Instance 1191 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "forever at index 46: [0.7099274396896362, 1.3501956462860107, 0.14756979048252106, 1.6073074340820312, 2.1684892177581787]\n",
      "Grand sum of 826 tensor sets is: [253.09178161621094, 636.1327514648438, -182.12557983398438, 35.75917053222656, -1200.95166015625]\n",
      "\n",
      "Instance 1192 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1193 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 5: [0.9449121356010437, 0.5821481943130493, 0.18746080994606018, 1.0183961391448975, 0.06548896431922913]\n",
      "Grand sum of 827 tensor sets is: [254.0366973876953, 636.7149047851562, -181.93812561035156, 36.777565002441406, -1200.88623046875]\n",
      "\n",
      "Instance 1194 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1195 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 4: [-0.07390961050987244, 2.185049533843994, 0.22666983306407928, -1.3678489923477173, 0.8578280210494995]\n",
      "Grand sum of 828 tensor sets is: [253.96278381347656, 638.8999633789062, -181.71145629882812, 35.40971755981445, -1200.0284423828125]\n",
      "\n",
      "Instance 1196 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 26: [-0.9898470044136047, 0.25101903080940247, -0.8326247930526733, 1.0976895093917847, -4.337558269500732]\n",
      "Grand sum of 829 tensor sets is: [252.97293090820312, 639.1510009765625, -182.54408264160156, 36.507408142089844, -1204.365966796875]\n",
      "\n",
      "Instance 1197 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 19: [-0.0619477704167366, 2.22285795211792, 0.8686966300010681, 0.5488012433052063, -2.841062307357788]\n",
      "Grand sum of 830 tensor sets is: [252.91098022460938, 641.3738403320312, -181.67538452148438, 37.056209564208984, -1207.20703125]\n",
      "\n",
      "Instance 1198 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 10: [0.7584355473518372, -0.16352814435958862, 0.19690290093421936, -0.8029332160949707, -2.318375587463379]\n",
      "Grand sum of 831 tensor sets is: [253.66941833496094, 641.2103271484375, -181.47848510742188, 36.25327682495117, -1209.525390625]\n",
      "\n",
      "Instance 1199 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 832 tensor sets is: [253.77084350585938, 642.9496459960938, -182.1165008544922, 35.13789367675781, -1213.822021484375]\n",
      "\n",
      "Instance 1200 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 25: [0.2739219069480896, 0.5808514356613159, -0.1160622388124466, -1.478981852531433, -4.579621315002441]\n",
      "Grand sum of 833 tensor sets is: [254.04476928710938, 643.530517578125, -182.23255920410156, 33.658912658691406, -1218.401611328125]\n",
      "\n",
      "Instance 1201 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [73, 306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([366, 13, 768])\n",
      "Shape of summed layers is: 366 x 768\n",
      "forever at index 73: [-0.2401716262102127, 2.2776107788085938, -0.08971258997917175, 0.43860936164855957, -0.1759650707244873]\n",
      "forever at index 306: [0.04323674738407135, 2.133942127227783, -0.3427940011024475, -0.28511229157447815, -3.5771842002868652]\n",
      "Grand sum of 834 tensor sets is: [253.94630432128906, 645.7362670898438, -182.4488067626953, 33.735660552978516, -1220.2781982421875]\n",
      "\n",
      "Instance 1202 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1203 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 5: [0.48542577028274536, 1.2541369199752808, -1.407567024230957, 1.5818785429000854, -4.121397018432617]\n",
      "Grand sum of 835 tensor sets is: [254.43173217773438, 646.9904174804688, -183.8563690185547, 35.31753921508789, -1224.3995361328125]\n",
      "\n",
      "Instance 1204 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1205 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 4: [0.7037645578384399, 1.6744920015335083, -0.6147169470787048, 0.9604010581970215, -2.68399977684021]\n",
      "Grand sum of 836 tensor sets is: [255.135498046875, 648.6649169921875, -184.47108459472656, 36.27793884277344, -1227.08349609375]\n",
      "\n",
      "Instance 1206 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1207 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 29: [-0.7432736754417419, 0.39003321528434753, -1.1465339660644531, -0.49671924114227295, -3.3246891498565674]\n",
      "Grand sum of 837 tensor sets is: [254.39222717285156, 649.054931640625, -185.61761474609375, 35.781219482421875, -1230.408203125]\n",
      "\n",
      "Instance 1208 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [64]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "forever at index 64: [0.523896336555481, -1.347061038017273, -0.5596960783004761, -1.804034948348999, -3.1485958099365234]\n",
      "Grand sum of 838 tensor sets is: [254.91612243652344, 647.7078857421875, -186.17730712890625, 33.9771842956543, -1233.5567626953125]\n",
      "\n",
      "Instance 1209 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1210 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1211 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1212 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 30: [0.02663789689540863, 1.8673721551895142, -0.9335192441940308, -0.9731557369232178, -1.9721131324768066]\n",
      "Grand sum of 839 tensor sets is: [254.94276428222656, 649.5752563476562, -187.11082458496094, 33.0040283203125, -1235.5289306640625]\n",
      "\n",
      "Instance 1213 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 11: [-0.23031939566135406, 0.1300508975982666, -0.024886637926101685, 0.7884435653686523, -4.2321295738220215]\n",
      "Grand sum of 840 tensor sets is: [254.7124481201172, 649.705322265625, -187.13571166992188, 33.79247283935547, -1239.7611083984375]\n",
      "\n",
      "Instance 1214 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 3: [1.5629125833511353, -0.6846093535423279, -0.7997795343399048, -0.7897446155548096, 1.4620361328125]\n",
      "Grand sum of 841 tensor sets is: [256.2753601074219, 649.0206909179688, -187.93548583984375, 33.00272750854492, -1238.299072265625]\n",
      "\n",
      "Instance 1215 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 9: [0.6344406604766846, 1.6315698623657227, 0.4478471279144287, -1.5792721509933472, -5.181726455688477]\n",
      "Grand sum of 842 tensor sets is: [256.9097900390625, 650.6522827148438, -187.48764038085938, 31.4234561920166, -1243.4808349609375]\n",
      "\n",
      "Instance 1216 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 24: [1.0735924243927002, 0.7960044145584106, -0.37208420038223267, -1.7589421272277832, -6.567999839782715]\n",
      "Grand sum of 843 tensor sets is: [257.9833679199219, 651.4483032226562, -187.85972595214844, 29.664514541625977, -1250.048828125]\n",
      "\n",
      "Instance 1217 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1218 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1219 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 10: [1.5420037508010864, 0.5990746021270752, -0.552238404750824, 1.1692397594451904, -2.3368043899536133]\n",
      "Grand sum of 844 tensor sets is: [259.5253601074219, 652.04736328125, -188.41195678710938, 30.83375358581543, -1252.3856201171875]\n",
      "\n",
      "Instance 1220 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [73]\n",
      "Size of token embeddings is torch.Size([502, 13, 768])\n",
      "Shape of summed layers is: 502 x 768\n",
      "forever at index 73: [-0.4114202857017517, 1.2488502264022827, 0.9239303469657898, 0.5437033772468567, -0.7385613322257996]\n",
      "Grand sum of 845 tensor sets is: [259.11395263671875, 653.2962036132812, -187.48802185058594, 31.377456665039062, -1253.1241455078125]\n",
      "\n",
      "Instance 1221 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 18: [-0.34996646642684937, 1.189697504043579, -1.9847074747085571, 0.02247941493988037, -5.916849136352539]\n",
      "Grand sum of 846 tensor sets is: [258.76397705078125, 654.4859008789062, -189.4727325439453, 31.39993667602539, -1259.041015625]\n",
      "\n",
      "Instance 1222 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 13: [0.4114418625831604, 1.1240171194076538, -0.319175124168396, 0.5671614408493042, 0.11368386447429657]\n",
      "Grand sum of 847 tensor sets is: [259.1754150390625, 655.6099243164062, -189.79190063476562, 31.967098236083984, -1258.9273681640625]\n",
      "\n",
      "Instance 1223 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([78, 13, 768])\n",
      "Shape of summed layers is: 78 x 768\n",
      "forever at index 13: [0.2407979965209961, -0.17049762606620789, -0.029205873608589172, 0.75691819190979, -2.7755680084228516]\n",
      "Grand sum of 848 tensor sets is: [259.41619873046875, 655.439453125, -189.82110595703125, 32.72401809692383, -1261.702880859375]\n",
      "\n",
      "Instance 1224 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [168]\n",
      "Size of token embeddings is torch.Size([325, 13, 768])\n",
      "Shape of summed layers is: 325 x 768\n",
      "forever at index 168: [0.22241482138633728, 1.3674943447113037, -0.475735068321228, 0.758650004863739, -4.7400689125061035]\n",
      "Grand sum of 849 tensor sets is: [259.63861083984375, 656.8069458007812, -190.29684448242188, 33.482669830322266, -1266.4429931640625]\n",
      "\n",
      "Instance 1225 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1226 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 35: [-0.8352028727531433, 1.3812878131866455, 0.15275518596172333, -1.3601136207580566, -0.1853448450565338]\n",
      "Grand sum of 850 tensor sets is: [258.80340576171875, 658.188232421875, -190.1440887451172, 32.122554779052734, -1266.6282958984375]\n",
      "\n",
      "Instance 1227 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [0.5422123670578003, -0.37714433670043945, -1.0815092325210571, 0.34070926904678345, -2.6716675758361816]\n",
      "Grand sum of 851 tensor sets is: [259.3456115722656, 657.8110961914062, -191.22560119628906, 32.46326446533203, -1269.2999267578125]\n",
      "\n",
      "Instance 1228 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 2: [1.1141968965530396, -0.23476240038871765, -0.9401029944419861, 1.9605684280395508, -1.1663790941238403]\n",
      "Grand sum of 852 tensor sets is: [260.4598083496094, 657.5763549804688, -192.16571044921875, 34.423831939697266, -1270.46630859375]\n",
      "\n",
      "Instance 1229 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1230 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1231 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 42: [1.5460656881332397, -0.17537465691566467, 1.3580548763275146, -0.09351888298988342, 1.986833095550537]\n",
      "Grand sum of 853 tensor sets is: [262.005859375, 657.4010009765625, -190.8076629638672, 34.33031463623047, -1268.4794921875]\n",
      "\n",
      "Instance 1232 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 20: [-0.9336524605751038, -1.08174729347229, -0.7083820104598999, -0.9412457942962646, -1.4941601753234863]\n",
      "Grand sum of 854 tensor sets is: [261.07220458984375, 656.3192749023438, -191.51605224609375, 33.389068603515625, -1269.9736328125]\n",
      "\n",
      "Instance 1233 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [0.9832466840744019, 0.19009733200073242, -0.32321181893348694, 0.42473360896110535, 0.5483078360557556]\n",
      "Grand sum of 855 tensor sets is: [262.0554504394531, 656.5093994140625, -191.83926391601562, 33.81380081176758, -1269.42529296875]\n",
      "\n",
      "Instance 1234 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1235 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "forever at index 6: [0.6685612201690674, 1.1300132274627686, -1.6400933265686035, 0.49850982427597046, -5.162750244140625]\n",
      "Grand sum of 856 tensor sets is: [262.7239990234375, 657.639404296875, -193.47935485839844, 34.31230926513672, -1274.5880126953125]\n",
      "\n",
      "Instance 1236 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1237 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 6: [1.3005774021148682, 0.15465453267097473, -0.22541363537311554, -0.9007368087768555, -2.019552230834961]\n",
      "Grand sum of 857 tensor sets is: [264.0245666503906, 657.7940673828125, -193.70477294921875, 33.41157150268555, -1276.6075439453125]\n",
      "\n",
      "Instance 1238 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1239 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1240 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 37: [0.3767627477645874, 1.2150057554244995, -0.40364792943000793, -1.365105390548706, -1.5889462232589722]\n",
      "Grand sum of 858 tensor sets is: [264.4013366699219, 659.0090942382812, -194.10841369628906, 32.04646682739258, -1278.196533203125]\n",
      "\n",
      "Instance 1241 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 6: [0.27036136388778687, 1.5498942136764526, 0.38521963357925415, -0.8504982590675354, -4.2729291915893555]\n",
      "Grand sum of 859 tensor sets is: [264.67169189453125, 660.5589599609375, -193.7231903076172, 31.195968627929688, -1282.469482421875]\n",
      "\n",
      "Instance 1242 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1243 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 65: [-0.4072004556655884, -1.165315866470337, -1.116049885749817, -1.6974555253982544, -1.7651728391647339]\n",
      "Grand sum of 860 tensor sets is: [264.2644958496094, 659.3936157226562, -194.8392333984375, 29.498512268066406, -1284.234619140625]\n",
      "\n",
      "Instance 1244 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "forever at index 3: [0.05327366292476654, -1.2412495613098145, -0.5227065682411194, 2.1619515419006348, 2.5576298236846924]\n",
      "Grand sum of 861 tensor sets is: [264.3177795410156, 658.15234375, -195.3619384765625, 31.660463333129883, -1281.677001953125]\n",
      "\n",
      "Instance 1245 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1246 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 19: [0.6814960241317749, 2.8259117603302, -0.15198871493339539, 1.7890695333480835, -1.6233561038970947]\n",
      "Grand sum of 862 tensor sets is: [264.999267578125, 660.978271484375, -195.51393127441406, 33.44953155517578, -1283.3004150390625]\n",
      "\n",
      "Instance 1247 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 42: [0.6252152323722839, 0.10107462853193283, 0.32814085483551025, -3.465648889541626, -5.785473823547363]\n",
      "Grand sum of 863 tensor sets is: [265.6244812011719, 661.079345703125, -195.185791015625, 29.983882904052734, -1289.0859375]\n",
      "\n",
      "Instance 1248 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 4: [0.6740239262580872, 1.81118905544281, 1.0216253995895386, -1.2709662914276123, -0.24852022528648376]\n",
      "Grand sum of 864 tensor sets is: [266.2984924316406, 662.8905639648438, -194.16416931152344, 28.71291732788086, -1289.33447265625]\n",
      "\n",
      "Instance 1249 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1250 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 16: [0.5835648775100708, -0.6977444291114807, -0.4682287871837616, 0.40074285864830017, -2.8543694019317627]\n",
      "Grand sum of 865 tensor sets is: [266.8820495605469, 662.1928100585938, -194.6324005126953, 29.11366081237793, -1292.1888427734375]\n",
      "\n",
      "Instance 1251 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 5: [1.2801661491394043, 1.412265419960022, -0.05210113525390625, 0.8605825304985046, -1.6986912488937378]\n",
      "Grand sum of 866 tensor sets is: [268.1622009277344, 663.6051025390625, -194.68450927734375, 29.9742431640625, -1293.8875732421875]\n",
      "\n",
      "Instance 1252 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 867 tensor sets is: [269.95355224609375, 662.0504760742188, -195.3667449951172, 29.128318786621094, -1293.4345703125]\n",
      "\n",
      "Instance 1253 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 25: [-0.29436904191970825, 2.367140054702759, 0.07900681346654892, 0.11936947703361511, -0.9916372299194336]\n",
      "Grand sum of 868 tensor sets is: [269.6591796875, 664.4176025390625, -195.28773498535156, 29.24768829345703, -1294.4261474609375]\n",
      "\n",
      "Instance 1254 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 22: [0.28840765357017517, 1.579850673675537, 0.3439999520778656, -0.041129112243652344, -0.4002249836921692]\n",
      "Grand sum of 869 tensor sets is: [269.9476013183594, 665.9974365234375, -194.94374084472656, 29.206558227539062, -1294.826416015625]\n",
      "\n",
      "Instance 1255 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1256 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 6: [0.44430556893348694, 2.078617811203003, -1.4057191610336304, 0.9681902527809143, -1.6914286613464355]\n",
      "Grand sum of 870 tensor sets is: [270.39190673828125, 668.0760498046875, -196.34945678710938, 30.17474937438965, -1296.517822265625]\n",
      "\n",
      "Instance 1257 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [-0.6199858784675598, 1.3548524379730225, 0.3231000602245331, 2.669618606567383, -5.943930625915527]\n",
      "Grand sum of 871 tensor sets is: [269.77191162109375, 669.430908203125, -196.02635192871094, 32.84436798095703, -1302.4617919921875]\n",
      "\n",
      "Instance 1258 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1259 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "forever at index 33: [0.36935457587242126, 1.7818799018859863, 0.06176447123289108, -0.38573402166366577, 0.2531820833683014]\n",
      "Grand sum of 872 tensor sets is: [270.1412658691406, 671.2127685546875, -195.96458435058594, 32.45863342285156, -1302.2086181640625]\n",
      "\n",
      "Instance 1260 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 11: [0.03765258565545082, 0.801445484161377, 0.8542230725288391, -0.12458936870098114, -0.9831475019454956]\n",
      "Grand sum of 873 tensor sets is: [270.1789245605469, 672.0142211914062, -195.11036682128906, 32.33404541015625, -1303.1917724609375]\n",
      "\n",
      "Instance 1261 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1262 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 14: [0.21717102825641632, 1.0272125005722046, 0.001334354281425476, 0.6784761548042297, 0.5428685545921326]\n",
      "Grand sum of 874 tensor sets is: [270.3960876464844, 673.0414428710938, -195.10903930664062, 33.01251983642578, -1302.64892578125]\n",
      "\n",
      "Instance 1263 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 26: [0.37860313057899475, -0.8008484840393066, -0.2810317873954773, 0.23155392706394196, -1.51703679561615]\n",
      "Grand sum of 875 tensor sets is: [270.7746887207031, 672.2406005859375, -195.39007568359375, 33.24407196044922, -1304.166015625]\n",
      "\n",
      "Instance 1264 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "forever at index 22: [0.6365607976913452, 1.492620587348938, -0.9187334775924683, 0.43143990635871887, -3.4707634449005127]\n",
      "Grand sum of 876 tensor sets is: [271.4112548828125, 673.7332153320312, -196.30880737304688, 33.67551040649414, -1307.63671875]\n",
      "\n",
      "Instance 1265 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 30: [-0.3938977122306824, 2.333895444869995, -0.10837904363870621, -0.41284722089767456, -0.6024755239486694]\n",
      "Grand sum of 877 tensor sets is: [271.0173645019531, 676.067138671875, -196.4171905517578, 33.262664794921875, -1308.2391357421875]\n",
      "\n",
      "Instance 1266 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 28: [0.6911503076553345, 1.9049209356307983, 0.15113846957683563, 0.8103886246681213, -0.4799545407295227]\n",
      "Grand sum of 878 tensor sets is: [271.7085266113281, 677.9720458984375, -196.26605224609375, 34.073055267333984, -1308.7191162109375]\n",
      "\n",
      "Instance 1267 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1268 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 5: [0.8438927531242371, 1.0786609649658203, 0.031205222010612488, 1.0534193515777588, -4.412059783935547]\n",
      "Grand sum of 879 tensor sets is: [272.55242919921875, 679.0507202148438, -196.23484802246094, 35.1264762878418, -1313.1312255859375]\n",
      "\n",
      "Instance 1269 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [78]\n",
      "Size of token embeddings is torch.Size([269, 13, 768])\n",
      "Shape of summed layers is: 269 x 768\n",
      "forever at index 78: [-0.8362959623336792, 1.3877527713775635, -0.461038202047348, 1.362511396408081, 3.3866772651672363]\n",
      "Grand sum of 880 tensor sets is: [271.71612548828125, 680.4384765625, -196.69589233398438, 36.48898696899414, -1309.7445068359375]\n",
      "\n",
      "Instance 1270 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 18: [0.21055816113948822, 1.0222302675247192, -1.0073379278182983, 0.3582121729850769, -6.145112991333008]\n",
      "Grand sum of 881 tensor sets is: [271.92669677734375, 681.460693359375, -197.70323181152344, 36.847198486328125, -1315.8896484375]\n",
      "\n",
      "Instance 1271 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 20: [0.698106050491333, 0.9285587668418884, 0.14148208498954773, 0.41985008120536804, -5.235246658325195]\n",
      "Grand sum of 882 tensor sets is: [272.62481689453125, 682.3892822265625, -197.56175231933594, 37.26704788208008, -1321.1248779296875]\n",
      "\n",
      "Instance 1272 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 43: [0.5692609548568726, 1.2013427019119263, -1.1910302639007568, -0.8984402418136597, -0.0712243914604187]\n",
      "Grand sum of 883 tensor sets is: [273.194091796875, 683.5906372070312, -198.75277709960938, 36.36860656738281, -1321.196044921875]\n",
      "\n",
      "Instance 1273 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 884 tensor sets is: [273.58978271484375, 684.5993041992188, -198.82235717773438, 35.68504333496094, -1319.8997802734375]\n",
      "\n",
      "Instance 1274 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 13: [0.18004052340984344, 1.7283211946487427, 0.35098713636398315, -0.1875414103269577, -3.998912811279297]\n",
      "Grand sum of 885 tensor sets is: [273.76983642578125, 686.32763671875, -198.47137451171875, 35.497501373291016, -1323.898681640625]\n",
      "\n",
      "Instance 1275 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 11: [-0.04714576154947281, 1.5581154823303223, 0.622721791267395, 3.1942262649536133, 0.7621023654937744]\n",
      "Grand sum of 886 tensor sets is: [273.7226867675781, 687.8857421875, -197.84864807128906, 38.69172668457031, -1323.1365966796875]\n",
      "\n",
      "Instance 1276 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "forever at index 19: [0.2299095243215561, 2.5271973609924316, 0.847969651222229, 2.2347757816314697, 1.2160415649414062]\n",
      "Grand sum of 887 tensor sets is: [273.9526062011719, 690.4129638671875, -197.00067138671875, 40.9265022277832, -1321.9205322265625]\n",
      "\n",
      "Instance 1277 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 20: [-1.0332770347595215, 2.440816879272461, -1.1048206090927124, -0.876291811466217, -1.0043970346450806]\n",
      "Grand sum of 888 tensor sets is: [272.9193420410156, 692.853759765625, -198.10549926757812, 40.050209045410156, -1322.9249267578125]\n",
      "\n",
      "Instance 1278 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 8: [1.2000463008880615, 1.268071174621582, 0.010718397796154022, 0.13532419502735138, -2.1222612857818604]\n",
      "Grand sum of 889 tensor sets is: [274.119384765625, 694.121826171875, -198.09478759765625, 40.18553161621094, -1325.0472412109375]\n",
      "\n",
      "Instance 1279 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 13: [1.2330877780914307, 2.2382659912109375, -1.0750439167022705, -1.175226092338562, -0.9880310297012329]\n",
      "Grand sum of 890 tensor sets is: [275.35247802734375, 696.360107421875, -199.16983032226562, 39.01030731201172, -1326.0352783203125]\n",
      "\n",
      "Instance 1280 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1281 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 20: [0.4685673713684082, -1.0994218587875366, -0.014241397380828857, -2.4418156147003174, 0.3341516852378845]\n",
      "Grand sum of 891 tensor sets is: [275.821044921875, 695.2606811523438, -199.18406677246094, 36.5684928894043, -1325.701171875]\n",
      "\n",
      "Instance 1282 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([106, 13, 768])\n",
      "Shape of summed layers is: 106 x 768\n",
      "forever at index 50: [0.16063275933265686, 3.064882278442383, -0.13956615328788757, 1.9016424417495728, -0.9146145582199097]\n",
      "Grand sum of 892 tensor sets is: [275.981689453125, 698.3255615234375, -199.32363891601562, 38.47013473510742, -1326.6158447265625]\n",
      "\n",
      "Instance 1283 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 25: [-0.6801314353942871, 1.295346975326538, 0.36650550365448, 0.2508595585823059, -3.4538605213165283]\n",
      "Grand sum of 893 tensor sets is: [275.3015441894531, 699.6209106445312, -198.95713806152344, 38.72099304199219, -1330.0697021484375]\n",
      "\n",
      "Instance 1284 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 6: [0.44430556893348694, 2.078617811203003, -1.4057191610336304, 0.9681902527809143, -1.6914286613464355]\n",
      "Grand sum of 894 tensor sets is: [275.745849609375, 701.6995239257812, -200.36285400390625, 39.68918228149414, -1331.7611083984375]\n",
      "\n",
      "Instance 1285 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 8: [0.3341563940048218, -0.45155027508735657, -0.36073899269104004, -2.4051058292388916, -1.8670011758804321]\n",
      "Grand sum of 895 tensor sets is: [276.08001708984375, 701.2479858398438, -200.7235870361328, 37.28407669067383, -1333.6280517578125]\n",
      "\n",
      "Instance 1286 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 13: [-0.04012918472290039, 2.1536567211151123, 0.21173818409442902, 1.9154763221740723, -2.992147445678711]\n",
      "Grand sum of 896 tensor sets is: [276.0398864746094, 703.4016723632812, -200.51185607910156, 39.199554443359375, -1336.6202392578125]\n",
      "\n",
      "Instance 1287 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1288 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "forever at index 15: [0.041224196553230286, 0.4727190136909485, -1.810550332069397, 0.5664258599281311, -5.136180877685547]\n",
      "Grand sum of 897 tensor sets is: [276.08111572265625, 703.8743896484375, -202.32240295410156, 39.7659797668457, -1341.7564697265625]\n",
      "\n",
      "Instance 1289 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 15: [0.41094425320625305, 1.688399076461792, 0.7384616732597351, 0.4671602249145508, -0.7715574502944946]\n",
      "Grand sum of 898 tensor sets is: [276.4920654296875, 705.5628051757812, -201.5839385986328, 40.23313903808594, -1342.528076171875]\n",
      "\n",
      "Instance 1290 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1291 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 15: [-0.4766021966934204, 1.74265718460083, 0.30620285868644714, 0.2167445719242096, -3.2190253734588623]\n",
      "Grand sum of 899 tensor sets is: [276.0154724121094, 707.3054809570312, -201.27774047851562, 40.44988250732422, -1345.7470703125]\n",
      "\n",
      "Instance 1292 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1293 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 4: [0.021256737411022186, -2.063232898712158, -1.3741142749786377, 0.8098586797714233, -2.1835286617279053]\n",
      "Grand sum of 900 tensor sets is: [276.0367431640625, 705.2422485351562, -202.65185546875, 41.259742736816406, -1347.9305419921875]\n",
      "\n",
      "Instance 1294 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([93, 13, 768])\n",
      "Shape of summed layers is: 93 x 768\n",
      "forever at index 38: [-1.1777037382125854, -0.2888803780078888, -0.2941676676273346, -0.5361020565032959, -3.4033100605010986]\n",
      "Grand sum of 901 tensor sets is: [274.8590393066406, 704.953369140625, -202.94602966308594, 40.72364044189453, -1351.3338623046875]\n",
      "\n",
      "Instance 1295 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 14: [0.33168551325798035, 0.6055267453193665, -1.1701728105545044, 0.7475870847702026, 4.666085243225098]\n",
      "Grand sum of 902 tensor sets is: [275.19073486328125, 705.5588989257812, -204.11619567871094, 41.47122573852539, -1346.667724609375]\n",
      "\n",
      "Instance 1296 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 15: [0.08447905629873276, 1.1184141635894775, 1.2626866102218628, -0.3538554906845093, -0.8582282066345215]\n",
      "Grand sum of 903 tensor sets is: [275.27520751953125, 706.6773071289062, -202.853515625, 41.11737060546875, -1347.5260009765625]\n",
      "\n",
      "Instance 1297 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "forever at index 26: [1.2489303350448608, 1.0866363048553467, -1.0488841533660889, -0.14282746613025665, 2.9000048637390137]\n",
      "Grand sum of 904 tensor sets is: [276.5241394042969, 707.763916015625, -203.90240478515625, 40.974544525146484, -1344.6259765625]\n",
      "\n",
      "Instance 1298 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 37: [-0.13357192277908325, 2.9697277545928955, -0.41853833198547363, 2.7901008129119873, -2.214669704437256]\n",
      "Grand sum of 905 tensor sets is: [276.39056396484375, 710.733642578125, -204.32093811035156, 43.764644622802734, -1346.8406982421875]\n",
      "\n",
      "Instance 1299 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "forever at index 39: [0.47293514013290405, 0.8743501901626587, -0.23747868835926056, -0.19662964344024658, -2.3182320594787598]\n",
      "Grand sum of 906 tensor sets is: [276.8634948730469, 711.6079711914062, -204.55841064453125, 43.568016052246094, -1349.158935546875]\n",
      "\n",
      "Instance 1300 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 3: [-0.43819522857666016, -0.7502033710479736, -0.5464555621147156, 0.7693041563034058, 0.39141640067100525]\n",
      "Grand sum of 907 tensor sets is: [276.42529296875, 710.8577880859375, -205.10487365722656, 44.337318420410156, -1348.767578125]\n",
      "\n",
      "Instance 1301 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [1.6994589567184448, -0.21792569756507874, 0.2326381653547287, -0.911447286605835, -4.0184526443481445]\n",
      "Grand sum of 908 tensor sets is: [278.124755859375, 710.639892578125, -204.8722381591797, 43.425872802734375, -1352.7860107421875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 1302 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1303 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1304 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1305 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1306 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 909 tensor sets is: [278.2261657714844, 712.3792114257812, -205.51025390625, 42.310489654541016, -1357.0826416015625]\n",
      "\n",
      "Instance 1307 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 8: [0.15089261531829834, -0.1330181062221527, 0.5545244216918945, -0.2366374135017395, -0.017552614212036133]\n",
      "Grand sum of 910 tensor sets is: [278.3770446777344, 712.2462158203125, -204.9557342529297, 42.0738525390625, -1357.1002197265625]\n",
      "\n",
      "Instance 1308 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 11: [-0.5674815773963928, 1.6017507314682007, -0.6321079134941101, 1.8035888671875, -0.8080515265464783]\n",
      "Grand sum of 911 tensor sets is: [277.8095703125, 713.8479614257812, -205.5878448486328, 43.87744140625, -1357.9083251953125]\n",
      "\n",
      "Instance 1309 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 16: [0.06773807108402252, -0.40363970398902893, -0.82701575756073, -0.7246317863464355, -3.970832347869873]\n",
      "Grand sum of 912 tensor sets is: [277.8773193359375, 713.4443359375, -206.41485595703125, 43.152809143066406, -1361.879150390625]\n",
      "\n",
      "Instance 1310 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 29: [-0.274652361869812, 2.0151851177215576, -0.4892542064189911, 4.439785957336426, -0.8764396905899048]\n",
      "Grand sum of 913 tensor sets is: [277.6026611328125, 715.4595336914062, -206.90411376953125, 47.592594146728516, -1362.755615234375]\n",
      "\n",
      "Instance 1311 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 3: [0.3422595262527466, 0.6172819137573242, -0.277499794960022, -0.5219694375991821, -1.5368154048919678]\n",
      "Grand sum of 914 tensor sets is: [277.9449157714844, 716.0768432617188, -207.18161010742188, 47.07062530517578, -1364.29248046875]\n",
      "\n",
      "Instance 1312 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 35: [0.9000182151794434, 1.8428376913070679, -0.43364787101745605, 0.40837475657463074, 1.6893153190612793]\n",
      "Grand sum of 915 tensor sets is: [278.8449401855469, 717.919677734375, -207.61526489257812, 47.479000091552734, -1362.6031494140625]\n",
      "\n",
      "Instance 1313 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 28: [-0.051965489983558655, 0.625558614730835, -0.39054369926452637, 0.7781293392181396, -2.600097179412842]\n",
      "Grand sum of 916 tensor sets is: [278.79296875, 718.5452270507812, -208.0058135986328, 48.25712966918945, -1365.2032470703125]\n",
      "\n",
      "Instance 1314 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 2: [0.9920616149902344, 2.5151543617248535, -0.4167712330818176, -0.8444349765777588, 1.1123470067977905]\n",
      "Grand sum of 917 tensor sets is: [279.7850341796875, 721.0603637695312, -208.42259216308594, 47.41269302368164, -1364.0909423828125]\n",
      "\n",
      "Instance 1315 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 9: [0.4059956967830658, 0.11968661844730377, 0.3304876387119293, 0.12825962901115417, -2.889920234680176]\n",
      "Grand sum of 918 tensor sets is: [280.1910400390625, 721.1800537109375, -208.09210205078125, 47.540950775146484, -1366.9808349609375]\n",
      "\n",
      "Instance 1316 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1317 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1318 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1319 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 919 tensor sets is: [280.2924499511719, 722.9193725585938, -208.73011779785156, 46.425567626953125, -1371.2774658203125]\n",
      "\n",
      "Instance 1320 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1321 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 5: [0.1326322704553604, 0.6617443561553955, -0.1646389663219452, -0.03879991173744202, 0.3773798942565918]\n",
      "Grand sum of 920 tensor sets is: [280.4250793457031, 723.5811157226562, -208.89476013183594, 46.38676834106445, -1370.900146484375]\n",
      "\n",
      "Instance 1322 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 14: [0.28411349654197693, 1.142496109008789, 0.08922789990901947, 0.9021301865577698, -5.300507068634033]\n",
      "Grand sum of 921 tensor sets is: [280.7091979980469, 724.7236328125, -208.80552673339844, 47.28889846801758, -1376.20068359375]\n",
      "\n",
      "Instance 1323 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1324 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 40: [0.4996374249458313, -0.28994956612586975, -0.46465548872947693, -2.637877941131592, -3.694197416305542]\n",
      "Grand sum of 922 tensor sets is: [281.2088317871094, 724.4336547851562, -209.2701873779297, 44.65102005004883, -1379.8948974609375]\n",
      "\n",
      "Instance 1325 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 9: [-0.11461029201745987, 2.2825093269348145, -0.05906650424003601, -0.7383939027786255, -2.830176591873169]\n",
      "Grand sum of 923 tensor sets is: [281.0942077636719, 726.7161865234375, -209.32925415039062, 43.91262435913086, -1382.72509765625]\n",
      "\n",
      "Instance 1326 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 4: [0.509709894657135, 0.5476845502853394, -0.3580619692802429, -0.7765750885009766, 1.192441701889038]\n",
      "Grand sum of 924 tensor sets is: [281.6039123535156, 727.2638549804688, -209.68731689453125, 43.13604736328125, -1381.53271484375]\n",
      "\n",
      "Instance 1327 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 24: [0.7988220453262329, 3.1725170612335205, 0.07258296012878418, 0.3906957805156708, -0.4708312451839447]\n",
      "Grand sum of 925 tensor sets is: [282.4027404785156, 730.4364013671875, -209.61473083496094, 43.5267448425293, -1382.0035400390625]\n",
      "\n",
      "Instance 1328 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 26: [0.1136123538017273, 1.4083458185195923, -0.5988785028457642, 0.3098926544189453, 1.6027343273162842]\n",
      "Grand sum of 926 tensor sets is: [282.516357421875, 731.8447265625, -210.21360778808594, 43.836639404296875, -1380.4007568359375]\n",
      "\n",
      "Instance 1329 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 11: [0.2499324381351471, 1.8358641862869263, -0.18933779001235962, -0.5996129512786865, -0.037381529808044434]\n",
      "Grand sum of 927 tensor sets is: [282.76629638671875, 733.6806030273438, -210.40293884277344, 43.23702621459961, -1380.4381103515625]\n",
      "\n",
      "Instance 1330 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 14: [1.030806541442871, 1.589138150215149, -0.06643886864185333, 0.8846340775489807, -0.5989537239074707]\n",
      "Grand sum of 928 tensor sets is: [283.7970886230469, 735.2697143554688, -210.46937561035156, 44.12166213989258, -1381.037109375]\n",
      "\n",
      "Instance 1331 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 23: [0.25613653659820557, 1.7356373071670532, -0.0017609372735023499, 1.8842601776123047, -1.2285208702087402]\n",
      "Grand sum of 929 tensor sets is: [284.05322265625, 737.00537109375, -210.47113037109375, 46.00592041015625, -1382.265625]\n",
      "\n",
      "Instance 1332 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "forever at index 41: [-0.26200538873672485, 0.9476540088653564, -0.07361551374197006, 0.1973760724067688, -1.4690473079681396]\n",
      "Grand sum of 930 tensor sets is: [283.7912292480469, 737.9530029296875, -210.54473876953125, 46.20329666137695, -1383.734619140625]\n",
      "\n",
      "Instance 1333 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 21: [0.5398508906364441, -0.4480217695236206, 0.27523067593574524, 0.5248483419418335, -2.4460389614105225]\n",
      "Grand sum of 931 tensor sets is: [284.3310852050781, 737.5050048828125, -210.26950073242188, 46.728145599365234, -1386.1806640625]\n",
      "\n",
      "Instance 1334 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [0.637782633304596, -0.49636951088905334, -0.8109208941459656, -2.298466205596924, -0.9923750162124634]\n",
      "Grand sum of 932 tensor sets is: [284.9688720703125, 737.0086059570312, -211.08042907714844, 44.42967987060547, -1387.173095703125]\n",
      "\n",
      "Instance 1335 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1336 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 20: [0.4931730329990387, 1.5171045064926147, 0.6801640391349792, 1.8614325523376465, 0.38397416472435]\n",
      "Grand sum of 933 tensor sets is: [285.4620361328125, 738.5256958007812, -210.4002685546875, 46.29111099243164, -1386.7890625]\n",
      "\n",
      "Instance 1337 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 6: [0.013124272227287292, 4.0368781089782715, 0.6150313019752502, -0.4008553624153137, 0.7887859344482422]\n",
      "Grand sum of 934 tensor sets is: [285.47515869140625, 742.5625610351562, -209.7852325439453, 45.890254974365234, -1386.000244140625]\n",
      "\n",
      "Instance 1338 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 10: [-0.3521346151828766, 0.6553964018821716, 1.1258656978607178, -0.6946448087692261, -1.368619680404663]\n",
      "Grand sum of 935 tensor sets is: [285.1230163574219, 743.2179565429688, -208.65936279296875, 45.19561004638672, -1387.368896484375]\n",
      "\n",
      "Instance 1339 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 15: [0.7567368149757385, 1.0763189792633057, -1.2812873125076294, -0.030185654759407043, 0.10610336065292358]\n",
      "Grand sum of 936 tensor sets is: [285.8797607421875, 744.2942504882812, -209.94064331054688, 45.16542434692383, -1387.2628173828125]\n",
      "\n",
      "Instance 1340 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [62]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 62: [0.3974354565143585, 0.7841166257858276, -0.5701643228530884, 0.5546071529388428, -6.074431419372559]\n",
      "Grand sum of 937 tensor sets is: [286.2771911621094, 745.078369140625, -210.51080322265625, 45.72003173828125, -1393.3372802734375]\n",
      "\n",
      "Instance 1341 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([94, 13, 768])\n",
      "Shape of summed layers is: 94 x 768\n",
      "forever at index 47: [0.4236904978752136, 0.880145788192749, 0.16488711535930634, -2.1832940578460693, -1.794419527053833]\n",
      "Grand sum of 938 tensor sets is: [286.70086669921875, 745.95849609375, -210.34591674804688, 43.536739349365234, -1395.1317138671875]\n",
      "\n",
      "Instance 1342 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1343 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 18: [0.37068963050842285, 1.295629858970642, -0.519830584526062, 1.7396597862243652, 1.4336200952529907]\n",
      "Grand sum of 939 tensor sets is: [287.0715637207031, 747.254150390625, -210.86575317382812, 45.276397705078125, -1393.6981201171875]\n",
      "\n",
      "Instance 1344 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 16: [0.21682173013687134, -0.22476503252983093, 0.5279460549354553, -1.051147222518921, -1.5116342306137085]\n",
      "Grand sum of 940 tensor sets is: [287.28839111328125, 747.0293579101562, -210.3378143310547, 44.225250244140625, -1395.209716796875]\n",
      "\n",
      "Instance 1345 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [1.6994589567184448, -0.21792569756507874, 0.2326381653547287, -0.911447286605835, -4.0184526443481445]\n",
      "Grand sum of 941 tensor sets is: [288.98785400390625, 746.8114624023438, -210.1051788330078, 43.313804626464844, -1399.2281494140625]\n",
      "\n",
      "Instance 1346 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1347 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4, 12]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "forever at index 4: [0.19769474864006042, 0.9239522218704224, 0.0995880514383316, -0.08179770410060883, 0.22700658440589905]\n",
      "forever at index 12: [0.6060097217559814, 0.5739008784294128, 0.5269936323165894, -1.1561846733093262, -0.11992436647415161]\n",
      "Grand sum of 942 tensor sets is: [289.38970947265625, 747.5603637695312, -209.79188537597656, 42.6948127746582, -1399.174560546875]\n",
      "\n",
      "Instance 1348 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 18: [0.12791003286838531, 1.8617236614227295, -0.19235803186893463, -0.327569842338562, 2.1581406593322754]\n",
      "Grand sum of 943 tensor sets is: [289.5176086425781, 749.4220581054688, -209.98423767089844, 42.367244720458984, -1397.0164794921875]\n",
      "\n",
      "Instance 1349 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 22: [0.11140218377113342, 1.1916406154632568, 0.15353934466838837, 1.0046420097351074, -3.2607672214508057]\n",
      "Grand sum of 944 tensor sets is: [289.6289978027344, 750.6137084960938, -209.83070373535156, 43.37188720703125, -1400.2772216796875]\n",
      "\n",
      "Instance 1350 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 65: [-0.4072004556655884, -1.165315866470337, -1.116049885749817, -1.6974555253982544, -1.7651728391647339]\n",
      "Grand sum of 945 tensor sets is: [289.2218017578125, 749.4483642578125, -210.94674682617188, 41.67443084716797, -1402.0423583984375]\n",
      "\n",
      "Instance 1351 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1352 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 23: [0.25613653659820557, 1.7356373071670532, -0.0017609372735023499, 1.8842601776123047, -1.2285208702087402]\n",
      "Grand sum of 946 tensor sets is: [289.4779357910156, 751.1840209960938, -210.94850158691406, 43.558692932128906, -1403.2708740234375]\n",
      "\n",
      "Instance 1353 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 3: [0.30890002846717834, 0.14745070040225983, 0.18268349766731262, -0.7755210399627686, -1.3056597709655762]\n",
      "Grand sum of 947 tensor sets is: [289.7868347167969, 751.3314819335938, -210.7658233642578, 42.783172607421875, -1404.5765380859375]\n",
      "\n",
      "Instance 1354 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 9: [0.645723819732666, 1.1926629543304443, -1.4760838747024536, -0.2098546326160431, -0.8657086491584778]\n",
      "Grand sum of 948 tensor sets is: [290.43255615234375, 752.524169921875, -212.24191284179688, 42.57331848144531, -1405.4422607421875]\n",
      "\n",
      "Instance 1355 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1356 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 949 tensor sets is: [290.8282470703125, 753.5328369140625, -212.31149291992188, 41.88975524902344, -1404.14599609375]\n",
      "\n",
      "Instance 1357 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 5: [0.6598741412162781, 0.01355201005935669, -0.7253003120422363, -0.25299155712127686, -0.5309762358665466]\n",
      "Grand sum of 950 tensor sets is: [291.4881286621094, 753.54638671875, -213.0367889404297, 41.63676452636719, -1404.677001953125]\n",
      "\n",
      "Instance 1358 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1359 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 8: [0.7191540598869324, 2.597564220428467, -0.13535168766975403, -1.43770432472229, 1.2078138589859009]\n",
      "Grand sum of 951 tensor sets is: [292.207275390625, 756.1439208984375, -213.17213439941406, 40.199058532714844, -1403.46923828125]\n",
      "\n",
      "Instance 1360 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1361 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 12: [-0.8262344598770142, 2.2327284812927246, 0.5385805368423462, 1.2192440032958984, -0.5886880159378052]\n",
      "Grand sum of 952 tensor sets is: [291.38104248046875, 758.3766479492188, -212.63356018066406, 41.418304443359375, -1404.0579833984375]\n",
      "\n",
      "Instance 1362 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1363 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1364 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 13: [0.8849242925643921, -0.41294944286346436, -0.5244792103767395, -0.22344878315925598, 1.4269490242004395]\n",
      "Grand sum of 953 tensor sets is: [292.2659606933594, 757.9636840820312, -213.1580352783203, 41.194854736328125, -1402.6309814453125]\n",
      "\n",
      "Instance 1365 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [0.3498929738998413, 0.9748251438140869, -1.374105453491211, -0.0809759795665741, -5.644731044769287]\n",
      "Grand sum of 954 tensor sets is: [292.6158447265625, 758.9385375976562, -214.53213500976562, 41.1138801574707, -1408.2757568359375]\n",
      "\n",
      "Instance 1366 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 27: [-0.17498338222503662, 1.9268269538879395, -0.06196216121315956, 2.414579153060913, -3.185361385345459]\n",
      "Grand sum of 955 tensor sets is: [292.44085693359375, 760.8653564453125, -214.59410095214844, 43.52845764160156, -1411.4610595703125]\n",
      "\n",
      "Instance 1367 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [0.7601019144058228, 1.7670985460281372, 0.6998885273933411, 1.0222638845443726, -0.9792816638946533]\n",
      "Grand sum of 956 tensor sets is: [293.2009582519531, 762.6324462890625, -213.8942108154297, 44.55072021484375, -1412.4403076171875]\n",
      "\n",
      "Instance 1368 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 16: [-0.09482137113809586, 1.070288896560669, -0.22235515713691711, -0.295080304145813, -1.8595144748687744]\n",
      "Grand sum of 957 tensor sets is: [293.10614013671875, 763.7027587890625, -214.11656188964844, 44.255638122558594, -1414.2998046875]\n",
      "\n",
      "Instance 1369 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 958 tensor sets is: [293.6500549316406, 763.0679321289062, -214.76611328125, 43.1134147644043, -1413.178466796875]\n",
      "\n",
      "Instance 1370 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1371 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 14: [0.5760253071784973, 2.096881151199341, 0.032204583287239075, 1.6949357986450195, -2.210397243499756]\n",
      "Grand sum of 959 tensor sets is: [294.22607421875, 765.164794921875, -214.73390197753906, 44.808349609375, -1415.388916015625]\n",
      "\n",
      "Instance 1372 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1373 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 25: [0.38160982728004456, 0.9152438640594482, -0.16408610343933105, -0.708044707775116, -0.770586371421814]\n",
      "Grand sum of 960 tensor sets is: [294.6076965332031, 766.0800170898438, -214.8979949951172, 44.100303649902344, -1416.1595458984375]\n",
      "\n",
      "Instance 1374 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 4: [0.41019314527511597, 2.023369550704956, 0.45522284507751465, -0.9443496465682983, -1.4270883798599243]\n",
      "Grand sum of 961 tensor sets is: [295.01788330078125, 768.1033935546875, -214.44277954101562, 43.15595245361328, -1417.586669921875]\n",
      "\n",
      "Instance 1375 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 962 tensor sets is: [296.8092346191406, 766.5487670898438, -215.12501525878906, 42.310028076171875, -1417.1336669921875]\n",
      "\n",
      "Instance 1376 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 4: [0.09941698610782623, 1.8239550590515137, -0.5195322632789612, 0.18484993278980255, -1.5472288131713867]\n",
      "Grand sum of 963 tensor sets is: [296.9086608886719, 768.3727416992188, -215.64454650878906, 42.494876861572266, -1418.680908203125]\n",
      "\n",
      "Instance 1377 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1378 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 9: [-0.3284541666507721, 1.6693605184555054, -1.4137835502624512, 0.2971886992454529, -0.48168522119522095]\n",
      "Grand sum of 964 tensor sets is: [296.5802001953125, 770.0421142578125, -217.05833435058594, 42.79206466674805, -1419.16259765625]\n",
      "\n",
      "Instance 1379 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 15: [-0.2670670747756958, 1.8069642782211304, -0.7715129852294922, 0.9764424562454224, -2.426302433013916]\n",
      "Grand sum of 965 tensor sets is: [296.3131408691406, 771.8490600585938, -217.82984924316406, 43.76850891113281, -1421.5888671875]\n",
      "\n",
      "Instance 1380 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 3: [1.0642578601837158, 0.15850503742694855, 0.22358757257461548, -1.1559677124023438, 1.0597320795059204]\n",
      "Grand sum of 966 tensor sets is: [297.3774108886719, 772.007568359375, -217.60626220703125, 42.61254119873047, -1420.5291748046875]\n",
      "\n",
      "Instance 1381 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 7: [0.4628858268260956, 0.2479752004146576, -0.09962954372167587, -1.9385676383972168, -1.836854100227356]\n",
      "Grand sum of 967 tensor sets is: [297.8403015136719, 772.2555541992188, -217.7058868408203, 40.673973083496094, -1422.3660888671875]\n",
      "\n",
      "Instance 1382 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.5882247090339661, 1.6285029649734497, -1.007781982421875, -0.12465835362672806, 0.7376689910888672]\n",
      "Grand sum of 968 tensor sets is: [298.42852783203125, 773.884033203125, -218.7136688232422, 40.54931640625, -1421.62841796875]\n",
      "\n",
      "Instance 1383 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 12: [0.48360347747802734, 0.22091403603553772, -0.5059034824371338, -1.3258872032165527, -2.1742491722106934]\n",
      "Grand sum of 969 tensor sets is: [298.9121398925781, 774.1049194335938, -219.21957397460938, 39.22343063354492, -1423.8026123046875]\n",
      "\n",
      "Instance 1384 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1385 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [-1.4442365169525146, 0.6222109794616699, -0.42531248927116394, 0.6119827628135681, -1.3534350395202637]\n",
      "Grand sum of 970 tensor sets is: [297.4678955078125, 774.7271118164062, -219.64488220214844, 39.83541488647461, -1425.156005859375]\n",
      "\n",
      "Instance 1386 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 16: [-0.21027418971061707, 0.6597233414649963, -0.8309154510498047, 0.21221476793289185, -1.1626111268997192]\n",
      "Grand sum of 971 tensor sets is: [297.25762939453125, 775.3868408203125, -220.47579956054688, 40.047630310058594, -1426.318603515625]\n",
      "\n",
      "Instance 1387 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 2: [1.1141968965530396, -0.23476240038871765, -0.9401029944419861, 1.9605684280395508, -1.1663790941238403]\n",
      "Grand sum of 972 tensor sets is: [298.371826171875, 775.152099609375, -221.41590881347656, 42.00819778442383, -1427.4849853515625]\n",
      "\n",
      "Instance 1388 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 26: [-0.1691516935825348, 1.1859240531921387, -0.009297594428062439, -0.7480151057243347, -2.977574586868286]\n",
      "Grand sum of 973 tensor sets is: [298.2026672363281, 776.3380126953125, -221.42520141601562, 41.26018142700195, -1430.4625244140625]\n",
      "\n",
      "Instance 1389 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 974 tensor sets is: [298.3040771484375, 778.0773315429688, -222.06321716308594, 40.144798278808594, -1434.7591552734375]\n",
      "\n",
      "Instance 1390 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 30: [1.0676368474960327, 1.828202486038208, -0.6096475720405579, -0.5763261914253235, 1.0746411085128784]\n",
      "Grand sum of 975 tensor sets is: [299.3717041015625, 779.905517578125, -222.67286682128906, 39.56847381591797, -1433.6845703125]\n",
      "\n",
      "Instance 1391 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 9: [0.14687219262123108, 1.005770206451416, -1.4417880773544312, -1.2884998321533203, -2.731337308883667]\n",
      "Grand sum of 976 tensor sets is: [299.5185852050781, 780.9113159179688, -224.11465454101562, 38.27997589111328, -1436.4158935546875]\n",
      "\n",
      "Instance 1392 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 27: [-0.15077653527259827, 1.065521001815796, -0.8151657581329346, -2.031930685043335, -2.950840473175049]\n",
      "Grand sum of 977 tensor sets is: [299.3677978515625, 781.976806640625, -224.92982482910156, 36.248046875, -1439.36669921875]\n",
      "\n",
      "Instance 1393 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1394 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1395 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1396 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [0.4478001594543457, -1.7374184131622314, -0.07038390636444092, -0.09785052388906479, -2.7656736373901367]\n",
      "Grand sum of 978 tensor sets is: [299.81561279296875, 780.2393798828125, -225.00021362304688, 36.15019607543945, -1442.13232421875]\n",
      "\n",
      "Instance 1397 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1398 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 21: [0.4555101990699768, -1.95773446559906, 0.23268721997737885, -1.3022263050079346, -2.553041458129883]\n",
      "Grand sum of 979 tensor sets is: [300.2711181640625, 778.2816162109375, -224.7675323486328, 34.84796905517578, -1444.6854248046875]\n",
      "\n",
      "Instance 1399 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 9: [0.5232775211334229, 1.5345979928970337, -0.7823975682258606, -0.6098713874816895, -4.3637590408325195]\n",
      "Grand sum of 980 tensor sets is: [300.7944030761719, 779.8162231445312, -225.5499267578125, 34.23809814453125, -1449.0491943359375]\n",
      "\n",
      "Instance 1400 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 6: [-0.0659170001745224, 0.18661877512931824, -0.8259607553482056, 2.5733258724212646, -2.0173346996307373]\n",
      "Grand sum of 981 tensor sets is: [300.7284851074219, 780.0028686523438, -226.37588500976562, 36.811424255371094, -1451.0665283203125]\n",
      "\n",
      "Instance 1401 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1402 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 982 tensor sets is: [300.8665466308594, 780.6570434570312, -226.81646728515625, 36.514102935791016, -1453.9168701171875]\n",
      "\n",
      "Instance 1403 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 27: [-0.02238532155752182, 1.8369990587234497, -0.7945443391799927, 0.0921863317489624, -6.391986846923828]\n",
      "Grand sum of 983 tensor sets is: [300.8441467285156, 782.4940185546875, -227.6110076904297, 36.60628890991211, -1460.308837890625]\n",
      "\n",
      "Instance 1404 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1405 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1406 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 24: [-0.21152673661708832, 2.2357053756713867, 0.25774696469306946, 0.32821524143218994, -0.830132246017456]\n",
      "Grand sum of 984 tensor sets is: [300.63262939453125, 784.729736328125, -227.35325622558594, 36.934505462646484, -1461.138916015625]\n",
      "\n",
      "Instance 1407 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 32: [-0.6230658292770386, 0.5303539037704468, -1.0711599588394165, -0.12893027067184448, -3.2016351222991943]\n",
      "Grand sum of 985 tensor sets is: [300.0095520019531, 785.2600708007812, -228.42442321777344, 36.80557632446289, -1464.340576171875]\n",
      "\n",
      "Instance 1408 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([102, 13, 768])\n",
      "Shape of summed layers is: 102 x 768\n",
      "forever at index 74: [-0.4757104814052582, 1.8463609218597412, -1.1018388271331787, 0.4659477770328522, -5.439427375793457]\n",
      "Grand sum of 986 tensor sets is: [299.5338439941406, 787.1064453125, -229.52626037597656, 37.271522521972656, -1469.780029296875]\n",
      "\n",
      "Instance 1409 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6171743273735046, -0.8320705890655518, -0.5786696076393127, -1.0672661066055298, -1.8662216663360596]\n",
      "Grand sum of 987 tensor sets is: [300.1510314941406, 786.2743530273438, -230.1049346923828, 36.20425796508789, -1471.646240234375]\n",
      "\n",
      "Instance 1410 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 27: [0.14846257865428925, 1.6767393350601196, -0.3980403542518616, 0.2485012710094452, -2.700578212738037]\n",
      "Grand sum of 988 tensor sets is: [300.29949951171875, 787.9511108398438, -230.5029754638672, 36.4527587890625, -1474.3468017578125]\n",
      "\n",
      "Instance 1411 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 4: [0.06811598688364029, 1.8061293363571167, -1.664454460144043, 0.7161114811897278, -4.349098205566406]\n",
      "Grand sum of 989 tensor sets is: [300.36761474609375, 789.7572631835938, -232.1674346923828, 37.16886901855469, -1478.6959228515625]\n",
      "\n",
      "Instance 1412 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 6: [0.7223087549209595, 2.0113162994384766, -0.4096926748752594, -0.750458836555481, -2.567059278488159]\n",
      "Grand sum of 990 tensor sets is: [301.0899353027344, 791.7685546875, -232.57713317871094, 36.41841125488281, -1481.262939453125]\n",
      "\n",
      "Instance 1413 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 25: [0.9020901918411255, 0.7690482139587402, -0.09132668375968933, -0.5933252573013306, -2.636047840118408]\n",
      "Grand sum of 991 tensor sets is: [301.9920349121094, 792.53759765625, -232.66845703125, 35.8250846862793, -1483.8990478515625]\n",
      "\n",
      "Instance 1414 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1415 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([93, 13, 768])\n",
      "Shape of summed layers is: 93 x 768\n",
      "forever at index 38: [-1.1777037382125854, -0.2888803780078888, -0.2941676676273346, -0.5361020565032959, -3.4033100605010986]\n",
      "Grand sum of 992 tensor sets is: [300.8143310546875, 792.2487182617188, -232.96263122558594, 35.28898239135742, -1487.3023681640625]\n",
      "\n",
      "Instance 1416 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 19: [-0.3271608352661133, 0.0988818109035492, 0.09290455281734467, -1.5138518810272217, -0.33509814739227295]\n",
      "Grand sum of 993 tensor sets is: [300.4871826171875, 792.3475952148438, -232.86972045898438, 33.77513122558594, -1487.637451171875]\n",
      "\n",
      "Instance 1417 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [0.11919157952070236, 1.3053935766220093, -0.15041474997997284, -0.6852971315383911, -1.8793830871582031]\n",
      "Grand sum of 994 tensor sets is: [300.60638427734375, 793.6530151367188, -233.0201416015625, 33.0898323059082, -1489.516845703125]\n",
      "\n",
      "Instance 1418 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1419 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1420 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "forever at index 15: [0.25772348046302795, 0.006301030516624451, 0.3681877553462982, -0.3072165250778198, -0.6652044057846069]\n",
      "Grand sum of 995 tensor sets is: [300.8641052246094, 793.6593017578125, -232.65194702148438, 32.782615661621094, -1490.1820068359375]\n",
      "\n",
      "Instance 1421 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 6: [-0.0659170001745224, 0.18661877512931824, -0.8259607553482056, 2.5733258724212646, -2.0173346996307373]\n",
      "Grand sum of 996 tensor sets is: [300.7981872558594, 793.845947265625, -233.4779052734375, 35.35594177246094, -1492.1993408203125]\n",
      "\n",
      "Instance 1422 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 11: [-0.08562301099300385, 2.960667371749878, 0.7840063571929932, 1.4838330745697021, 0.6219831705093384]\n",
      "Grand sum of 997 tensor sets is: [300.7125549316406, 796.806640625, -232.6938934326172, 36.83977508544922, -1491.577392578125]\n",
      "\n",
      "Instance 1423 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 12: [-0.739756166934967, 1.7382248640060425, -0.7249546647071838, -0.30038225650787354, -3.4597768783569336]\n",
      "Grand sum of 998 tensor sets is: [299.9728088378906, 798.5448608398438, -233.41885375976562, 36.53939437866211, -1495.037109375]\n",
      "\n",
      "Instance 1424 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1425 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [1.6994589567184448, -0.21792569756507874, 0.2326381653547287, -0.911447286605835, -4.0184526443481445]\n",
      "Grand sum of 999 tensor sets is: [301.6722717285156, 798.3269653320312, -233.18621826171875, 35.62794876098633, -1499.0555419921875]\n",
      "\n",
      "Instance 1426 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 6: [0.013124272227287292, 4.0368781089782715, 0.6150313019752502, -0.4008553624153137, 0.7887859344482422]\n",
      "Grand sum of 1000 tensor sets is: [301.6853942871094, 802.3638305664062, -232.57118225097656, 35.22709274291992, -1498.2667236328125]\n",
      "\n",
      "Instance 1427 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 1001 tensor sets is: [303.47674560546875, 800.8092041015625, -233.25341796875, 34.381168365478516, -1497.813720703125]\n",
      "\n",
      "Instance 1428 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 11: [-0.034517958760261536, 1.8235187530517578, -0.742537260055542, 0.09464952349662781, -1.425381064414978]\n",
      "Grand sum of 1002 tensor sets is: [303.4422302246094, 802.6327514648438, -233.99595642089844, 34.4758186340332, -1499.2391357421875]\n",
      "\n",
      "Instance 1429 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1430 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1431 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 36: [1.0657345056533813, 1.7169830799102783, -0.7159639000892639, 0.7175295352935791, -6.755459785461426]\n",
      "Grand sum of 1003 tensor sets is: [304.5079650878906, 804.3497314453125, -234.7119140625, 35.1933479309082, -1505.99462890625]\n",
      "\n",
      "Instance 1432 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1433 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 12: [0.10720492899417877, 2.3102097511291504, -0.5020813345909119, 0.46319037675857544, -2.2476463317871094]\n",
      "Grand sum of 1004 tensor sets is: [304.61517333984375, 806.659912109375, -235.2139892578125, 35.65653991699219, -1508.2423095703125]\n",
      "\n",
      "Instance 1434 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1435 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([92, 13, 768])\n",
      "Shape of summed layers is: 92 x 768\n",
      "forever at index 31: [0.17473964393138885, 0.7279466390609741, 0.0008538663387298584, 1.456054925918579, -3.3420746326446533]\n",
      "Grand sum of 1005 tensor sets is: [304.7899169921875, 807.3878784179688, -235.213134765625, 37.11259460449219, -1511.5843505859375]\n",
      "\n",
      "Instance 1436 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 1006 tensor sets is: [305.4847106933594, 808.5211791992188, -234.00999450683594, 35.843231201171875, -1512.474609375]\n",
      "\n",
      "Instance 1437 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1438 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 16: [-0.10990479588508606, 2.2434630393981934, 0.5143247842788696, -0.8192417025566101, -1.139312744140625]\n",
      "Grand sum of 1007 tensor sets is: [305.37481689453125, 810.7646484375, -233.49566650390625, 35.023990631103516, -1513.6138916015625]\n",
      "\n",
      "Instance 1439 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 16: [0.6992561221122742, 0.8384290933609009, 1.310780644416809, -0.8525620102882385, -3.343515157699585]\n",
      "Grand sum of 1008 tensor sets is: [306.0740661621094, 811.6030883789062, -232.1848907470703, 34.17142868041992, -1516.9573974609375]\n",
      "\n",
      "Instance 1440 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1441 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1442 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 36: [-0.39902058243751526, 1.9074503183364868, -0.18964794278144836, 1.0663508176803589, 1.0789954662322998]\n",
      "Grand sum of 1009 tensor sets is: [305.675048828125, 813.5105590820312, -232.37454223632812, 35.23777770996094, -1515.87841796875]\n",
      "\n",
      "Instance 1443 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 7: [0.6086311340332031, 0.8077070713043213, -0.32924193143844604, -0.5165956020355225, -0.19776973128318787]\n",
      "Grand sum of 1010 tensor sets is: [306.28369140625, 814.3182373046875, -232.7037811279297, 34.72118377685547, -1516.076171875]\n",
      "\n",
      "Instance 1444 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1445 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 5: [0.2599857449531555, 1.9206509590148926, -0.15723568201065063, -0.7429513931274414, -1.2063709497451782]\n",
      "Grand sum of 1011 tensor sets is: [306.5436706542969, 816.2388916015625, -232.86102294921875, 33.978233337402344, -1517.2825927734375]\n",
      "\n",
      "Instance 1446 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 15: [0.2262648195028305, 1.381420612335205, 0.09431253373622894, 0.7179683446884155, -1.6734687089920044]\n",
      "Grand sum of 1012 tensor sets is: [306.7699279785156, 817.6203002929688, -232.76670837402344, 34.69620132446289, -1518.9560546875]\n",
      "\n",
      "Instance 1447 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1448 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1449 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 17: [0.3291964828968048, -0.16444239020347595, -0.7733607888221741, -0.16987565159797668, 0.5619012713432312]\n",
      "Grand sum of 1013 tensor sets is: [307.09912109375, 817.4558715820312, -233.54006958007812, 34.52632522583008, -1518.3941650390625]\n",
      "\n",
      "Instance 1450 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "forever at index 49: [0.7447512745857239, -0.6039154529571533, -1.642414927482605, 1.3922590017318726, -2.948355197906494]\n",
      "Grand sum of 1014 tensor sets is: [307.8438720703125, 816.8519287109375, -235.18247985839844, 35.918582916259766, -1521.342529296875]\n",
      "\n",
      "Instance 1451 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1452 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 37: [-0.142353355884552, 1.1452974081039429, -0.9252950549125671, -0.7546662092208862, -7.564059734344482]\n",
      "Grand sum of 1015 tensor sets is: [307.7015075683594, 817.9972534179688, -236.10777282714844, 35.163917541503906, -1528.9066162109375]\n",
      "\n",
      "Instance 1453 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1454 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1455 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 3: [1.411973476409912, -0.3463144600391388, 0.9556238055229187, 0.11789199709892273, 2.1909661293029785]\n",
      "Grand sum of 1016 tensor sets is: [309.1134948730469, 817.6509399414062, -235.1521453857422, 35.28181076049805, -1526.7156982421875]\n",
      "\n",
      "Instance 1456 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1457 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 24: [0.6742856502532959, 0.4920651316642761, -0.2599599361419678, -0.43063870072364807, 1.245353102684021]\n",
      "Grand sum of 1017 tensor sets is: [309.78778076171875, 818.1430053710938, -235.412109375, 34.851173400878906, -1525.4703369140625]\n",
      "\n",
      "Instance 1458 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1459 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 24: [0.1437293440103531, 2.5468316078186035, 0.44560348987579346, -0.5118263363838196, -2.9352357387542725]\n",
      "Grand sum of 1018 tensor sets is: [309.9315185546875, 820.6898193359375, -234.9665069580078, 34.33934783935547, -1528.405517578125]\n",
      "\n",
      "Instance 1460 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 8: [0.6932922601699829, 0.8551253080368042, -0.2990806996822357, -0.13126246631145477, -0.7800002098083496]\n",
      "Grand sum of 1019 tensor sets is: [310.62481689453125, 821.544921875, -235.26559448242188, 34.20808410644531, -1529.185546875]\n",
      "\n",
      "Instance 1461 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 9: [0.5218549370765686, 0.9484233856201172, 0.1863338053226471, 0.2602981925010681, -3.169445037841797]\n",
      "Grand sum of 1020 tensor sets is: [311.14666748046875, 822.4933471679688, -235.07925415039062, 34.4683837890625, -1532.35498046875]\n",
      "\n",
      "Instance 1462 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 11: [3.572493076324463, 1.318234920501709, 0.3219805359840393, 1.770577073097229, -0.6667544841766357]\n",
      "Grand sum of 1021 tensor sets is: [314.7191467285156, 823.8115844726562, -234.7572784423828, 36.23896026611328, -1533.021728515625]\n",
      "\n",
      "Instance 1463 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [1.0301811695098877, 0.09672722220420837, -1.850982904434204, 0.9610074162483215, -2.2480239868164062]\n",
      "Grand sum of 1022 tensor sets is: [315.74932861328125, 823.9083251953125, -236.60826110839844, 37.19996643066406, -1535.269775390625]\n",
      "\n",
      "Instance 1464 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 53: [0.666554868221283, -0.7873085737228394, -1.431145191192627, 1.6047497987747192, -2.4387478828430176]\n",
      "Grand sum of 1023 tensor sets is: [316.4158935546875, 823.1210327148438, -238.03941345214844, 38.804718017578125, -1537.70849609375]\n",
      "\n",
      "Instance 1465 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 21: [0.7414695024490356, 1.9673219919204712, -1.5429937839508057, -0.42146068811416626, -5.9759135246276855]\n",
      "Grand sum of 1024 tensor sets is: [317.1573486328125, 825.08837890625, -239.58241271972656, 38.38325881958008, -1543.6844482421875]\n",
      "\n",
      "Instance 1466 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [66]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "forever at index 66: [-0.4971312880516052, 0.4068295955657959, 2.0872578620910645, 1.1197913885116577, -3.6161367893218994]\n",
      "Grand sum of 1025 tensor sets is: [316.66021728515625, 825.4951782226562, -237.49514770507812, 39.5030517578125, -1547.300537109375]\n",
      "\n",
      "Instance 1467 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 9: [1.16624915599823, 0.9549139738082886, 0.911909818649292, 1.068943738937378, 2.0268280506134033]\n",
      "Grand sum of 1026 tensor sets is: [317.82647705078125, 826.4500732421875, -236.58323669433594, 40.57199478149414, -1545.273681640625]\n",
      "\n",
      "Instance 1468 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1469 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1470 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 13: [0.09151513874530792, 0.49524205923080444, -0.07919028401374817, -1.013672113418579, -3.673577308654785]\n",
      "Grand sum of 1027 tensor sets is: [317.9179992675781, 826.9453125, -236.6624298095703, 39.55832290649414, -1548.947265625]\n",
      "\n",
      "Instance 1471 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1472 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 4: [0.12334029376506805, -0.016619056463241577, 1.3963911533355713, -0.04796069860458374, -1.8958925008773804]\n",
      "Grand sum of 1028 tensor sets is: [318.0413513183594, 826.9287109375, -235.2660369873047, 39.51036071777344, -1550.8431396484375]\n",
      "\n",
      "Instance 1473 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 19: [0.4928104877471924, -0.07274767756462097, -0.6433916091918945, -0.5077632665634155, -0.6468086838722229]\n",
      "Grand sum of 1029 tensor sets is: [318.5341491699219, 826.85595703125, -235.909423828125, 39.00259780883789, -1551.489990234375]\n",
      "\n",
      "Instance 1474 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1475 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 7: [-0.5174009203910828, 2.139543056488037, -0.2060706913471222, 0.29931503534317017, -1.3929206132888794]\n",
      "Grand sum of 1030 tensor sets is: [318.0167541503906, 828.9954833984375, -236.11549377441406, 39.30191421508789, -1552.8829345703125]\n",
      "\n",
      "Instance 1476 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 1031 tensor sets is: [319.80810546875, 827.4408569335938, -236.7977294921875, 38.455989837646484, -1552.429931640625]\n",
      "\n",
      "Instance 1477 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1478 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 3: [0.8706580400466919, 1.0311617851257324, -0.5358052849769592, -0.3652939796447754, 1.386218547821045]\n",
      "Grand sum of 1032 tensor sets is: [320.67877197265625, 828.4720458984375, -237.3335418701172, 38.090694427490234, -1551.043701171875]\n",
      "\n",
      "Instance 1479 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 22: [0.9892910718917847, 2.055809259414673, -0.2223549634218216, 1.243882417678833, -2.250450372695923]\n",
      "Grand sum of 1033 tensor sets is: [321.6680603027344, 830.52783203125, -237.55589294433594, 39.33457565307617, -1553.294189453125]\n",
      "\n",
      "Instance 1480 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1481 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 5: [-0.2770821750164032, -0.7397264242172241, -0.3901328146457672, 2.112226724624634, -7.215446949005127]\n",
      "Grand sum of 1034 tensor sets is: [321.3909912109375, 829.7880859375, -237.94602966308594, 41.44680404663086, -1560.5096435546875]\n",
      "\n",
      "Instance 1482 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 1035 tensor sets is: [321.529052734375, 830.4422607421875, -238.38661193847656, 41.14948272705078, -1563.3599853515625]\n",
      "\n",
      "Instance 1483 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 21: [0.3565308749675751, 0.13220976293087006, -1.0269287824630737, 2.582505464553833, -5.253737449645996]\n",
      "Grand sum of 1036 tensor sets is: [321.8855895996094, 830.574462890625, -239.41354370117188, 43.73198699951172, -1568.61376953125]\n",
      "\n",
      "Instance 1484 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 26: [0.07784063369035721, -0.018808722496032715, -0.5828002691268921, -1.7179555892944336, 0.10246801376342773]\n",
      "Grand sum of 1037 tensor sets is: [321.96343994140625, 830.5556640625, -239.996337890625, 42.01403045654297, -1568.5113525390625]\n",
      "\n",
      "Instance 1485 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "forever at index 32: [0.7274446487426758, -0.08199131488800049, -0.7311338782310486, 2.050051689147949, 2.093731641769409]\n",
      "Grand sum of 1038 tensor sets is: [322.6908874511719, 830.4736938476562, -240.72747802734375, 44.064083099365234, -1566.4176025390625]\n",
      "\n",
      "Instance 1486 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 18: [-0.25948262214660645, 1.2170281410217285, -0.2492808848619461, -0.20740871131420135, -1.7799415588378906]\n",
      "Grand sum of 1039 tensor sets is: [322.431396484375, 831.6907348632812, -240.9767608642578, 43.85667419433594, -1568.197509765625]\n",
      "\n",
      "Instance 1487 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 6: [0.11573264002799988, 0.6206953525543213, 0.08805956691503525, -1.5067120790481567, -2.556149959564209]\n",
      "Grand sum of 1040 tensor sets is: [322.547119140625, 832.3114013671875, -240.88870239257812, 42.34996032714844, -1570.753662109375]\n",
      "\n",
      "Instance 1488 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1489 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 22: [0.9013208150863647, 0.501822292804718, -0.2314746081829071, 0.13549034297466278, 0.9646273255348206]\n",
      "Grand sum of 1041 tensor sets is: [323.44842529296875, 832.813232421875, -241.12017822265625, 42.485450744628906, -1569.7890625]\n",
      "\n",
      "Instance 1490 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "forever at index 21: [0.10702623426914215, 3.617661952972412, 0.1723708212375641, 0.8347004055976868, -0.9059286117553711]\n",
      "Grand sum of 1042 tensor sets is: [323.5554504394531, 836.430908203125, -240.94781494140625, 43.320152282714844, -1570.6949462890625]\n",
      "\n",
      "Instance 1491 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 9: [0.934897780418396, 2.285919427871704, 0.15411318838596344, 0.6050851941108704, -3.018496513366699]\n",
      "Grand sum of 1043 tensor sets is: [324.4903564453125, 838.7168579101562, -240.793701171875, 43.925235748291016, -1573.7135009765625]\n",
      "\n",
      "Instance 1492 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 10: [0.041739728301763535, 0.037511616945266724, -1.0058053731918335, -0.8151471614837646, -0.08530163764953613]\n",
      "Grand sum of 1044 tensor sets is: [324.5321044921875, 838.75439453125, -241.79949951171875, 43.11008834838867, -1573.798828125]\n",
      "\n",
      "Instance 1493 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1494 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1495 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 15: [1.6844651699066162, 0.4943975806236267, 0.5821568965911865, 0.5689335465431213, 1.4781736135482788]\n",
      "Grand sum of 1045 tensor sets is: [326.2165832519531, 839.248779296875, -241.21734619140625, 43.67902374267578, -1572.3206787109375]\n",
      "\n",
      "Instance 1496 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [1.6994589567184448, -0.21792569756507874, 0.2326381653547287, -0.911447286605835, -4.0184526443481445]\n",
      "Grand sum of 1046 tensor sets is: [327.9160461425781, 839.0308837890625, -240.98471069335938, 42.767578125, -1576.339111328125]\n",
      "\n",
      "Instance 1497 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1498 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 15: [0.3202347457408905, 1.8221474885940552, -0.6219688057899475, -0.837870717048645, -5.175940036773682]\n",
      "Grand sum of 1047 tensor sets is: [328.23626708984375, 840.85302734375, -241.60667419433594, 41.92970657348633, -1581.5150146484375]\n",
      "\n",
      "Instance 1499 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 1048 tensor sets is: [329.07489013671875, 841.208984375, -242.00833129882812, 42.44333267211914, -1578.9759521484375]\n",
      "\n",
      "Instance 1500 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [0.7618420124053955, 0.8225862979888916, 0.4283001124858856, -0.34421825408935547, -0.9890788197517395]\n",
      "Grand sum of 1049 tensor sets is: [329.83673095703125, 842.0315551757812, -241.5800323486328, 42.09911346435547, -1579.965087890625]\n",
      "\n",
      "Instance 1501 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1502 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 16: [0.6233423352241516, 1.5153348445892334, 0.2717288136482239, 0.07681572437286377, -5.8123297691345215]\n",
      "Grand sum of 1050 tensor sets is: [330.4600830078125, 843.546875, -241.3083038330078, 42.17593002319336, -1585.7774658203125]\n",
      "\n",
      "Instance 1503 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 37: [0.9199132919311523, -0.09811678528785706, 0.11021116375923157, 0.599052906036377, -6.807407855987549]\n",
      "Grand sum of 1051 tensor sets is: [331.3800048828125, 843.44873046875, -241.19808959960938, 42.77498245239258, -1592.5848388671875]\n",
      "\n",
      "Instance 1504 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 20: [0.614175021648407, 0.6519899368286133, 0.18950217962265015, 0.3433239459991455, 1.3534419536590576]\n",
      "Grand sum of 1052 tensor sets is: [331.9941711425781, 844.1007080078125, -241.0085906982422, 43.11830520629883, -1591.2314453125]\n",
      "\n",
      "Instance 1505 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 11: [3.572493076324463, 1.318234920501709, 0.3219805359840393, 1.770577073097229, -0.6667544841766357]\n",
      "Grand sum of 1053 tensor sets is: [335.566650390625, 845.4189453125, -240.68661499023438, 44.88888168334961, -1591.898193359375]\n",
      "\n",
      "Instance 1506 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 22: [-0.19705107808113098, 1.1939126253128052, -0.2782352566719055, 0.03318857401609421, -5.209828853607178]\n",
      "Grand sum of 1054 tensor sets is: [335.3695983886719, 846.6128540039062, -240.96484375, 44.92206954956055, -1597.1080322265625]\n",
      "\n",
      "Instance 1507 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 1055 tensor sets is: [335.5076599121094, 847.2670288085938, -241.40542602539062, 44.62474822998047, -1599.9583740234375]\n",
      "\n",
      "Instance 1508 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 20: [2.1171700954437256, 1.0083293914794922, -1.0699901580810547, -1.064203143119812, 1.5197018384933472]\n",
      "Grand sum of 1056 tensor sets is: [337.62481689453125, 848.2753295898438, -242.4754180908203, 43.560546875, -1598.438720703125]\n",
      "\n",
      "Instance 1509 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 35: [0.6621032357215881, 0.3386150598526001, -0.06501524150371552, -0.8462619185447693, -2.64076566696167]\n",
      "Grand sum of 1057 tensor sets is: [338.28692626953125, 848.6139526367188, -242.54043579101562, 42.71428680419922, -1601.0794677734375]\n",
      "\n",
      "Instance 1510 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 23: [0.25613653659820557, 1.7356373071670532, -0.0017609372735023499, 1.8842601776123047, -1.2285208702087402]\n",
      "Grand sum of 1058 tensor sets is: [338.5430603027344, 850.349609375, -242.5421905517578, 44.598548889160156, -1602.3079833984375]\n",
      "\n",
      "Instance 1511 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 3: [1.2177218198776245, -0.5040438175201416, -0.27497974038124084, -0.19534257054328918, 1.986175775527954]\n",
      "Grand sum of 1059 tensor sets is: [339.7607727050781, 849.8455810546875, -242.81716918945312, 44.40320587158203, -1600.32177734375]\n",
      "\n",
      "Instance 1512 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 10: [-0.24185465276241302, 1.296627163887024, -0.16257280111312866, 1.0769593715667725, -1.8069483041763306]\n",
      "Grand sum of 1060 tensor sets is: [339.5189208984375, 851.1422119140625, -242.979736328125, 45.48016357421875, -1602.1287841796875]\n",
      "\n",
      "Instance 1513 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 14: [0.414150595664978, 2.727163314819336, -0.3142601549625397, -0.08707733452320099, -1.491782784461975]\n",
      "Grand sum of 1061 tensor sets is: [339.9330749511719, 853.869384765625, -243.2939910888672, 45.39308547973633, -1603.62060546875]\n",
      "\n",
      "Instance 1514 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 37: [-0.142353355884552, 1.1452974081039429, -0.9252950549125671, -0.7546662092208862, -7.564059734344482]\n",
      "Grand sum of 1062 tensor sets is: [339.79071044921875, 855.0147094726562, -244.2192840576172, 44.63842010498047, -1611.1846923828125]\n",
      "\n",
      "Instance 1515 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 10: [-0.3842359781265259, 2.4757752418518066, -0.00024133548140525818, -0.35836631059646606, -0.8739967942237854]\n",
      "Grand sum of 1063 tensor sets is: [339.4064636230469, 857.490478515625, -244.2195281982422, 44.280052185058594, -1612.0587158203125]\n",
      "\n",
      "Instance 1516 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "forever at index 23: [0.1948038637638092, -0.7633526921272278, 1.3962904214859009, -2.47843074798584, 3.2126736640930176]\n",
      "Grand sum of 1064 tensor sets is: [339.60125732421875, 856.7271118164062, -242.8232421875, 41.80162048339844, -1608.8460693359375]\n",
      "\n",
      "Instance 1517 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1518 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 15: [1.0035948753356934, 1.6475456953048706, -0.09258541464805603, 0.7471585273742676, -0.42909759283065796]\n",
      "Grand sum of 1065 tensor sets is: [340.6048583984375, 858.3746337890625, -242.91583251953125, 42.54877853393555, -1609.275146484375]\n",
      "\n",
      "Instance 1519 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 11: [0.6229428052902222, 2.7105860710144043, 0.7931156158447266, 1.218493938446045, -0.5476322770118713]\n",
      "Grand sum of 1066 tensor sets is: [341.2278137207031, 861.085205078125, -242.12271118164062, 43.76727294921875, -1609.82275390625]\n",
      "\n",
      "Instance 1520 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 32: [0.5580860376358032, 1.3533823490142822, -0.4834572374820709, -0.033720821142196655, 0.9561137557029724]\n",
      "Grand sum of 1067 tensor sets is: [341.785888671875, 862.4385986328125, -242.60617065429688, 43.733551025390625, -1608.86669921875]\n",
      "\n",
      "Instance 1521 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 29: [-0.08562163263559341, 1.6887550354003906, -0.6288149952888489, 1.618505597114563, -1.0774767398834229]\n",
      "Grand sum of 1068 tensor sets is: [341.70025634765625, 864.1273803710938, -243.2349853515625, 45.35205841064453, -1609.9442138671875]\n",
      "\n",
      "Instance 1522 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 24: [0.17382538318634033, 0.8897467255592346, 0.42547768354415894, 0.6133379340171814, -3.5955703258514404]\n",
      "Grand sum of 1069 tensor sets is: [341.87408447265625, 865.0171508789062, -242.80950927734375, 45.965396881103516, -1613.539794921875]\n",
      "\n",
      "Instance 1523 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 4: [0.8311793208122253, -2.1077728271484375, -0.9338191747665405, -1.4455567598342896, 0.3672388195991516]\n",
      "Grand sum of 1070 tensor sets is: [342.70526123046875, 862.9093627929688, -243.7433319091797, 44.519840240478516, -1613.172607421875]\n",
      "\n",
      "Instance 1524 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 30: [0.2754778563976288, 0.9682372808456421, -0.8505895137786865, -0.9398832321166992, -2.9780235290527344]\n",
      "Grand sum of 1071 tensor sets is: [342.9807434082031, 863.8776245117188, -244.5939178466797, 43.5799560546875, -1616.150634765625]\n",
      "\n",
      "Instance 1525 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 15: [0.08447905629873276, 1.1184141635894775, 1.2626866102218628, -0.3538554906845093, -0.8582282066345215]\n",
      "Grand sum of 1072 tensor sets is: [343.0652160644531, 864.9960327148438, -243.33123779296875, 43.22610092163086, -1617.0089111328125]\n",
      "\n",
      "Instance 1526 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 41: [0.3009492754936218, 1.6103100776672363, -0.5544747114181519, -1.3260111808776855, -0.3092454671859741]\n",
      "Grand sum of 1073 tensor sets is: [343.3661804199219, 866.6063232421875, -243.88571166992188, 41.900089263916016, -1617.318115234375]\n",
      "\n",
      "Instance 1527 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1528 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 15: [0.06102409213781357, 1.687580943107605, -0.013835631310939789, 0.16518719494342804, -1.3349659442901611]\n",
      "Grand sum of 1074 tensor sets is: [343.4272155761719, 868.2938842773438, -243.89955139160156, 42.065277099609375, -1618.653076171875]\n",
      "\n",
      "Instance 1529 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 3: [0.16092470288276672, 1.0392476320266724, -0.23780977725982666, -0.6785850524902344, -0.7763291597366333]\n",
      "Grand sum of 1075 tensor sets is: [343.588134765625, 869.3331298828125, -244.13735961914062, 41.38669204711914, -1619.429443359375]\n",
      "\n",
      "Instance 1530 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 65: [-0.4072004556655884, -1.165315866470337, -1.116049885749817, -1.6974555253982544, -1.7651728391647339]\n",
      "Grand sum of 1076 tensor sets is: [343.1809387207031, 868.1677856445312, -245.25340270996094, 39.68923568725586, -1621.194580078125]\n",
      "\n",
      "Instance 1531 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1532 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 8: [0.7191540598869324, 2.597564220428467, -0.13535168766975403, -1.43770432472229, 1.2078138589859009]\n",
      "Grand sum of 1077 tensor sets is: [343.90008544921875, 870.7653198242188, -245.3887481689453, 38.251529693603516, -1619.98681640625]\n",
      "\n",
      "Instance 1533 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1534 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([96, 13, 768])\n",
      "Shape of summed layers is: 96 x 768\n",
      "forever at index 74: [-0.4365505576133728, 2.724628448486328, -0.05427752435207367, 0.5233117938041687, -3.9231414794921875]\n",
      "Grand sum of 1078 tensor sets is: [343.4635314941406, 873.4899291992188, -245.44302368164062, 38.77484130859375, -1623.909912109375]\n",
      "\n",
      "Instance 1535 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 4: [-0.3498840630054474, 0.20364993810653687, -0.93637615442276, 2.0009140968322754, -5.576076030731201]\n",
      "Grand sum of 1079 tensor sets is: [343.1136474609375, 873.693603515625, -246.37939453125, 40.7757568359375, -1629.4859619140625]\n",
      "\n",
      "Instance 1536 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 28: [0.7438717484474182, 2.7962841987609863, 0.5236805081367493, -0.7688060998916626, -2.746089458465576]\n",
      "Grand sum of 1080 tensor sets is: [343.8575134277344, 876.4898681640625, -245.855712890625, 40.00695037841797, -1632.2320556640625]\n",
      "\n",
      "Instance 1537 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([86, 13, 768])\n",
      "Shape of summed layers is: 86 x 768\n",
      "forever at index 43: [0.3894956111907959, 1.1499978303909302, 0.40419888496398926, -0.04062250256538391, -1.4412838220596313]\n",
      "Grand sum of 1081 tensor sets is: [344.24700927734375, 877.639892578125, -245.45150756835938, 39.96632766723633, -1633.67333984375]\n",
      "\n",
      "Instance 1538 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 17: [-0.2090570330619812, 2.113945484161377, -0.37295180559158325, -0.6703647971153259, -5.1978044509887695]\n",
      "Grand sum of 1082 tensor sets is: [344.0379638671875, 879.7538452148438, -245.824462890625, 39.295963287353516, -1638.87109375]\n",
      "\n",
      "Instance 1539 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 37: [-0.1917235106229782, 1.4844772815704346, -1.540109395980835, 0.853671669960022, -0.9915447235107422]\n",
      "Grand sum of 1083 tensor sets is: [343.84625244140625, 881.2383422851562, -247.3645782470703, 40.149635314941406, -1639.8626708984375]\n",
      "\n",
      "Instance 1540 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1541 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 8: [0.3754754364490509, 0.3495880663394928, -0.5306942462921143, -1.1248395442962646, -0.996917724609375]\n",
      "Grand sum of 1084 tensor sets is: [344.22174072265625, 881.5879516601562, -247.89527893066406, 39.02479553222656, -1640.859619140625]\n",
      "\n",
      "Instance 1542 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "forever at index 49: [0.45053282380104065, -0.7395524978637695, -1.3806698322296143, 1.4619797468185425, -2.667952299118042]\n",
      "Grand sum of 1085 tensor sets is: [344.6722717285156, 880.848388671875, -249.2759552001953, 40.48677444458008, -1643.527587890625]\n",
      "\n",
      "Instance 1543 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [0.36479946970939636, 0.24682025611400604, -0.4848631024360657, -0.5248885154724121, -1.1551042795181274]\n",
      "Grand sum of 1086 tensor sets is: [345.0370788574219, 881.09521484375, -249.7608184814453, 39.96188735961914, -1644.6827392578125]\n",
      "\n",
      "Instance 1544 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 1087 tensor sets is: [344.3023376464844, 881.5000610351562, -249.369140625, 39.604827880859375, -1649.2435302734375]\n",
      "\n",
      "Instance 1545 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 18: [-0.21522369980812073, 0.12667641043663025, 0.13218742609024048, -0.6626375913619995, -2.2892203330993652]\n",
      "Grand sum of 1088 tensor sets is: [344.0871276855469, 881.626708984375, -249.23695373535156, 38.94219207763672, -1651.53271484375]\n",
      "\n",
      "Instance 1546 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1547 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1548 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 15: [0.17756690084934235, 1.0009195804595947, 0.10962549597024918, -0.57579505443573, -5.601437568664551]\n",
      "Grand sum of 1089 tensor sets is: [344.26470947265625, 882.6276245117188, -249.12733459472656, 38.366397857666016, -1657.1341552734375]\n",
      "\n",
      "Instance 1549 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [1.2450788021087646, 0.646034836769104, -0.3325129747390747, 0.27870291471481323, -0.7674711346626282]\n",
      "Grand sum of 1090 tensor sets is: [345.5097961425781, 883.273681640625, -249.45985412597656, 38.64509963989258, -1657.901611328125]\n",
      "\n",
      "Instance 1550 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 28: [1.6710373163223267, 2.4406800270080566, -1.2949292659759521, 1.2370859384536743, -3.6440844535827637]\n",
      "Grand sum of 1091 tensor sets is: [347.18084716796875, 885.71435546875, -250.75477600097656, 39.88218688964844, -1661.545654296875]\n",
      "\n",
      "Instance 1551 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1552 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1553 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1554 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 8: [0.7212356328964233, 1.736541986465454, -0.7470483779907227, -0.9353132247924805, -1.184665560722351]\n",
      "Grand sum of 1092 tensor sets is: [347.9020690917969, 887.450927734375, -251.5018310546875, 38.94687271118164, -1662.7303466796875]\n",
      "\n",
      "Instance 1555 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 17: [-0.02005722001194954, 1.3152607679367065, -1.4290310144424438, 0.4147929251194, -4.315427780151367]\n",
      "Grand sum of 1093 tensor sets is: [347.88201904296875, 888.7661743164062, -252.9308624267578, 39.361663818359375, -1667.0457763671875]\n",
      "\n",
      "Instance 1556 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1557 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 3: [0.4096089005470276, 0.26065558195114136, -1.32350492477417, 1.6990419626235962, 0.8962521553039551]\n",
      "Grand sum of 1094 tensor sets is: [348.2916259765625, 889.02685546875, -254.25436401367188, 41.060707092285156, -1666.1495361328125]\n",
      "\n",
      "Instance 1558 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [61]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "forever at index 61: [0.12918752431869507, -0.2972927987575531, 0.34967151284217834, 0.3797261118888855, 2.308488368988037]\n",
      "Grand sum of 1095 tensor sets is: [348.4208068847656, 888.7295532226562, -253.90469360351562, 41.440433502197266, -1663.841064453125]\n",
      "\n",
      "Instance 1559 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 11: [0.017625052481889725, -0.470812052488327, -0.7000343203544617, 1.0754239559173584, -4.280330181121826]\n",
      "Grand sum of 1096 tensor sets is: [348.4384460449219, 888.2587280273438, -254.60472106933594, 42.5158576965332, -1668.121337890625]\n",
      "\n",
      "Instance 1560 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 25: [1.0837713479995728, 1.074723482131958, -0.5189899802207947, 0.4627645015716553, -0.5006327629089355]\n",
      "Grand sum of 1097 tensor sets is: [349.522216796875, 889.3334350585938, -255.12371826171875, 42.97862243652344, -1668.6219482421875]\n",
      "\n",
      "Instance 1561 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 11: [0.14445868134498596, 0.3071666955947876, -0.5727315545082092, 1.459496021270752, 1.114387035369873]\n",
      "Grand sum of 1098 tensor sets is: [349.66668701171875, 889.640625, -255.6964569091797, 44.43811798095703, -1667.507568359375]\n",
      "\n",
      "Instance 1562 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 34: [0.5211111307144165, 1.9588099718093872, -0.9864837527275085, 3.792853832244873, -0.725614070892334]\n",
      "Grand sum of 1099 tensor sets is: [350.18780517578125, 891.5994262695312, -256.6829528808594, 48.23097229003906, -1668.233154296875]\n",
      "\n",
      "Instance 1563 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 12: [0.6088136434555054, 0.3975434899330139, 0.4411941170692444, 1.4886198043823242, -0.31663116812705994]\n",
      "Grand sum of 1100 tensor sets is: [350.796630859375, 891.9969482421875, -256.24176025390625, 49.7195930480957, -1668.5498046875]\n",
      "\n",
      "Instance 1564 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1565 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1566 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1567 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1568 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1101 tensor sets is: [350.8980407714844, 893.7362670898438, -256.8797912597656, 48.604209899902344, -1672.846435546875]\n",
      "\n",
      "Instance 1569 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 11: [-0.31641367077827454, 0.7202603220939636, 0.591779887676239, -0.8554326295852661, -3.843867778778076]\n",
      "Grand sum of 1102 tensor sets is: [350.5816345214844, 894.45654296875, -256.28802490234375, 47.748775482177734, -1676.6903076171875]\n",
      "\n",
      "Instance 1570 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([512, 13, 768])\n",
      "Shape of summed layers is: 512 x 768\n",
      "forever at index 489: [0.8344964981079102, 2.5291402339935303, 0.4705815613269806, -1.6730315685272217, 1.4032330513000488]\n",
      "Grand sum of 1103 tensor sets is: [351.4161376953125, 896.9856567382812, -255.81744384765625, 46.07574462890625, -1675.287109375]\n",
      "\n",
      "Instance 1571 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 26: [0.37860313057899475, -0.8008484840393066, -0.2810317873954773, 0.23155392706394196, -1.51703679561615]\n",
      "Grand sum of 1104 tensor sets is: [351.79473876953125, 896.184814453125, -256.0984802246094, 46.30729675292969, -1676.80419921875]\n",
      "\n",
      "Instance 1572 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 27: [2.6564555168151855, -0.28695887327194214, 0.9142811298370361, -1.1701536178588867, 1.5390628576278687]\n",
      "Grand sum of 1105 tensor sets is: [354.4512023925781, 895.8978271484375, -255.1842041015625, 45.137142181396484, -1675.26513671875]\n",
      "\n",
      "Instance 1573 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 10: [-0.19397953152656555, 0.4547581672668457, -0.6189611554145813, -1.6607660055160522, -2.7404651641845703]\n",
      "Grand sum of 1106 tensor sets is: [354.2572326660156, 896.3526000976562, -255.80316162109375, 43.476375579833984, -1678.005615234375]\n",
      "\n",
      "Instance 1574 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 15: [1.3035266399383545, 1.6241505146026611, -0.2983250021934509, -0.6585006713867188, -0.44310227036476135]\n",
      "Grand sum of 1107 tensor sets is: [355.5607604980469, 897.9767456054688, -256.10150146484375, 42.817874908447266, -1678.44873046875]\n",
      "\n",
      "Instance 1575 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1576 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 6: [1.2384650707244873, 1.389543890953064, 0.4943855404853821, -0.655780017375946, 0.33713728189468384]\n",
      "Grand sum of 1108 tensor sets is: [356.7992248535156, 899.3662719726562, -255.60711669921875, 42.16209411621094, -1678.111572265625]\n",
      "\n",
      "Instance 1577 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([141, 13, 768])\n",
      "Shape of summed layers is: 141 x 768\n",
      "forever at index 27: [1.287987232208252, 3.0086612701416016, 0.24911177158355713, 2.302521228790283, -0.7873912453651428]\n",
      "Grand sum of 1109 tensor sets is: [358.08721923828125, 902.3749389648438, -255.35800170898438, 44.46461486816406, -1678.89892578125]\n",
      "\n",
      "Instance 1578 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 12: [-0.5012862086296082, 1.3434875011444092, -0.002794206142425537, -0.6331725716590881, -3.9345502853393555]\n",
      "Grand sum of 1110 tensor sets is: [357.5859375, 903.7184448242188, -255.3607940673828, 43.831443786621094, -1682.83349609375]\n",
      "\n",
      "Instance 1579 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 9: [0.2615699768066406, -0.8733716607093811, -0.330057829618454, 0.8237425684928894, 2.332779884338379]\n",
      "Grand sum of 1111 tensor sets is: [357.8475036621094, 902.8450927734375, -255.69085693359375, 44.65518569946289, -1680.500732421875]\n",
      "\n",
      "Instance 1580 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 6: [0.3315807580947876, 1.0195425748825073, -1.2288215160369873, -0.6724015474319458, -5.118860721588135]\n",
      "Grand sum of 1112 tensor sets is: [358.1790771484375, 903.8646240234375, -256.919677734375, 43.982784271240234, -1685.61962890625]\n",
      "\n",
      "Instance 1581 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1582 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 15: [0.68857741355896, 0.552061140537262, -1.0038185119628906, 0.9418604373931885, -6.954274654388428]\n",
      "Grand sum of 1113 tensor sets is: [358.8676452636719, 904.4166870117188, -257.9234924316406, 44.924644470214844, -1692.5738525390625]\n",
      "\n",
      "Instance 1583 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 23: [0.25613653659820557, 1.7356373071670532, -0.0017609372735023499, 1.8842601776123047, -1.2285208702087402]\n",
      "Grand sum of 1114 tensor sets is: [359.123779296875, 906.15234375, -257.9252624511719, 46.80890655517578, -1693.8023681640625]\n",
      "\n",
      "Instance 1584 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "forever at index 17: [-0.34067654609680176, 0.9935600161552429, 0.2777979075908661, 0.4053666293621063, -1.9118859767913818]\n",
      "Grand sum of 1115 tensor sets is: [358.7831115722656, 907.1458740234375, -257.6474609375, 47.214271545410156, -1695.7142333984375]\n",
      "\n",
      "Instance 1585 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 1116 tensor sets is: [359.173828125, 908.2197265625, -257.87109375, 44.57332229614258, -1690.3414306640625]\n",
      "\n",
      "Instance 1586 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 34: [0.7136924266815186, 1.3583749532699585, -0.7377657294273376, -1.1533266305923462, -0.588890552520752]\n",
      "Grand sum of 1117 tensor sets is: [359.88751220703125, 909.578125, -258.6088562011719, 43.41999435424805, -1690.9302978515625]\n",
      "\n",
      "Instance 1587 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10, 21]\n",
      "Size of token embeddings is torch.Size([318, 13, 768])\n",
      "Shape of summed layers is: 318 x 768\n",
      "forever at index 10: [1.0840656757354736, -0.6625480651855469, 0.21505406498908997, 0.6509239673614502, -1.086319088935852]\n",
      "forever at index 21: [0.9328203797340393, 0.3705559968948364, 0.4868112802505493, 0.4545937180519104, -1.7759140729904175]\n",
      "Grand sum of 1118 tensor sets is: [360.8959655761719, 909.43212890625, -258.2579345703125, 43.97275161743164, -1692.3614501953125]\n",
      "\n",
      "Instance 1588 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1589 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 39: [-0.5993912220001221, 2.547759532928467, -0.03306249529123306, -0.952208399772644, 2.061579704284668]\n",
      "Grand sum of 1119 tensor sets is: [360.29656982421875, 911.9798583984375, -258.2909851074219, 43.02054214477539, -1690.2999267578125]\n",
      "\n",
      "Instance 1590 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1591 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 4: [0.06223367154598236, 3.160944700241089, 0.16775807738304138, 0.03181636333465576, -0.9675204753875732]\n",
      "Grand sum of 1120 tensor sets is: [360.3587951660156, 915.1408081054688, -258.12322998046875, 43.0523567199707, -1691.2674560546875]\n",
      "\n",
      "Instance 1592 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1593 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 9: [0.42478320002555847, 0.93290114402771, -1.7236628532409668, 0.5517960786819458, -6.107257843017578]\n",
      "Grand sum of 1121 tensor sets is: [360.7835693359375, 916.07373046875, -259.8468933105469, 43.60415267944336, -1697.374755859375]\n",
      "\n",
      "Instance 1594 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 1122 tensor sets is: [361.4783630371094, 917.20703125, -258.64373779296875, 42.33478927612305, -1698.2650146484375]\n",
      "\n",
      "Instance 1595 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1596 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([84, 13, 768])\n",
      "Shape of summed layers is: 84 x 768\n",
      "forever at index 40: [0.9684860706329346, 0.1456630825996399, 1.5928456783294678, -2.1146888732910156, 0.24556592106819153]\n",
      "Grand sum of 1123 tensor sets is: [362.44683837890625, 917.3527221679688, -257.0509033203125, 40.22010040283203, -1698.0194091796875]\n",
      "\n",
      "Instance 1597 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 1124 tensor sets is: [362.842529296875, 918.3613891601562, -257.1204833984375, 39.536537170410156, -1696.72314453125]\n",
      "\n",
      "Instance 1598 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [0.5422123670578003, -0.37714433670043945, -1.0815092325210571, 0.34070926904678345, -2.6716675758361816]\n",
      "Grand sum of 1125 tensor sets is: [363.3847351074219, 917.9842529296875, -258.2019958496094, 39.87724685668945, -1699.394775390625]\n",
      "\n",
      "Instance 1599 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 1126 tensor sets is: [362.6499938964844, 918.3890991210938, -257.810302734375, 39.52018737792969, -1703.95556640625]\n",
      "\n",
      "Instance 1600 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.05075901746749878, 2.21020770072937, 0.04688075929880142, -1.1270672082901, -1.3753117322921753]\n",
      "Grand sum of 1127 tensor sets is: [362.70074462890625, 920.5993041992188, -257.763427734375, 38.39311981201172, -1705.3309326171875]\n",
      "\n",
      "Instance 1601 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 11: [1.003737211227417, 1.6404950618743896, -0.5222038626670837, 0.9465548992156982, -1.5633914470672607]\n",
      "Grand sum of 1128 tensor sets is: [363.7044677734375, 922.2398071289062, -258.28564453125, 39.33967590332031, -1706.894287109375]\n",
      "\n",
      "Instance 1602 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [73]\n",
      "Size of token embeddings is torch.Size([84, 13, 768])\n",
      "Shape of summed layers is: 84 x 768\n",
      "forever at index 73: [-0.019175486639142036, -0.9730319976806641, -0.2739437520503998, 1.0227832794189453, -1.321164846420288]\n",
      "Grand sum of 1129 tensor sets is: [363.685302734375, 921.2667846679688, -258.5596008300781, 40.362457275390625, -1708.2154541015625]\n",
      "\n",
      "Instance 1603 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 14: [0.2762794494628906, 1.5158710479736328, 0.45176345109939575, -0.5854049921035767, 0.0990290641784668]\n",
      "Grand sum of 1130 tensor sets is: [363.9615783691406, 922.7826538085938, -258.10784912109375, 39.77705383300781, -1708.116455078125]\n",
      "\n",
      "Instance 1604 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1605 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1606 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 32: [-0.6230658292770386, 0.5303539037704468, -1.0711599588394165, -0.12893027067184448, -3.2016351222991943]\n",
      "Grand sum of 1131 tensor sets is: [363.3385009765625, 923.31298828125, -259.17901611328125, 39.64812469482422, -1711.318115234375]\n",
      "\n",
      "Instance 1607 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1608 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 31: [1.2444370985031128, 2.083610773086548, 0.012480378150939941, -0.08371381461620331, -0.07705679535865784]\n",
      "Grand sum of 1132 tensor sets is: [364.58294677734375, 925.3966064453125, -259.1665344238281, 39.56441116333008, -1711.3951416015625]\n",
      "\n",
      "Instance 1609 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 8: [0.7823385000228882, -0.7494251728057861, 0.24732759594917297, -0.025110512971878052, -1.7710593938827515]\n",
      "Grand sum of 1133 tensor sets is: [365.36529541015625, 924.6471557617188, -258.9192199707031, 39.53929901123047, -1713.166259765625]\n",
      "\n",
      "Instance 1610 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 27: [-0.14658421277999878, 1.7373427152633667, -0.1413535624742508, -0.6529784798622131, -2.432753086090088]\n",
      "Grand sum of 1134 tensor sets is: [365.2187194824219, 926.384521484375, -259.0605773925781, 38.886322021484375, -1715.5989990234375]\n",
      "\n",
      "Instance 1611 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 16: [0.38789165019989014, 0.892227292060852, -0.18999476730823517, 0.7209543585777283, -0.769005537033081]\n",
      "Grand sum of 1135 tensor sets is: [365.6065979003906, 927.2767333984375, -259.2505798339844, 39.607276916503906, -1716.3680419921875]\n",
      "\n",
      "Instance 1612 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1613 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 11: [3.572493076324463, 1.318234920501709, 0.3219805359840393, 1.770577073097229, -0.6667544841766357]\n",
      "Grand sum of 1136 tensor sets is: [369.1790771484375, 928.594970703125, -258.9285888671875, 41.37785339355469, -1717.0347900390625]\n",
      "\n",
      "Instance 1614 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 14: [0.4359724819660187, 1.7764891386032104, 0.3081552982330322, -1.0068445205688477, 0.8073806762695312]\n",
      "Grand sum of 1137 tensor sets is: [369.61505126953125, 930.3714599609375, -258.62042236328125, 40.371009826660156, -1716.2274169921875]\n",
      "\n",
      "Instance 1615 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 20: [0.29291456937789917, 2.619406223297119, -0.7125177383422852, 0.2201201319694519, -4.801540851593018]\n",
      "Grand sum of 1138 tensor sets is: [369.907958984375, 932.9908447265625, -259.33294677734375, 40.591129302978516, -1721.0289306640625]\n",
      "\n",
      "Instance 1616 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1617 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1618 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1619 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 14: [1.7092270851135254, -0.01818719506263733, -0.13765977323055267, -0.2920265793800354, -6.528299331665039]\n",
      "Grand sum of 1139 tensor sets is: [371.6171875, 932.97265625, -259.4706115722656, 40.299102783203125, -1727.5572509765625]\n",
      "\n",
      "Instance 1620 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [61]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "forever at index 61: [0.7652768492698669, 0.45849353075027466, -0.0454796701669693, 0.042384058237075806, -6.189453601837158]\n",
      "Grand sum of 1140 tensor sets is: [372.3824768066406, 933.43115234375, -259.5160827636719, 40.341487884521484, -1733.7467041015625]\n",
      "\n",
      "Instance 1621 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 21: [0.3565308749675751, 0.13220976293087006, -1.0269287824630737, 2.582505464553833, -5.253737449645996]\n",
      "Grand sum of 1141 tensor sets is: [372.739013671875, 933.5633544921875, -260.5429992675781, 42.92399215698242, -1739.00048828125]\n",
      "\n",
      "Instance 1622 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 26: [0.8065680265426636, 0.8415888547897339, 0.9611994624137878, -2.0984177589416504, -2.0199716091156006]\n",
      "Grand sum of 1142 tensor sets is: [373.54559326171875, 934.4049682617188, -259.581787109375, 40.8255729675293, -1741.0205078125]\n",
      "\n",
      "Instance 1623 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 9: [0.5218549370765686, 0.9484233856201172, 0.1863338053226471, 0.2602981925010681, -3.169445037841797]\n",
      "Grand sum of 1143 tensor sets is: [374.06744384765625, 935.3533935546875, -259.39544677734375, 41.085872650146484, -1744.18994140625]\n",
      "\n",
      "Instance 1624 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 9: [-0.8260839581489563, 1.9985580444335938, 0.23278620839118958, 0.6706464290618896, -1.7037709951400757]\n",
      "Grand sum of 1144 tensor sets is: [373.2413635253906, 937.3519287109375, -259.16265869140625, 41.75651931762695, -1745.8936767578125]\n",
      "\n",
      "Instance 1625 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "forever at index 2: [0.1687939465045929, 0.6238967180252075, -1.5755114555358887, 0.32566219568252563, -0.7747147083282471]\n",
      "Grand sum of 1145 tensor sets is: [373.41015625, 937.975830078125, -260.7381591796875, 42.08218002319336, -1746.6683349609375]\n",
      "\n",
      "Instance 1626 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 12: [1.1312917470932007, -0.40726321935653687, 0.6903752088546753, -0.8645679950714111, -4.637851715087891]\n",
      "Grand sum of 1146 tensor sets is: [374.54144287109375, 937.5685424804688, -260.04779052734375, 41.217613220214844, -1751.30615234375]\n",
      "\n",
      "Instance 1627 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 3: [1.2006046772003174, 0.5808532238006592, -0.24634915590286255, -0.06272569298744202, 3.2023799419403076]\n",
      "Grand sum of 1147 tensor sets is: [375.7420349121094, 938.1494140625, -260.29412841796875, 41.15488815307617, -1748.103759765625]\n",
      "\n",
      "Instance 1628 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1629 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 9: [0.4463651776313782, 2.2901675701141357, 0.7634134292602539, 2.1807525157928467, -0.303458034992218]\n",
      "Grand sum of 1148 tensor sets is: [376.1883850097656, 940.4395751953125, -259.53070068359375, 43.33563995361328, -1748.4072265625]\n",
      "\n",
      "Instance 1630 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 18: [-1.0150537490844727, 1.4278802871704102, 0.32866111397743225, -1.4641917943954468, -6.022346496582031]\n",
      "Grand sum of 1149 tensor sets is: [375.17333984375, 941.867431640625, -259.2020263671875, 41.8714485168457, -1754.4295654296875]\n",
      "\n",
      "Instance 1631 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 25: [0.10129414498806, 2.7727768421173096, 0.3695603013038635, -0.6778906583786011, -1.7446908950805664]\n",
      "Grand sum of 1150 tensor sets is: [375.2746276855469, 944.6401977539062, -258.83245849609375, 41.19355773925781, -1756.17431640625]\n",
      "\n",
      "Instance 1632 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 10: [1.010695219039917, 0.9581845998764038, 0.8642725944519043, 1.225913643836975, -0.134840190410614]\n",
      "Grand sum of 1151 tensor sets is: [376.2853088378906, 945.598388671875, -257.96820068359375, 42.419471740722656, -1756.3092041015625]\n",
      "\n",
      "Instance 1633 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 28: [-0.3301142454147339, 2.7885491847991943, 1.6111890077590942, 0.45580074191093445, 3.2045745849609375]\n",
      "Grand sum of 1152 tensor sets is: [375.9552001953125, 948.386962890625, -256.3570251464844, 42.87527084350586, -1753.1046142578125]\n",
      "\n",
      "Instance 1634 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 1153 tensor sets is: [376.35089111328125, 949.3956298828125, -256.4266052246094, 42.191707611083984, -1751.808349609375]\n",
      "\n",
      "Instance 1635 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 13: [-0.5899449586868286, -1.1071178913116455, 0.19296544790267944, 1.2115206718444824, -7.616400241851807]\n",
      "Grand sum of 1154 tensor sets is: [375.7609558105469, 948.2885131835938, -256.233642578125, 43.403228759765625, -1759.4248046875]\n",
      "\n",
      "Instance 1636 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 1155 tensor sets is: [376.5995788574219, 948.6444702148438, -256.63531494140625, 43.91685485839844, -1756.8857421875]\n",
      "\n",
      "Instance 1637 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 30: [0.9550421833992004, 0.40939247608184814, 0.7572590112686157, 1.0330222845077515, -2.8581228256225586]\n",
      "Grand sum of 1156 tensor sets is: [377.55462646484375, 949.0538330078125, -255.8780517578125, 44.94987869262695, -1759.743896484375]\n",
      "\n",
      "Instance 1638 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1639 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [54]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "forever at index 54: [-0.35940536856651306, -0.052329882979393005, -1.0900832414627075, 0.6745641231536865, -0.9731273651123047]\n",
      "Grand sum of 1157 tensor sets is: [377.1952209472656, 949.0015258789062, -256.9681396484375, 45.62444305419922, -1760.717041015625]\n",
      "\n",
      "Instance 1640 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 21: [0.2920752763748169, 2.034810781478882, -0.13773176074028015, 1.0095350742340088, -4.332393169403076]\n",
      "Grand sum of 1158 tensor sets is: [377.4873046875, 951.0363159179688, -257.1058654785156, 46.63397979736328, -1765.0494384765625]\n",
      "\n",
      "Instance 1641 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 28: [0.30456122756004333, 1.8290811777114868, -1.3457883596420288, 2.0160727500915527, -5.646443843841553]\n",
      "Grand sum of 1159 tensor sets is: [377.7918701171875, 952.8654174804688, -258.45166015625, 48.65005111694336, -1770.6959228515625]\n",
      "\n",
      "Instance 1642 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [55]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 55: [-0.24874716997146606, 1.0213277339935303, -0.8490011096000671, 1.623038649559021, -2.248537302017212]\n",
      "Grand sum of 1160 tensor sets is: [377.5431213378906, 953.88671875, -259.3006591796875, 50.27309036254883, -1772.9444580078125]\n",
      "\n",
      "Instance 1643 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1644 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 18: [-0.6471228003501892, 2.6363301277160645, 0.7268035411834717, 0.5757290124893188, -5.501775741577148]\n",
      "Grand sum of 1161 tensor sets is: [376.89599609375, 956.5230712890625, -258.5738525390625, 50.848819732666016, -1778.4462890625]\n",
      "\n",
      "Instance 1645 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1646 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 14: [0.8316928148269653, 0.3446013927459717, -0.09427622705698013, -0.6584572792053223, 0.08989842236042023]\n",
      "Grand sum of 1162 tensor sets is: [377.7276916503906, 956.86767578125, -258.6681213378906, 50.19036102294922, -1778.3564453125]\n",
      "\n",
      "Instance 1647 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 7: [-0.08373568207025528, 1.5458321571350098, -0.12904828786849976, 0.4991111755371094, 0.7249925136566162]\n",
      "Grand sum of 1163 tensor sets is: [377.6439514160156, 958.4135131835938, -258.79718017578125, 50.68947219848633, -1777.6314697265625]\n",
      "\n",
      "Instance 1648 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 11: [0.7115077972412109, 1.304237961769104, -0.18021690845489502, -0.7568598389625549, 0.20930039882659912]\n",
      "Grand sum of 1164 tensor sets is: [378.35546875, 959.7177734375, -258.9773864746094, 49.932613372802734, -1777.422119140625]\n",
      "\n",
      "Instance 1649 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1650 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([5, 13, 768])\n",
      "Shape of summed layers is: 5 x 768\n",
      "forever at index 2: [-0.33841848373413086, 2.1172003746032715, -0.7663429975509644, 0.02998320758342743, 1.156374216079712]\n",
      "Grand sum of 1165 tensor sets is: [378.0170593261719, 961.8349609375, -259.7437438964844, 49.96259689331055, -1776.2657470703125]\n",
      "\n",
      "Instance 1651 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1652 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 17: [0.3721219301223755, 0.8616467714309692, -2.163327932357788, 1.1021146774291992, -1.1164844036102295]\n",
      "Grand sum of 1166 tensor sets is: [378.3891906738281, 962.6965942382812, -261.9070739746094, 51.06471252441406, -1777.3822021484375]\n",
      "\n",
      "Instance 1653 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1654 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1655 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 1167 tensor sets is: [380.1805419921875, 961.1419677734375, -262.5893249511719, 50.218788146972656, -1776.92919921875]\n",
      "\n",
      "Instance 1656 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 15: [0.6035048961639404, 1.5045939683914185, -0.2767743766307831, 1.0063964128494263, -6.262855529785156]\n",
      "Grand sum of 1168 tensor sets is: [380.7840576171875, 962.6465454101562, -262.8660888671875, 51.22518539428711, -1783.1920166015625]\n",
      "\n",
      "Instance 1657 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 9: [0.14400172233581543, 1.5506548881530762, -0.07570625841617584, 0.9953963160514832, -1.9012373685836792]\n",
      "Grand sum of 1169 tensor sets is: [380.9280700683594, 964.1972045898438, -262.9418029785156, 52.2205810546875, -1785.09326171875]\n",
      "\n",
      "Instance 1658 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1659 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1660 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 19: [0.3759021461009979, 0.022357173264026642, -0.5604652166366577, -1.6594786643981934, -4.375090599060059]\n",
      "Grand sum of 1170 tensor sets is: [381.3039855957031, 964.2195434570312, -263.50225830078125, 50.56110382080078, -1789.4683837890625]\n",
      "\n",
      "Instance 1661 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1662 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 7: [0.8184988498687744, 1.226743459701538, -0.18018695712089539, 1.6873916387557983, -4.644508361816406]\n",
      "Grand sum of 1171 tensor sets is: [382.12249755859375, 965.4462890625, -263.68243408203125, 52.248497009277344, -1794.1129150390625]\n",
      "\n",
      "Instance 1663 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 8: [0.04481939971446991, 0.6415148377418518, -0.6004641056060791, -0.3571919798851013, -3.1906676292419434]\n",
      "Grand sum of 1172 tensor sets is: [382.1673278808594, 966.0878295898438, -264.28289794921875, 51.89130401611328, -1797.3035888671875]\n",
      "\n",
      "Instance 1664 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 36: [0.6477728486061096, 0.9316562414169312, -0.7127490639686584, 1.2018117904663086, -5.375816822052002]\n",
      "Grand sum of 1173 tensor sets is: [382.8150939941406, 967.0194702148438, -264.9956359863281, 53.093116760253906, -1802.679443359375]\n",
      "\n",
      "Instance 1665 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 3: [1.1526155471801758, -0.35349705815315247, -0.598080039024353, -0.6890103816986084, 1.427318811416626]\n",
      "Grand sum of 1174 tensor sets is: [383.96771240234375, 966.6659545898438, -265.5937194824219, 52.40410614013672, -1801.2520751953125]\n",
      "\n",
      "Instance 1666 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 9: [1.546895980834961, 0.019613951444625854, -0.9013502597808838, -0.16504448652267456, -0.5620764493942261]\n",
      "Grand sum of 1175 tensor sets is: [385.5146179199219, 966.685546875, -266.49505615234375, 52.23906326293945, -1801.814208984375]\n",
      "\n",
      "Instance 1667 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 13: [1.2787045240402222, -0.9359003901481628, -0.21519187092781067, -0.4537562131881714, 0.7751668691635132]\n",
      "Grand sum of 1176 tensor sets is: [386.7933349609375, 965.7496337890625, -266.7102355957031, 51.785308837890625, -1801.0390625]\n",
      "\n",
      "Instance 1668 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 1177 tensor sets is: [387.18902587890625, 966.75830078125, -266.7798156738281, 51.10174560546875, -1799.7427978515625]\n",
      "\n",
      "Instance 1669 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 6: [0.8421875238418579, 0.42780303955078125, 0.39635711908340454, 0.19624298810958862, -1.7869514226913452]\n",
      "Grand sum of 1178 tensor sets is: [388.0312194824219, 967.1860961914062, -266.3834533691406, 51.29798889160156, -1801.52978515625]\n",
      "\n",
      "Instance 1670 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1671 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1672 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1673 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1674 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 3: [0.032609812915325165, 0.9036938548088074, -0.5419735908508301, 0.9442896246910095, -3.3316798210144043]\n",
      "Grand sum of 1179 tensor sets is: [388.0638427734375, 968.0897827148438, -266.9254150390625, 52.242279052734375, -1804.8614501953125]\n",
      "\n",
      "Instance 1675 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "forever at index 33: [0.5447864532470703, 0.8691648244857788, -0.21205826103687286, -0.9309178590774536, 0.5619667768478394]\n",
      "Grand sum of 1180 tensor sets is: [388.608642578125, 968.9589233398438, -267.1374816894531, 51.31135940551758, -1804.2994384765625]\n",
      "\n",
      "Instance 1676 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1677 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 36: [0.06538059562444687, 0.44880592823028564, 0.23172646760940552, -0.7714033722877502, -2.221250534057617]\n",
      "Grand sum of 1181 tensor sets is: [388.67401123046875, 969.40771484375, -266.90576171875, 50.539955139160156, -1806.5206298828125]\n",
      "\n",
      "Instance 1678 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1679 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 9: [-0.6993216872215271, 0.1232987642288208, -0.05163644999265671, -0.3039001226425171, -2.182028293609619]\n",
      "Grand sum of 1182 tensor sets is: [387.9747009277344, 969.531005859375, -266.9573974609375, 50.236053466796875, -1808.70263671875]\n",
      "\n",
      "Instance 1680 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1681 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 9: [0.881157636642456, -0.2143162339925766, -1.8575067520141602, -1.0039076805114746, -2.2189066410064697]\n",
      "Grand sum of 1183 tensor sets is: [388.8558654785156, 969.3167114257812, -268.8149108886719, 49.232147216796875, -1810.9215087890625]\n",
      "\n",
      "Instance 1682 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1184 tensor sets is: [388.957275390625, 971.0560302734375, -269.45294189453125, 48.116764068603516, -1815.2181396484375]\n",
      "\n",
      "Instance 1683 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 26: [0.8086232542991638, 2.7507808208465576, 0.145180806517601, 1.8992515802383423, -0.6766523718833923]\n",
      "Grand sum of 1185 tensor sets is: [389.7658996582031, 973.8068237304688, -269.3077697753906, 50.016014099121094, -1815.894775390625]\n",
      "\n",
      "Instance 1684 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 25: [0.20626860857009888, 2.0086677074432373, -0.2991522252559662, -0.10997264087200165, 0.5599933862686157]\n",
      "Grand sum of 1186 tensor sets is: [389.97216796875, 975.8154907226562, -269.60693359375, 49.90604019165039, -1815.3348388671875]\n",
      "\n",
      "Instance 1685 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 7: [0.34615278244018555, 1.4784549474716187, -0.39662179350852966, -0.48693737387657166, -2.7038135528564453]\n",
      "Grand sum of 1187 tensor sets is: [390.3183288574219, 977.2939453125, -270.0035705566406, 49.41910171508789, -1818.0386962890625]\n",
      "\n",
      "Instance 1686 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "forever at index 31: [0.44526633620262146, 1.1814833879470825, -0.5583636164665222, -0.7470312118530273, -5.050550937652588]\n",
      "Grand sum of 1188 tensor sets is: [390.7635803222656, 978.4754028320312, -270.5619201660156, 48.67206954956055, -1823.0892333984375]\n",
      "\n",
      "Instance 1687 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 33: [-0.3104149401187897, 1.7804526090621948, -1.2166333198547363, 0.7740077972412109, 0.6117056608200073]\n",
      "Grand sum of 1189 tensor sets is: [390.4531555175781, 980.255859375, -271.778564453125, 49.446075439453125, -1822.4775390625]\n",
      "\n",
      "Instance 1688 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.5266655683517456, 0.2738105356693268, -0.20097297430038452, -0.9150396585464478, -1.1159794330596924]\n",
      "Grand sum of 1190 tensor sets is: [390.9798278808594, 980.5296630859375, -271.9795227050781, 48.531036376953125, -1823.593505859375]\n",
      "\n",
      "Instance 1689 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 12: [0.31702303886413574, 1.2536346912384033, -1.413166880607605, 0.07282432913780212, -5.246798992156982]\n",
      "Grand sum of 1191 tensor sets is: [391.2968444824219, 981.7833251953125, -273.3927001953125, 48.603858947753906, -1828.84033203125]\n",
      "\n",
      "Instance 1690 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1691 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1692 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 26: [0.35886162519454956, 1.6567275524139404, 0.4127803146839142, -0.6584648489952087, -0.9872159957885742]\n",
      "Grand sum of 1192 tensor sets is: [391.65570068359375, 983.4400634765625, -272.97991943359375, 47.94539260864258, -1829.8275146484375]\n",
      "\n",
      "Instance 1693 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1694 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [-0.20171532034873962, 2.932819128036499, -0.8074631690979004, -0.1686008721590042, -5.406799793243408]\n",
      "Grand sum of 1193 tensor sets is: [391.4539794921875, 986.3728637695312, -273.7873840332031, 47.776790618896484, -1835.234375]\n",
      "\n",
      "Instance 1695 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 6: [-0.0659170001745224, 0.18661877512931824, -0.8259607553482056, 2.5733258724212646, -2.0173346996307373]\n",
      "Grand sum of 1194 tensor sets is: [391.3880615234375, 986.5595092773438, -274.61334228515625, 50.35011672973633, -1837.251708984375]\n",
      "\n",
      "Instance 1696 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 16: [-0.40659934282302856, -0.034516215324401855, -0.18873190879821777, 0.6844087839126587, -4.421189308166504]\n",
      "Grand sum of 1195 tensor sets is: [390.9814758300781, 986.5249633789062, -274.80206298828125, 51.03452682495117, -1841.6728515625]\n",
      "\n",
      "Instance 1697 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 37: [0.6259977221488953, 1.2646294832229614, -0.2358868420124054, 2.2125396728515625, -0.14502069354057312]\n",
      "Grand sum of 1196 tensor sets is: [391.60748291015625, 987.7896118164062, -275.0379638671875, 53.247066497802734, -1841.81787109375]\n",
      "\n",
      "Instance 1698 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 10: [0.09152780473232269, 0.44090303778648376, -0.7119274735450745, 1.209414005279541, -4.417181015014648]\n",
      "Grand sum of 1197 tensor sets is: [391.6990051269531, 988.2305297851562, -275.7498779296875, 54.45648193359375, -1846.235107421875]\n",
      "\n",
      "Instance 1699 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 28: [0.6821977496147156, 1.6481544971466064, 0.14911490678787231, -0.8097460269927979, 0.9285337924957275]\n",
      "Grand sum of 1198 tensor sets is: [392.3811950683594, 989.878662109375, -275.60076904296875, 53.64673614501953, -1845.3065185546875]\n",
      "\n",
      "Instance 1700 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [0.9832466840744019, 0.19009733200073242, -0.32321181893348694, 0.42473360896110535, 0.5483078360557556]\n",
      "Grand sum of 1199 tensor sets is: [393.36444091796875, 990.0687866210938, -275.9239807128906, 54.071468353271484, -1844.7581787109375]\n",
      "\n",
      "Instance 1701 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1702 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1703 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [52]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "forever at index 52: [0.6576010584831238, -0.5561482906341553, -1.461075782775879, 1.6579723358154297, -2.0563032627105713]\n",
      "Grand sum of 1200 tensor sets is: [394.02203369140625, 989.5126342773438, -277.38507080078125, 55.72943878173828, -1846.814453125]\n",
      "\n",
      "Instance 1704 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 13: [1.0306488275527954, -0.006747938692569733, 0.1850205510854721, 0.4763759672641754, -1.3853257894515991]\n",
      "Grand sum of 1201 tensor sets is: [395.05267333984375, 989.505859375, -277.2000427246094, 56.205814361572266, -1848.1998291015625]\n",
      "\n",
      "Instance 1705 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([161, 13, 768])\n",
      "Shape of summed layers is: 161 x 768\n",
      "forever at index 106: [0.16450457274913788, -1.536022663116455, 0.6757814884185791, 0.9335299134254456, -1.0018373727798462]\n",
      "Grand sum of 1202 tensor sets is: [395.2171630859375, 987.9698486328125, -276.5242614746094, 57.13934326171875, -1849.20166015625]\n",
      "\n",
      "Instance 1706 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 42: [0.073648601770401, 0.28873270750045776, -1.1674768924713135, 2.215874671936035, 1.4167451858520508]\n",
      "Grand sum of 1203 tensor sets is: [395.2908020019531, 988.2586059570312, -277.6917419433594, 59.35521697998047, -1847.784912109375]\n",
      "\n",
      "Instance 1707 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 6: [0.6050586104393005, -0.3654163181781769, -0.8042585849761963, -0.7856321334838867, -5.909743309020996]\n",
      "Grand sum of 1204 tensor sets is: [395.8958740234375, 987.8931884765625, -278.4960021972656, 58.569583892822266, -1853.6947021484375]\n",
      "\n",
      "Instance 1708 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 25: [0.8050746917724609, 1.4370639324188232, -0.4833424687385559, -1.7373411655426025, 1.2307367324829102]\n",
      "Grand sum of 1205 tensor sets is: [396.7009582519531, 989.3302612304688, -278.9793395996094, 56.83224105834961, -1852.4639892578125]\n",
      "\n",
      "Instance 1709 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 22: [0.9013208150863647, 0.501822292804718, -0.2314746081829071, 0.13549034297466278, 0.9646273255348206]\n",
      "Grand sum of 1206 tensor sets is: [397.6022644042969, 989.8320922851562, -279.2108154296875, 56.96773147583008, -1851.4993896484375]\n",
      "\n",
      "Instance 1710 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 1207 tensor sets is: [396.9668273925781, 991.5538940429688, -278.4783020019531, 58.27350997924805, -1851.6761474609375]\n",
      "\n",
      "Instance 1711 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 17: [-0.17552907764911652, 1.279524326324463, -1.1725163459777832, 0.8971952795982361, -6.025716781616211]\n",
      "Grand sum of 1208 tensor sets is: [396.7912902832031, 992.8334350585938, -279.65081787109375, 59.17070388793945, -1857.701904296875]\n",
      "\n",
      "Instance 1712 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1713 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "forever at index 7: [0.45713508129119873, -0.6950680017471313, 0.08215746283531189, 0.8868474364280701, -3.7884974479675293]\n",
      "Grand sum of 1209 tensor sets is: [397.2484130859375, 992.1383666992188, -279.56866455078125, 60.057552337646484, -1861.4903564453125]\n",
      "\n",
      "Instance 1714 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1715 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1716 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 14: [1.1189937591552734, 1.9516034126281738, -1.0186811685562134, 2.6258020401000977, 1.9498555660247803]\n",
      "Grand sum of 1210 tensor sets is: [398.3674011230469, 994.0899658203125, -280.58734130859375, 62.683353424072266, -1859.54052734375]\n",
      "\n",
      "Instance 1717 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1718 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 7: [0.19168205559253693, 0.4769518971443176, -0.2902992069721222, -1.3021159172058105, -3.3055732250213623]\n",
      "Grand sum of 1211 tensor sets is: [398.55908203125, 994.56689453125, -280.8776550292969, 61.3812370300293, -1862.8460693359375]\n",
      "\n",
      "Instance 1719 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1720 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1721 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1722 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 11: [0.6822685599327087, 2.3814902305603027, -0.5504717826843262, -1.486860990524292, -3.1557974815368652]\n",
      "Grand sum of 1212 tensor sets is: [399.2413635253906, 996.9483642578125, -281.4281311035156, 59.89437484741211, -1866.0018310546875]\n",
      "\n",
      "Instance 1723 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 7: [0.6276252269744873, -0.07134130597114563, 0.011950615793466568, 0.44015973806381226, -3.6122031211853027]\n",
      "Grand sum of 1213 tensor sets is: [399.8689880371094, 996.8770141601562, -281.4161682128906, 60.33453369140625, -1869.614013671875]\n",
      "\n",
      "Instance 1724 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1725 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1726 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "forever at index 31: [0.7797679305076599, 1.519208550453186, -0.4485541880130768, 1.3213329315185547, -0.5561649799346924]\n",
      "Grand sum of 1214 tensor sets is: [400.64874267578125, 998.396240234375, -281.8647155761719, 61.65586853027344, -1870.170166015625]\n",
      "\n",
      "Instance 1727 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 65: [-0.4072004556655884, -1.165315866470337, -1.116049885749817, -1.6974555253982544, -1.7651728391647339]\n",
      "Grand sum of 1215 tensor sets is: [400.2415466308594, 997.2308959960938, -282.98077392578125, 59.958412170410156, -1871.935302734375]\n",
      "\n",
      "Instance 1728 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 1216 tensor sets is: [400.78546142578125, 996.5960693359375, -283.63031005859375, 58.81618881225586, -1870.81396484375]\n",
      "\n",
      "Instance 1729 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1730 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7, 21]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 7: [0.5436997413635254, -0.6106987595558167, 1.338125228881836, 1.21939218044281, 1.0076067447662354]\n",
      "forever at index 21: [1.223418116569519, -0.21843191981315613, 1.7668100595474243, 0.419169157743454, 0.46715521812438965]\n",
      "Grand sum of 1217 tensor sets is: [401.66900634765625, 996.1815185546875, -282.0778503417969, 59.63547134399414, -1870.0765380859375]\n",
      "\n",
      "Instance 1731 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 26: [0.4730048179626465, 1.1848609447479248, -0.523694634437561, -1.0575138330459595, -1.040798544883728]\n",
      "Grand sum of 1218 tensor sets is: [402.1419982910156, 997.3663940429688, -282.6015319824219, 58.57795715332031, -1871.1173095703125]\n",
      "\n",
      "Instance 1732 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [1.0301811695098877, 0.09672722220420837, -1.850982904434204, 0.9610074162483215, -2.2480239868164062]\n",
      "Grand sum of 1219 tensor sets is: [403.17218017578125, 997.463134765625, -284.4525146484375, 59.538963317871094, -1873.3653564453125]\n",
      "\n",
      "Instance 1733 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 19: [0.47943368554115295, 1.5855116844177246, 0.40771910548210144, -0.6598290205001831, -5.621279239654541]\n",
      "Grand sum of 1220 tensor sets is: [403.651611328125, 999.0486450195312, -284.0447998046875, 58.87913513183594, -1878.9866943359375]\n",
      "\n",
      "Instance 1734 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 19: [0.31799232959747314, 2.0471644401550293, -0.1606685370206833, 0.40147292613983154, -2.119386672973633]\n",
      "Grand sum of 1221 tensor sets is: [403.9696044921875, 1001.0958251953125, -284.2054748535156, 59.280609130859375, -1881.1060791015625]\n",
      "\n",
      "Instance 1735 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1736 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1737 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1738 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 4: [0.13308516144752502, 1.3706238269805908, 0.2873152494430542, -0.1941385120153427, -1.353935956954956]\n",
      "Grand sum of 1222 tensor sets is: [404.1026916503906, 1002.4664306640625, -283.91815185546875, 59.08647155761719, -1882.4599609375]\n",
      "\n",
      "Instance 1739 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10, 26]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 10: [0.15990924835205078, -0.7313010096549988, 1.160194754600525, -0.4916960299015045, -1.2919011116027832]\n",
      "forever at index 26: [-0.22681547701358795, -0.6101900339126587, 1.2631909847259521, -0.34333181381225586, -0.9942067861557007]\n",
      "Grand sum of 1223 tensor sets is: [404.0692443847656, 1001.7957153320312, -282.7064514160156, 58.6689567565918, -1883.60302734375]\n",
      "\n",
      "Instance 1740 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1741 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 11: [0.3670167028903961, 1.1460521221160889, 0.8667268753051758, 1.5957746505737305, -1.049910545349121]\n",
      "Grand sum of 1224 tensor sets is: [404.4362487792969, 1002.9417724609375, -281.8397216796875, 60.264732360839844, -1884.6529541015625]\n",
      "\n",
      "Instance 1742 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 12: [0.4669851064682007, 0.16052097082138062, -0.7754212617874146, -0.18280163407325745, -1.9563658237457275]\n",
      "Grand sum of 1225 tensor sets is: [404.9032287597656, 1003.102294921875, -282.6151428222656, 60.081932067871094, -1886.609375]\n",
      "\n",
      "Instance 1743 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 16: [-0.02942943572998047, -0.3098869323730469, 0.09064827859401703, -0.9771841764450073, -0.36661186814308167]\n",
      "Grand sum of 1226 tensor sets is: [404.8738098144531, 1002.7924194335938, -282.5245056152344, 59.1047477722168, -1886.9759521484375]\n",
      "\n",
      "Instance 1744 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 1227 tensor sets is: [406.6651611328125, 1001.23779296875, -283.2067565917969, 58.25882339477539, -1886.52294921875]\n",
      "\n",
      "Instance 1745 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 7: [0.8293657302856445, -0.4950600266456604, 0.12013155221939087, 0.5050804615020752, -5.512665748596191]\n",
      "Grand sum of 1228 tensor sets is: [407.4945373535156, 1000.7427368164062, -283.0866394042969, 58.7639045715332, -1892.03564453125]\n",
      "\n",
      "Instance 1746 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 22: [-0.19705107808113098, 1.1939126253128052, -0.2782352566719055, 0.03318857401609421, -5.209828853607178]\n",
      "Grand sum of 1229 tensor sets is: [407.2974853515625, 1001.9366455078125, -283.3648681640625, 58.79709243774414, -1897.2454833984375]\n",
      "\n",
      "Instance 1747 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 16: [1.371143102645874, 0.7203837037086487, -0.2224923074245453, -3.099541664123535, -1.7773008346557617]\n",
      "Grand sum of 1230 tensor sets is: [408.66864013671875, 1002.6570434570312, -283.5873718261719, 55.69755172729492, -1899.0228271484375]\n",
      "\n",
      "Instance 1748 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 5: [0.632073163986206, 0.5161945819854736, -0.4973262548446655, 0.3163191080093384, 0.6705003380775452]\n",
      "Grand sum of 1231 tensor sets is: [409.30072021484375, 1003.1732177734375, -284.0846862792969, 56.01387023925781, -1898.352294921875]\n",
      "\n",
      "Instance 1749 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([116, 13, 768])\n",
      "Shape of summed layers is: 116 x 768\n",
      "forever at index 26: [0.526923656463623, -0.9061081409454346, -1.1990087032318115, -1.9131766557693481, -3.142043352127075]\n",
      "Grand sum of 1232 tensor sets is: [409.82763671875, 1002.26708984375, -285.28369140625, 54.10069274902344, -1901.494384765625]\n",
      "\n",
      "Instance 1750 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 8: [0.9287663102149963, 1.650160312652588, 0.06110204756259918, -0.10928745567798615, -3.2204158306121826]\n",
      "Grand sum of 1233 tensor sets is: [410.75640869140625, 1003.917236328125, -285.22259521484375, 53.99140548706055, -1904.71484375]\n",
      "\n",
      "Instance 1751 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 34: [0.15755833685398102, 1.1902210712432861, -0.19558139145374298, 0.6964802742004395, -5.006905555725098]\n",
      "Grand sum of 1234 tensor sets is: [410.9139709472656, 1005.1074829101562, -285.4181823730469, 54.68788528442383, -1909.7218017578125]\n",
      "\n",
      "Instance 1752 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1753 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1754 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 24: [0.08103205263614655, 1.1681417226791382, 0.6959716081619263, -0.9055914878845215, 0.8613457679748535]\n",
      "Grand sum of 1235 tensor sets is: [410.9949951171875, 1006.275634765625, -284.7221984863281, 53.78229522705078, -1908.8604736328125]\n",
      "\n",
      "Instance 1755 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 12: [-0.6171454787254333, 1.280904769897461, -0.05586175620555878, -0.6360867023468018, 0.8563989400863647]\n",
      "Grand sum of 1236 tensor sets is: [410.3778381347656, 1007.5565185546875, -284.7780456542969, 53.146209716796875, -1908.0040283203125]\n",
      "\n",
      "Instance 1756 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1757 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1758 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1759 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1760 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 1237 tensor sets is: [411.2164611816406, 1007.9124755859375, -285.1797180175781, 53.65983581542969, -1905.4649658203125]\n",
      "\n",
      "Instance 1761 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1762 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 7: [0.3813270330429077, 0.8420504331588745, 0.0075440555810928345, -1.1220741271972656, -0.17658895254135132]\n",
      "Grand sum of 1238 tensor sets is: [411.5977783203125, 1008.7545166015625, -285.17218017578125, 52.53776168823242, -1905.6416015625]\n",
      "\n",
      "Instance 1763 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 12: [0.9770734310150146, 1.9923278093338013, 0.8326442241668701, 0.22303128242492676, 3.1740176677703857]\n",
      "Grand sum of 1239 tensor sets is: [412.5748596191406, 1010.746826171875, -284.33953857421875, 52.76079177856445, -1902.467529296875]\n",
      "\n",
      "Instance 1764 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([65, 13, 768])\n",
      "Shape of summed layers is: 65 x 768\n",
      "forever at index 54: [0.5403580665588379, 0.8530066013336182, -0.6903316378593445, -1.8289856910705566, -1.7782424688339233]\n",
      "Grand sum of 1240 tensor sets is: [413.1152038574219, 1011.599853515625, -285.0298767089844, 50.93180465698242, -1904.2457275390625]\n",
      "\n",
      "Instance 1765 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 23: [0.029262596741318703, -0.2194613218307495, -1.8841884136199951, 0.44647181034088135, -0.4749624729156494]\n",
      "Grand sum of 1241 tensor sets is: [413.14447021484375, 1011.38037109375, -286.9140625, 51.37827682495117, -1904.720703125]\n",
      "\n",
      "Instance 1766 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1767 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 4: [1.312066674232483, 0.42236000299453735, -1.404955506324768, 0.9254209399223328, 2.749570608139038]\n",
      "Grand sum of 1242 tensor sets is: [414.45654296875, 1011.802734375, -288.31903076171875, 52.3036994934082, -1901.97119140625]\n",
      "\n",
      "Instance 1768 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1769 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1770 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [237]\n",
      "Size of token embeddings is torch.Size([296, 13, 768])\n",
      "Shape of summed layers is: 296 x 768\n",
      "forever at index 237: [0.17014473676681519, 2.033351421356201, 1.225629448890686, -0.11607648432254791, 0.8086017370223999]\n",
      "Grand sum of 1243 tensor sets is: [414.6266784667969, 1013.8360595703125, -287.0934143066406, 52.1876220703125, -1901.16259765625]\n",
      "\n",
      "Instance 1771 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 21: [-0.11575540155172348, -0.021986573934555054, 0.7579510807991028, 0.6702896952629089, -2.6634607315063477]\n",
      "Grand sum of 1244 tensor sets is: [414.51092529296875, 1013.8140869140625, -286.33544921875, 52.85791015625, -1903.8260498046875]\n",
      "\n",
      "Instance 1772 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 16: [0.2405356764793396, 0.9870584011077881, -0.6175406575202942, 1.038504719734192, -3.4679009914398193]\n",
      "Grand sum of 1245 tensor sets is: [414.75146484375, 1014.8011474609375, -286.9530029296875, 53.89641571044922, -1907.2939453125]\n",
      "\n",
      "Instance 1773 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 12: [0.2413463294506073, 1.3632383346557617, -0.9988118410110474, -0.15744256973266602, -1.7616164684295654]\n",
      "Grand sum of 1246 tensor sets is: [414.9927978515625, 1016.1643676757812, -287.9518127441406, 53.73897171020508, -1909.0555419921875]\n",
      "\n",
      "Instance 1774 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 13: [0.7027958035469055, 0.03634129464626312, 0.024510741233825684, -1.492360234260559, 0.3972865343093872]\n",
      "Grand sum of 1247 tensor sets is: [415.6955871582031, 1016.20068359375, -287.92730712890625, 52.246612548828125, -1908.658203125]\n",
      "\n",
      "Instance 1775 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [60]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "forever at index 60: [-0.7991319894790649, 1.3573554754257202, -0.4611288011074066, -0.13487036526203156, -1.5192917585372925]\n",
      "Grand sum of 1248 tensor sets is: [414.8964538574219, 1017.5580444335938, -288.388427734375, 52.11174392700195, -1910.177490234375]\n",
      "\n",
      "Instance 1776 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 13: [0.6747141480445862, 2.8518266677856445, 0.23841488361358643, 1.497554063796997, -0.9243980646133423]\n",
      "Grand sum of 1249 tensor sets is: [415.5711669921875, 1020.4098510742188, -288.1500244140625, 53.60929870605469, -1911.1019287109375]\n",
      "\n",
      "Instance 1777 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 29: [-0.03502638638019562, 0.8892831206321716, 0.2642863690853119, -0.5952756404876709, -1.6554361581802368]\n",
      "Grand sum of 1250 tensor sets is: [415.5361328125, 1021.2991333007812, -287.8857421875, 53.01402282714844, -1912.75732421875]\n",
      "\n",
      "Instance 1778 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 16: [0.20596185326576233, -0.13367056846618652, 0.09476975351572037, -1.2574779987335205, -2.7590904235839844]\n",
      "Grand sum of 1251 tensor sets is: [415.7420959472656, 1021.1654663085938, -287.7909851074219, 51.75654602050781, -1915.516357421875]\n",
      "\n",
      "Instance 1779 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 7: [0.16704101860523224, 0.6705337762832642, 0.5885030031204224, 0.08536241203546524, -2.7121994495391846]\n",
      "Grand sum of 1252 tensor sets is: [415.9091491699219, 1021.8359985351562, -287.2024841308594, 51.8419075012207, -1918.228515625]\n",
      "\n",
      "Instance 1780 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1781 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 11: [3.572493076324463, 1.318234920501709, 0.3219805359840393, 1.770577073097229, -0.6667544841766357]\n",
      "Grand sum of 1253 tensor sets is: [419.48162841796875, 1023.1542358398438, -286.8804931640625, 53.612483978271484, -1918.895263671875]\n",
      "\n",
      "Instance 1782 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1783 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 29: [0.7626990079879761, 1.267268180847168, 0.16719061136245728, -0.46277379989624023, -1.8552031517028809]\n",
      "Grand sum of 1254 tensor sets is: [420.24432373046875, 1024.4215087890625, -286.7132873535156, 53.14971160888672, -1920.75048828125]\n",
      "\n",
      "Instance 1784 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 12: [-0.04751661419868469, 1.864916443824768, -0.5872056484222412, 1.306915044784546, 1.5287903547286987]\n",
      "Grand sum of 1255 tensor sets is: [420.1968078613281, 1026.286376953125, -287.3005065917969, 54.456626892089844, -1919.2216796875]\n",
      "\n",
      "Instance 1785 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 3: [-0.22920706868171692, 0.314419150352478, -2.243889093399048, 1.0864968299865723, -4.454908847808838]\n",
      "Grand sum of 1256 tensor sets is: [419.96759033203125, 1026.600830078125, -289.5444030761719, 55.54312515258789, -1923.6766357421875]\n",
      "\n",
      "Instance 1786 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "forever at index 25: [0.006092488765716553, -0.42101430892944336, -0.6545708179473877, 0.13731373846530914, 2.861842155456543]\n",
      "Grand sum of 1257 tensor sets is: [419.97369384765625, 1026.1798095703125, -290.198974609375, 55.68043899536133, -1920.8148193359375]\n",
      "\n",
      "Instance 1787 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 9: [0.40147775411605835, 3.293123960494995, 1.0489404201507568, -1.5550727844238281, -0.46700525283813477]\n",
      "Grand sum of 1258 tensor sets is: [420.37518310546875, 1029.472900390625, -289.1500244140625, 54.1253662109375, -1921.2818603515625]\n",
      "\n",
      "Instance 1788 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 24: [-0.31462737917900085, 1.32406747341156, 0.24591270089149475, -0.18309703469276428, -0.5427312850952148]\n",
      "Grand sum of 1259 tensor sets is: [420.060546875, 1030.7969970703125, -288.90411376953125, 53.94226837158203, -1921.8245849609375]\n",
      "\n",
      "Instance 1789 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 1260 tensor sets is: [419.3258056640625, 1031.201904296875, -288.5124206542969, 53.585208892822266, -1926.3853759765625]\n",
      "\n",
      "Instance 1790 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 3: [0.7400802969932556, 0.339777410030365, -0.3671029508113861, -0.4082692563533783, 1.5671052932739258]\n",
      "Grand sum of 1261 tensor sets is: [420.0658874511719, 1031.5416259765625, -288.8795166015625, 53.17694091796875, -1924.8182373046875]\n",
      "\n",
      "Instance 1791 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 12: [0.679350733757019, 2.280841588973999, 0.09799298644065857, 3.661510944366455, -0.17466215789318085]\n",
      "Grand sum of 1262 tensor sets is: [420.7452392578125, 1033.822509765625, -288.7815246582031, 56.83845138549805, -1924.992919921875]\n",
      "\n",
      "Instance 1792 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1793 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [0.25580787658691406, 0.9257710576057434, -0.29591256380081177, 0.7308024168014526, -5.34088659286499]\n",
      "Grand sum of 1263 tensor sets is: [421.00103759765625, 1034.748291015625, -289.0774230957031, 57.569252014160156, -1930.3338623046875]\n",
      "\n",
      "Instance 1794 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1795 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 22: [0.9892910718917847, 2.055809259414673, -0.2223549634218216, 1.243882417678833, -2.250450372695923]\n",
      "Grand sum of 1264 tensor sets is: [421.9903259277344, 1036.8040771484375, -289.2997741699219, 58.813133239746094, -1932.5843505859375]\n",
      "\n",
      "Instance 1796 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 16: [0.30324578285217285, 0.493397980928421, -0.4766228199005127, 0.17180414497852325, -4.456072807312012]\n",
      "Grand sum of 1265 tensor sets is: [422.2935791015625, 1037.2974853515625, -289.7763977050781, 58.98493576049805, -1937.0404052734375]\n",
      "\n",
      "Instance 1797 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1798 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1799 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 7: [-0.11389224231243134, 1.4060347080230713, -0.56422358751297, 1.7137997150421143, -5.163950443267822]\n",
      "Grand sum of 1266 tensor sets is: [422.1796875, 1038.7034912109375, -290.3406066894531, 60.698734283447266, -1942.204345703125]\n",
      "\n",
      "Instance 1800 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1801 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 14: [0.4290889501571655, 1.7175090312957764, 0.3998156785964966, 1.4641659259796143, 2.3899803161621094]\n",
      "Grand sum of 1267 tensor sets is: [422.6087646484375, 1040.4210205078125, -289.9407958984375, 62.162899017333984, -1939.8143310546875]\n",
      "\n",
      "Instance 1802 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 1268 tensor sets is: [421.8740234375, 1040.825927734375, -289.5491027832031, 61.80583953857422, -1944.3751220703125]\n",
      "\n",
      "Instance 1803 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 1269 tensor sets is: [423.6653747558594, 1039.271240234375, -290.2313537597656, 60.95991516113281, -1943.922119140625]\n",
      "\n",
      "Instance 1804 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 9: [0.5218549370765686, 0.9484233856201172, 0.1863338053226471, 0.2602981925010681, -3.169445037841797]\n",
      "Grand sum of 1270 tensor sets is: [424.1872253417969, 1040.2196044921875, -290.0450134277344, 61.22021484375, -1947.091552734375]\n",
      "\n",
      "Instance 1805 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 32: [0.13950887322425842, 0.3649469017982483, -0.22916531562805176, 0.5904614925384521, 2.6145646572113037]\n",
      "Grand sum of 1271 tensor sets is: [424.32672119140625, 1040.5845947265625, -290.274169921875, 61.81067657470703, -1944.4769287109375]\n",
      "\n",
      "Instance 1806 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 29: [0.29655832052230835, 1.1898669004440308, 0.20614232122898102, -1.214375615119934, -2.7721962928771973]\n",
      "Grand sum of 1272 tensor sets is: [424.623291015625, 1041.7744140625, -290.0680236816406, 60.5963020324707, -1947.2491455078125]\n",
      "\n",
      "Instance 1807 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "forever at index 38: [0.4558597803115845, 0.858635425567627, -0.10579365491867065, 0.11026450991630554, -2.7813806533813477]\n",
      "Grand sum of 1273 tensor sets is: [425.07916259765625, 1042.633056640625, -290.173828125, 60.706565856933594, -1950.030517578125]\n",
      "\n",
      "Instance 1808 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1809 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1810 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1274 tensor sets is: [425.1805725097656, 1044.3724365234375, -290.8118591308594, 59.591182708740234, -1954.3271484375]\n",
      "\n",
      "Instance 1811 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 5: [0.37776729464530945, -1.0611239671707153, -1.3574273586273193, 0.4046775698661804, -2.6271872520446777]\n",
      "Grand sum of 1275 tensor sets is: [425.558349609375, 1043.311279296875, -292.1692810058594, 59.9958610534668, -1956.954345703125]\n",
      "\n",
      "Instance 1812 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1813 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 3: [1.1526155471801758, -0.35349705815315247, -0.598080039024353, -0.6890103816986084, 1.427318811416626]\n",
      "Grand sum of 1276 tensor sets is: [426.7109680175781, 1042.957763671875, -292.7673645019531, 59.30685043334961, -1955.5269775390625]\n",
      "\n",
      "Instance 1814 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1815 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 14: [0.40740811824798584, -0.42447060346603394, 1.1734654903411865, 0.5220872759819031, 2.0731029510498047]\n",
      "Grand sum of 1277 tensor sets is: [427.1183776855469, 1042.5333251953125, -291.5939025878906, 59.82893753051758, -1953.453857421875]\n",
      "\n",
      "Instance 1816 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 26: [0.37860313057899475, -0.8008484840393066, -0.2810317873954773, 0.23155392706394196, -1.51703679561615]\n",
      "Grand sum of 1278 tensor sets is: [427.4969787597656, 1041.732421875, -291.87493896484375, 60.060489654541016, -1954.970947265625]\n",
      "\n",
      "Instance 1817 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [1.6994589567184448, -0.21792569756507874, 0.2326381653547287, -0.911447286605835, -4.0184526443481445]\n",
      "Grand sum of 1279 tensor sets is: [429.1964416503906, 1041.5145263671875, -291.6423034667969, 59.149044036865234, -1958.9893798828125]\n",
      "\n",
      "Instance 1818 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 4: [0.597179651260376, 2.085334062576294, 0.26162225008010864, 1.508796215057373, -1.487855315208435]\n",
      "Grand sum of 1280 tensor sets is: [429.7936096191406, 1043.599853515625, -291.38067626953125, 60.657840728759766, -1960.477294921875]\n",
      "\n",
      "Instance 1819 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 12: [1.3971847295761108, 2.6847918033599854, -0.8556854724884033, 1.8453903198242188, 2.479337215423584]\n",
      "Grand sum of 1281 tensor sets is: [431.1907958984375, 1046.28466796875, -292.2363586425781, 62.503231048583984, -1957.9979248046875]\n",
      "\n",
      "Instance 1820 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1821 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 26: [1.3161708116531372, 0.7910457253456116, 0.03404364734888077, -0.2547284960746765, -6.300019264221191]\n",
      "Grand sum of 1282 tensor sets is: [432.5069580078125, 1047.07568359375, -292.2023010253906, 62.24850082397461, -1964.2979736328125]\n",
      "\n",
      "Instance 1822 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 1283 tensor sets is: [431.772216796875, 1047.4805908203125, -291.81060791015625, 61.891441345214844, -1968.8587646484375]\n",
      "\n",
      "Instance 1823 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14, 46]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 14: [0.1682974398136139, 1.5269901752471924, 0.23025944828987122, -1.3031113147735596, -0.33381855487823486]\n",
      "forever at index 46: [0.5073516368865967, -1.001587152481079, -0.0033099204301834106, -1.2680394649505615, -4.438116550445557]\n",
      "Grand sum of 1284 tensor sets is: [432.11004638671875, 1047.7432861328125, -291.6971435546875, 60.605865478515625, -1971.2447509765625]\n",
      "\n",
      "Instance 1824 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 1285 tensor sets is: [431.474609375, 1049.465087890625, -290.9646301269531, 61.911643981933594, -1971.4215087890625]\n",
      "\n",
      "Instance 1825 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 17: [0.9977977871894836, 0.3386944532394409, 0.3083459138870239, -2.57766056060791, 1.5883357524871826]\n",
      "Grand sum of 1286 tensor sets is: [432.472412109375, 1049.8038330078125, -290.6562805175781, 59.333984375, -1969.8331298828125]\n",
      "\n",
      "Instance 1826 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1827 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 18: [0.2555389702320099, 2.32997727394104, -0.5315282940864563, -1.0904674530029297, -5.921359062194824]\n",
      "Grand sum of 1287 tensor sets is: [432.72796630859375, 1052.1337890625, -291.18780517578125, 58.24351501464844, -1975.7545166015625]\n",
      "\n",
      "Instance 1828 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 4: [0.11524893343448639, 0.7392664551734924, -0.12497269362211227, -0.6717877388000488, 0.398396760225296]\n",
      "Grand sum of 1288 tensor sets is: [432.84320068359375, 1052.873046875, -291.3127746582031, 57.57172775268555, -1975.3560791015625]\n",
      "\n",
      "Instance 1829 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 9: [-0.7930142879486084, 0.862140417098999, -0.4280673563480377, 0.37278544902801514, -3.8681788444519043]\n",
      "Grand sum of 1289 tensor sets is: [432.0502014160156, 1053.7352294921875, -291.7408447265625, 57.94451141357422, -1979.2242431640625]\n",
      "\n",
      "Instance 1830 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1831 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 7: [0.29507502913475037, 1.8510929346084595, -0.01439964771270752, 0.17857331037521362, -4.131156921386719]\n",
      "Grand sum of 1290 tensor sets is: [432.34527587890625, 1055.5863037109375, -291.7552490234375, 58.123085021972656, -1983.3553466796875]\n",
      "\n",
      "Instance 1832 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [102, 167]\n",
      "Size of token embeddings is torch.Size([186, 13, 768])\n",
      "Shape of summed layers is: 186 x 768\n",
      "forever at index 102: [0.15189987421035767, 0.9312858581542969, -1.0129108428955078, 0.15915696322917938, -2.6133224964141846]\n",
      "forever at index 167: [0.8973044157028198, 2.0149521827697754, 0.6894101500511169, -1.1778972148895264, -2.75626277923584]\n",
      "Grand sum of 1291 tensor sets is: [432.869873046875, 1057.0594482421875, -291.9169921875, 57.61371612548828, -1986.0401611328125]\n",
      "\n",
      "Instance 1833 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1834 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 8: [1.4019694328308105, 0.7361456155776978, 0.21667999029159546, 2.0872418880462646, 3.896575927734375]\n",
      "Grand sum of 1292 tensor sets is: [434.2718505859375, 1057.795654296875, -291.7003173828125, 59.700958251953125, -1982.1435546875]\n",
      "\n",
      "Instance 1835 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 5: [-0.12427559494972229, 1.8362431526184082, -0.1262708604335785, -0.14181195199489594, -0.9123669266700745]\n",
      "Grand sum of 1293 tensor sets is: [434.1475830078125, 1059.6319580078125, -291.82659912109375, 59.559146881103516, -1983.055908203125]\n",
      "\n",
      "Instance 1836 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 21: [-0.5127941370010376, 2.2104854583740234, -0.3321448564529419, 0.5112597346305847, -3.007030487060547]\n",
      "Grand sum of 1294 tensor sets is: [433.6347961425781, 1061.8424072265625, -292.15875244140625, 60.07040786743164, -1986.06298828125]\n",
      "\n",
      "Instance 1837 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 1295 tensor sets is: [434.32958984375, 1062.9757080078125, -290.9555969238281, 58.80104446411133, -1986.9532470703125]\n",
      "\n",
      "Instance 1838 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 16: [0.5529628396034241, -1.7828621864318848, -0.9644438624382019, -2.0412192344665527, -1.4600043296813965]\n",
      "Grand sum of 1296 tensor sets is: [434.8825378417969, 1061.19287109375, -291.9200439453125, 56.75982666015625, -1988.4132080078125]\n",
      "\n",
      "Instance 1839 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 31: [0.5960530638694763, -0.03966358304023743, -0.2580677270889282, 1.39520263671875, -0.7516476511955261]\n",
      "Grand sum of 1297 tensor sets is: [435.47857666015625, 1061.1531982421875, -292.1781005859375, 58.155029296875, -1989.164794921875]\n",
      "\n",
      "Instance 1840 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 27: [0.07942651212215424, 2.2006492614746094, -0.4972972869873047, -0.32676222920417786, -0.061111271381378174]\n",
      "Grand sum of 1298 tensor sets is: [435.5580139160156, 1063.3538818359375, -292.6753845214844, 57.82826614379883, -1989.2259521484375]\n",
      "\n",
      "Instance 1841 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 8: [0.14640235900878906, 0.10831528156995773, 0.14789485931396484, 1.5315980911254883, 2.0381650924682617]\n",
      "Grand sum of 1299 tensor sets is: [435.70440673828125, 1063.462158203125, -292.5274963378906, 59.35986328125, -1987.187744140625]\n",
      "\n",
      "Instance 1842 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1843 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 1300 tensor sets is: [436.0951232910156, 1064.5360107421875, -292.7511291503906, 56.71891403198242, -1981.81494140625]\n",
      "\n",
      "Instance 1844 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 16: [-0.23736338317394257, -0.1111365556716919, -0.2509633004665375, -1.059692144393921, -0.8565080165863037]\n",
      "Grand sum of 1301 tensor sets is: [435.8577575683594, 1064.4249267578125, -293.0021057128906, 55.65922164916992, -1982.6715087890625]\n",
      "\n",
      "Instance 1845 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([71, 13, 768])\n",
      "Shape of summed layers is: 71 x 768\n",
      "forever at index 12: [0.8910912275314331, 1.8350791931152344, -1.093894124031067, 4.271853923797607, -0.01114201545715332]\n",
      "Grand sum of 1302 tensor sets is: [436.74884033203125, 1066.260009765625, -294.09600830078125, 59.93107604980469, -1982.6826171875]\n",
      "\n",
      "Instance 1846 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1847 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 22: [-0.2055673450231552, 1.8835264444351196, 0.4980594515800476, -0.6265937089920044, -2.109809398651123]\n",
      "Grand sum of 1303 tensor sets is: [436.54327392578125, 1068.1435546875, -293.59796142578125, 59.304481506347656, -1984.79248046875]\n",
      "\n",
      "Instance 1848 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 11: [-0.17691078782081604, 1.5645350217819214, -0.98503577709198, 0.5344499349594116, -1.0687930583953857]\n",
      "Grand sum of 1304 tensor sets is: [436.3663635253906, 1069.7081298828125, -294.5830078125, 59.838932037353516, -1985.861328125]\n",
      "\n",
      "Instance 1849 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1850 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 9: [-0.46422648429870605, 2.4089653491973877, -0.4569409489631653, -0.7450731992721558, 2.6945481300354004]\n",
      "Grand sum of 1305 tensor sets is: [435.9021301269531, 1072.1170654296875, -295.0399475097656, 59.0938606262207, -1983.166748046875]\n",
      "\n",
      "Instance 1851 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1852 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 1306 tensor sets is: [435.1673889160156, 1072.52197265625, -294.64825439453125, 58.73680114746094, -1987.7275390625]\n",
      "\n",
      "Instance 1853 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 21: [-0.14795778691768646, 0.2837488055229187, -1.5400350093841553, 1.0726945400238037, -4.729109764099121]\n",
      "Grand sum of 1307 tensor sets is: [435.0194396972656, 1072.8056640625, -296.18829345703125, 59.80949401855469, -1992.4566650390625]\n",
      "\n",
      "Instance 1854 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "forever at index 30: [-0.1166355237364769, 1.3978617191314697, 0.3407113552093506, -0.055488526821136475, -1.0360134840011597]\n",
      "Grand sum of 1308 tensor sets is: [434.9028015136719, 1074.2034912109375, -295.84759521484375, 59.754005432128906, -1993.49267578125]\n",
      "\n",
      "Instance 1855 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 11: [-0.31641367077827454, 0.7202603220939636, 0.591779887676239, -0.8554326295852661, -3.843867778778076]\n",
      "Grand sum of 1309 tensor sets is: [434.5863952636719, 1074.9237060546875, -295.2558288574219, 58.8985710144043, -1997.3365478515625]\n",
      "\n",
      "Instance 1856 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 8: [0.9603164196014404, -1.1454706192016602, -1.5383920669555664, -0.7735167741775513, 0.09273940324783325]\n",
      "Grand sum of 1310 tensor sets is: [435.5467224121094, 1073.7781982421875, -296.7942199707031, 58.12505340576172, -1997.2437744140625]\n",
      "\n",
      "Instance 1857 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 23: [-1.4012020826339722, 2.0466442108154297, -1.8467037677764893, 1.689387321472168, -0.9971396923065186]\n",
      "Grand sum of 1311 tensor sets is: [434.1455078125, 1075.8248291015625, -298.64093017578125, 59.8144416809082, -1998.240966796875]\n",
      "\n",
      "Instance 1858 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [66]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "forever at index 66: [0.10178609192371368, 1.8100552558898926, 0.9943283200263977, -2.8297181129455566, -2.516984462738037]\n",
      "Grand sum of 1312 tensor sets is: [434.2472839355469, 1077.6348876953125, -297.6466064453125, 56.98472213745117, -2000.7579345703125]\n",
      "\n",
      "Instance 1859 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1860 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [83]\n",
      "Size of token embeddings is torch.Size([110, 13, 768])\n",
      "Shape of summed layers is: 110 x 768\n",
      "forever at index 83: [0.06891366839408875, 0.7293775081634521, 0.3836304843425751, 1.6173467636108398, -2.1564314365386963]\n",
      "Grand sum of 1313 tensor sets is: [434.3161926269531, 1078.3642578125, -297.2629699707031, 58.60206985473633, -2002.914306640625]\n",
      "\n",
      "Instance 1861 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 26: [0.7321347594261169, 1.7756788730621338, -1.268976092338562, 3.34544038772583, -1.3405930995941162]\n",
      "Grand sum of 1314 tensor sets is: [435.04833984375, 1080.139892578125, -298.5319519042969, 61.947509765625, -2004.2548828125]\n",
      "\n",
      "Instance 1862 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 10: [-0.003194287419319153, 3.0020017623901367, -0.9934726357460022, -0.005951300263404846, -0.08032992482185364]\n",
      "Grand sum of 1315 tensor sets is: [435.0451354980469, 1083.141845703125, -299.5254211425781, 61.941558837890625, -2004.335205078125]\n",
      "\n",
      "Instance 1863 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 34: [0.7136924266815186, 1.3583749532699585, -0.7377657294273376, -1.1533266305923462, -0.588890552520752]\n",
      "Grand sum of 1316 tensor sets is: [435.7588195800781, 1084.500244140625, -300.26318359375, 60.788230895996094, -2004.924072265625]\n",
      "\n",
      "Instance 1864 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 19: [0.6337724924087524, 0.5371542572975159, -0.2246326506137848, -2.2136449813842773, 0.20330657064914703]\n",
      "Grand sum of 1317 tensor sets is: [436.392578125, 1085.037353515625, -300.4878234863281, 58.5745849609375, -2004.7208251953125]\n",
      "\n",
      "Instance 1865 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1866 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 9: [-0.3493066430091858, 2.6084718704223633, -0.4450094997882843, 0.8467295169830322, -1.6018298864364624]\n",
      "Grand sum of 1318 tensor sets is: [436.04327392578125, 1087.6458740234375, -300.9328308105469, 59.42131423950195, -2006.3226318359375]\n",
      "\n",
      "Instance 1867 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1868 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 15: [0.31887394189834595, 1.6021109819412231, -0.8715335130691528, -1.5683071613311768, -1.3572468757629395]\n",
      "Grand sum of 1319 tensor sets is: [436.3621520996094, 1089.2479248046875, -301.8043518066406, 57.85300827026367, -2007.679931640625]\n",
      "\n",
      "Instance 1869 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 6: [-0.7263126373291016, 1.2893990278244019, -0.6344245672225952, 0.3867553472518921, -1.0809133052825928]\n",
      "Grand sum of 1320 tensor sets is: [435.6358337402344, 1090.537353515625, -302.43878173828125, 58.23976516723633, -2008.7608642578125]\n",
      "\n",
      "Instance 1870 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1871 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 25: [0.6529316902160645, 0.5405619144439697, -0.43905335664749146, 0.17718994617462158, -2.965202569961548]\n",
      "Grand sum of 1321 tensor sets is: [436.28875732421875, 1091.077880859375, -302.8778381347656, 58.416954040527344, -2011.72607421875]\n",
      "\n",
      "Instance 1872 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1873 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 17: [0.2510674297809601, 1.3207827806472778, 0.46162551641464233, -0.40096092224121094, -4.772617816925049]\n",
      "Grand sum of 1322 tensor sets is: [436.5398254394531, 1092.398681640625, -302.41619873046875, 58.0159912109375, -2016.4986572265625]\n",
      "\n",
      "Instance 1874 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 5: [0.4627998471260071, 1.5189412832260132, -0.44054344296455383, -1.145575761795044, 1.9087803363800049]\n",
      "Grand sum of 1323 tensor sets is: [437.00262451171875, 1093.9176025390625, -302.85675048828125, 56.87041473388672, -2014.58984375]\n",
      "\n",
      "Instance 1875 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1876 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "forever at index 46: [-0.021052014082670212, 1.3995059728622437, -0.08996768295764923, 0.0869903564453125, -1.0271573066711426]\n",
      "Grand sum of 1324 tensor sets is: [436.9815673828125, 1095.317138671875, -302.94671630859375, 56.95740509033203, -2015.616943359375]\n",
      "\n",
      "Instance 1877 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1878 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "forever at index 16: [0.5073181986808777, 0.42760926485061646, -0.5461349487304688, 0.5990097522735596, -0.5067334175109863]\n",
      "Grand sum of 1325 tensor sets is: [437.4888916015625, 1095.7447509765625, -303.49285888671875, 57.55641555786133, -2016.1236572265625]\n",
      "\n",
      "Instance 1879 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 2: [0.30317509174346924, 2.4331958293914795, -0.37424516677856445, -1.4089643955230713, 2.133906602859497]\n",
      "Grand sum of 1326 tensor sets is: [437.79205322265625, 1098.177978515625, -303.8670959472656, 56.1474494934082, -2013.98974609375]\n",
      "\n",
      "Instance 1880 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 8: [1.6764886379241943, 1.6666738986968994, 0.08508042246103287, 1.1984151601791382, 0.20278644561767578]\n",
      "Grand sum of 1327 tensor sets is: [439.4685363769531, 1099.8446044921875, -303.7820129394531, 57.345863342285156, -2013.7869873046875]\n",
      "\n",
      "Instance 1881 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 1328 tensor sets is: [439.8642272949219, 1100.853271484375, -303.8515930175781, 56.66230010986328, -2012.49072265625]\n",
      "\n",
      "Instance 1882 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1883 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1884 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 21: [0.2830163538455963, -0.405086487531662, -0.3646240830421448, 0.5609540939331055, -3.5869486331939697]\n",
      "Grand sum of 1329 tensor sets is: [440.1472473144531, 1100.4482421875, -304.2162170410156, 57.2232551574707, -2016.07763671875]\n",
      "\n",
      "Instance 1885 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1886 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1887 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 18: [0.8116636872291565, 0.06893207877874374, -1.5251628160476685, -0.02945449948310852, 2.6440649032592773]\n",
      "Grand sum of 1330 tensor sets is: [440.95892333984375, 1100.5172119140625, -305.74139404296875, 57.19380187988281, -2013.43359375]\n",
      "\n",
      "Instance 1888 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 4: [0.23130017518997192, 3.0733137130737305, -1.6750054359436035, 0.9826681017875671, 0.04277367889881134]\n",
      "Grand sum of 1331 tensor sets is: [441.1902160644531, 1103.590576171875, -307.4164123535156, 58.17647171020508, -2013.390869140625]\n",
      "\n",
      "Instance 1889 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "forever at index 13: [0.3704926371574402, -0.86744225025177, 0.03499317169189453, -0.22266435623168945, -1.6869419813156128]\n",
      "Grand sum of 1332 tensor sets is: [441.5606994628906, 1102.72314453125, -307.38140869140625, 57.95380783081055, -2015.0777587890625]\n",
      "\n",
      "Instance 1890 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 16: [0.2767907381057739, 1.2694132328033447, -0.4388395845890045, -0.37906399369239807, -1.1787409782409668]\n",
      "Grand sum of 1333 tensor sets is: [441.8374938964844, 1103.9925537109375, -307.82025146484375, 57.574745178222656, -2016.2564697265625]\n",
      "\n",
      "Instance 1891 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 13: [0.6599693298339844, 1.3709321022033691, 0.3216383159160614, 2.243821620941162, -1.825993299484253]\n",
      "Grand sum of 1334 tensor sets is: [442.4974670410156, 1105.363525390625, -307.4986267089844, 59.818565368652344, -2018.08251953125]\n",
      "\n",
      "Instance 1892 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 23: [0.25613653659820557, 1.7356373071670532, -0.0017609372735023499, 1.8842601776123047, -1.2285208702087402]\n",
      "Grand sum of 1335 tensor sets is: [442.75360107421875, 1107.09912109375, -307.5003967285156, 61.70282745361328, -2019.31103515625]\n",
      "\n",
      "Instance 1893 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1894 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 6: [0.33444744348526, -0.11728088557720184, 0.3616103231906891, 0.7342945337295532, -1.0521466732025146]\n",
      "Grand sum of 1336 tensor sets is: [443.0880432128906, 1106.9818115234375, -307.1387939453125, 62.4371223449707, -2020.3631591796875]\n",
      "\n",
      "Instance 1895 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1896 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 26: [0.5102909803390503, 0.7774418592453003, 0.9991793632507324, -1.3198788166046143, -5.471462726593018]\n",
      "Grand sum of 1337 tensor sets is: [443.59832763671875, 1107.75927734375, -306.1396179199219, 61.117244720458984, -2025.8345947265625]\n",
      "\n",
      "Instance 1897 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([81, 13, 768])\n",
      "Shape of summed layers is: 81 x 768\n",
      "forever at index 59: [0.5151832103729248, 2.1541244983673096, -1.1704514026641846, 2.161311388015747, -3.896782159805298]\n",
      "Grand sum of 1338 tensor sets is: [444.113525390625, 1109.9134521484375, -307.31005859375, 63.27855682373047, -2029.7313232421875]\n",
      "\n",
      "Instance 1898 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1899 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 12: [0.5227048397064209, -0.30003082752227783, 0.2295801341533661, -1.0547748804092407, -1.7778465747833252]\n",
      "Grand sum of 1339 tensor sets is: [444.63623046875, 1109.6134033203125, -307.0804748535156, 62.22378158569336, -2031.5091552734375]\n",
      "\n",
      "Instance 1900 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 16: [0.3014290928840637, 1.5356453657150269, 0.3597206175327301, -0.06196311116218567, -7.572225093841553]\n",
      "Grand sum of 1340 tensor sets is: [444.9376525878906, 1111.1490478515625, -306.72076416015625, 62.16181945800781, -2039.0814208984375]\n",
      "\n",
      "Instance 1901 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1902 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 2: [0.5687731504440308, -0.030098557472229004, -0.4558166563510895, -0.9705744981765747, -3.498289108276367]\n",
      "Grand sum of 1341 tensor sets is: [445.5064392089844, 1111.118896484375, -307.17657470703125, 61.191246032714844, -2042.5797119140625]\n",
      "\n",
      "Instance 1903 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 65: [-0.4072004556655884, -1.165315866470337, -1.116049885749817, -1.6974555253982544, -1.7651728391647339]\n",
      "Grand sum of 1342 tensor sets is: [445.0992431640625, 1109.95361328125, -308.2926330566406, 59.49378967285156, -2044.3448486328125]\n",
      "\n",
      "Instance 1904 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 4: [1.3253533840179443, 1.2372300624847412, 0.5217266082763672, 0.32533255219459534, 2.5591914653778076]\n",
      "Grand sum of 1343 tensor sets is: [446.4245910644531, 1111.1907958984375, -307.7709045410156, 59.819122314453125, -2041.78564453125]\n",
      "\n",
      "Instance 1905 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 34: [0.7136924266815186, 1.3583749532699585, -0.7377657294273376, -1.1533266305923462, -0.588890552520752]\n",
      "Grand sum of 1344 tensor sets is: [447.1382751464844, 1112.5491943359375, -308.5086669921875, 58.665794372558594, -2042.37451171875]\n",
      "\n",
      "Instance 1906 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1907 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [66]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "forever at index 66: [-0.4971312880516052, 0.4068295955657959, 2.0872578620910645, 1.1197913885116577, -3.6161367893218994]\n",
      "Grand sum of 1345 tensor sets is: [446.6411437988281, 1112.9560546875, -306.4214172363281, 59.785587310791016, -2045.9906005859375]\n",
      "\n",
      "Instance 1908 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [1.373034954071045, -0.19460828602313995, -0.33495208621025085, -0.5347614288330078, 0.8759350180625916]\n",
      "Grand sum of 1346 tensor sets is: [448.0141906738281, 1112.761474609375, -306.7563781738281, 59.250823974609375, -2045.1146240234375]\n",
      "\n",
      "Instance 1909 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 25: [-0.5632249712944031, 2.357093095779419, -0.5206774473190308, -1.370469570159912, -1.9047770500183105]\n",
      "Grand sum of 1347 tensor sets is: [447.4509582519531, 1115.1185302734375, -307.2770690917969, 57.88035583496094, -2047.0194091796875]\n",
      "\n",
      "Instance 1910 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 10: [-0.003194287419319153, 3.0020017623901367, -0.9934726357460022, -0.005951300263404846, -0.08032992482185364]\n",
      "Grand sum of 1348 tensor sets is: [447.44775390625, 1118.1204833984375, -308.2705383300781, 57.87440490722656, -2047.0997314453125]\n",
      "\n",
      "Instance 1911 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1912 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1913 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1914 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 11: [-0.5674815773963928, 1.6017507314682007, -0.6321079134941101, 1.8035888671875, -0.8080515265464783]\n",
      "Grand sum of 1349 tensor sets is: [446.8802795410156, 1119.7222900390625, -308.90264892578125, 59.67799377441406, -2047.9078369140625]\n",
      "\n",
      "Instance 1915 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 11: [-0.6461827754974365, 0.022598501294851303, -2.208751916885376, -2.117126941680908, -4.1703362464904785]\n",
      "Grand sum of 1350 tensor sets is: [446.2341003417969, 1119.744873046875, -311.11138916015625, 57.56086730957031, -2052.078125]\n",
      "\n",
      "Instance 1916 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 13: [1.2330877780914307, 2.2382659912109375, -1.0750439167022705, -1.175226092338562, -0.9880310297012329]\n",
      "Grand sum of 1351 tensor sets is: [447.4671936035156, 1121.983154296875, -312.1864318847656, 56.385643005371094, -2053.066162109375]\n",
      "\n",
      "Instance 1917 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "forever at index 20: [0.6764991283416748, -0.20368163287639618, -0.2635296881198883, -1.3565479516983032, 1.6593703031539917]\n",
      "Grand sum of 1352 tensor sets is: [448.1437072753906, 1121.7794189453125, -312.449951171875, 55.02909469604492, -2051.40673828125]\n",
      "\n",
      "Instance 1918 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1919 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1920 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 18: [0.23628199100494385, 0.234147846698761, 0.3020831346511841, 1.5849838256835938, -3.6293108463287354]\n",
      "Grand sum of 1353 tensor sets is: [448.3799743652344, 1122.0135498046875, -312.1478576660156, 56.614078521728516, -2055.0361328125]\n",
      "\n",
      "Instance 1921 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 3: [0.14751428365707397, 2.126890182495117, -0.8581041097640991, 0.35677435994148254, -0.5929785966873169]\n",
      "Grand sum of 1354 tensor sets is: [448.5274963378906, 1124.140380859375, -313.0059509277344, 56.97085189819336, -2055.629150390625]\n",
      "\n",
      "Instance 1922 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1923 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [0.21350616216659546, 0.967560887336731, 0.32559120655059814, 0.07132753729820251, 0.1406002640724182]\n",
      "Grand sum of 1355 tensor sets is: [448.7409973144531, 1125.10791015625, -312.68035888671875, 57.042179107666016, -2055.488525390625]\n",
      "\n",
      "Instance 1924 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1925 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 8: [0.3460031747817993, 0.8408506512641907, -1.0601550340652466, 0.2392045110464096, -7.836584091186523]\n",
      "Grand sum of 1356 tensor sets is: [449.0870056152344, 1125.94873046875, -313.7405090332031, 57.2813835144043, -2063.3251953125]\n",
      "\n",
      "Instance 1926 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1927 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1928 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 12: [0.5707752108573914, 1.9606306552886963, -0.31371596455574036, 0.008518531918525696, -2.51194167137146]\n",
      "Grand sum of 1357 tensor sets is: [449.65777587890625, 1127.9093017578125, -314.0542297363281, 57.28990173339844, -2065.837158203125]\n",
      "\n",
      "Instance 1929 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 5: [0.3561139702796936, 1.5587464570999146, -0.2110653668642044, 0.756371796131134, 1.6190857887268066]\n",
      "Grand sum of 1358 tensor sets is: [450.0138854980469, 1129.468017578125, -314.2652893066406, 58.04627227783203, -2064.218017578125]\n",
      "\n",
      "Instance 1930 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1931 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 37: [-0.142353355884552, 1.1452974081039429, -0.9252950549125671, -0.7546662092208862, -7.564059734344482]\n",
      "Grand sum of 1359 tensor sets is: [449.87152099609375, 1130.61328125, -315.1905822753906, 57.29160690307617, -2071.781982421875]\n",
      "\n",
      "Instance 1932 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 25: [-0.11344346404075623, 1.1253985166549683, 0.3885439336299896, 1.5087603330612183, -1.993820071220398]\n",
      "Grand sum of 1360 tensor sets is: [449.7580871582031, 1131.7386474609375, -314.8020324707031, 58.80036544799805, -2073.77587890625]\n",
      "\n",
      "Instance 1933 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 1361 tensor sets is: [450.302001953125, 1131.103759765625, -315.4515686035156, 57.65814208984375, -2072.654541015625]\n",
      "\n",
      "Instance 1934 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 4: [0.7427921295166016, 1.4259026050567627, -0.5795949697494507, -0.6174664497375488, -1.9336100816726685]\n",
      "Grand sum of 1362 tensor sets is: [451.0447998046875, 1132.5296630859375, -316.0311584472656, 57.04067611694336, -2074.588134765625]\n",
      "\n",
      "Instance 1935 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 20: [1.0445581674575806, 0.8767131567001343, 0.029001906514167786, 0.6887302398681641, 1.1150587797164917]\n",
      "Grand sum of 1363 tensor sets is: [452.08935546875, 1133.4063720703125, -316.0021667480469, 57.729408264160156, -2073.47314453125]\n",
      "\n",
      "Instance 1936 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 1364 tensor sets is: [453.8807067871094, 1131.8516845703125, -316.6844177246094, 56.88348388671875, -2073.02001953125]\n",
      "\n",
      "Instance 1937 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1938 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [1]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 1: [0.4295527935028076, 0.2844044268131256, 0.23688557744026184, 0.00736980140209198, 0.9091325402259827]\n",
      "Grand sum of 1365 tensor sets is: [454.3102722167969, 1132.1361083984375, -316.4475402832031, 56.89085388183594, -2072.11083984375]\n",
      "\n",
      "Instance 1939 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 32: [0.9629029035568237, 0.01942431926727295, -0.8555898666381836, 0.38378092646598816, 1.1695311069488525]\n",
      "Grand sum of 1366 tensor sets is: [455.2731628417969, 1132.155517578125, -317.3031311035156, 57.274635314941406, -2070.94140625]\n",
      "\n",
      "Instance 1940 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 28: [0.7666438221931458, 3.3937082290649414, -0.4433043301105499, -1.5617042779922485, -0.3821541666984558]\n",
      "Grand sum of 1367 tensor sets is: [456.039794921875, 1135.5491943359375, -317.7464294433594, 55.71293258666992, -2071.323486328125]\n",
      "\n",
      "Instance 1941 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1942 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1943 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1944 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 13: [0.7122939825057983, -0.5834611654281616, -0.7117363214492798, 0.011450424790382385, -0.9720163941383362]\n",
      "Grand sum of 1368 tensor sets is: [456.7520751953125, 1134.9656982421875, -318.4581604003906, 55.72438430786133, -2072.29541015625]\n",
      "\n",
      "Instance 1945 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 15: [-0.21757999062538147, -0.6876397728919983, 0.05092805624008179, -0.5392184257507324, -1.8225153684616089]\n",
      "Grand sum of 1369 tensor sets is: [456.53448486328125, 1134.278076171875, -318.4072265625, 55.18516540527344, -2074.117919921875]\n",
      "\n",
      "Instance 1946 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 19: [0.26421770453453064, -0.03174152970314026, -0.22444935142993927, -3.3928732872009277, -2.147773027420044]\n",
      "Grand sum of 1370 tensor sets is: [456.7987060546875, 1134.246337890625, -318.6316833496094, 51.792293548583984, -2076.265625]\n",
      "\n",
      "Instance 1947 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 35: [0.8164530396461487, 2.5037410259246826, -0.35468339920043945, -0.23923945426940918, 0.07596150040626526]\n",
      "Grand sum of 1371 tensor sets is: [457.61517333984375, 1136.7501220703125, -318.9863586425781, 51.55305480957031, -2076.189697265625]\n",
      "\n",
      "Instance 1948 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 26: [0.400550901889801, 0.7189286947250366, -0.6075277328491211, -1.4737600088119507, -2.4382126331329346]\n",
      "Grand sum of 1372 tensor sets is: [458.0157165527344, 1137.468994140625, -319.5938720703125, 50.07929611206055, -2078.6279296875]\n",
      "\n",
      "Instance 1949 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 31: [1.7967220544815063, 0.625460147857666, -0.7580216526985168, -0.9709306955337524, 0.11974747478961945]\n",
      "Grand sum of 1373 tensor sets is: [459.81243896484375, 1138.094482421875, -320.3518981933594, 49.10836410522461, -2078.50830078125]\n",
      "\n",
      "Instance 1950 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 2: [0.22390148043632507, -1.0350204706192017, -1.2310749292373657, 0.5405985116958618, 0.9379860162734985]\n",
      "Grand sum of 1374 tensor sets is: [460.0363464355469, 1137.0594482421875, -321.5829772949219, 49.648963928222656, -2077.5703125]\n",
      "\n",
      "Instance 1951 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 5: [0.05049986392259598, 2.6426186561584473, -0.3606257140636444, 1.1219706535339355, 2.215026378631592]\n",
      "Grand sum of 1375 tensor sets is: [460.08685302734375, 1139.7020263671875, -321.943603515625, 50.77093505859375, -2075.355224609375]\n",
      "\n",
      "Instance 1952 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1953 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 31: [0.4022781252861023, 1.9779874086380005, -1.1320397853851318, 0.7771404385566711, 0.6989696025848389]\n",
      "Grand sum of 1376 tensor sets is: [460.4891357421875, 1141.6800537109375, -323.0756530761719, 51.54807662963867, -2074.65625]\n",
      "\n",
      "Instance 1954 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 1377 tensor sets is: [460.8798522949219, 1142.75390625, -323.2992858886719, 48.907127380371094, -2069.283447265625]\n",
      "\n",
      "Instance 1955 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 32: [-0.6230658292770386, 0.5303539037704468, -1.0711599588394165, -0.12893027067184448, -3.2016351222991943]\n",
      "Grand sum of 1378 tensor sets is: [460.25677490234375, 1143.2843017578125, -324.3704528808594, 48.7781982421875, -2072.485107421875]\n",
      "\n",
      "Instance 1956 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [83]\n",
      "Size of token embeddings is torch.Size([126, 13, 768])\n",
      "Shape of summed layers is: 126 x 768\n",
      "forever at index 83: [0.07103681564331055, 1.174595832824707, 1.155388355255127, 0.10309364646673203, -2.150132417678833]\n",
      "Grand sum of 1379 tensor sets is: [460.32781982421875, 1144.4588623046875, -323.2150573730469, 48.881290435791016, -2074.63525390625]\n",
      "\n",
      "Instance 1957 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1958 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1959 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 9: [0.5127362608909607, 1.6839463710784912, -1.7059210538864136, -1.3932702541351318, -2.300595998764038]\n",
      "Grand sum of 1380 tensor sets is: [460.8405456542969, 1146.142822265625, -324.9209899902344, 47.48802185058594, -2076.935791015625]\n",
      "\n",
      "Instance 1960 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([81, 13, 768])\n",
      "Shape of summed layers is: 81 x 768\n",
      "forever at index 34: [0.32066968083381653, 2.315840721130371, 0.42728233337402344, -0.1014619916677475, -0.9035521745681763]\n",
      "Grand sum of 1381 tensor sets is: [461.1612243652344, 1148.4586181640625, -324.49371337890625, 47.386558532714844, -2077.83935546875]\n",
      "\n",
      "Instance 1961 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "forever at index 38: [0.30992865562438965, 0.1553918719291687, 0.19817928969860077, -1.953077793121338, -2.195916175842285]\n",
      "Grand sum of 1382 tensor sets is: [461.4711608886719, 1148.614013671875, -324.2955322265625, 45.43347930908203, -2080.03515625]\n",
      "\n",
      "Instance 1962 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 9: [0.09808286279439926, 2.6105146408081055, -0.8324180245399475, 0.27597784996032715, -0.5580456256866455]\n",
      "Grand sum of 1383 tensor sets is: [461.5692443847656, 1151.2244873046875, -325.1279602050781, 45.70945739746094, -2080.59326171875]\n",
      "\n",
      "Instance 1963 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 6: [-2.3178889751434326, 2.0723695755004883, 0.9701051115989685, -1.0434398651123047, -0.6485276818275452]\n",
      "Grand sum of 1384 tensor sets is: [459.2513427734375, 1153.296875, -324.1578674316406, 44.666015625, -2081.24169921875]\n",
      "\n",
      "Instance 1964 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 7: [0.48358893394470215, 2.3055455684661865, -0.031198740005493164, 0.5478510856628418, -6.491182327270508]\n",
      "Grand sum of 1385 tensor sets is: [459.73492431640625, 1155.6024169921875, -324.1890563964844, 45.2138671875, -2087.73291015625]\n",
      "\n",
      "Instance 1965 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [58]\n",
      "Size of token embeddings is torch.Size([78, 13, 768])\n",
      "Shape of summed layers is: 78 x 768\n",
      "forever at index 58: [0.3089781403541565, 1.0574235916137695, -0.18703095614910126, 2.958552837371826, 0.4714229106903076]\n",
      "Grand sum of 1386 tensor sets is: [460.0439147949219, 1156.6597900390625, -324.3760986328125, 48.172420501708984, -2087.261474609375]\n",
      "\n",
      "Instance 1966 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [1.6994589567184448, -0.21792569756507874, 0.2326381653547287, -0.911447286605835, -4.0184526443481445]\n",
      "Grand sum of 1387 tensor sets is: [461.7433776855469, 1156.44189453125, -324.1434631347656, 47.2609748840332, -2091.280029296875]\n",
      "\n",
      "Instance 1967 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 5: [0.16878627240657806, -0.10807082056999207, 0.8378866314888, -0.6610498428344727, -1.7766801118850708]\n",
      "Grand sum of 1388 tensor sets is: [461.91217041015625, 1156.3338623046875, -323.3055725097656, 46.59992599487305, -2093.056640625]\n",
      "\n",
      "Instance 1968 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 23: [0.6096040606498718, -1.2785965204238892, -0.8943328857421875, 0.38729673624038696, 3.0688958168029785]\n",
      "Grand sum of 1389 tensor sets is: [462.52178955078125, 1155.0552978515625, -324.19989013671875, 46.98722457885742, -2089.98779296875]\n",
      "\n",
      "Instance 1969 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 1390 tensor sets is: [462.9125061035156, 1156.129150390625, -324.42352294921875, 44.346275329589844, -2084.614990234375]\n",
      "\n",
      "Instance 1970 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 9: [0.5934656262397766, 1.1140263080596924, -0.4252891540527344, 1.1003504991531372, -3.073045253753662]\n",
      "Grand sum of 1391 tensor sets is: [463.5059814453125, 1157.2431640625, -324.84881591796875, 45.446624755859375, -2087.68798828125]\n",
      "\n",
      "Instance 1971 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [118]\n",
      "Size of token embeddings is torch.Size([170, 13, 768])\n",
      "Shape of summed layers is: 170 x 768\n",
      "forever at index 118: [1.1302268505096436, 0.6251394748687744, 0.08368626981973648, -1.331012487411499, -1.806151032447815]\n",
      "Grand sum of 1392 tensor sets is: [464.6361999511719, 1157.8682861328125, -324.76513671875, 44.1156120300293, -2089.494140625]\n",
      "\n",
      "Instance 1972 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [1, 2]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 1: [-0.08904105424880981, -0.936232328414917, -0.3478843569755554, -1.4821186065673828, 1.0253021717071533]\n",
      "forever at index 2: [-0.31744512915611267, -1.0318900346755981, 0.30648064613342285, -0.7331597805023193, 0.6265822649002075]\n",
      "Grand sum of 1393 tensor sets is: [464.4329528808594, 1156.88427734375, -324.78582763671875, 43.007972717285156, -2088.668212890625]\n",
      "\n",
      "Instance 1973 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1974 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 10: [0.9104049205780029, 0.766831636428833, -0.21603699028491974, -0.5190383195877075, -4.156322479248047]\n",
      "Grand sum of 1394 tensor sets is: [465.3433532714844, 1157.651123046875, -325.0018615722656, 42.48893356323242, -2092.824462890625]\n",
      "\n",
      "Instance 1975 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [90]\n",
      "Size of token embeddings is torch.Size([95, 13, 768])\n",
      "Shape of summed layers is: 95 x 768\n",
      "forever at index 90: [0.8284138441085815, -0.8050498962402344, -0.20533408224582672, 0.802809476852417, 2.2268428802490234]\n",
      "Grand sum of 1395 tensor sets is: [466.1717529296875, 1156.8460693359375, -325.2071838378906, 43.291744232177734, -2090.59765625]\n",
      "\n",
      "Instance 1976 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1977 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1978 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 8: [0.7191540598869324, 2.597564220428467, -0.13535168766975403, -1.43770432472229, 1.2078138589859009]\n",
      "Grand sum of 1396 tensor sets is: [466.8908996582031, 1159.443603515625, -325.342529296875, 41.85403823852539, -2089.389892578125]\n",
      "\n",
      "Instance 1979 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 20: [0.11697055399417877, 2.4457435607910156, -0.4542301297187805, 2.0087621212005615, -2.385776996612549]\n",
      "Grand sum of 1397 tensor sets is: [467.00787353515625, 1161.889404296875, -325.7967529296875, 43.86280059814453, -2091.775634765625]\n",
      "\n",
      "Instance 1980 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 20: [0.3660978078842163, 0.7815632820129395, -1.7551839351654053, 2.035003900527954, -4.049980163574219]\n",
      "Grand sum of 1398 tensor sets is: [467.37396240234375, 1162.6710205078125, -327.55194091796875, 45.897804260253906, -2095.82568359375]\n",
      "\n",
      "Instance 1981 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1982 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1983 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [1.6994589567184448, -0.21792569756507874, 0.2326381653547287, -0.911447286605835, -4.0184526443481445]\n",
      "Grand sum of 1399 tensor sets is: [469.07342529296875, 1162.453125, -327.3193054199219, 44.986358642578125, -2099.84423828125]\n",
      "\n",
      "Instance 1984 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1400 tensor sets is: [469.1748352050781, 1164.1925048828125, -327.95733642578125, 43.870975494384766, -2104.140869140625]\n",
      "\n",
      "Instance 1985 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 1401 tensor sets is: [469.5705261230469, 1165.201171875, -328.02691650390625, 43.18741226196289, -2102.844482421875]\n",
      "\n",
      "Instance 1986 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1987 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1988 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 14: [0.4658612012863159, -0.6436519026756287, -0.2748093008995056, 1.8496003150939941, -4.528522491455078]\n",
      "Grand sum of 1402 tensor sets is: [470.036376953125, 1164.5574951171875, -328.3017272949219, 45.03701400756836, -2107.373046875]\n",
      "\n",
      "Instance 1989 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [0.3498929738998413, 0.9748251438140869, -1.374105453491211, -0.0809759795665741, -5.644731044769287]\n",
      "Grand sum of 1403 tensor sets is: [470.3862609863281, 1165.5323486328125, -329.67584228515625, 44.95603942871094, -2113.017822265625]\n",
      "\n",
      "Instance 1990 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1991 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 53: [0.666554868221283, -0.7873085737228394, -1.431145191192627, 1.6047497987747192, -2.4387478828430176]\n",
      "Grand sum of 1404 tensor sets is: [471.0528259277344, 1164.7449951171875, -331.10699462890625, 46.560791015625, -2115.45654296875]\n",
      "\n",
      "Instance 1992 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 3: [-0.9431154131889343, 0.30131253600120544, -1.1991311311721802, 2.3228437900543213, 0.5337315797805786]\n",
      "Grand sum of 1405 tensor sets is: [470.1097106933594, 1165.0462646484375, -332.3061218261719, 48.883636474609375, -2114.9228515625]\n",
      "\n",
      "Instance 1993 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "forever at index 3: [1.0051188468933105, -0.13949325680732727, -0.5028639435768127, -0.42343443632125854, 1.4101810455322266]\n",
      "Grand sum of 1406 tensor sets is: [471.1148376464844, 1164.90673828125, -332.8089904785156, 48.460201263427734, -2113.5126953125]\n",
      "\n",
      "Instance 1994 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1407 tensor sets is: [471.21624755859375, 1166.6461181640625, -333.447021484375, 47.344818115234375, -2117.809326171875]\n",
      "\n",
      "Instance 1995 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1996 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 1997 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([98, 13, 768])\n",
      "Shape of summed layers is: 98 x 768\n",
      "forever at index 5: [0.36085912585258484, 0.3790145516395569, -0.9976582527160645, 0.6957803964614868, -0.40552234649658203]\n",
      "Grand sum of 1408 tensor sets is: [471.5771179199219, 1167.025146484375, -334.4446716308594, 48.04059982299805, -2118.21484375]\n",
      "\n",
      "Instance 1998 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [54]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "forever at index 54: [-0.35940536856651306, -0.052329882979393005, -1.0900832414627075, 0.6745641231536865, -0.9731273651123047]\n",
      "Grand sum of 1409 tensor sets is: [471.21771240234375, 1166.9727783203125, -335.5347595214844, 48.71516418457031, -2119.18798828125]\n",
      "\n",
      "Instance 1999 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2000 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 1410 tensor sets is: [471.6084289550781, 1168.046630859375, -335.7583923339844, 46.074214935302734, -2113.815185546875]\n",
      "\n",
      "Instance 2001 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2002 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 10: [0.5423617362976074, 0.9779613614082336, -0.372332900762558, 0.1429896056652069, -1.6070632934570312]\n",
      "Grand sum of 1411 tensor sets is: [472.1507873535156, 1169.0245361328125, -336.1307373046875, 46.21720504760742, -2115.42236328125]\n",
      "\n",
      "Instance 2003 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2004 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 1412 tensor sets is: [471.4160461425781, 1169.429443359375, -335.7390441894531, 45.860145568847656, -2119.983154296875]\n",
      "\n",
      "Instance 2005 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2006 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 8: [0.24743735790252686, 2.153682231903076, 0.010694362223148346, -0.2478337436914444, -1.5513173341751099]\n",
      "Grand sum of 1413 tensor sets is: [471.6634826660156, 1171.5831298828125, -335.7283630371094, 45.61231231689453, -2121.534423828125]\n",
      "\n",
      "Instance 2007 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 8: [0.8391567468643188, -0.0043168216943740845, -0.6980368494987488, -1.0319147109985352, -0.7486999034881592]\n",
      "Grand sum of 1414 tensor sets is: [472.50262451171875, 1171.578857421875, -336.4263916015625, 44.58039855957031, -2122.283203125]\n",
      "\n",
      "Instance 2008 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2009 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 39: [-0.24740281701087952, 1.3252360820770264, 0.2622608542442322, 0.4683770537376404, -1.09104323387146]\n",
      "Grand sum of 1415 tensor sets is: [472.2552185058594, 1172.904052734375, -336.16412353515625, 45.04877471923828, -2123.374267578125]\n",
      "\n",
      "Instance 2010 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 21: [0.3565308749675751, 0.13220976293087006, -1.0269287824630737, 2.582505464553833, -5.253737449645996]\n",
      "Grand sum of 1416 tensor sets is: [472.61175537109375, 1173.0362548828125, -337.1910400390625, 47.63127899169922, -2128.6279296875]\n",
      "\n",
      "Instance 2011 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 9: [0.36403074860572815, 0.7289369106292725, 0.08295699954032898, 0.27647843956947327, -3.562440872192383]\n",
      "Grand sum of 1417 tensor sets is: [472.9757995605469, 1173.76513671875, -337.10809326171875, 47.90775680541992, -2132.1904296875]\n",
      "\n",
      "Instance 2012 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "forever at index 33: [0.5447864532470703, 0.8691648244857788, -0.21205826103687286, -0.9309178590774536, 0.5619667768478394]\n",
      "Grand sum of 1418 tensor sets is: [473.5205993652344, 1174.63427734375, -337.3201599121094, 46.976837158203125, -2131.62841796875]\n",
      "\n",
      "Instance 2013 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 15: [0.7992382645606995, 0.6638726592063904, -0.6857155561447144, 0.5731008648872375, -4.835704326629639]\n",
      "Grand sum of 1419 tensor sets is: [474.31982421875, 1175.298095703125, -338.0058898925781, 47.5499382019043, -2136.464111328125]\n",
      "\n",
      "Instance 2014 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 12: [-0.18325351178646088, -0.9651882648468018, -0.25119882822036743, -0.06060896813869476, -2.8344850540161133]\n",
      "Grand sum of 1420 tensor sets is: [474.1365661621094, 1174.3328857421875, -338.257080078125, 47.48933029174805, -2139.298583984375]\n",
      "\n",
      "Instance 2015 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 4: [0.6326205730438232, 0.38100263476371765, -0.47913551330566406, -0.44007933139801025, -0.6237162351608276]\n",
      "Grand sum of 1421 tensor sets is: [474.7691955566406, 1174.7138671875, -338.7362060546875, 47.049251556396484, -2139.92236328125]\n",
      "\n",
      "Instance 2016 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 5: [-0.8601359128952026, 1.4352011680603027, -0.21128728985786438, -0.23573458194732666, -2.3410472869873047]\n",
      "Grand sum of 1422 tensor sets is: [473.9090576171875, 1176.1490478515625, -338.9474792480469, 46.81351852416992, -2142.263427734375]\n",
      "\n",
      "Instance 2017 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 18: [-0.08627288788557053, 1.184529423713684, -0.7141148447990417, 1.5770025253295898, -3.429564952850342]\n",
      "Grand sum of 1423 tensor sets is: [473.8227844238281, 1177.3336181640625, -339.6615905761719, 48.39052200317383, -2145.69287109375]\n",
      "\n",
      "Instance 2018 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 29: [1.132653832435608, 2.2863540649414062, 0.4666453003883362, 2.9122204780578613, -2.8947372436523438]\n",
      "Grand sum of 1424 tensor sets is: [474.9554443359375, 1179.6199951171875, -339.1949462890625, 51.30274200439453, -2148.587646484375]\n",
      "\n",
      "Instance 2019 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4, 12]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "forever at index 4: [0.19769474864006042, 0.9239522218704224, 0.0995880514383316, -0.08179770410060883, 0.22700658440589905]\n",
      "forever at index 12: [0.6060097217559814, 0.5739008784294128, 0.5269936323165894, -1.1561846733093262, -0.11992436647415161]\n",
      "Grand sum of 1425 tensor sets is: [475.3572998046875, 1180.368896484375, -338.88165283203125, 50.68375015258789, -2148.5341796875]\n",
      "\n",
      "Instance 2020 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 34: [-0.2564598619937897, -0.3268887400627136, -1.9103718996047974, -0.6442441940307617, 1.3184504508972168]\n",
      "Grand sum of 1426 tensor sets is: [475.100830078125, 1180.0419921875, -340.7920227050781, 50.03950500488281, -2147.2158203125]\n",
      "\n",
      "Instance 2021 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 1427 tensor sets is: [476.8921813964844, 1178.4873046875, -341.4742736816406, 49.193580627441406, -2146.7626953125]\n",
      "\n",
      "Instance 2022 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([78, 13, 768])\n",
      "Shape of summed layers is: 78 x 768\n",
      "forever at index 74: [-0.9621662497520447, -0.31729865074157715, -1.135249376296997, 1.1995973587036133, -1.9001832008361816]\n",
      "Grand sum of 1428 tensor sets is: [475.9300231933594, 1178.1700439453125, -342.6095275878906, 50.3931770324707, -2148.662841796875]\n",
      "\n",
      "Instance 2023 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "forever at index 12: [-0.09824617207050323, 0.9465703964233398, 0.6908197402954102, 2.9915778636932373, 0.21310952305793762]\n",
      "Grand sum of 1429 tensor sets is: [475.831787109375, 1179.1165771484375, -341.918701171875, 53.3847541809082, -2148.44970703125]\n",
      "\n",
      "Instance 2024 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [62]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 62: [0.04090709611773491, -1.670753836631775, 0.9987567663192749, 0.09906500577926636, 0.702258825302124]\n",
      "Grand sum of 1430 tensor sets is: [475.8726806640625, 1177.44580078125, -340.9199523925781, 53.48381805419922, -2147.74755859375]\n",
      "\n",
      "Instance 2025 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 7: [-0.14925307035446167, 0.5280888676643372, 0.020995140075683594, -0.7849578261375427, 1.5560033321380615]\n",
      "Grand sum of 1431 tensor sets is: [475.7234191894531, 1177.973876953125, -340.8989562988281, 52.69886016845703, -2146.191650390625]\n",
      "\n",
      "Instance 2026 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "forever at index 43: [0.705483615398407, -1.8049049377441406, 0.18964418768882751, 1.611323356628418, -4.160096645355225]\n",
      "Grand sum of 1432 tensor sets is: [476.42889404296875, 1176.1689453125, -340.7093200683594, 54.310184478759766, -2150.351806640625]\n",
      "\n",
      "Instance 2027 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 28: [0.7538031339645386, -0.5053645372390747, -0.044456735253334045, 1.4420287609100342, -2.9960527420043945]\n",
      "Grand sum of 1433 tensor sets is: [477.1827087402344, 1175.66357421875, -340.7537841796875, 55.75221252441406, -2153.347900390625]\n",
      "\n",
      "Instance 2028 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 53: [0.666554868221283, -0.7873085737228394, -1.431145191192627, 1.6047497987747192, -2.4387478828430176]\n",
      "Grand sum of 1434 tensor sets is: [477.8492736816406, 1174.876220703125, -342.1849365234375, 57.356964111328125, -2155.78662109375]\n",
      "\n",
      "Instance 2029 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2030 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 3: [0.8674190044403076, 0.6085760593414307, -0.22337208688259125, -0.9356646537780762, 0.4696657359600067]\n",
      "Grand sum of 1435 tensor sets is: [478.7167053222656, 1175.4847412109375, -342.4082946777344, 56.42129898071289, -2155.31689453125]\n",
      "\n",
      "Instance 2031 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 8: [0.19374868273735046, 0.8961356282234192, -1.3206279277801514, 0.003491729497909546, -2.188938856124878]\n",
      "Grand sum of 1436 tensor sets is: [478.91046142578125, 1176.380859375, -343.7289123535156, 56.42478942871094, -2157.505859375]\n",
      "\n",
      "Instance 2032 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 7: [-0.41640105843544006, 1.9023795127868652, -1.5473904609680176, 1.0560188293457031, -4.434470176696777]\n",
      "Grand sum of 1437 tensor sets is: [478.4940490722656, 1178.283203125, -345.27630615234375, 57.48080825805664, -2161.9404296875]\n",
      "\n",
      "Instance 2033 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2034 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 10: [0.45649904012680054, 1.2778434753417969, -0.8464846611022949, 1.4232455492019653, -0.05080398917198181]\n",
      "Grand sum of 1438 tensor sets is: [478.9505615234375, 1179.56103515625, -346.122802734375, 58.904052734375, -2161.9912109375]\n",
      "\n",
      "Instance 2035 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2036 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 23: [0.25613653659820557, 1.7356373071670532, -0.0017609372735023499, 1.8842601776123047, -1.2285208702087402]\n",
      "Grand sum of 1439 tensor sets is: [479.2066955566406, 1181.296630859375, -346.12457275390625, 60.78831481933594, -2163.2197265625]\n",
      "\n",
      "Instance 2037 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1440 tensor sets is: [479.30810546875, 1183.0360107421875, -346.7626037597656, 59.67293167114258, -2167.516357421875]\n",
      "\n",
      "Instance 2038 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 1441 tensor sets is: [479.4461669921875, 1183.690185546875, -347.20318603515625, 59.3756103515625, -2170.36669921875]\n",
      "\n",
      "Instance 2039 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 9: [0.5218549370765686, 0.9484233856201172, 0.1863338053226471, 0.2602981925010681, -3.169445037841797]\n",
      "Grand sum of 1442 tensor sets is: [479.968017578125, 1184.6385498046875, -347.016845703125, 59.63591003417969, -2173.5361328125]\n",
      "\n",
      "Instance 2040 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 5: [0.11453734338283539, 2.597841739654541, -0.3012343943119049, 0.2762383222579956, 0.931027889251709]\n",
      "Grand sum of 1443 tensor sets is: [480.0825500488281, 1187.2364501953125, -347.3180847167969, 59.912147521972656, -2172.605224609375]\n",
      "\n",
      "Instance 2041 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 26: [0.37860313057899475, -0.8008484840393066, -0.2810317873954773, 0.23155392706394196, -1.51703679561615]\n",
      "Grand sum of 1444 tensor sets is: [480.4611511230469, 1186.435546875, -347.59912109375, 60.143699645996094, -2174.122314453125]\n",
      "\n",
      "Instance 2042 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 9: [1.2111119031906128, 0.8916640281677246, 0.942943811416626, -0.15736126899719238, 0.11870068311691284]\n",
      "Grand sum of 1445 tensor sets is: [481.6722717285156, 1187.3272705078125, -346.65618896484375, 59.9863395690918, -2174.003662109375]\n",
      "\n",
      "Instance 2043 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 4: [-0.5188701748847961, -0.6284850835800171, 0.3127037286758423, 0.7452778816223145, 0.7820901870727539]\n",
      "Grand sum of 1446 tensor sets is: [481.1534118652344, 1186.69873046875, -346.3434753417969, 60.73161697387695, -2173.2216796875]\n",
      "\n",
      "Instance 2044 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 24: [0.48892319202423096, 0.8739367127418518, -0.35139983892440796, -0.37098991870880127, -1.8001636266708374]\n",
      "Grand sum of 1447 tensor sets is: [481.642333984375, 1187.5726318359375, -346.69488525390625, 60.360626220703125, -2175.021728515625]\n",
      "\n",
      "Instance 2045 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 3: [0.14191044867038727, 1.746381163597107, -0.49761220812797546, -0.027949512004852295, -0.42751356959342957]\n",
      "Grand sum of 1448 tensor sets is: [481.78424072265625, 1189.3189697265625, -347.1925048828125, 60.33267593383789, -2175.44921875]\n",
      "\n",
      "Instance 2046 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 1449 tensor sets is: [482.179931640625, 1190.32763671875, -347.2620849609375, 59.649112701416016, -2174.15283203125]\n",
      "\n",
      "Instance 2047 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 3: [0.262912780046463, 0.6668047904968262, -0.9871349334716797, 2.2932703495025635, -4.634162425994873]\n",
      "Grand sum of 1450 tensor sets is: [482.4428405761719, 1190.994384765625, -348.24920654296875, 61.9423828125, -2178.787109375]\n",
      "\n",
      "Instance 2048 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 23: [0.6096040606498718, -1.2785965204238892, -0.8943328857421875, 0.38729673624038696, 3.0688958168029785]\n",
      "Grand sum of 1451 tensor sets is: [483.0524597167969, 1189.7158203125, -349.1435546875, 62.329681396484375, -2175.71826171875]\n",
      "\n",
      "Instance 2049 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 8: [0.1305556446313858, 1.8028452396392822, -1.4802212715148926, -2.1369388103485107, 0.09891679883003235]\n",
      "Grand sum of 1452 tensor sets is: [483.1830139160156, 1191.5186767578125, -350.623779296875, 60.19274139404297, -2175.619384765625]\n",
      "\n",
      "Instance 2050 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2051 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 12: [0.43202492594718933, -1.139236330986023, -0.40867727994918823, -0.44313716888427734, 1.4331287145614624]\n",
      "Grand sum of 1453 tensor sets is: [483.61505126953125, 1190.37939453125, -351.032470703125, 59.749603271484375, -2174.186279296875]\n",
      "\n",
      "Instance 2052 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2053 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 15: [0.7152518033981323, 3.0281548500061035, 0.9368008375167847, 2.4919745922088623, -0.9967325329780579]\n",
      "Grand sum of 1454 tensor sets is: [484.3302917480469, 1193.4075927734375, -350.0956726074219, 62.2415771484375, -2175.18310546875]\n",
      "\n",
      "Instance 2054 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2055 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 9: [1.098562479019165, -0.07149630784988403, -0.65312260389328, -1.7348510026931763, -1.0558137893676758]\n",
      "Grand sum of 1455 tensor sets is: [485.4288635253906, 1193.3360595703125, -350.7488098144531, 60.5067253112793, -2176.239013671875]\n",
      "\n",
      "Instance 2056 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2057 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1456 tensor sets is: [485.5302734375, 1195.075439453125, -351.3868408203125, 59.39134216308594, -2180.53564453125]\n",
      "\n",
      "Instance 2058 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [1.366894245147705, 0.3037130534648895, 0.7437591552734375, 0.26792025566101074, -0.9232490062713623]\n",
      "Grand sum of 1457 tensor sets is: [486.89715576171875, 1195.379150390625, -350.64306640625, 59.659263610839844, -2181.458984375]\n",
      "\n",
      "Instance 2059 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 3: [0.8838528394699097, 0.36088302731513977, -0.15678158402442932, -0.5789933800697327, 1.9426205158233643]\n",
      "Grand sum of 1458 tensor sets is: [487.781005859375, 1195.739990234375, -350.7998352050781, 59.08026885986328, -2179.516357421875]\n",
      "\n",
      "Instance 2060 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 22: [1.8938905000686646, 1.784278154373169, -1.436119794845581, -1.1105173826217651, 1.7212132215499878]\n",
      "Grand sum of 1459 tensor sets is: [489.6748962402344, 1197.5242919921875, -352.2359619140625, 57.96975326538086, -2177.795166015625]\n",
      "\n",
      "Instance 2061 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 9: [1.0918303728103638, 1.546337604522705, 0.7251821756362915, -0.6038042902946472, 3.6248083114624023]\n",
      "Grand sum of 1460 tensor sets is: [490.7667236328125, 1199.0706787109375, -351.5107727050781, 57.36594772338867, -2174.17041015625]\n",
      "\n",
      "Instance 2062 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 8: [0.5321953892707825, 0.43688249588012695, 0.27559006214141846, 0.5427164435386658, -2.0752856731414795]\n",
      "Grand sum of 1461 tensor sets is: [491.2989196777344, 1199.507568359375, -351.23516845703125, 57.90866470336914, -2176.24560546875]\n",
      "\n",
      "Instance 2063 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 3: [0.7214680910110474, -0.5151770114898682, -0.23026883602142334, -1.1771399974822998, 1.4443559646606445]\n",
      "Grand sum of 1462 tensor sets is: [492.0203857421875, 1198.992431640625, -351.4654235839844, 56.73152542114258, -2174.80126953125]\n",
      "\n",
      "Instance 2064 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2065 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [63]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "forever at index 63: [-0.3769170641899109, -0.25676125288009644, -1.0251142978668213, 0.10496726632118225, -0.19741638004779816]\n",
      "Grand sum of 1463 tensor sets is: [491.6434631347656, 1198.7357177734375, -352.49053955078125, 56.83649444580078, -2174.998779296875]\n",
      "\n",
      "Instance 2066 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2067 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 29: [0.5993239879608154, 0.14783580601215363, -0.2562742829322815, 0.12871547043323517, -0.7251304984092712]\n",
      "Grand sum of 1464 tensor sets is: [492.2427978515625, 1198.883544921875, -352.746826171875, 56.9652099609375, -2175.723876953125]\n",
      "\n",
      "Instance 2068 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([142, 13, 768])\n",
      "Shape of summed layers is: 142 x 768\n",
      "forever at index 11: [0.15067969262599945, 0.7939843535423279, -1.5837278366088867, -1.1507213115692139, 0.3493054509162903]\n",
      "Grand sum of 1465 tensor sets is: [492.3934631347656, 1199.677490234375, -354.33056640625, 55.81448745727539, -2175.37451171875]\n",
      "\n",
      "Instance 2069 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [1]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 1: [-0.3276218771934509, -0.7526265978813171, 0.5264058709144592, -0.5372047424316406, -0.7454492449760437]\n",
      "Grand sum of 1466 tensor sets is: [492.0658264160156, 1198.9248046875, -353.8041687011719, 55.27728271484375, -2176.119873046875]\n",
      "\n",
      "Instance 2070 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 1467 tensor sets is: [492.45654296875, 1199.9986572265625, -354.0278015136719, 52.63633346557617, -2170.7470703125]\n",
      "\n",
      "Instance 2071 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2072 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2073 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 6: [-0.785726010799408, 1.2101291418075562, -0.08649443835020065, 2.3100814819335938, -0.05883467197418213]\n",
      "Grand sum of 1468 tensor sets is: [491.6708068847656, 1201.208740234375, -354.1142883300781, 54.946414947509766, -2170.805908203125]\n",
      "\n",
      "Instance 2074 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 1469 tensor sets is: [492.2147216796875, 1200.5738525390625, -354.7638244628906, 53.80419158935547, -2169.6845703125]\n",
      "\n",
      "Instance 2075 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 3: [0.40155142545700073, 0.40877315402030945, 0.7246789932250977, 0.4708523154258728, -1.8498430252075195]\n",
      "Grand sum of 1470 tensor sets is: [492.61627197265625, 1200.982666015625, -354.0391540527344, 54.27504348754883, -2171.534423828125]\n",
      "\n",
      "Instance 2076 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2077 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 9: [0.16780094802379608, 0.3673933148384094, -0.6137426495552063, 1.908109426498413, -4.417931079864502]\n",
      "Grand sum of 1471 tensor sets is: [492.7840881347656, 1201.35009765625, -354.65289306640625, 56.18315124511719, -2175.952392578125]\n",
      "\n",
      "Instance 2078 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 33: [0.3907316327095032, 1.0738276243209839, -0.22364214062690735, -2.64094877243042, 5.372750282287598]\n",
      "Grand sum of 1472 tensor sets is: [493.1748046875, 1202.4239501953125, -354.87652587890625, 53.54220199584961, -2170.57958984375]\n",
      "\n",
      "Instance 2079 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 7: [0.3480818271636963, 2.406264066696167, -0.6363531947135925, 2.3194010257720947, -0.1331101953983307]\n",
      "Grand sum of 1473 tensor sets is: [493.52288818359375, 1204.8302001953125, -355.51287841796875, 55.861602783203125, -2170.712646484375]\n",
      "\n",
      "Instance 2080 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2081 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 6: [0.6458348035812378, 1.251787781715393, 0.0830710381269455, 2.747269630432129, -3.8478469848632812]\n",
      "Grand sum of 1474 tensor sets is: [494.1687316894531, 1206.08203125, -355.4298095703125, 58.60887145996094, -2174.560546875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 2082 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 5: [-0.3383725583553314, 1.851874828338623, 0.2177410125732422, -1.0514795780181885, -1.3264408111572266]\n",
      "Grand sum of 1475 tensor sets is: [493.8303527832031, 1207.9339599609375, -355.2120666503906, 57.55739212036133, -2175.886962890625]\n",
      "\n",
      "Instance 2083 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2084 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([75, 13, 768])\n",
      "Shape of summed layers is: 75 x 768\n",
      "forever at index 7: [0.5589639544487, 3.0550198554992676, 0.05146200954914093, 0.2576420307159424, -1.6746045351028442]\n",
      "Grand sum of 1476 tensor sets is: [494.3893127441406, 1210.989013671875, -355.1606140136719, 57.815032958984375, -2177.5615234375]\n",
      "\n",
      "Instance 2085 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 23: [0.25613653659820557, 1.7356373071670532, -0.0017609372735023499, 1.8842601776123047, -1.2285208702087402]\n",
      "Grand sum of 1477 tensor sets is: [494.64544677734375, 1212.724609375, -355.1623840332031, 59.69929504394531, -2178.7900390625]\n",
      "\n",
      "Instance 2086 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [1.0301811695098877, 0.09672722220420837, -1.850982904434204, 0.9610074162483215, -2.2480239868164062]\n",
      "Grand sum of 1478 tensor sets is: [495.6756286621094, 1212.8212890625, -357.01336669921875, 60.660301208496094, -2181.0380859375]\n",
      "\n",
      "Instance 2087 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 1479 tensor sets is: [495.0401916503906, 1214.5430908203125, -356.2808532714844, 61.96607971191406, -2181.21484375]\n",
      "\n",
      "Instance 2088 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "forever at index 4: [-0.3200491666793823, 0.7375553250312805, 1.6378397941589355, 1.5015676021575928, -0.7497329711914062]\n",
      "Grand sum of 1480 tensor sets is: [494.72015380859375, 1215.2806396484375, -354.64300537109375, 63.467647552490234, -2181.964599609375]\n",
      "\n",
      "Instance 2089 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "forever at index 16: [-0.3696151673793793, 2.6771321296691895, 0.09934107959270477, 1.744758129119873, -0.7530254125595093]\n",
      "Grand sum of 1481 tensor sets is: [494.35052490234375, 1217.957763671875, -354.5436706542969, 65.21240234375, -2182.717529296875]\n",
      "\n",
      "Instance 2090 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2091 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2092 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2093 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2094 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 14: [0.170328289270401, 0.9191398620605469, -0.2894980311393738, -1.3060963153839111, -1.6856714487075806]\n",
      "Grand sum of 1482 tensor sets is: [494.5208435058594, 1218.876953125, -354.8331604003906, 63.906307220458984, -2184.4033203125]\n",
      "\n",
      "Instance 2095 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 5: [-0.38718658685684204, -0.7276805639266968, -0.4920441210269928, -0.22040963172912598, 0.40566539764404297]\n",
      "Grand sum of 1483 tensor sets is: [494.1336669921875, 1218.1492919921875, -355.3251953125, 63.68589782714844, -2183.99755859375]\n",
      "\n",
      "Instance 2096 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 8: [0.4675549566745758, 0.5072417259216309, 0.08221422135829926, -0.4410611391067505, -0.252644419670105]\n",
      "Grand sum of 1484 tensor sets is: [494.6012268066406, 1218.656494140625, -355.24298095703125, 63.244834899902344, -2184.250244140625]\n",
      "\n",
      "Instance 2097 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "forever at index 18: [0.7844703793525696, 0.370402991771698, -0.12934918701648712, -0.17644335329532623, -0.3598279654979706]\n",
      "Grand sum of 1485 tensor sets is: [495.3857116699219, 1219.02685546875, -355.3723449707031, 63.068389892578125, -2184.610107421875]\n",
      "\n",
      "Instance 2098 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 10: [-0.42309683561325073, -0.07070571184158325, -0.6382231116294861, -1.026694416999817, -1.658251404762268]\n",
      "Grand sum of 1486 tensor sets is: [494.9626159667969, 1218.9561767578125, -356.01055908203125, 62.04169464111328, -2186.268310546875]\n",
      "\n",
      "Instance 2099 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2100 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 41: [0.05608724057674408, 2.2491841316223145, -0.8168084025382996, 0.29389894008636475, -3.87876033782959]\n",
      "Grand sum of 1487 tensor sets is: [495.0187072753906, 1221.205322265625, -356.8273620605469, 62.335594177246094, -2190.14697265625]\n",
      "\n",
      "Instance 2101 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([93, 13, 768])\n",
      "Shape of summed layers is: 93 x 768\n",
      "forever at index 38: [-1.1777037382125854, -0.2888803780078888, -0.2941676676273346, -0.5361020565032959, -3.4033100605010986]\n",
      "Grand sum of 1488 tensor sets is: [493.84100341796875, 1220.9163818359375, -357.12152099609375, 61.79949188232422, -2193.55029296875]\n",
      "\n",
      "Instance 2102 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "forever at index 53: [0.5529572367668152, -0.5191553235054016, -0.2242467999458313, -0.7202454805374146, -0.8007594347000122]\n",
      "Grand sum of 1489 tensor sets is: [494.3939514160156, 1220.397216796875, -357.34576416015625, 61.079246520996094, -2194.35107421875]\n",
      "\n",
      "Instance 2103 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2104 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 6: [0.39659440517425537, 2.2821192741394043, 0.09638229012489319, -0.1714290827512741, -1.2504308223724365]\n",
      "Grand sum of 1490 tensor sets is: [494.7905578613281, 1222.6793212890625, -357.2493896484375, 60.90781784057617, -2195.6015625]\n",
      "\n",
      "Instance 2105 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2106 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.05075901746749878, 2.21020770072937, 0.04688075929880142, -1.1270672082901, -1.3753117322921753]\n",
      "Grand sum of 1491 tensor sets is: [494.84130859375, 1224.8895263671875, -357.2025146484375, 59.7807502746582, -2196.976806640625]\n",
      "\n",
      "Instance 2107 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 1492 tensor sets is: [494.1065673828125, 1225.29443359375, -356.8108215332031, 59.42369079589844, -2201.53759765625]\n",
      "\n",
      "Instance 2108 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 1493 tensor sets is: [493.47113037109375, 1227.0162353515625, -356.07830810546875, 60.729469299316406, -2201.71435546875]\n",
      "\n",
      "Instance 2109 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 14: [-0.02545209974050522, 1.3907983303070068, -0.3009229898452759, 3.4477641582489014, 0.6799418330192566]\n",
      "Grand sum of 1494 tensor sets is: [493.4456787109375, 1228.406982421875, -356.3792419433594, 64.17723083496094, -2201.034423828125]\n",
      "\n",
      "Instance 2110 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 5: [0.0016269870102405548, -0.9829532504081726, -0.3052789866924286, -0.25711590051651, -3.3363444805145264]\n",
      "Grand sum of 1495 tensor sets is: [493.4472961425781, 1227.424072265625, -356.68450927734375, 63.92011642456055, -2204.370849609375]\n",
      "\n",
      "Instance 2111 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "forever at index 7: [0.45713508129119873, -0.6950680017471313, 0.08215746283531189, 0.8868474364280701, -3.7884974479675293]\n",
      "Grand sum of 1496 tensor sets is: [493.9044189453125, 1226.72900390625, -356.60235595703125, 64.80696105957031, -2208.159423828125]\n",
      "\n",
      "Instance 2112 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2113 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 3: [0.3684597909450531, 0.3012693524360657, -0.03804786503314972, 1.4354119300842285, -4.2325439453125]\n",
      "Grand sum of 1497 tensor sets is: [494.27288818359375, 1227.0302734375, -356.6404113769531, 66.24237060546875, -2212.39208984375]\n",
      "\n",
      "Instance 2114 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 15: [0.1990681290626526, 2.2539663314819336, -0.8350521326065063, -1.8975818157196045, -2.130544662475586]\n",
      "Grand sum of 1498 tensor sets is: [494.4719543457031, 1229.2841796875, -357.4754638671875, 64.34478759765625, -2214.522705078125]\n",
      "\n",
      "Instance 2115 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2116 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "forever at index 14: [0.5092223286628723, 1.3658702373504639, -0.5270277261734009, 0.37430793046951294, -2.304713487625122]\n",
      "Grand sum of 1499 tensor sets is: [494.9811706542969, 1230.6500244140625, -358.00250244140625, 64.7190933227539, -2216.827392578125]\n",
      "\n",
      "Instance 2117 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 19: [0.8453393578529358, 0.5756961703300476, 0.3677171468734741, -0.46797752380371094, -2.994748592376709]\n",
      "Grand sum of 1500 tensor sets is: [495.8265075683594, 1231.2257080078125, -357.6347961425781, 64.25111389160156, -2219.822021484375]\n",
      "\n",
      "Instance 2118 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2119 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2120 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2121 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2122 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2123 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1501 tensor sets is: [495.92791748046875, 1232.965087890625, -358.2728271484375, 63.1357307434082, -2224.11865234375]\n",
      "\n",
      "Instance 2124 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 35: [0.557374119758606, 1.2128506898880005, 0.23274052143096924, 2.601256847381592, 0.09828796982765198]\n",
      "Grand sum of 1502 tensor sets is: [496.48529052734375, 1234.177978515625, -358.04010009765625, 65.73698425292969, -2224.020263671875]\n",
      "\n",
      "Instance 2125 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2126 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2127 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2128 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 5: [0.36143025755882263, 1.75130033493042, 0.16450771689414978, 0.5068372488021851, 0.5175354480743408]\n",
      "Grand sum of 1503 tensor sets is: [496.8467102050781, 1235.9293212890625, -357.8755798339844, 66.24382019042969, -2223.502685546875]\n",
      "\n",
      "Instance 2129 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 1504 tensor sets is: [497.390625, 1235.29443359375, -358.5251159667969, 65.10160064697266, -2222.38134765625]\n",
      "\n",
      "Instance 2130 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 53: [0.666554868221283, -0.7873085737228394, -1.431145191192627, 1.6047497987747192, -2.4387478828430176]\n",
      "Grand sum of 1505 tensor sets is: [498.05718994140625, 1234.507080078125, -359.9562683105469, 66.70635223388672, -2224.820068359375]\n",
      "\n",
      "Instance 2131 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2132 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 16: [0.20596185326576233, -0.13367056846618652, 0.09476975351572037, -1.2574779987335205, -2.7590904235839844]\n",
      "Grand sum of 1506 tensor sets is: [498.2631530761719, 1234.3734130859375, -359.86151123046875, 65.4488754272461, -2227.5791015625]\n",
      "\n",
      "Instance 2133 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 3: [0.16565468907356262, 1.0928062200546265, -1.2412376403808594, 0.7928088307380676, -2.8227076530456543]\n",
      "Grand sum of 1507 tensor sets is: [498.4288024902344, 1235.4661865234375, -361.1027526855469, 66.24168395996094, -2230.40185546875]\n",
      "\n",
      "Instance 2134 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 18: [-0.6697186827659607, -0.40208446979522705, -1.0951238870620728, -0.043754324316978455, -3.164599895477295]\n",
      "Grand sum of 1508 tensor sets is: [497.75909423828125, 1235.0640869140625, -362.1978759765625, 66.19792938232422, -2233.56640625]\n",
      "\n",
      "Instance 2135 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 18: [0.5254415273666382, 1.2009477615356445, -0.24099573493003845, 0.7186393141746521, -3.1880369186401367]\n",
      "Grand sum of 1509 tensor sets is: [498.2845458984375, 1236.2650146484375, -362.4388732910156, 66.91656494140625, -2236.75439453125]\n",
      "\n",
      "Instance 2136 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 20: [-0.2862831652164459, 2.7898988723754883, -0.4999125599861145, 0.21247658133506775, -2.473128318786621]\n",
      "Grand sum of 1510 tensor sets is: [497.9982604980469, 1239.054931640625, -362.93878173828125, 67.12904357910156, -2239.2275390625]\n",
      "\n",
      "Instance 2137 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2138 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 3: [0.32994553446769714, 0.18613892793655396, -0.5860008597373962, 0.7834742069244385, 1.6237897872924805]\n",
      "Grand sum of 1511 tensor sets is: [498.3282165527344, 1239.2410888671875, -363.5247802734375, 67.91252136230469, -2237.603759765625]\n",
      "\n",
      "Instance 2139 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 17: [-0.02005722001194954, 1.3152607679367065, -1.4290310144424438, 0.4147929251194, -4.315427780151367]\n",
      "Grand sum of 1512 tensor sets is: [498.30816650390625, 1240.556396484375, -364.95379638671875, 68.32731628417969, -2241.919189453125]\n",
      "\n",
      "Instance 2140 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 14: [-0.18250900506973267, 0.47290676832199097, -0.9022859334945679, -0.08502942323684692, -1.3994040489196777]\n",
      "Grand sum of 1513 tensor sets is: [498.12567138671875, 1241.029296875, -365.8560791015625, 68.2422866821289, -2243.318603515625]\n",
      "\n",
      "Instance 2141 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 7: [0.31466546654701233, 2.745884418487549, -1.275192379951477, -1.161094307899475, -3.676356315612793]\n",
      "Grand sum of 1514 tensor sets is: [498.4403381347656, 1243.775146484375, -367.13128662109375, 67.08119201660156, -2246.994873046875]\n",
      "\n",
      "Instance 2142 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 7: [0.5530528426170349, 1.0379085540771484, -1.1313152313232422, 0.3623736500740051, -2.8000447750091553]\n",
      "Grand sum of 1515 tensor sets is: [498.9933776855469, 1244.8131103515625, -368.2626037597656, 67.44356536865234, -2249.794921875]\n",
      "\n",
      "Instance 2143 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 16: [-0.22236871719360352, 1.6409229040145874, -0.2629823088645935, -0.46249115467071533, -3.5786025524139404]\n",
      "Grand sum of 1516 tensor sets is: [498.77099609375, 1246.4539794921875, -368.52557373046875, 66.98107147216797, -2253.37353515625]\n",
      "\n",
      "Instance 2144 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 23: [0.25613653659820557, 1.7356373071670532, -0.0017609372735023499, 1.8842601776123047, -1.2285208702087402]\n",
      "Grand sum of 1517 tensor sets is: [499.0271301269531, 1248.1895751953125, -368.52734375, 68.8653335571289, -2254.60205078125]\n",
      "\n",
      "Instance 2145 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 12: [-0.2123539000749588, 1.661030888557434, 1.3314051628112793, -0.20076999068260193, -1.49067223072052]\n",
      "Grand sum of 1518 tensor sets is: [498.8147888183594, 1249.8505859375, -367.1959533691406, 68.66456604003906, -2256.0927734375]\n",
      "\n",
      "Instance 2146 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 1519 tensor sets is: [499.35870361328125, 1249.2156982421875, -367.8454895019531, 67.52234649658203, -2254.971435546875]\n",
      "\n",
      "Instance 2147 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 11: [-0.5490189790725708, 2.0408101081848145, -0.7467520236968994, -0.41066399216651917, -2.4551258087158203]\n",
      "Grand sum of 1520 tensor sets is: [498.8096923828125, 1251.2564697265625, -368.5922546386719, 67.11167907714844, -2257.426513671875]\n",
      "\n",
      "Instance 2148 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 26: [0.4612100422382355, 2.2966549396514893, 0.3864597678184509, 1.6454845666885376, -1.5285221338272095]\n",
      "Grand sum of 1521 tensor sets is: [499.2709045410156, 1253.5531005859375, -368.2057800292969, 68.75716400146484, -2258.955078125]\n",
      "\n",
      "Instance 2149 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 10: [0.5306530594825745, 0.3977932333946228, -0.7976622581481934, -1.348623514175415, -2.0144762992858887]\n",
      "Grand sum of 1522 tensor sets is: [499.8015441894531, 1253.950927734375, -369.0034484863281, 67.40853881835938, -2260.969482421875]\n",
      "\n",
      "Instance 2150 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([148, 13, 768])\n",
      "Shape of summed layers is: 148 x 768\n",
      "forever at index 35: [1.2565830945968628, 0.28820666670799255, 0.9543382525444031, 0.6283606886863708, -2.3538753986358643]\n",
      "Grand sum of 1523 tensor sets is: [501.0581359863281, 1254.2391357421875, -368.0491027832031, 68.03689575195312, -2263.3232421875]\n",
      "\n",
      "Instance 2151 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2152 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 6: [-0.11940082907676697, -0.6697089076042175, -0.41439202427864075, 2.2658207416534424, -2.07173752784729]\n",
      "Grand sum of 1524 tensor sets is: [500.938720703125, 1253.5694580078125, -368.4635009765625, 70.30271911621094, -2265.39501953125]\n",
      "\n",
      "Instance 2153 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2154 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 6: [0.4861705005168915, 0.4429602026939392, -0.9512537717819214, -1.3777185678482056, -4.834580898284912]\n",
      "Grand sum of 1525 tensor sets is: [501.4248962402344, 1254.012451171875, -369.4147644042969, 68.92500305175781, -2270.2294921875]\n",
      "\n",
      "Instance 2155 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 16: [0.6446890830993652, -0.2635232210159302, 0.5160491466522217, -1.6629115343093872, -3.6011862754821777]\n",
      "Grand sum of 1526 tensor sets is: [502.069580078125, 1253.7489013671875, -368.8987121582031, 67.26209259033203, -2273.83056640625]\n",
      "\n",
      "Instance 2156 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [59]\n",
      "Size of token embeddings is torch.Size([119, 13, 768])\n",
      "Shape of summed layers is: 119 x 768\n",
      "forever at index 59: [-0.1983756572008133, 0.09491465985774994, 0.4357684552669525, -0.010033950209617615, 1.5795191526412964]\n",
      "Grand sum of 1527 tensor sets is: [501.8712158203125, 1253.8438720703125, -368.46295166015625, 67.25205993652344, -2272.2509765625]\n",
      "\n",
      "Instance 2157 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [166]\n",
      "Size of token embeddings is torch.Size([220, 13, 768])\n",
      "Shape of summed layers is: 220 x 768\n",
      "forever at index 166: [0.5145673155784607, 0.5686100721359253, -0.7473867535591125, -1.649482011795044, -0.11683367192745209]\n",
      "Grand sum of 1528 tensor sets is: [502.3857727050781, 1254.4124755859375, -369.2103271484375, 65.60257720947266, -2272.367919921875]\n",
      "\n",
      "Instance 2158 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2159 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 20: [0.9474145174026489, 2.839916229248047, 0.9322200417518616, 2.646975040435791, -0.14312544465065002]\n",
      "Grand sum of 1529 tensor sets is: [503.33319091796875, 1257.25244140625, -368.2781066894531, 68.24954986572266, -2272.510986328125]\n",
      "\n",
      "Instance 2160 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2161 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 4: [-0.4510653614997864, 1.36069917678833, 0.07783521711826324, 0.3840061128139496, 0.05771535634994507]\n",
      "Grand sum of 1530 tensor sets is: [502.8821105957031, 1258.6131591796875, -368.20025634765625, 68.63355255126953, -2272.453369140625]\n",
      "\n",
      "Instance 2162 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 26: [1.3161708116531372, 0.7910457253456116, 0.03404364734888077, -0.2547284960746765, -6.300019264221191]\n",
      "Grand sum of 1531 tensor sets is: [504.1982727050781, 1259.4041748046875, -368.16619873046875, 68.37882232666016, -2278.75341796875]\n",
      "\n",
      "Instance 2163 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 7: [1.298396348953247, 0.7237108945846558, -0.6683068871498108, -1.9852122068405151, 0.9474582076072693]\n",
      "Grand sum of 1532 tensor sets is: [505.4966735839844, 1260.1279296875, -368.8345031738281, 66.39360809326172, -2277.805908203125]\n",
      "\n",
      "Instance 2164 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 21: [0.8141884803771973, 1.5951849222183228, -0.7927660346031189, -0.4851362705230713, -3.9368739128112793]\n",
      "Grand sum of 1533 tensor sets is: [506.31085205078125, 1261.72314453125, -369.62725830078125, 65.9084701538086, -2281.74267578125]\n",
      "\n",
      "Instance 2165 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 5: [0.744665265083313, -0.45003652572631836, 0.36066246032714844, 0.8296575546264648, 1.1457334756851196]\n",
      "Grand sum of 1534 tensor sets is: [507.0555114746094, 1261.2730712890625, -369.2666015625, 66.73812866210938, -2280.596923828125]\n",
      "\n",
      "Instance 2166 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2167 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 12: [-0.1362672597169876, 1.3054125308990479, 0.7543923258781433, -0.08231033384799957, -1.3851341009140015]\n",
      "Grand sum of 1535 tensor sets is: [506.91925048828125, 1262.5784912109375, -368.51220703125, 66.65581512451172, -2281.982177734375]\n",
      "\n",
      "Instance 2168 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [0.9832466840744019, 0.19009733200073242, -0.32321181893348694, 0.42473360896110535, 0.5483078360557556]\n",
      "Grand sum of 1536 tensor sets is: [507.9024963378906, 1262.7685546875, -368.8354187011719, 67.08055114746094, -2281.433837890625]\n",
      "\n",
      "Instance 2169 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 1537 tensor sets is: [508.5972900390625, 1263.90185546875, -367.63226318359375, 65.81118774414062, -2282.323974609375]\n",
      "\n",
      "Instance 2170 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 18: [0.2604178786277771, 1.165994644165039, -0.8623663187026978, 1.2040090560913086, -4.899868488311768]\n",
      "Grand sum of 1538 tensor sets is: [508.8576965332031, 1265.06787109375, -368.49462890625, 67.01519775390625, -2287.223876953125]\n",
      "\n",
      "Instance 2171 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 1539 tensor sets is: [509.2533874511719, 1266.0765380859375, -368.564208984375, 66.33163452148438, -2285.927490234375]\n",
      "\n",
      "Instance 2172 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [207]\n",
      "Size of token embeddings is torch.Size([214, 13, 768])\n",
      "Shape of summed layers is: 214 x 768\n",
      "forever at index 207: [0.19219869375228882, 0.8073763847351074, -0.2543439269065857, -1.9308363199234009, -0.6630887985229492]\n",
      "Grand sum of 1540 tensor sets is: [509.4455871582031, 1266.8839111328125, -368.81854248046875, 64.40079498291016, -2286.590576171875]\n",
      "\n",
      "Instance 2173 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 3: [1.1526155471801758, -0.35349705815315247, -0.598080039024353, -0.6890103816986084, 1.427318811416626]\n",
      "Grand sum of 1541 tensor sets is: [510.59820556640625, 1266.5303955078125, -369.4166259765625, 63.71178436279297, -2285.163330078125]\n",
      "\n",
      "Instance 2174 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 40: [0.34789401292800903, 0.6223324537277222, -0.025321558117866516, -0.18347570300102234, 0.32842326164245605]\n",
      "Grand sum of 1542 tensor sets is: [510.94610595703125, 1267.1527099609375, -369.44195556640625, 63.5283088684082, -2284.8349609375]\n",
      "\n",
      "Instance 2175 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2176 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 11: [-0.17476999759674072, -0.5574918985366821, 0.17218592762947083, 1.3741354942321777, 3.7136359214782715]\n",
      "Grand sum of 1543 tensor sets is: [510.7713317871094, 1266.59521484375, -369.269775390625, 64.9024429321289, -2281.121337890625]\n",
      "\n",
      "Instance 2177 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 1544 tensor sets is: [510.9093933105469, 1267.2493896484375, -369.7103576660156, 64.6051254272461, -2283.9716796875]\n",
      "\n",
      "Instance 2178 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2179 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.3464672267436981, 1.030474066734314, -0.4848339557647705, -1.0788788795471191, -1.025876760482788]\n",
      "Grand sum of 1545 tensor sets is: [511.255859375, 1268.2799072265625, -370.1951904296875, 63.5262451171875, -2284.99755859375]\n",
      "\n",
      "Instance 2180 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [1.6994589567184448, -0.21792569756507874, 0.2326381653547287, -0.911447286605835, -4.0184526443481445]\n",
      "Grand sum of 1546 tensor sets is: [512.955322265625, 1268.06201171875, -369.9625549316406, 62.61479949951172, -2289.01611328125]\n",
      "\n",
      "Instance 2181 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2182 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2183 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 9: [0.6288964152336121, 2.667804479598999, -0.11283165216445923, 2.0305159091949463, -2.095258951187134]\n",
      "Grand sum of 1547 tensor sets is: [513.584228515625, 1270.7298583984375, -370.07537841796875, 64.64531707763672, -2291.111328125]\n",
      "\n",
      "Instance 2184 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 9: [1.1694749593734741, 0.20091408491134644, -0.5279715657234192, 0.28439900279045105, -0.5493965744972229]\n",
      "Grand sum of 1548 tensor sets is: [514.7537231445312, 1270.9307861328125, -370.6033630371094, 64.92971801757812, -2291.66064453125]\n",
      "\n",
      "Instance 2185 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 8: [0.37268006801605225, 1.6455166339874268, -0.32577067613601685, 0.932145357131958, -5.992197036743164]\n",
      "Grand sum of 1549 tensor sets is: [515.1264038085938, 1272.5762939453125, -370.92913818359375, 65.86186218261719, -2297.65283203125]\n",
      "\n",
      "Instance 2186 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 1550 tensor sets is: [514.490966796875, 1274.298095703125, -370.1966247558594, 67.16764068603516, -2297.82958984375]\n",
      "\n",
      "Instance 2187 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 15: [0.7327563762664795, 2.2250590324401855, -0.5446501970291138, -0.4336801767349243, -5.1940412521362305]\n",
      "Grand sum of 1551 tensor sets is: [515.2236938476562, 1276.523193359375, -370.74127197265625, 66.73396301269531, -2303.023681640625]\n",
      "\n",
      "Instance 2188 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([121, 13, 768])\n",
      "Shape of summed layers is: 121 x 768\n",
      "forever at index 46: [0.41324031352996826, 1.1886684894561768, -0.8633221387863159, -0.13346849381923676, -1.5512607097625732]\n",
      "Grand sum of 1552 tensor sets is: [515.636962890625, 1277.7119140625, -371.6045837402344, 66.60049438476562, -2304.574951171875]\n",
      "\n",
      "Instance 2189 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2190 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "forever at index 28: [0.2257017344236374, -0.30034658312797546, 0.4386584460735321, -1.103806495666504, 0.34561118483543396]\n",
      "Grand sum of 1553 tensor sets is: [515.8626708984375, 1277.41162109375, -371.1659240722656, 65.49668884277344, -2304.229248046875]\n",
      "\n",
      "Instance 2191 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 1554 tensor sets is: [516.4065551757812, 1276.7767333984375, -371.8154602050781, 64.3544692993164, -2303.10791015625]\n",
      "\n",
      "Instance 2192 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 19: [0.8666878938674927, 1.6490929126739502, -0.808843731880188, 0.03622845560312271, -3.7226786613464355]\n",
      "Grand sum of 1555 tensor sets is: [517.2732543945312, 1278.42578125, -372.6242980957031, 64.39070129394531, -2306.83056640625]\n",
      "\n",
      "Instance 2193 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "forever at index 34: [-0.2403811812400818, 0.628974199295044, -0.4796023964881897, -2.744699001312256, -0.174118772149086]\n",
      "Grand sum of 1556 tensor sets is: [517.0328979492188, 1279.0548095703125, -373.1039123535156, 61.64600372314453, -2307.004638671875]\n",
      "\n",
      "Instance 2194 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [78]\n",
      "Size of token embeddings is torch.Size([269, 13, 768])\n",
      "Shape of summed layers is: 269 x 768\n",
      "forever at index 78: [-0.8362959623336792, 1.3877527713775635, -0.461038202047348, 1.362511396408081, 3.3866772651672363]\n",
      "Grand sum of 1557 tensor sets is: [516.1965942382812, 1280.4425048828125, -373.56494140625, 63.008514404296875, -2303.617919921875]\n",
      "\n",
      "Instance 2195 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 24: [1.0735924243927002, 0.7960044145584106, -0.37208420038223267, -1.7589421272277832, -6.567999839782715]\n",
      "Grand sum of 1558 tensor sets is: [517.2702026367188, 1281.238525390625, -373.93701171875, 61.24957275390625, -2310.18603515625]\n",
      "\n",
      "Instance 2196 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 11: [0.5462900996208191, 0.9441671967506409, 0.007722698152065277, -1.065494179725647, 0.9867439270019531]\n",
      "Grand sum of 1559 tensor sets is: [517.8164672851562, 1282.1827392578125, -373.9292907714844, 60.184078216552734, -2309.19921875]\n",
      "\n",
      "Instance 2197 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 7: [1.4188060760498047, 0.6245797872543335, -0.23938952386379242, -0.5617891550064087, -1.201753854751587]\n",
      "Grand sum of 1560 tensor sets is: [519.2352905273438, 1282.807373046875, -374.1686706542969, 59.62228775024414, -2310.40087890625]\n",
      "\n",
      "Instance 2198 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 12: [0.19806820154190063, -1.5504539012908936, -2.282001256942749, -0.4363165497779846, 1.770513892173767]\n",
      "Grand sum of 1561 tensor sets is: [519.433349609375, 1281.2569580078125, -376.45068359375, 59.185970306396484, -2308.63037109375]\n",
      "\n",
      "Instance 2199 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2200 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 20: [0.5913089513778687, 1.1588599681854248, -0.258926659822464, -1.144754409790039, -1.6046878099441528]\n",
      "Grand sum of 1562 tensor sets is: [520.024658203125, 1282.415771484375, -376.7096252441406, 58.04121398925781, -2310.235107421875]\n",
      "\n",
      "Instance 2201 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2202 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 1563 tensor sets is: [519.2899169921875, 1282.8206787109375, -376.31793212890625, 57.68415451049805, -2314.7958984375]\n",
      "\n",
      "Instance 2203 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2204 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 34: [0.9863830804824829, 0.37368401885032654, -0.9962220191955566, -1.3218932151794434, -2.493481159210205]\n",
      "Grand sum of 1564 tensor sets is: [520.2763061523438, 1283.1943359375, -377.31414794921875, 56.36226272583008, -2317.289306640625]\n",
      "\n",
      "Instance 2205 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 13: [1.0306488275527954, -0.006747938692569733, 0.1850205510854721, 0.4763759672641754, -1.3853257894515991]\n",
      "Grand sum of 1565 tensor sets is: [521.3069458007812, 1283.1876220703125, -377.1291198730469, 56.83863830566406, -2318.674560546875]\n",
      "\n",
      "Instance 2206 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 28: [0.5659814476966858, 2.6657001972198486, 0.4558942914009094, 1.2157108783721924, -1.1450613737106323]\n",
      "Grand sum of 1566 tensor sets is: [521.8729248046875, 1285.853271484375, -376.6732177734375, 58.05434799194336, -2319.819580078125]\n",
      "\n",
      "Instance 2207 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 3: [0.04570348188281059, 0.9863324165344238, -0.21525335311889648, -0.8483102917671204, -1.6665058135986328]\n",
      "Grand sum of 1567 tensor sets is: [521.9186401367188, 1286.839599609375, -376.8884582519531, 57.20603942871094, -2321.486083984375]\n",
      "\n",
      "Instance 2208 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 7: [0.43185460567474365, 1.854201316833496, -0.3217552900314331, 0.156417578458786, -0.6675945520401001]\n",
      "Grand sum of 1568 tensor sets is: [522.3505249023438, 1288.69384765625, -377.210205078125, 57.362457275390625, -2322.153564453125]\n",
      "\n",
      "Instance 2209 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "forever at index 45: [-0.002792656421661377, 0.10096898674964905, -1.0778956413269043, -0.4323015511035919, -4.247395992279053]\n",
      "Grand sum of 1569 tensor sets is: [522.3477172851562, 1288.7947998046875, -378.2880859375, 56.93015670776367, -2326.40087890625]\n",
      "\n",
      "Instance 2210 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 36: [-0.050302959978580475, 0.42136991024017334, -0.15843471884727478, -0.33366659283638, -0.636167049407959]\n",
      "Grand sum of 1570 tensor sets is: [522.2974243164062, 1289.2161865234375, -378.446533203125, 56.59648895263672, -2327.037109375]\n",
      "\n",
      "Instance 2211 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 9: [0.00775802880525589, -0.4664250314235687, -1.2065333127975464, 0.47917211055755615, -4.387584686279297]\n",
      "Grand sum of 1571 tensor sets is: [522.30517578125, 1288.749755859375, -379.653076171875, 57.075660705566406, -2331.4248046875]\n",
      "\n",
      "Instance 2212 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2213 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 13: [0.01943383552134037, 0.2157527059316635, 0.496049165725708, 0.008870452642440796, -0.749788761138916]\n",
      "Grand sum of 1572 tensor sets is: [522.3245849609375, 1288.9654541015625, -379.1570129394531, 57.084529876708984, -2332.174560546875]\n",
      "\n",
      "Instance 2214 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2215 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 9: [-0.09352827817201614, 1.6670969724655151, 0.03003399819135666, 2.511651039123535, 0.5485068559646606]\n",
      "Grand sum of 1573 tensor sets is: [522.2310791015625, 1290.632568359375, -379.1269836425781, 59.5961799621582, -2331.6259765625]\n",
      "\n",
      "Instance 2216 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 6: [-0.785726010799408, 1.2101291418075562, -0.08649443835020065, 2.3100814819335938, -0.05883467197418213]\n",
      "Grand sum of 1574 tensor sets is: [521.4453735351562, 1291.8426513671875, -379.2134704589844, 61.9062614440918, -2331.684814453125]\n",
      "\n",
      "Instance 2217 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2218 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 15: [-0.6729450225830078, -0.15572404861450195, -0.38405486941337585, -0.2244773507118225, -2.2405636310577393]\n",
      "Grand sum of 1575 tensor sets is: [520.7723999023438, 1291.6868896484375, -379.5975341796875, 61.681785583496094, -2333.92529296875]\n",
      "\n",
      "Instance 2219 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 17: [0.33422285318374634, -0.8155922293663025, -0.537778913974762, -0.07142467796802521, -3.95515775680542]\n",
      "Grand sum of 1576 tensor sets is: [521.1066284179688, 1290.871337890625, -380.13531494140625, 61.61035919189453, -2337.88037109375]\n",
      "\n",
      "Instance 2220 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2221 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2222 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 12: [0.24443423748016357, 1.426192045211792, -0.15776565670967102, 0.023366376757621765, 0.8883479833602905]\n",
      "Grand sum of 1577 tensor sets is: [521.35107421875, 1292.2974853515625, -380.2930908203125, 61.633724212646484, -2336.991943359375]\n",
      "\n",
      "Instance 2223 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 5: [1.2257450819015503, 0.7711949348449707, -0.6281892657279968, 0.12119488418102264, -4.068622589111328]\n",
      "Grand sum of 1578 tensor sets is: [522.5768432617188, 1293.0687255859375, -380.9212951660156, 61.754920959472656, -2341.060546875]\n",
      "\n",
      "Instance 2224 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2225 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 14: [0.1075509637594223, -0.07534673810005188, 0.15395355224609375, 0.1763014793395996, -3.741509437561035]\n",
      "Grand sum of 1579 tensor sets is: [522.6843872070312, 1292.993408203125, -380.767333984375, 61.93122100830078, -2344.802001953125]\n",
      "\n",
      "Instance 2226 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 4: [0.2655646800994873, 1.3385205268859863, -0.6681448817253113, 0.9340962767601013, 1.4512591361999512]\n",
      "Grand sum of 1580 tensor sets is: [522.949951171875, 1294.3319091796875, -381.43548583984375, 62.865318298339844, -2343.350830078125]\n",
      "\n",
      "Instance 2227 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 3: [1.2232654094696045, 0.0008663088083267212, -1.3222897052764893, -0.19839344918727875, -2.0205676555633545]\n",
      "Grand sum of 1581 tensor sets is: [524.1732177734375, 1294.332763671875, -382.7577819824219, 62.66692352294922, -2345.371337890625]\n",
      "\n",
      "Instance 2228 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2229 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 12: [0.5575101971626282, -0.5566350221633911, -0.1650124490261078, 1.0452404022216797, 0.3103427290916443]\n",
      "Grand sum of 1582 tensor sets is: [524.730712890625, 1293.776123046875, -382.92279052734375, 63.71216583251953, -2345.06103515625]\n",
      "\n",
      "Instance 2230 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "forever at index 9: [0.22057148814201355, 0.0323236882686615, -0.1333530992269516, 2.0842604637145996, 0.17189154028892517]\n",
      "Grand sum of 1583 tensor sets is: [524.9512939453125, 1293.8084716796875, -383.05615234375, 65.79642486572266, -2344.88916015625]\n",
      "\n",
      "Instance 2231 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 18: [-0.2552477717399597, 0.8495585918426514, -1.1730486154556274, 2.1181576251983643, -0.262847363948822]\n",
      "Grand sum of 1584 tensor sets is: [524.696044921875, 1294.6580810546875, -384.22918701171875, 67.91458129882812, -2345.152099609375]\n",
      "\n",
      "Instance 2232 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2233 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2234 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 20: [0.6016704440116882, -0.12500640749931335, -0.41209590435028076, 0.29743170738220215, -2.734790802001953]\n",
      "Grand sum of 1585 tensor sets is: [525.2977294921875, 1294.5330810546875, -384.64129638671875, 68.2120132446289, -2347.886962890625]\n",
      "\n",
      "Instance 2235 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2236 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 3: [1.212202548980713, 0.6425241231918335, -0.6932804584503174, -0.2644960880279541, 0.26515984535217285]\n",
      "Grand sum of 1586 tensor sets is: [526.5099487304688, 1295.1756591796875, -385.3345642089844, 67.94751739501953, -2347.621826171875]\n",
      "\n",
      "Instance 2237 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2238 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 17: [0.29927998781204224, 1.086247205734253, 1.4000462293624878, 1.1797261238098145, -6.230565547943115]\n",
      "Grand sum of 1587 tensor sets is: [526.8092041015625, 1296.261962890625, -383.93450927734375, 69.12724304199219, -2353.852294921875]\n",
      "\n",
      "Instance 2239 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2, 26]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 2: [-0.05583298206329346, -2.450472593307495, 1.0485458374023438, -0.8533215522766113, 0.004646044224500656]\n",
      "forever at index 26: [-0.607017993927002, -2.455716609954834, 0.732523500919342, -2.302199363708496, -1.8982460498809814]\n",
      "Grand sum of 1588 tensor sets is: [526.477783203125, 1293.808837890625, -383.0439758300781, 67.54948425292969, -2354.799072265625]\n",
      "\n",
      "Instance 2240 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 8: [0.27112728357315063, 2.2254085540771484, 0.03218047320842743, 0.2721783518791199, -2.89788818359375]\n",
      "Grand sum of 1589 tensor sets is: [526.7489013671875, 1296.0343017578125, -383.0118103027344, 67.82166290283203, -2357.697021484375]\n",
      "\n",
      "Instance 2241 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 12: [0.7967674136161804, 3.093390464782715, -0.12921175360679626, -0.6532351970672607, 1.4192748069763184]\n",
      "Grand sum of 1590 tensor sets is: [527.545654296875, 1299.127685546875, -383.1410217285156, 67.16842651367188, -2356.27783203125]\n",
      "\n",
      "Instance 2242 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2243 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2244 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 9: [-0.5344382524490356, 1.0036393404006958, 0.9070356488227844, 0.601829469203949, -4.288227081298828]\n",
      "Grand sum of 1591 tensor sets is: [527.01123046875, 1300.13134765625, -382.2339782714844, 67.77025604248047, -2360.566162109375]\n",
      "\n",
      "Instance 2245 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 8: [0.2729629576206207, 1.8921456336975098, 0.4650682210922241, 1.0739167928695679, -0.9087559580802917]\n",
      "Grand sum of 1592 tensor sets is: [527.2841796875, 1302.0234375, -381.7689208984375, 68.84416961669922, -2361.474853515625]\n",
      "\n",
      "Instance 2246 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2247 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 26: [0.624078631401062, 0.5144463777542114, -0.10020378232002258, 0.05382432043552399, 0.8459208607673645]\n",
      "Grand sum of 1593 tensor sets is: [527.9082641601562, 1302.537841796875, -381.8691101074219, 68.89799499511719, -2360.62890625]\n",
      "\n",
      "Instance 2248 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 21: [0.3565308749675751, 0.13220976293087006, -1.0269287824630737, 2.582505464553833, -5.253737449645996]\n",
      "Grand sum of 1594 tensor sets is: [528.2647705078125, 1302.6700439453125, -382.8960266113281, 71.48049926757812, -2365.882568359375]\n",
      "\n",
      "Instance 2249 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 8: [0.7375310659408569, -0.21126458048820496, -0.4835295081138611, -1.9114234447479248, 0.16114282608032227]\n",
      "Grand sum of 1595 tensor sets is: [529.0023193359375, 1302.458740234375, -383.3795471191406, 69.56907653808594, -2365.721435546875]\n",
      "\n",
      "Instance 2250 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 4: [0.03114640712738037, 0.5670510530471802, -0.06291399896144867, -1.1785025596618652, -0.7667499780654907]\n",
      "Grand sum of 1596 tensor sets is: [529.033447265625, 1303.0257568359375, -383.4424743652344, 68.39057159423828, -2366.48828125]\n",
      "\n",
      "Instance 2251 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 10: [0.5037148594856262, 1.282475233078003, -1.7165502309799194, 0.1755542904138565, -3.3468384742736816]\n",
      "Grand sum of 1597 tensor sets is: [529.5371704101562, 1304.3082275390625, -385.1590270996094, 68.56612396240234, -2369.835205078125]\n",
      "\n",
      "Instance 2252 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 10: [0.2967793941497803, 2.2937967777252197, -0.09183014184236526, 3.471863031387329, 1.7914891242980957]\n",
      "Grand sum of 1598 tensor sets is: [529.8339233398438, 1306.60205078125, -385.2508544921875, 72.0379867553711, -2368.043701171875]\n",
      "\n",
      "Instance 2253 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 46: [1.1710206270217896, 0.07187464833259583, -0.4213065505027771, -3.149962902069092, -1.5643279552459717]\n",
      "Grand sum of 1599 tensor sets is: [531.0049438476562, 1306.6739501953125, -385.6721496582031, 68.88802337646484, -2369.60791015625]\n",
      "\n",
      "Instance 2254 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 18: [-0.7294908761978149, 2.864431858062744, 0.37590494751930237, 0.5331430435180664, -1.1344996690750122]\n",
      "Grand sum of 1600 tensor sets is: [530.2754516601562, 1309.538330078125, -385.2962341308594, 69.4211654663086, -2370.742431640625]\n",
      "\n",
      "Instance 2255 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 29: [0.14236760139465332, 2.899231433868408, 1.0018742084503174, -0.9774129390716553, -0.25950098037719727]\n",
      "Grand sum of 1601 tensor sets is: [530.4178466796875, 1312.4376220703125, -384.29437255859375, 68.44375610351562, -2371.001953125]\n",
      "\n",
      "Instance 2256 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 21: [0.24626079201698303, 2.3013858795166016, -0.029095901176333427, -1.2818257808685303, 2.092871904373169]\n",
      "Grand sum of 1602 tensor sets is: [530.6641235351562, 1314.739013671875, -384.3234558105469, 67.16193389892578, -2368.9091796875]\n",
      "\n",
      "Instance 2257 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 1603 tensor sets is: [529.9293823242188, 1315.1439208984375, -383.9317626953125, 66.80487060546875, -2373.469970703125]\n",
      "\n",
      "Instance 2258 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 16: [0.8183431625366211, 2.033780097961426, 0.3634833097457886, 0.1977071315050125, -4.468682289123535]\n",
      "Grand sum of 1604 tensor sets is: [530.7477416992188, 1317.177734375, -383.5682678222656, 67.00257873535156, -2377.938720703125]\n",
      "\n",
      "Instance 2259 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 11: [-0.3108530044555664, -0.4150739312171936, 0.2029424011707306, -0.798915684223175, -0.9822593331336975]\n",
      "Grand sum of 1605 tensor sets is: [530.4368896484375, 1316.7626953125, -383.3653259277344, 66.20366668701172, -2378.9208984375]\n",
      "\n",
      "Instance 2260 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 26: [1.3161708116531372, 0.7910457253456116, 0.03404364734888077, -0.2547284960746765, -6.300019264221191]\n",
      "Grand sum of 1606 tensor sets is: [531.7530517578125, 1317.5537109375, -383.3312683105469, 65.94893646240234, -2385.220947265625]\n",
      "\n",
      "Instance 2261 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2262 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2263 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 4: [1.3859190940856934, -0.6459774971008301, 0.8364301323890686, -0.17226360738277435, -0.439924955368042]\n",
      "Grand sum of 1607 tensor sets is: [533.1389770507812, 1316.90771484375, -382.4948425292969, 65.77667236328125, -2385.660888671875]\n",
      "\n",
      "Instance 2264 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2265 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 9: [1.081651210784912, 0.4458416700363159, -0.3356822729110718, 0.2362014651298523, -4.7088236808776855]\n",
      "Grand sum of 1608 tensor sets is: [534.2206420898438, 1317.353515625, -382.8305358886719, 66.01287078857422, -2390.36962890625]\n",
      "\n",
      "Instance 2266 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2267 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 20: [0.28634506464004517, 1.7352755069732666, 0.39188024401664734, 1.932461142539978, -0.7038207650184631]\n",
      "Grand sum of 1609 tensor sets is: [534.5069580078125, 1319.0887451171875, -382.43865966796875, 67.9453353881836, -2391.073486328125]\n",
      "\n",
      "Instance 2268 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2269 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 19: [0.5340545177459717, 2.9708878993988037, 0.21614599227905273, -1.419804573059082, -3.9419522285461426]\n",
      "Grand sum of 1610 tensor sets is: [535.041015625, 1322.0596923828125, -382.2225036621094, 66.52552795410156, -2395.015380859375]\n",
      "\n",
      "Instance 2270 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 1611 tensor sets is: [535.1790771484375, 1322.7138671875, -382.6630859375, 66.22821044921875, -2397.86572265625]\n",
      "\n",
      "Instance 2271 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 20: [0.6300421953201294, 1.8560326099395752, 0.22185774147510529, 0.3324981927871704, -0.6448638439178467]\n",
      "Grand sum of 1612 tensor sets is: [535.8091430664062, 1324.5699462890625, -382.44122314453125, 66.56070709228516, -2398.510498046875]\n",
      "\n",
      "Instance 2272 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 12: [-0.07138007879257202, 0.49957647919654846, -0.3032335638999939, 0.583499014377594, -5.019226551055908]\n",
      "Grand sum of 1613 tensor sets is: [535.73779296875, 1325.069580078125, -382.74444580078125, 67.14420318603516, -2403.52978515625]\n",
      "\n",
      "Instance 2273 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2274 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2275 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [0.5091382265090942, 0.9314974546432495, -0.584216833114624, -0.4129290282726288, -0.6319780349731445]\n",
      "Grand sum of 1614 tensor sets is: [536.2469482421875, 1326.0010986328125, -383.32867431640625, 66.73127746582031, -2404.161865234375]\n",
      "\n",
      "Instance 2276 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 5: [-0.014664754271507263, -1.2344988584518433, 0.22723159193992615, 2.0939176082611084, -2.6420178413391113]\n",
      "Grand sum of 1615 tensor sets is: [536.2322998046875, 1324.7666015625, -383.1014404296875, 68.8251953125, -2406.803955078125]\n",
      "\n",
      "Instance 2277 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2278 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 9: [1.7284691333770752, 0.3919599950313568, 0.053386494517326355, -0.5530070066452026, -2.3724308013916016]\n",
      "Grand sum of 1616 tensor sets is: [537.9607543945312, 1325.1585693359375, -383.0480651855469, 68.27218627929688, -2409.17626953125]\n",
      "\n",
      "Instance 2279 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 9: [0.3343687355518341, -0.23698492348194122, -0.4109577536582947, 0.7118002772331238, -1.3364067077636719]\n",
      "Grand sum of 1617 tensor sets is: [538.2951049804688, 1324.921630859375, -383.4590148925781, 68.9839859008789, -2410.5126953125]\n",
      "\n",
      "Instance 2280 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 14: [1.5132617950439453, -0.3562914729118347, 0.8674231767654419, -0.27675992250442505, 0.23524819314479828]\n",
      "Grand sum of 1618 tensor sets is: [539.808349609375, 1324.5653076171875, -382.5915832519531, 68.70722961425781, -2410.27734375]\n",
      "\n",
      "Instance 2281 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2282 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 8: [-0.21621108055114746, 1.8166568279266357, -0.22995060682296753, -1.5340235233306885, -1.1060597896575928]\n",
      "Grand sum of 1619 tensor sets is: [539.5921630859375, 1326.3819580078125, -382.821533203125, 67.17320251464844, -2411.38330078125]\n",
      "\n",
      "Instance 2283 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 14: [1.3130872249603271, -0.8553817272186279, -0.006187885999679565, 0.6111485958099365, -4.508805751800537]\n",
      "Grand sum of 1620 tensor sets is: [540.9052734375, 1325.526611328125, -382.8277282714844, 67.78434753417969, -2415.89208984375]\n",
      "\n",
      "Instance 2284 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2285 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2286 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 35: [0.35695403814315796, 0.6343450546264648, 0.14175790548324585, -2.1033759117126465, -2.9931697845458984]\n",
      "Grand sum of 1621 tensor sets is: [541.26220703125, 1326.1610107421875, -382.68597412109375, 65.68096923828125, -2418.88525390625]\n",
      "\n",
      "Instance 2287 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 22: [-0.4180574119091034, 1.09161376953125, -0.5062294602394104, -1.4953339099884033, -0.68163001537323]\n",
      "Grand sum of 1622 tensor sets is: [540.8441772460938, 1327.252685546875, -383.19219970703125, 64.18563842773438, -2419.56689453125]\n",
      "\n",
      "Instance 2288 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 1623 tensor sets is: [541.6828002929688, 1327.608642578125, -383.5938720703125, 64.69926452636719, -2417.02783203125]\n",
      "\n",
      "Instance 2289 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 22: [0.3079040050506592, 1.3369406461715698, 0.2242584526538849, -0.08212898671627045, 0.3777461647987366]\n",
      "Grand sum of 1624 tensor sets is: [541.99072265625, 1328.945556640625, -383.3695983886719, 64.61713409423828, -2416.650146484375]\n",
      "\n",
      "Instance 2290 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 16: [0.35739976167678833, 1.2515888214111328, -0.23953871428966522, 1.2756701707839966, -1.2794626951217651]\n",
      "Grand sum of 1625 tensor sets is: [542.34814453125, 1330.1971435546875, -383.609130859375, 65.89280700683594, -2417.9296875]\n",
      "\n",
      "Instance 2291 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 30: [0.20347031950950623, 0.430131733417511, -0.18520504236221313, 1.9607232809066772, 0.7505477666854858]\n",
      "Grand sum of 1626 tensor sets is: [542.5516357421875, 1330.6273193359375, -383.7943420410156, 67.85353088378906, -2417.17919921875]\n",
      "\n",
      "Instance 2292 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2293 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [69]\n",
      "Size of token embeddings is torch.Size([100, 13, 768])\n",
      "Shape of summed layers is: 100 x 768\n",
      "forever at index 69: [-0.06599497050046921, 0.4619430899620056, 1.361464500427246, 1.5638530254364014, -2.9723103046417236]\n",
      "Grand sum of 1627 tensor sets is: [542.4856567382812, 1331.0892333984375, -382.4328918457031, 69.4173812866211, -2420.151611328125]\n",
      "\n",
      "Instance 2294 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 13: [-0.05442998558282852, 0.4815368056297302, -0.05137649178504944, -0.13467103242874146, 0.7260201573371887]\n",
      "Grand sum of 1628 tensor sets is: [542.4312133789062, 1331.57080078125, -382.4842834472656, 69.28270721435547, -2419.425537109375]\n",
      "\n",
      "Instance 2295 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 8: [0.6981178522109985, 0.4359075725078583, -1.667092204093933, -1.716860055923462, -5.664602279663086]\n",
      "Grand sum of 1629 tensor sets is: [543.1293334960938, 1332.0067138671875, -384.1513671875, 67.56584930419922, -2425.090087890625]\n",
      "\n",
      "Instance 2296 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 29: [0.8736487627029419, 1.0319050550460815, -0.4528752565383911, -0.5258032083511353, 0.3675333857536316]\n",
      "Grand sum of 1630 tensor sets is: [544.0029907226562, 1333.03857421875, -384.604248046875, 67.04004669189453, -2424.72265625]\n",
      "\n",
      "Instance 2297 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 8: [0.5657870769500732, -0.611755907535553, 1.1318800449371338, 0.2753438949584961, -3.3274996280670166]\n",
      "Grand sum of 1631 tensor sets is: [544.5687866210938, 1332.4267578125, -383.4723815917969, 67.31539154052734, -2428.050048828125]\n",
      "\n",
      "Instance 2298 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 19: [0.4393460750579834, 0.6378673315048218, -0.6363399028778076, 1.3333278894424438, -4.173364162445068]\n",
      "Grand sum of 1632 tensor sets is: [545.0081176757812, 1333.0645751953125, -384.1087341308594, 68.64871978759766, -2432.223388671875]\n",
      "\n",
      "Instance 2299 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 9: [0.14595423638820648, 0.46701353788375854, -1.6866143941879272, 0.039805829524993896, -6.4555983543396]\n",
      "Grand sum of 1633 tensor sets is: [545.154052734375, 1333.5316162109375, -385.79534912109375, 68.68852233886719, -2438.678955078125]\n",
      "\n",
      "Instance 2300 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [0.651089072227478, -0.28721681237220764, 0.24158483743667603, 1.1413365602493286, -1.226405143737793]\n",
      "Grand sum of 1634 tensor sets is: [545.8051147460938, 1333.244384765625, -385.55377197265625, 69.8298568725586, -2439.9052734375]\n",
      "\n",
      "Instance 2301 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 1635 tensor sets is: [545.0703735351562, 1333.6492919921875, -385.1620788574219, 69.47279357910156, -2444.466064453125]\n",
      "\n",
      "Instance 2302 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 5: [-0.34756311774253845, 2.200188159942627, -0.42201775312423706, 0.9223399758338928, -1.8390556573867798]\n",
      "Grand sum of 1636 tensor sets is: [544.7228393554688, 1335.8494873046875, -385.5841064453125, 70.39513397216797, -2446.30517578125]\n",
      "\n",
      "Instance 2303 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 12: [0.7102779150009155, 1.920969843864441, -1.764746069908142, -0.8756726384162903, 2.618021011352539]\n",
      "Grand sum of 1637 tensor sets is: [545.43310546875, 1337.7705078125, -387.3488464355469, 69.51946258544922, -2443.687255859375]\n",
      "\n",
      "Instance 2304 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 11: [0.4216592311859131, 0.5738847851753235, 0.6547420620918274, -2.1167144775390625, -0.5588846802711487]\n",
      "Grand sum of 1638 tensor sets is: [545.854736328125, 1338.3443603515625, -386.694091796875, 67.40274810791016, -2444.24609375]\n",
      "\n",
      "Instance 2305 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 15: [0.010465174913406372, 2.7747514247894287, 0.35163673758506775, -1.5717573165893555, -0.594995379447937]\n",
      "Grand sum of 1639 tensor sets is: [545.8651733398438, 1341.119140625, -386.34246826171875, 65.83099365234375, -2444.841064453125]\n",
      "\n",
      "Instance 2306 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2307 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 8: [0.3464864492416382, 0.41073763370513916, 0.5279500484466553, 1.2198702096939087, -1.7025293111801147]\n",
      "Grand sum of 1640 tensor sets is: [546.211669921875, 1341.5299072265625, -385.81451416015625, 67.05086517333984, -2446.543701171875]\n",
      "\n",
      "Instance 2308 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 17: [-0.4070477783679962, 1.9511057138442993, -0.3937692642211914, 1.7272112369537354, 1.4193044900894165]\n",
      "Grand sum of 1641 tensor sets is: [545.8046264648438, 1343.48095703125, -386.2082824707031, 68.778076171875, -2445.12451171875]\n",
      "\n",
      "Instance 2309 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2310 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2311 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 41: [0.28834885358810425, 1.8783077001571655, 0.42420679330825806, 1.5375562906265259, -0.601883053779602]\n",
      "Grand sum of 1642 tensor sets is: [546.0929565429688, 1345.3592529296875, -385.7840881347656, 70.31563568115234, -2445.726318359375]\n",
      "\n",
      "Instance 2312 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2313 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 16: [0.32691341638565063, 0.05485007166862488, 0.2463381439447403, 1.0896027088165283, -1.4257802963256836]\n",
      "Grand sum of 1643 tensor sets is: [546.4198608398438, 1345.4140625, -385.5377502441406, 71.40523529052734, -2447.152099609375]\n",
      "\n",
      "Instance 2314 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2315 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 14: [0.601614773273468, 0.8601912260055542, -0.9699770212173462, -0.7300403714179993, -5.347193717956543]\n",
      "Grand sum of 1644 tensor sets is: [547.021484375, 1346.2742919921875, -386.5077209472656, 70.6751937866211, -2452.499267578125]\n",
      "\n",
      "Instance 2316 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 3: [0.5314648151397705, 1.5862371921539307, 0.8151748180389404, 1.1217023134231567, 3.0352444648742676]\n",
      "Grand sum of 1645 tensor sets is: [547.552978515625, 1347.8604736328125, -385.6925354003906, 71.7968978881836, -2449.464111328125]\n",
      "\n",
      "Instance 2317 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4, 10, 16]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 4: [0.45127588510513306, -2.1319243907928467, 0.01613999903202057, -0.26633787155151367, 0.6631009578704834]\n",
      "forever at index 10: [-0.1643257439136505, -1.195698618888855, 0.9429879188537598, 0.11874464154243469, 0.757766604423523]\n",
      "forever at index 16: [0.5451870560646057, -2.493384838104248, 0.6574508547782898, -1.3917913436889648, -0.7511448264122009]\n",
      "Grand sum of 1646 tensor sets is: [547.8303833007812, 1345.920166015625, -385.1536865234375, 71.28376770019531, -2449.240966796875]\n",
      "\n",
      "Instance 2318 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2319 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 6: [-0.10695799440145493, 0.20047977566719055, -0.328433632850647, -1.4192694425582886, -1.9729769229888916]\n",
      "Grand sum of 1647 tensor sets is: [547.7234497070312, 1346.12060546875, -385.48211669921875, 69.864501953125, -2451.2138671875]\n",
      "\n",
      "Instance 2320 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 27: [0.05093938112258911, 1.6734373569488525, 0.04887692630290985, 0.5390750169754028, -1.0309727191925049]\n",
      "Grand sum of 1648 tensor sets is: [547.7744140625, 1347.7940673828125, -385.4332275390625, 70.40357971191406, -2452.244873046875]\n",
      "\n",
      "Instance 2321 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2322 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1649 tensor sets is: [547.8758544921875, 1349.533447265625, -386.0712585449219, 69.28820037841797, -2456.54150390625]\n",
      "\n",
      "Instance 2323 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 3: [0.7058653831481934, 0.06750103831291199, 0.2829538583755493, -1.225460410118103, 2.4409291744232178]\n",
      "Grand sum of 1650 tensor sets is: [548.5817260742188, 1349.6009521484375, -385.7882995605469, 68.06273651123047, -2454.1005859375]\n",
      "\n",
      "Instance 2324 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2325 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 8: [1.0249220132827759, -0.7690837383270264, -0.2713632583618164, 0.24680562317371368, -1.8286933898925781]\n",
      "Grand sum of 1651 tensor sets is: [549.6066284179688, 1348.8319091796875, -386.0596618652344, 68.30953979492188, -2455.92919921875]\n",
      "\n",
      "Instance 2326 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 17: [0.4191344678401947, 1.385462760925293, -0.6033922433853149, -0.3351379632949829, -0.8355673551559448]\n",
      "Grand sum of 1652 tensor sets is: [550.0257568359375, 1350.2174072265625, -386.6630554199219, 67.97440338134766, -2456.7646484375]\n",
      "\n",
      "Instance 2327 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 8: [1.2925320863723755, -0.9379924535751343, 0.3297255337238312, -0.29196038842201233, -2.133418560028076]\n",
      "Grand sum of 1653 tensor sets is: [551.3182983398438, 1349.2794189453125, -386.3333435058594, 67.68244171142578, -2458.89794921875]\n",
      "\n",
      "Instance 2328 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 9: [0.2697189152240753, -0.07250749319791794, -0.6321370005607605, -0.9708994626998901, 0.9368044137954712]\n",
      "Grand sum of 1654 tensor sets is: [551.5880126953125, 1349.2069091796875, -386.9654846191406, 66.71154022216797, -2457.961181640625]\n",
      "\n",
      "Instance 2329 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([66, 13, 768])\n",
      "Shape of summed layers is: 66 x 768\n",
      "forever at index 33: [0.49434930086135864, -0.1679779291152954, -0.4529811143875122, -1.7525060176849365, -3.1207823753356934]\n",
      "Grand sum of 1655 tensor sets is: [552.0823364257812, 1349.0389404296875, -387.41845703125, 64.95903778076172, -2461.08203125]\n",
      "\n",
      "Instance 2330 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2331 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2332 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2333 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 13: [1.6380696296691895, 0.6868185997009277, 0.9034686088562012, 0.16040384769439697, 1.7017096281051636]\n",
      "Grand sum of 1656 tensor sets is: [553.7203979492188, 1349.7257080078125, -386.5149841308594, 65.11943817138672, -2459.38037109375]\n",
      "\n",
      "Instance 2334 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2335 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2336 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 10: [1.1494827270507812, 0.5018416047096252, -1.0069793462753296, -0.37739160656929016, -1.7439427375793457]\n",
      "Grand sum of 1657 tensor sets is: [554.869873046875, 1350.2275390625, -387.52197265625, 64.74205017089844, -2461.124267578125]\n",
      "\n",
      "Instance 2337 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 13: [-0.15089312195777893, 2.6270782947540283, -0.9360987544059753, -0.5314923524856567, -2.521505355834961]\n",
      "Grand sum of 1658 tensor sets is: [554.718994140625, 1352.8546142578125, -388.45806884765625, 64.21055603027344, -2463.645751953125]\n",
      "\n",
      "Instance 2338 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2339 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2340 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 4: [0.509709894657135, 0.5476845502853394, -0.3580619692802429, -0.7765750885009766, 1.192441701889038]\n",
      "Grand sum of 1659 tensor sets is: [555.2286987304688, 1353.40234375, -388.8161315917969, 63.433982849121094, -2462.453369140625]\n",
      "\n",
      "Instance 2341 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 1660 tensor sets is: [557.02001953125, 1351.84765625, -389.4983825683594, 62.58805847167969, -2462.000244140625]\n",
      "\n",
      "Instance 2342 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [62]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 62: [0.3974354565143585, 0.7841166257858276, -0.5701643228530884, 0.5546071529388428, -6.074431419372559]\n",
      "Grand sum of 1661 tensor sets is: [557.41748046875, 1352.6317138671875, -390.06854248046875, 63.14266586303711, -2468.07470703125]\n",
      "\n",
      "Instance 2343 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2344 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 8: [-0.5450443625450134, 1.418305516242981, -0.3329024314880371, 0.22072848677635193, -1.0245592594146729]\n",
      "Grand sum of 1662 tensor sets is: [556.8724365234375, 1354.050048828125, -390.4014587402344, 63.36339569091797, -2469.099365234375]\n",
      "\n",
      "Instance 2345 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 17: [0.23919184505939484, 1.1192561388015747, -0.4922682046890259, 1.9374676942825317, -5.913750648498535]\n",
      "Grand sum of 1663 tensor sets is: [557.1116333007812, 1355.1693115234375, -390.89373779296875, 65.30086517333984, -2475.01318359375]\n",
      "\n",
      "Instance 2346 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2347 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 3: [0.5314648151397705, 1.5862371921539307, 0.8151748180389404, 1.1217023134231567, 3.0352444648742676]\n",
      "Grand sum of 1664 tensor sets is: [557.6431274414062, 1356.7554931640625, -390.07855224609375, 66.42256927490234, -2471.97802734375]\n",
      "\n",
      "Instance 2348 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2349 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1665 tensor sets is: [557.7445678710938, 1358.494873046875, -390.7165832519531, 65.30718994140625, -2476.274658203125]\n",
      "\n",
      "Instance 2350 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 4: [-0.19992440938949585, 1.6869374513626099, 0.8323917984962463, -1.4075615406036377, -0.4147946238517761]\n",
      "Grand sum of 1666 tensor sets is: [557.5446166992188, 1360.1817626953125, -389.8841857910156, 63.899627685546875, -2476.689453125]\n",
      "\n",
      "Instance 2351 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2352 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2353 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 7: [0.7712624669075012, 0.8864405155181885, -0.9746192097663879, 0.5618910789489746, -2.8577651977539062]\n",
      "Grand sum of 1667 tensor sets is: [558.3158569335938, 1361.0682373046875, -390.8587951660156, 64.46151733398438, -2479.547119140625]\n",
      "\n",
      "Instance 2354 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 16: [-0.06181986629962921, 1.9252158403396606, -0.8812121152877808, 1.9643388986587524, -0.9304258227348328]\n",
      "Grand sum of 1668 tensor sets is: [558.2540283203125, 1362.993408203125, -391.7400207519531, 66.42585754394531, -2480.4775390625]\n",
      "\n",
      "Instance 2355 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2356 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 21: [-0.2615050971508026, 1.8707685470581055, 0.10674327611923218, 0.21677672863006592, -1.642950415611267]\n",
      "Grand sum of 1669 tensor sets is: [557.9925537109375, 1364.8641357421875, -391.6332702636719, 66.64263153076172, -2482.12060546875]\n",
      "\n",
      "Instance 2357 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 12: [0.5227048397064209, -0.30003082752227783, 0.2295801341533661, -1.0547748804092407, -1.7778465747833252]\n",
      "Grand sum of 1670 tensor sets is: [558.5152587890625, 1364.5640869140625, -391.4036865234375, 65.58786010742188, -2483.8984375]\n",
      "\n",
      "Instance 2358 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 10: [0.05301333963871002, 0.6695453524589539, -0.8211275935173035, 0.10052984952926636, -1.5100544691085815]\n",
      "Grand sum of 1671 tensor sets is: [558.5682983398438, 1365.233642578125, -392.2248229980469, 65.68839263916016, -2485.408447265625]\n",
      "\n",
      "Instance 2359 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [0.7351399064064026, 0.7444392442703247, -0.1695406436920166, -0.40575534105300903, 0.029354900121688843]\n",
      "Grand sum of 1672 tensor sets is: [559.303466796875, 1365.97802734375, -392.3943786621094, 65.28263854980469, -2485.379150390625]\n",
      "\n",
      "Instance 2360 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2361 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 19: [-0.015988275408744812, 0.1840602308511734, -0.689448893070221, 0.5662415623664856, -2.1616933345794678]\n",
      "Grand sum of 1673 tensor sets is: [559.2874755859375, 1366.162109375, -393.0838317871094, 65.848876953125, -2487.540771484375]\n",
      "\n",
      "Instance 2362 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 9: [-0.08726581931114197, -0.5950591564178467, -0.6482129693031311, 0.0736592710018158, -1.4238046407699585]\n",
      "Grand sum of 1674 tensor sets is: [559.2001953125, 1365.5670166015625, -393.7320556640625, 65.92253875732422, -2488.964599609375]\n",
      "\n",
      "Instance 2363 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 6: [0.6458348035812378, 1.251787781715393, 0.0830710381269455, 2.747269630432129, -3.8478469848632812]\n",
      "Grand sum of 1675 tensor sets is: [559.8460083007812, 1366.81884765625, -393.64898681640625, 68.66980743408203, -2492.8125]\n",
      "\n",
      "Instance 2364 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4, 20]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 4: [0.8358464241027832, 0.7150712013244629, 0.7084684371948242, -0.2078755646944046, -0.2161574363708496]\n",
      "forever at index 20: [0.3439469337463379, 0.797486424446106, 0.14562106132507324, -0.45669180154800415, -1.8482286930084229]\n",
      "Grand sum of 1676 tensor sets is: [560.4359130859375, 1367.5750732421875, -393.2219543457031, 68.3375244140625, -2493.8447265625]\n",
      "\n",
      "Instance 2365 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 13: [0.21638931334018707, 2.0103137493133545, 0.42065590620040894, 1.2595754861831665, -0.249765545129776]\n",
      "Grand sum of 1677 tensor sets is: [560.6522827148438, 1369.5853271484375, -392.8013000488281, 69.59709930419922, -2494.094482421875]\n",
      "\n",
      "Instance 2366 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 9: [0.4314788579940796, 1.141649603843689, -0.21452708542346954, 0.37738391757011414, -1.0009021759033203]\n",
      "Grand sum of 1678 tensor sets is: [561.083740234375, 1370.7269287109375, -393.0158386230469, 69.97447967529297, -2495.095458984375]\n",
      "\n",
      "Instance 2367 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 11: [-0.5674815773963928, 1.6017507314682007, -0.6321079134941101, 1.8035888671875, -0.8080515265464783]\n",
      "Grand sum of 1679 tensor sets is: [560.5162353515625, 1372.3287353515625, -393.64794921875, 71.77806854248047, -2495.903564453125]\n",
      "\n",
      "Instance 2368 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([93, 13, 768])\n",
      "Shape of summed layers is: 93 x 768\n",
      "forever at index 38: [-1.1777037382125854, -0.2888803780078888, -0.2941676676273346, -0.5361020565032959, -3.4033100605010986]\n",
      "Grand sum of 1680 tensor sets is: [559.3385620117188, 1372.039794921875, -393.9421081542969, 71.2419662475586, -2499.306884765625]\n",
      "\n",
      "Instance 2369 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 1681 tensor sets is: [558.703125, 1373.7615966796875, -393.2095947265625, 72.54774475097656, -2499.483642578125]\n",
      "\n",
      "Instance 2370 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [-0.15920022130012512, 2.5161070823669434, 0.06372873485088348, -0.12602898478507996, -2.1756668090820312]\n",
      "Grand sum of 1682 tensor sets is: [558.5439453125, 1376.2777099609375, -393.1458740234375, 72.42171478271484, -2501.659423828125]\n",
      "\n",
      "Instance 2371 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 16: [-0.18407320976257324, -1.2640435695648193, -0.2070121318101883, 0.5587940216064453, -3.5886526107788086]\n",
      "Grand sum of 1683 tensor sets is: [558.35986328125, 1375.013671875, -393.3528747558594, 72.98050689697266, -2505.248046875]\n",
      "\n",
      "Instance 2372 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 1684 tensor sets is: [559.0546264648438, 1376.14697265625, -392.14971923828125, 71.71114349365234, -2506.13818359375]\n",
      "\n",
      "Instance 2373 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [67]\n",
      "Size of token embeddings is torch.Size([160, 13, 768])\n",
      "Shape of summed layers is: 160 x 768\n",
      "forever at index 67: [0.6513000130653381, -1.9387506246566772, 0.3571016192436218, 0.6380136013031006, -3.0931806564331055]\n",
      "Grand sum of 1685 tensor sets is: [559.7059326171875, 1374.208251953125, -391.7926025390625, 72.34915924072266, -2509.2314453125]\n",
      "\n",
      "Instance 2374 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 16: [0.522491991519928, 2.209319591522217, -0.21301689743995667, -0.12179283797740936, -1.9968183040618896]\n",
      "Grand sum of 1686 tensor sets is: [560.2284545898438, 1376.4176025390625, -392.005615234375, 72.22736358642578, -2511.228271484375]\n",
      "\n",
      "Instance 2375 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 25: [0.24522359669208527, -1.3580260276794434, 0.3945884108543396, -2.298550844192505, 3.2596116065979004]\n",
      "Grand sum of 1687 tensor sets is: [560.4736938476562, 1375.0595703125, -391.61102294921875, 69.9288101196289, -2507.96875]\n",
      "\n",
      "Instance 2376 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 21: [0.24626079201698303, 2.3013858795166016, -0.029095901176333427, -1.2818257808685303, 2.092871904373169]\n",
      "Grand sum of 1688 tensor sets is: [560.719970703125, 1377.3609619140625, -391.6401062011719, 68.64698791503906, -2505.8759765625]\n",
      "\n",
      "Instance 2377 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 10: [0.39235013723373413, 0.5195966362953186, -0.04650300741195679, 0.8064181208610535, -5.031660556793213]\n",
      "Grand sum of 1689 tensor sets is: [561.1123046875, 1377.880615234375, -391.6866149902344, 69.45340728759766, -2510.90771484375]\n",
      "\n",
      "Instance 2378 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2379 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 9: [-0.2991176247596741, 0.9662394523620605, -1.0727510452270508, 1.3681936264038086, -6.349564075469971]\n",
      "Grand sum of 1690 tensor sets is: [560.8131713867188, 1378.8468017578125, -392.7593688964844, 70.82160186767578, -2517.25732421875]\n",
      "\n",
      "Instance 2380 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [64]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "forever at index 64: [-0.6975685358047485, -0.4966183304786682, -2.045245409011841, -0.34151142835617065, 1.625889539718628]\n",
      "Grand sum of 1691 tensor sets is: [560.1156005859375, 1378.3502197265625, -394.80462646484375, 70.48008728027344, -2515.63134765625]\n",
      "\n",
      "Instance 2381 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2382 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2383 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2384 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 14: [0.601614773273468, 0.8601912260055542, -0.9699770212173462, -0.7300403714179993, -5.347193717956543]\n",
      "Grand sum of 1692 tensor sets is: [560.7172241210938, 1379.21044921875, -395.77459716796875, 69.75004577636719, -2520.978515625]\n",
      "\n",
      "Instance 2385 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2386 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 4: [-0.4807726740837097, 0.7814627885818481, -0.7893418669700623, 1.3447903394699097, -5.202075004577637]\n",
      "Grand sum of 1693 tensor sets is: [560.2364501953125, 1379.991943359375, -396.5639343261719, 71.09483337402344, -2526.1806640625]\n",
      "\n",
      "Instance 2387 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2388 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([129, 13, 768])\n",
      "Shape of summed layers is: 129 x 768\n",
      "forever at index 38: [0.313968688249588, -0.4891098439693451, -0.8076965808868408, -1.2177201509475708, -3.608511209487915]\n",
      "Grand sum of 1694 tensor sets is: [560.5504150390625, 1379.5028076171875, -397.37164306640625, 69.87711334228516, -2529.7890625]\n",
      "\n",
      "Instance 2389 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 23: [0.6091084480285645, 0.3695499897003174, 0.28370538353919983, 0.3370553255081177, -1.2195183038711548]\n",
      "Grand sum of 1695 tensor sets is: [561.1595458984375, 1379.872314453125, -397.08795166015625, 70.21417236328125, -2531.008544921875]\n",
      "\n",
      "Instance 2390 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 1696 tensor sets is: [561.7034301757812, 1379.2374267578125, -397.73748779296875, 69.07195281982422, -2529.88720703125]\n",
      "\n",
      "Instance 2391 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 1697 tensor sets is: [561.0679931640625, 1380.959228515625, -397.0049743652344, 70.37773132324219, -2530.06396484375]\n",
      "\n",
      "Instance 2392 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2393 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 8: [0.6559311151504517, 1.1381438970565796, -0.2866974472999573, -1.94203782081604, -3.1938211917877197]\n",
      "Grand sum of 1698 tensor sets is: [561.7239379882812, 1382.097412109375, -397.29168701171875, 68.4356918334961, -2533.2578125]\n",
      "\n",
      "Instance 2394 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 17: [0.8741735219955444, -0.2898051142692566, -0.5300635695457458, -0.36382320523262024, -2.9599013328552246]\n",
      "Grand sum of 1699 tensor sets is: [562.5980834960938, 1381.8076171875, -397.8217468261719, 68.07186889648438, -2536.2177734375]\n",
      "\n",
      "Instance 2395 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 44: [0.8309445381164551, -0.458668053150177, -1.3386505842208862, -2.54099702835083, -3.6211836338043213]\n",
      "Grand sum of 1700 tensor sets is: [563.4290161132812, 1381.3489990234375, -399.160400390625, 65.53086853027344, -2539.8388671875]\n",
      "\n",
      "Instance 2396 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 13: [1.0306488275527954, -0.006747938692569733, 0.1850205510854721, 0.4763759672641754, -1.3853257894515991]\n",
      "Grand sum of 1701 tensor sets is: [564.4596557617188, 1381.34228515625, -398.9753723144531, 66.00724792480469, -2541.22412109375]\n",
      "\n",
      "Instance 2397 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 23: [0.25613653659820557, 1.7356373071670532, -0.0017609372735023499, 1.8842601776123047, -1.2285208702087402]\n",
      "Grand sum of 1702 tensor sets is: [564.7158203125, 1383.077880859375, -398.9771423339844, 67.89151000976562, -2542.45263671875]\n",
      "\n",
      "Instance 2398 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 3: [0.8830519914627075, 0.23959997296333313, -0.4618088901042938, 0.40850967168807983, 1.6340255737304688]\n",
      "Grand sum of 1703 tensor sets is: [565.598876953125, 1383.3175048828125, -399.43896484375, 68.30001831054688, -2540.818603515625]\n",
      "\n",
      "Instance 2399 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 4: [0.3934631049633026, 0.8280083537101746, -1.4028171300888062, -0.05869141221046448, -2.6398086547851562]\n",
      "Grand sum of 1704 tensor sets is: [565.9923095703125, 1384.1455078125, -400.841796875, 68.24132537841797, -2543.45849609375]\n",
      "\n",
      "Instance 2400 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 1705 tensor sets is: [567.7836303710938, 1382.5908203125, -401.5240478515625, 67.39540100097656, -2543.00537109375]\n",
      "\n",
      "Instance 2401 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2402 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2403 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "forever at index 24: [1.0409573316574097, -0.19595374166965485, -0.9018043279647827, 0.581130862236023, 0.07262459397315979]\n",
      "Grand sum of 1706 tensor sets is: [568.8245849609375, 1382.3948974609375, -402.42584228515625, 67.97653198242188, -2542.932861328125]\n",
      "\n",
      "Instance 2404 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 19: [-0.5621359348297119, 0.7418949604034424, 0.007165774703025818, -0.4962702989578247, -9.285123825073242]\n",
      "Grand sum of 1707 tensor sets is: [568.262451171875, 1383.1368408203125, -402.4186706542969, 67.48026275634766, -2552.218017578125]\n",
      "\n",
      "Instance 2405 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 2: [-0.5513662099838257, 1.679462194442749, -0.07373695075511932, 1.2606719732284546, -3.93057918548584]\n",
      "Grand sum of 1708 tensor sets is: [567.7110595703125, 1384.8162841796875, -402.4924011230469, 68.74093627929688, -2556.148681640625]\n",
      "\n",
      "Instance 2406 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 1709 tensor sets is: [568.1067504882812, 1385.824951171875, -402.5619812011719, 68.057373046875, -2554.852294921875]\n",
      "\n",
      "Instance 2407 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2408 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 1710 tensor sets is: [568.9453735351562, 1386.180908203125, -402.9636535644531, 68.57099914550781, -2552.313232421875]\n",
      "\n",
      "Instance 2409 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2410 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2411 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 22: [-0.19705107808113098, 1.1939126253128052, -0.2782352566719055, 0.03318857401609421, -5.209828853607178]\n",
      "Grand sum of 1711 tensor sets is: [568.7483520507812, 1387.3748779296875, -403.24188232421875, 68.60418701171875, -2557.52294921875]\n",
      "\n",
      "Instance 2412 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 12: [-0.44998496770858765, 0.33025655150413513, 0.15094809234142303, 1.6428210735321045, 1.0242094993591309]\n",
      "Grand sum of 1712 tensor sets is: [568.29833984375, 1387.705078125, -403.0909423828125, 70.24700927734375, -2556.498779296875]\n",
      "\n",
      "Instance 2413 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 12: [-0.1357840597629547, 0.35787302255630493, -0.2289954274892807, 0.5837039947509766, -2.39477276802063]\n",
      "Grand sum of 1713 tensor sets is: [568.1625366210938, 1388.06298828125, -403.3199462890625, 70.8307113647461, -2558.8935546875]\n",
      "\n",
      "Instance 2414 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2415 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2416 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2417 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 13: [-0.6518627405166626, 3.101691484451294, -0.44571447372436523, -0.8201470375061035, 0.39190858602523804]\n",
      "Grand sum of 1714 tensor sets is: [567.5106811523438, 1391.1646728515625, -403.7656555175781, 70.01056671142578, -2558.501708984375]\n",
      "\n",
      "Instance 2418 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 13: [0.23092828691005707, 0.5423389077186584, -0.06063485145568848, 1.3572322130203247, -3.804450750350952]\n",
      "Grand sum of 1715 tensor sets is: [567.7416381835938, 1391.70703125, -403.8262939453125, 71.3677978515625, -2562.30615234375]\n",
      "\n",
      "Instance 2419 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2420 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2421 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [43]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "forever at index 43: [0.36196672916412354, -0.20208777487277985, -0.6770387291908264, -1.1453596353530884, -2.4529056549072266]\n",
      "Grand sum of 1716 tensor sets is: [568.1035766601562, 1391.5048828125, -404.5033264160156, 70.2224349975586, -2564.759033203125]\n",
      "\n",
      "Instance 2422 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 20: [0.6114815473556519, 0.635421633720398, -0.7407627105712891, 1.4102864265441895, 1.5775883197784424]\n",
      "Grand sum of 1717 tensor sets is: [568.715087890625, 1392.1402587890625, -405.24407958984375, 71.63272094726562, -2563.181396484375]\n",
      "\n",
      "Instance 2423 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2424 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 13: [1.0306488275527954, -0.006747938692569733, 0.1850205510854721, 0.4763759672641754, -1.3853257894515991]\n",
      "Grand sum of 1718 tensor sets is: [569.7457275390625, 1392.133544921875, -405.0590515136719, 72.10910034179688, -2564.566650390625]\n",
      "\n",
      "Instance 2425 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 1719 tensor sets is: [569.1102905273438, 1393.8553466796875, -404.3265380859375, 73.41487884521484, -2564.743408203125]\n",
      "\n",
      "Instance 2426 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2427 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 3: [-0.6723930239677429, 0.9160648584365845, -0.702422559261322, 0.699920654296875, -1.9733576774597168]\n",
      "Grand sum of 1720 tensor sets is: [568.4379272460938, 1394.7713623046875, -405.0289611816406, 74.11479949951172, -2566.716796875]\n",
      "\n",
      "Instance 2428 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 1721 tensor sets is: [567.802490234375, 1396.4931640625, -404.29644775390625, 75.42057800292969, -2566.8935546875]\n",
      "\n",
      "Instance 2429 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 3: [0.7770174741744995, 0.14506244659423828, -0.6678030490875244, -1.1164658069610596, -1.178963541984558]\n",
      "Grand sum of 1722 tensor sets is: [568.5795288085938, 1396.63818359375, -404.9642639160156, 74.30411529541016, -2568.072509765625]\n",
      "\n",
      "Instance 2430 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "forever at index 8: [0.020567793399095535, -1.135591983795166, 1.0719499588012695, 1.1741585731506348, -1.135704517364502]\n",
      "Grand sum of 1723 tensor sets is: [568.60009765625, 1395.5025634765625, -403.8923034667969, 75.478271484375, -2569.208251953125]\n",
      "\n",
      "Instance 2431 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 40: [0.4158547520637512, 1.8590456247329712, -1.6150810718536377, 0.3023301959037781, -3.461760997772217]\n",
      "Grand sum of 1724 tensor sets is: [569.0159301757812, 1397.361572265625, -405.50738525390625, 75.78060150146484, -2572.669921875]\n",
      "\n",
      "Instance 2432 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2433 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 5: [0.6022050380706787, 2.4245588779449463, -0.6253792643547058, 1.0847046375274658, 0.46901166439056396]\n",
      "Grand sum of 1725 tensor sets is: [569.6181640625, 1399.7861328125, -406.13275146484375, 76.86530303955078, -2572.200927734375]\n",
      "\n",
      "Instance 2434 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 34: [0.2311011254787445, 0.3443058133125305, -0.023906365036964417, -0.627159595489502, -2.6861281394958496]\n",
      "Grand sum of 1726 tensor sets is: [569.8492431640625, 1400.1304931640625, -406.1566467285156, 76.23814392089844, -2574.886962890625]\n",
      "\n",
      "Instance 2435 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 14: [1.094424843788147, 0.529801607131958, -0.4525042176246643, -0.19177165627479553, 1.8896480798721313]\n",
      "Grand sum of 1727 tensor sets is: [570.9436645507812, 1400.6602783203125, -406.6091613769531, 76.04637145996094, -2572.997314453125]\n",
      "\n",
      "Instance 2436 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 15: [-0.4766021966934204, 1.74265718460083, 0.30620285868644714, 0.2167445719242096, -3.2190253734588623]\n",
      "Grand sum of 1728 tensor sets is: [570.467041015625, 1402.4029541015625, -406.3029479980469, 76.26311492919922, -2576.21630859375]\n",
      "\n",
      "Instance 2437 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 20: [0.5693416595458984, 3.100731611251831, 0.18758946657180786, -1.058305025100708, -1.9403722286224365]\n",
      "Grand sum of 1729 tensor sets is: [571.036376953125, 1405.503662109375, -406.1153564453125, 75.2048110961914, -2578.15673828125]\n",
      "\n",
      "Instance 2438 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2439 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2440 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 13: [0.6949979066848755, 0.9895399808883667, -0.32517850399017334, -1.1428334712982178, -2.624631881713867]\n",
      "Grand sum of 1730 tensor sets is: [571.7313842773438, 1406.4931640625, -406.4405212402344, 74.06198120117188, -2580.78125]\n",
      "\n",
      "Instance 2441 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 16: [-0.10990479588508606, 2.2434630393981934, 0.5143247842788696, -0.8192417025566101, -1.139312744140625]\n",
      "Grand sum of 1731 tensor sets is: [571.6214599609375, 1408.736572265625, -405.92620849609375, 73.24273681640625, -2581.920654296875]\n",
      "\n",
      "Instance 2442 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 11: [0.3863116502761841, 2.0772664546966553, -1.4187146425247192, 2.1403534412384033, -5.384565353393555]\n",
      "Grand sum of 1732 tensor sets is: [572.0077514648438, 1410.8138427734375, -407.34490966796875, 75.38308715820312, -2587.30517578125]\n",
      "\n",
      "Instance 2443 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1733 tensor sets is: [572.1091918945312, 1412.55322265625, -407.9829406738281, 74.26770782470703, -2591.601806640625]\n",
      "\n",
      "Instance 2444 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 7: [-0.3437940180301666, 0.7498482465744019, -0.24309854209423065, -0.16130584478378296, -3.86409854888916]\n",
      "Grand sum of 1734 tensor sets is: [571.765380859375, 1413.3031005859375, -408.2260437011719, 74.10639953613281, -2595.4658203125]\n",
      "\n",
      "Instance 2445 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [50]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "forever at index 50: [-0.21470855176448822, -1.3488097190856934, 1.817403793334961, 0.6378251314163208, 3.061790704727173]\n",
      "Grand sum of 1735 tensor sets is: [571.5506591796875, 1411.954345703125, -406.40863037109375, 74.74422454833984, -2592.404052734375]\n",
      "\n",
      "Instance 2446 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1736 tensor sets is: [571.652099609375, 1413.6937255859375, -407.0466613769531, 73.62884521484375, -2596.70068359375]\n",
      "\n",
      "Instance 2447 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 14: [0.5944801568984985, 1.4684652090072632, -0.20519837737083435, 0.5984058976173401, 0.2883116602897644]\n",
      "Grand sum of 1737 tensor sets is: [572.24658203125, 1415.1622314453125, -407.2518615722656, 74.22724914550781, -2596.412353515625]\n",
      "\n",
      "Instance 2448 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 4: [0.7231817245483398, 0.994153618812561, -0.4197235107421875, -0.523115336894989, 0.4192184805870056]\n",
      "Grand sum of 1738 tensor sets is: [572.9697875976562, 1416.1563720703125, -407.67156982421875, 73.70413208007812, -2595.9931640625]\n",
      "\n",
      "Instance 2449 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2450 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 25: [0.05614999681711197, 0.3012913763523102, 0.5342416167259216, -0.09334723651409149, -0.31925931572914124]\n",
      "Grand sum of 1739 tensor sets is: [573.0259399414062, 1416.4576416015625, -407.1373291015625, 73.61078643798828, -2596.3125]\n",
      "\n",
      "Instance 2451 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2452 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 1740 tensor sets is: [574.8172607421875, 1414.9029541015625, -407.819580078125, 72.76486206054688, -2595.859375]\n",
      "\n",
      "Instance 2453 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 19: [0.2712671458721161, 2.6482737064361572, 0.6870505809783936, -1.2366394996643066, -0.8429713249206543]\n",
      "Grand sum of 1741 tensor sets is: [575.0885009765625, 1417.55126953125, -407.1325378417969, 71.5282211303711, -2596.702392578125]\n",
      "\n",
      "Instance 2454 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2455 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "forever at index 9: [0.5218549370765686, 0.9484233856201172, 0.1863338053226471, 0.2602981925010681, -3.169445037841797]\n",
      "Grand sum of 1742 tensor sets is: [575.6103515625, 1418.4996337890625, -406.9461975097656, 71.78852081298828, -2599.871826171875]\n",
      "\n",
      "Instance 2456 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [53]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 53: [1.0640228986740112, 0.9880425333976746, -0.4146309494972229, 0.1579052209854126, -3.353015899658203]\n",
      "Grand sum of 1743 tensor sets is: [576.6743774414062, 1419.4876708984375, -407.36083984375, 71.94642639160156, -2603.224853515625]\n",
      "\n",
      "Instance 2457 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2458 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2459 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 9: [1.098562479019165, -0.07149630784988403, -0.65312260389328, -1.7348510026931763, -1.0558137893676758]\n",
      "Grand sum of 1744 tensor sets is: [577.77294921875, 1419.4161376953125, -408.01397705078125, 70.21157836914062, -2604.28076171875]\n",
      "\n",
      "Instance 2460 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2461 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 18: [0.3713524639606476, 1.1481904983520508, -1.1194250583648682, 1.6352049112319946, -4.920462131500244]\n",
      "Grand sum of 1745 tensor sets is: [578.144287109375, 1420.5643310546875, -409.1333923339844, 71.84678649902344, -2609.201171875]\n",
      "\n",
      "Instance 2462 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2463 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 31: [0.37884777784347534, 1.1321529150009155, -0.5322197079658508, -1.4627447128295898, 0.023972660303115845]\n",
      "Grand sum of 1746 tensor sets is: [578.5231323242188, 1421.696533203125, -409.6656188964844, 70.38404083251953, -2609.17724609375]\n",
      "\n",
      "Instance 2464 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2465 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2466 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26, 58]\n",
      "Size of token embeddings is torch.Size([73, 13, 768])\n",
      "Shape of summed layers is: 73 x 768\n",
      "forever at index 26: [0.4369354248046875, -1.9200717210769653, 1.201796293258667, 2.1940929889678955, 0.35944876074790955]\n",
      "forever at index 58: [0.3083455264568329, -1.659688115119934, 1.0000965595245361, 2.016608476638794, 0.31671157479286194]\n",
      "Grand sum of 1747 tensor sets is: [578.895751953125, 1419.9066162109375, -408.5646667480469, 72.48939514160156, -2608.839111328125]\n",
      "\n",
      "Instance 2467 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 24: [0.25700488686561584, -0.03141859173774719, 0.20902836322784424, -1.3262306451797485, -2.632232904434204]\n",
      "Grand sum of 1748 tensor sets is: [579.1527709960938, 1419.875244140625, -408.35565185546875, 71.16316223144531, -2611.471435546875]\n",
      "\n",
      "Instance 2468 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2469 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 9: [-0.8845191597938538, 0.9074761867523193, -0.364807665348053, 0.1613493710756302, -3.214406967163086]\n",
      "Grand sum of 1749 tensor sets is: [578.2682495117188, 1420.78271484375, -408.720458984375, 71.32450866699219, -2614.685791015625]\n",
      "\n",
      "Instance 2470 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2471 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2472 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 4: [-0.1960567831993103, 0.1344730257987976, 0.15601155161857605, -0.6826155185699463, -0.594531238079071]\n",
      "Grand sum of 1750 tensor sets is: [578.0722045898438, 1420.917236328125, -408.564453125, 70.64189147949219, -2615.2802734375]\n",
      "\n",
      "Instance 2473 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 14: [0.7616899609565735, 1.167149543762207, -0.2753746509552002, 0.7795141935348511, -3.990046977996826]\n",
      "Grand sum of 1751 tensor sets is: [578.8339233398438, 1422.0843505859375, -408.8398132324219, 71.42140197753906, -2619.270263671875]\n",
      "\n",
      "Instance 2474 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([81, 13, 768])\n",
      "Shape of summed layers is: 81 x 768\n",
      "forever at index 65: [0.2005038857460022, 0.1780579388141632, -0.19022947549819946, 0.24201560020446777, -2.0446221828460693]\n",
      "Grand sum of 1752 tensor sets is: [579.034423828125, 1422.262451171875, -409.030029296875, 71.66341400146484, -2621.31494140625]\n",
      "\n",
      "Instance 2475 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2476 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2477 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2478 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 16: [-0.06181986629962921, 1.9252158403396606, -0.8812121152877808, 1.9643388986587524, -0.9304258227348328]\n",
      "Grand sum of 1753 tensor sets is: [578.9725952148438, 1424.1876220703125, -409.9112548828125, 73.62775421142578, -2622.245361328125]\n",
      "\n",
      "Instance 2479 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 23: [0.8718481659889221, -0.4836075007915497, 0.1424952894449234, -1.0510263442993164, -4.597509860992432]\n",
      "Grand sum of 1754 tensor sets is: [579.8444213867188, 1423.7039794921875, -409.7687683105469, 72.57672882080078, -2626.8427734375]\n",
      "\n",
      "Instance 2480 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2481 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2482 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 10: [-0.7215121984481812, 1.91921865940094, -1.330257534980774, -0.021542400121688843, -0.9067964553833008]\n",
      "Grand sum of 1755 tensor sets is: [579.1229248046875, 1425.6231689453125, -411.0990295410156, 72.55518341064453, -2627.74951171875]\n",
      "\n",
      "Instance 2483 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2484 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 8: [-0.341831773519516, 2.0853500366210938, -0.45807117223739624, 1.5271732807159424, -2.01289701461792]\n",
      "Grand sum of 1756 tensor sets is: [578.7810668945312, 1427.70849609375, -411.5570983886719, 74.08235931396484, -2629.762451171875]\n",
      "\n",
      "Instance 2485 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 29: [1.0472074747085571, 1.7720599174499512, -0.3437556326389313, 2.94246244430542, 5.7358856201171875]\n",
      "Grand sum of 1757 tensor sets is: [579.8282470703125, 1429.4805908203125, -411.9008483886719, 77.02481842041016, -2624.026611328125]\n",
      "\n",
      "Instance 2486 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 17: [0.4191344678401947, 1.385462760925293, -0.6033922433853149, -0.3351379632949829, -0.8355673551559448]\n",
      "Grand sum of 1758 tensor sets is: [580.2473754882812, 1430.8660888671875, -412.5042419433594, 76.68968200683594, -2624.862060546875]\n",
      "\n",
      "Instance 2487 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([75, 13, 768])\n",
      "Shape of summed layers is: 75 x 768\n",
      "forever at index 9: [0.6720932722091675, 0.5254420042037964, -0.37496328353881836, 0.06730225682258606, -3.1615846157073975]\n",
      "Grand sum of 1759 tensor sets is: [580.9194946289062, 1431.3914794921875, -412.87921142578125, 76.7569808959961, -2628.023681640625]\n",
      "\n",
      "Instance 2488 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 3: [-0.003689780831336975, 0.6785387992858887, 0.22607024013996124, -0.7985621690750122, -0.3594732880592346]\n",
      "Grand sum of 1760 tensor sets is: [580.9158325195312, 1432.070068359375, -412.65313720703125, 75.95841979980469, -2628.383056640625]\n",
      "\n",
      "Instance 2489 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2490 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 3: [-0.2502080202102661, 0.5505109429359436, -0.9030846953392029, 2.0250155925750732, -3.5836191177368164]\n",
      "Grand sum of 1761 tensor sets is: [580.6656494140625, 1432.62060546875, -413.55621337890625, 77.98343658447266, -2631.966796875]\n",
      "\n",
      "Instance 2491 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 3: [0.32994553446769714, 0.18613892793655396, -0.5860008597373962, 0.7834742069244385, 1.6237897872924805]\n",
      "Grand sum of 1762 tensor sets is: [580.99560546875, 1432.8067626953125, -414.1422119140625, 78.76691436767578, -2630.343017578125]\n",
      "\n",
      "Instance 2492 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2493 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 9: [0.6248182058334351, 0.42348727583885193, 1.256920576095581, -0.2616877853870392, -0.8839870691299438]\n",
      "Grand sum of 1763 tensor sets is: [581.6204223632812, 1433.230224609375, -412.8852844238281, 78.5052261352539, -2631.22705078125]\n",
      "\n",
      "Instance 2494 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2495 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 40: [0.4996374249458313, -0.28994956612586975, -0.46465548872947693, -2.637877941131592, -3.694197416305542]\n",
      "Grand sum of 1764 tensor sets is: [582.1200561523438, 1432.9403076171875, -413.3499450683594, 75.86734771728516, -2634.921142578125]\n",
      "\n",
      "Instance 2496 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [36, 75]\n",
      "Size of token embeddings is torch.Size([94, 13, 768])\n",
      "Shape of summed layers is: 94 x 768\n",
      "forever at index 36: [0.5397933721542358, 0.9954855442047119, -0.6757387518882751, -0.4997861981391907, -3.3372044563293457]\n",
      "forever at index 75: [1.5084513425827026, 0.2468351423740387, 0.9808189868927002, -1.4100486040115356, -0.7715275287628174]\n",
      "Grand sum of 1765 tensor sets is: [583.1441650390625, 1433.5615234375, -413.1974182128906, 74.91242980957031, -2636.9755859375]\n",
      "\n",
      "Instance 2497 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 8: [0.3341563940048218, -0.45155027508735657, -0.36073899269104004, -2.4051058292388916, -1.8670011758804321]\n",
      "Grand sum of 1766 tensor sets is: [583.4783325195312, 1433.1099853515625, -413.55816650390625, 72.50732421875, -2638.842529296875]\n",
      "\n",
      "Instance 2498 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 8: [0.6250044107437134, 2.5987792015075684, 0.050587937235832214, -0.44628196954727173, -3.5335609912872314]\n",
      "Grand sum of 1767 tensor sets is: [584.1033325195312, 1435.708740234375, -413.507568359375, 72.06104278564453, -2642.3759765625]\n",
      "\n",
      "Instance 2499 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2500 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 9: [0.00775802880525589, -0.4664250314235687, -1.2065333127975464, 0.47917211055755615, -4.387584686279297]\n",
      "Grand sum of 1768 tensor sets is: [584.111083984375, 1435.2423095703125, -414.714111328125, 72.54021453857422, -2646.763671875]\n",
      "\n",
      "Instance 2501 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 9: [0.1591707170009613, 2.595632314682007, 0.058137670159339905, 0.0837814211845398, -1.8375955820083618]\n",
      "Grand sum of 1769 tensor sets is: [584.270263671875, 1437.837890625, -414.6559753417969, 72.62399291992188, -2648.601318359375]\n",
      "\n",
      "Instance 2502 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 3: [0.34910479187965393, 1.9760431051254272, -0.1503932923078537, -1.044787049293518, 0.361553430557251]\n",
      "Grand sum of 1770 tensor sets is: [584.619384765625, 1439.81396484375, -414.8063659667969, 71.57920837402344, -2648.23974609375]\n",
      "\n",
      "Instance 2503 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2504 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 12: [0.5227048397064209, -0.30003082752227783, 0.2295801341533661, -1.0547748804092407, -1.7778465747833252]\n",
      "Grand sum of 1771 tensor sets is: [585.14208984375, 1439.513916015625, -414.5767822265625, 70.5244369506836, -2650.017578125]\n",
      "\n",
      "Instance 2505 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2506 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 6: [0.3127959370613098, 0.4363399147987366, 0.08336375653743744, -0.6534538269042969, 0.0912485122680664]\n",
      "Grand sum of 1772 tensor sets is: [585.4548950195312, 1439.9501953125, -414.493408203125, 69.87098693847656, -2649.92626953125]\n",
      "\n",
      "Instance 2507 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2508 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 37: [1.0011470317840576, 2.278724193572998, 0.7192291021347046, 2.7144124507904053, -2.003579616546631]\n",
      "Grand sum of 1773 tensor sets is: [586.4560546875, 1442.2288818359375, -413.774169921875, 72.58539581298828, -2651.929931640625]\n",
      "\n",
      "Instance 2509 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2510 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1774 tensor sets is: [586.5574951171875, 1443.96826171875, -414.4122009277344, 71.47001647949219, -2656.2265625]\n",
      "\n",
      "Instance 2511 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 17: [-0.5531822443008423, 1.25632643699646, -1.4030699729919434, -0.2432505488395691, -4.426941394805908]\n",
      "Grand sum of 1775 tensor sets is: [586.0043334960938, 1445.224609375, -415.8152770996094, 71.22676849365234, -2660.653564453125]\n",
      "\n",
      "Instance 2512 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 5: [-0.38718658685684204, -0.7276805639266968, -0.4920441210269928, -0.22040963172912598, 0.40566539764404297]\n",
      "Grand sum of 1776 tensor sets is: [585.6171264648438, 1444.4969482421875, -416.30731201171875, 71.00635528564453, -2660.247802734375]\n",
      "\n",
      "Instance 2513 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 8: [0.6043856739997864, 1.5832488536834717, -1.650606393814087, 0.2807258367538452, -0.3111696243286133]\n",
      "Grand sum of 1777 tensor sets is: [586.2214965820312, 1446.0802001953125, -417.9579162597656, 71.28707885742188, -2660.55908203125]\n",
      "\n",
      "Instance 2514 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2515 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2516 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 4: [-0.19992440938949585, 1.6869374513626099, 0.8323917984962463, -1.4075615406036377, -0.4147946238517761]\n",
      "Grand sum of 1778 tensor sets is: [586.0215454101562, 1447.76708984375, -417.1255187988281, 69.8795166015625, -2660.973876953125]\n",
      "\n",
      "Instance 2517 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [59]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "forever at index 59: [0.3317471146583557, -0.4785270392894745, -1.4570953845977783, 1.5113109350204468, -2.7488739490509033]\n",
      "Grand sum of 1779 tensor sets is: [586.353271484375, 1447.28857421875, -418.5826110839844, 71.39083099365234, -2663.72265625]\n",
      "\n",
      "Instance 2518 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2519 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2520 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 32: [0.29929107427597046, 1.331392765045166, -0.16339021921157837, -1.0112459659576416, -2.6980886459350586]\n",
      "Grand sum of 1780 tensor sets is: [586.652587890625, 1448.6199951171875, -418.7460021972656, 70.37958526611328, -2666.420654296875]\n",
      "\n",
      "Instance 2521 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 15: [-0.4766021966934204, 1.74265718460083, 0.30620285868644714, 0.2167445719242096, -3.2190253734588623]\n",
      "Grand sum of 1781 tensor sets is: [586.1759643554688, 1450.3626708984375, -418.4397888183594, 70.59632873535156, -2669.6396484375]\n",
      "\n",
      "Instance 2522 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 12: [0.4129360318183899, 1.9724130630493164, -0.0389808714389801, 0.43249136209487915, -6.537036895751953]\n",
      "Grand sum of 1782 tensor sets is: [586.5889282226562, 1452.3350830078125, -418.478759765625, 71.02882385253906, -2676.1767578125]\n",
      "\n",
      "Instance 2523 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "forever at index 20: [0.10454433411359787, 0.7871823310852051, 0.0858149453997612, -0.7315887808799744, -2.443013906478882]\n",
      "Grand sum of 1783 tensor sets is: [586.6934814453125, 1453.122314453125, -418.3929443359375, 70.29723358154297, -2678.619873046875]\n",
      "\n",
      "Instance 2524 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2525 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2526 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 14: [0.8222966194152832, 1.4864437580108643, -0.862433910369873, 0.03989380598068237, 1.162133812904358]\n",
      "Grand sum of 1784 tensor sets is: [587.5158081054688, 1454.6087646484375, -419.25537109375, 70.33712768554688, -2677.457763671875]\n",
      "\n",
      "Instance 2527 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [174]\n",
      "Size of token embeddings is torch.Size([181, 13, 768])\n",
      "Shape of summed layers is: 181 x 768\n",
      "forever at index 174: [-0.18140292167663574, 0.28570640087127686, -0.2314547300338745, -2.0777368545532227, 1.524895191192627]\n",
      "Grand sum of 1785 tensor sets is: [587.3344116210938, 1454.89453125, -419.48681640625, 68.25939178466797, -2675.932861328125]\n",
      "\n",
      "Instance 2528 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 3: [-0.5556266903877258, 0.6667457818984985, -1.7795963287353516, 0.5510233044624329, -3.3520665168762207]\n",
      "Grand sum of 1786 tensor sets is: [586.77880859375, 1455.561279296875, -421.26641845703125, 68.81041717529297, -2679.284912109375]\n",
      "\n",
      "Instance 2529 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13, 23]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "forever at index 13: [1.2997686862945557, -1.3014837503433228, 1.1049504280090332, -0.21790069341659546, -0.2693304419517517]\n",
      "forever at index 23: [1.0805879831314087, -0.8612240552902222, 1.184114933013916, -1.9300768375396729, -0.6622325778007507]\n",
      "Grand sum of 1787 tensor sets is: [587.968994140625, 1454.47998046875, -420.12188720703125, 67.7364273071289, -2679.750732421875]\n",
      "\n",
      "Instance 2530 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 9: [0.698822557926178, 0.5805623531341553, -0.05982381850481033, 0.17691010236740112, -2.963899612426758]\n",
      "Grand sum of 1788 tensor sets is: [588.6678466796875, 1455.060546875, -420.18170166015625, 67.91333770751953, -2682.714599609375]\n",
      "\n",
      "Instance 2531 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 26: [0.3894859850406647, 0.028996914625167847, -0.1615557223558426, -0.8795301914215088, -0.5065106153488159]\n",
      "Grand sum of 1789 tensor sets is: [589.0573120117188, 1455.089599609375, -420.34326171875, 67.03380584716797, -2683.22119140625]\n",
      "\n",
      "Instance 2532 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2533 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 10: [0.2941908836364746, 1.790968894958496, -1.4262081384658813, -0.48040974140167236, 0.7198961973190308]\n",
      "Grand sum of 1790 tensor sets is: [589.3515014648438, 1456.880615234375, -421.76947021484375, 66.55339813232422, -2682.501220703125]\n",
      "\n",
      "Instance 2534 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 27: [0.01696310192346573, 0.752837061882019, -0.19700458645820618, 0.4498496651649475, -0.060307830572128296]\n",
      "Grand sum of 1791 tensor sets is: [589.3684692382812, 1457.6334228515625, -421.9664611816406, 67.00325012207031, -2682.5615234375]\n",
      "\n",
      "Instance 2535 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 8: [0.4658259451389313, 1.1462438106536865, -1.0548607110977173, 0.726665198802948, -0.346773624420166]\n",
      "Grand sum of 1792 tensor sets is: [589.8342895507812, 1458.7796630859375, -423.0213317871094, 67.72991180419922, -2682.908203125]\n",
      "\n",
      "Instance 2536 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 17: [0.6881306171417236, 1.0999705791473389, -0.6750742197036743, 0.2830566167831421, -2.437903881072998]\n",
      "Grand sum of 1793 tensor sets is: [590.5223999023438, 1459.879638671875, -423.6964111328125, 68.01296997070312, -2685.34619140625]\n",
      "\n",
      "Instance 2537 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "forever at index 37: [0.22523927688598633, 0.8947292566299438, 0.2814337909221649, -1.0550615787506104, -2.871880292892456]\n",
      "Grand sum of 1794 tensor sets is: [590.7476196289062, 1460.7744140625, -423.41497802734375, 66.9579086303711, -2688.218017578125]\n",
      "\n",
      "Instance 2538 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 32: [2.051858901977539, 0.8482000827789307, 0.5979204773902893, 1.884175181388855, 4.851768493652344]\n",
      "Grand sum of 1795 tensor sets is: [592.7994995117188, 1461.62255859375, -422.8170471191406, 68.84208679199219, -2683.3662109375]\n",
      "\n",
      "Instance 2539 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 8: [0.7191540598869324, 2.597564220428467, -0.13535168766975403, -1.43770432472229, 1.2078138589859009]\n",
      "Grand sum of 1796 tensor sets is: [593.5186767578125, 1464.2200927734375, -422.952392578125, 67.40438079833984, -2682.158447265625]\n",
      "\n",
      "Instance 2540 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [71]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "forever at index 71: [0.10519235581159592, 0.43187275528907776, -0.6278825998306274, -0.10609352588653564, -0.456643283367157]\n",
      "Grand sum of 1797 tensor sets is: [593.6238403320312, 1464.6519775390625, -423.58026123046875, 67.29828643798828, -2682.614990234375]\n",
      "\n",
      "Instance 2541 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2542 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 10: [0.2453213632106781, 0.7938261032104492, -0.931047260761261, 1.5417791604995728, -5.021829605102539]\n",
      "Grand sum of 1798 tensor sets is: [593.869140625, 1465.44580078125, -424.5113220214844, 68.8400650024414, -2687.63671875]\n",
      "\n",
      "Instance 2543 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 10: [0.09152780473232269, 0.44090303778648376, -0.7119274735450745, 1.209414005279541, -4.417181015014648]\n",
      "Grand sum of 1799 tensor sets is: [593.960693359375, 1465.88671875, -425.2232360839844, 70.04947662353516, -2692.053955078125]\n",
      "\n",
      "Instance 2544 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1800 tensor sets is: [594.0621337890625, 1467.6260986328125, -425.86126708984375, 68.93409729003906, -2696.3505859375]\n",
      "\n",
      "Instance 2545 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 39: [0.19261828064918518, -0.3796582818031311, -0.33681121468544006, -0.2852908670902252, -3.456049680709839]\n",
      "Grand sum of 1801 tensor sets is: [594.2547607421875, 1467.2464599609375, -426.1980895996094, 68.6488037109375, -2699.806640625]\n",
      "\n",
      "Instance 2546 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2547 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 26: [0.37860313057899475, -0.8008484840393066, -0.2810317873954773, 0.23155392706394196, -1.51703679561615]\n",
      "Grand sum of 1802 tensor sets is: [594.6333618164062, 1466.445556640625, -426.4791259765625, 68.88035583496094, -2701.32373046875]\n",
      "\n",
      "Instance 2548 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 15: [0.9696832895278931, 0.3698159456253052, -0.568975567817688, 0.39616239070892334, -0.6662353277206421]\n",
      "Grand sum of 1803 tensor sets is: [595.60302734375, 1466.8154296875, -427.048095703125, 69.27651977539062, -2701.989990234375]\n",
      "\n",
      "Instance 2549 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 7: [0.20609144866466522, 0.2815585136413574, -0.6479869484901428, 0.7129466533660889, -2.6273627281188965]\n",
      "Grand sum of 1804 tensor sets is: [595.8091430664062, 1467.0970458984375, -427.6960754394531, 69.98946380615234, -2704.617431640625]\n",
      "\n",
      "Instance 2550 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2551 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2552 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [54]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "forever at index 54: [-0.35940536856651306, -0.052329882979393005, -1.0900832414627075, 0.6745641231536865, -0.9731273651123047]\n",
      "Grand sum of 1805 tensor sets is: [595.4497680664062, 1467.044677734375, -428.7861633300781, 70.66402435302734, -2705.590576171875]\n",
      "\n",
      "Instance 2553 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2554 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "forever at index 34: [-0.2403811812400818, 0.628974199295044, -0.4796023964881897, -2.744699001312256, -0.174118772149086]\n",
      "Grand sum of 1806 tensor sets is: [595.2094116210938, 1467.6737060546875, -429.2657775878906, 67.91932678222656, -2705.7646484375]\n",
      "\n",
      "Instance 2555 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2556 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2557 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 25: [0.38160982728004456, 0.9152438640594482, -0.16408610343933105, -0.708044707775116, -0.770586371421814]\n",
      "Grand sum of 1807 tensor sets is: [595.5910034179688, 1468.5889892578125, -429.42987060546875, 67.2112808227539, -2706.53515625]\n",
      "\n",
      "Instance 2558 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 52: [-0.25532668828964233, 1.0798821449279785, 0.5748259425163269, -1.4162236452102661, -1.8875761032104492]\n",
      "Grand sum of 1808 tensor sets is: [595.335693359375, 1469.6688232421875, -428.85504150390625, 65.79505920410156, -2708.4228515625]\n",
      "\n",
      "Instance 2559 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 1809 tensor sets is: [594.7002563476562, 1471.390625, -428.1225280761719, 67.10083770751953, -2708.599609375]\n",
      "\n",
      "Instance 2560 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 10: [0.1278500109910965, 0.008798539638519287, -1.3999611139297485, -0.37128132581710815, -1.098502278327942]\n",
      "Grand sum of 1810 tensor sets is: [594.828125, 1471.3994140625, -429.5224914550781, 66.72955322265625, -2709.697998046875]\n",
      "\n",
      "Instance 2561 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 11: [0.32559436559677124, -0.18058784306049347, -0.21059739589691162, 0.329372763633728, -1.1785783767700195]\n",
      "Grand sum of 1811 tensor sets is: [595.1537475585938, 1471.2188720703125, -429.73309326171875, 67.05892944335938, -2710.87646484375]\n",
      "\n",
      "Instance 2562 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 3: [0.7765663862228394, 0.014637395739555359, -0.059301622211933136, -0.668442964553833, 1.743051290512085]\n",
      "Grand sum of 1812 tensor sets is: [595.9302978515625, 1471.2335205078125, -429.7923889160156, 66.39048767089844, -2709.13330078125]\n",
      "\n",
      "Instance 2563 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 7: [0.12614160776138306, 0.3488052189350128, -0.4339976906776428, 0.10429178178310394, -4.513529300689697]\n",
      "Grand sum of 1813 tensor sets is: [596.0564575195312, 1471.582275390625, -430.22637939453125, 66.49478149414062, -2713.646728515625]\n",
      "\n",
      "Instance 2564 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2565 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2566 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 4: [-0.3752632141113281, 1.7071202993392944, 0.5718411803245544, -1.2132822275161743, -3.142397880554199]\n",
      "Grand sum of 1814 tensor sets is: [595.6812133789062, 1473.2894287109375, -429.654541015625, 65.28150177001953, -2716.7890625]\n",
      "\n",
      "Instance 2567 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 8: [0.14101146161556244, 1.2000112533569336, -1.462010145187378, -0.07150418311357498, -1.5163934230804443]\n",
      "Grand sum of 1815 tensor sets is: [595.8222045898438, 1474.4893798828125, -431.1165466308594, 65.20999908447266, -2718.305419921875]\n",
      "\n",
      "Instance 2568 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 9: [1.098562479019165, -0.07149630784988403, -0.65312260389328, -1.7348510026931763, -1.0558137893676758]\n",
      "Grand sum of 1816 tensor sets is: [596.9207763671875, 1474.4178466796875, -431.7696838378906, 63.47514724731445, -2719.361328125]\n",
      "\n",
      "Instance 2569 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2570 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2571 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.07251130044460297, -1.8579487800598145, -0.7446154952049255, -1.0451340675354004, -3.531836748123169]\n",
      "Grand sum of 1817 tensor sets is: [596.9932861328125, 1472.5599365234375, -432.5143127441406, 62.43001174926758, -2722.89306640625]\n",
      "\n",
      "Instance 2572 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 8: [-0.17230816185474396, 0.854118824005127, 0.21219289302825928, -0.7595388889312744, -5.266519069671631]\n",
      "Grand sum of 1818 tensor sets is: [596.8209838867188, 1473.4140625, -432.3021240234375, 61.67047119140625, -2728.15966796875]\n",
      "\n",
      "Instance 2573 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2574 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 23: [0.7232486009597778, 1.3866437673568726, 0.3417642116546631, -0.1827470064163208, 1.173323631286621]\n",
      "Grand sum of 1819 tensor sets is: [597.5442504882812, 1474.8006591796875, -431.9603576660156, 61.48772430419922, -2726.986328125]\n",
      "\n",
      "Instance 2575 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2576 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 18: [0.6374046802520752, 4.245052337646484, -0.8385918140411377, 2.127424955368042, -2.159917116165161]\n",
      "Grand sum of 1820 tensor sets is: [598.181640625, 1479.045654296875, -432.7989501953125, 63.615150451660156, -2729.146240234375]\n",
      "\n",
      "Instance 2577 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "forever at index 22: [0.2096656858921051, 2.394752264022827, -0.6821109652519226, -0.9293566942214966, -3.2947778701782227]\n",
      "Grand sum of 1821 tensor sets is: [598.3912963867188, 1481.4404296875, -433.4810485839844, 62.685794830322266, -2732.44091796875]\n",
      "\n",
      "Instance 2578 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2579 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2580 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2581 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2582 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 4: [-0.33897265791893005, 1.3033950328826904, -0.49549606442451477, -1.7480592727661133, -2.181185007095337]\n",
      "Grand sum of 1822 tensor sets is: [598.0523071289062, 1482.7437744140625, -433.9765319824219, 60.93773651123047, -2734.6220703125]\n",
      "\n",
      "Instance 2583 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 9: [-0.554456353187561, 1.755728006362915, 0.36366307735443115, 0.1381741464138031, -1.6970949172973633]\n",
      "Grand sum of 1823 tensor sets is: [597.4978637695312, 1484.49951171875, -433.61285400390625, 61.07591247558594, -2736.319091796875]\n",
      "\n",
      "Instance 2584 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 17: [-0.11085617542266846, 1.6629451513290405, 0.06083974987268448, 0.9826996922492981, -1.9294296503067017]\n",
      "Grand sum of 1824 tensor sets is: [597.3870239257812, 1486.1624755859375, -433.552001953125, 62.05861282348633, -2738.24853515625]\n",
      "\n",
      "Instance 2585 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [60]\n",
      "Size of token embeddings is torch.Size([91, 13, 768])\n",
      "Shape of summed layers is: 91 x 768\n",
      "forever at index 60: [0.0031675174832344055, 1.3741670846939087, 0.37984713912010193, -1.0696520805358887, 0.40278616547584534]\n",
      "Grand sum of 1825 tensor sets is: [597.3901977539062, 1487.53662109375, -433.1721496582031, 60.98896026611328, -2737.845703125]\n",
      "\n",
      "Instance 2586 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 19: [0.4410718083381653, 0.8252122402191162, 0.8226633071899414, -0.8931680917739868, -1.9953457117080688]\n",
      "Grand sum of 1826 tensor sets is: [597.831298828125, 1488.36181640625, -432.3494873046875, 60.09579086303711, -2739.841064453125]\n",
      "\n",
      "Instance 2587 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2588 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 14: [0.2395603507757187, 0.49061837792396545, 1.7143605947494507, 1.0437121391296387, -0.7463366985321045]\n",
      "Grand sum of 1827 tensor sets is: [598.0708618164062, 1488.8524169921875, -430.6351318359375, 61.139503479003906, -2740.58740234375]\n",
      "\n",
      "Instance 2589 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 11: [0.9418429136276245, 2.6807937622070312, 0.4933071732521057, 0.05821502208709717, -0.5153200626373291]\n",
      "Grand sum of 1828 tensor sets is: [599.0126953125, 1491.533203125, -430.1418151855469, 61.19771957397461, -2741.102783203125]\n",
      "\n",
      "Instance 2590 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2591 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 27: [0.8955196142196655, 2.008474349975586, -0.44899702072143555, 1.5108089447021484, 1.4069082736968994]\n",
      "Grand sum of 1829 tensor sets is: [599.908203125, 1493.5416259765625, -430.5908203125, 62.708526611328125, -2739.69580078125]\n",
      "\n",
      "Instance 2592 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2593 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2594 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2595 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 10: [-0.053110845386981964, 0.9487161636352539, -1.8552852869033813, -0.6267894506454468, -2.6495132446289062]\n",
      "Grand sum of 1830 tensor sets is: [599.8551025390625, 1494.4903564453125, -432.44610595703125, 62.08173751831055, -2742.34521484375]\n",
      "\n",
      "Instance 2596 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2597 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2598 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 13: [0.6390613913536072, 1.3397406339645386, -0.5036189556121826, 1.9189852476119995, 3.7281599044799805]\n",
      "Grand sum of 1831 tensor sets is: [600.494140625, 1495.830078125, -432.9497375488281, 64.00072479248047, -2738.616943359375]\n",
      "\n",
      "Instance 2599 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2600 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 12: [-0.4925311803817749, -0.11043353378772736, -1.3283264636993408, -2.147111177444458, -3.6141746044158936]\n",
      "Grand sum of 1832 tensor sets is: [600.0015869140625, 1495.7196044921875, -434.278076171875, 61.853614807128906, -2742.231201171875]\n",
      "\n",
      "Instance 2601 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 5: [0.9175915718078613, -0.1897631138563156, -1.3345837593078613, 0.7062106132507324, 1.241743564605713]\n",
      "Grand sum of 1833 tensor sets is: [600.919189453125, 1495.52978515625, -435.6126708984375, 62.5598258972168, -2740.989501953125]\n",
      "\n",
      "Instance 2602 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2603 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 1834 tensor sets is: [601.7578125, 1495.8857421875, -436.01434326171875, 63.07345199584961, -2738.450439453125]\n",
      "\n",
      "Instance 2604 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "forever at index 4: [0.6587233543395996, 2.4285759925842285, 0.4839630126953125, 0.38338181376457214, -1.2988861799240112]\n",
      "Grand sum of 1835 tensor sets is: [602.4165649414062, 1498.3143310546875, -435.5303955078125, 63.45683288574219, -2739.749267578125]\n",
      "\n",
      "Instance 2605 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2606 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 17: [0.11122555285692215, 0.22489987313747406, 1.0439949035644531, -0.29273611307144165, -5.155889511108398]\n",
      "Grand sum of 1836 tensor sets is: [602.5277709960938, 1498.5391845703125, -434.48638916015625, 63.16409683227539, -2744.9052734375]\n",
      "\n",
      "Instance 2607 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 4: [0.18669968843460083, 0.1423853635787964, 0.5554900765419006, 0.634471595287323, -1.5151008367538452]\n",
      "Grand sum of 1837 tensor sets is: [602.7144775390625, 1498.6815185546875, -433.930908203125, 63.79856872558594, -2746.42041015625]\n",
      "\n",
      "Instance 2608 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 5: [0.7922149896621704, 0.11639181524515152, -0.6452752947807312, -2.474534749984741, -0.8326596021652222]\n",
      "Grand sum of 1838 tensor sets is: [603.5067138671875, 1498.7978515625, -434.576171875, 61.32403564453125, -2747.253173828125]\n",
      "\n",
      "Instance 2609 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2610 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 11: [-0.17476999759674072, -0.5574918985366821, 0.17218592762947083, 1.3741354942321777, 3.7136359214782715]\n",
      "Grand sum of 1839 tensor sets is: [603.3319702148438, 1498.2403564453125, -434.40399169921875, 62.69816970825195, -2743.53955078125]\n",
      "\n",
      "Instance 2611 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [51]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "forever at index 51: [1.2722183465957642, 1.0126348733901978, 0.22556179761886597, -0.7708761692047119, -5.509192943572998]\n",
      "Grand sum of 1840 tensor sets is: [604.6041870117188, 1499.2530517578125, -434.1784362792969, 61.92729187011719, -2749.048828125]\n",
      "\n",
      "Instance 2612 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2613 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "forever at index 37: [0.7120872735977173, 1.3848528861999512, 0.5738686323165894, 0.30191102623939514, 0.32392406463623047]\n",
      "Grand sum of 1841 tensor sets is: [605.3162841796875, 1500.637939453125, -433.60455322265625, 62.22920227050781, -2748.724853515625]\n",
      "\n",
      "Instance 2614 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 8: [0.4433378577232361, 2.0096654891967773, 0.000878162682056427, 1.0241150856018066, 1.7132691144943237]\n",
      "Grand sum of 1842 tensor sets is: [605.7596435546875, 1502.6475830078125, -433.6036682128906, 63.253318786621094, -2747.011474609375]\n",
      "\n",
      "Instance 2615 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2616 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2617 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "forever at index 40: [0.4996374249458313, -0.28994956612586975, -0.46465548872947693, -2.637877941131592, -3.694197416305542]\n",
      "Grand sum of 1843 tensor sets is: [606.25927734375, 1502.357666015625, -434.0683288574219, 60.615440368652344, -2750.70556640625]\n",
      "\n",
      "Instance 2618 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 18: [-0.36745208501815796, 1.631134033203125, -0.9954196810722351, -0.02650301158428192, -6.949418544769287]\n",
      "Grand sum of 1844 tensor sets is: [605.891845703125, 1503.98876953125, -435.0637512207031, 60.58893585205078, -2757.655029296875]\n",
      "\n",
      "Instance 2619 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2620 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 42: [0.08395320177078247, 0.6060254573822021, 0.10077250748872757, 0.5573438405990601, 2.1399402618408203]\n",
      "Grand sum of 1845 tensor sets is: [605.9757690429688, 1504.5948486328125, -434.9629821777344, 61.146278381347656, -2755.51513671875]\n",
      "\n",
      "Instance 2621 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 13: [0.702001690864563, -0.24760305881500244, -0.012464519590139389, 2.746957778930664, -2.8226592540740967]\n",
      "Grand sum of 1846 tensor sets is: [606.6777954101562, 1504.3472900390625, -434.9754333496094, 63.89323425292969, -2758.337890625]\n",
      "\n",
      "Instance 2622 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 17: [-0.5430637001991272, 0.11462897062301636, -1.7717318534851074, -0.20631565153598785, -5.750796318054199]\n",
      "Grand sum of 1847 tensor sets is: [606.1347045898438, 1504.4619140625, -436.7471618652344, 63.686920166015625, -2764.088623046875]\n",
      "\n",
      "Instance 2623 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 26: [1.3161708116531372, 0.7910457253456116, 0.03404364734888077, -0.2547284960746765, -6.300019264221191]\n",
      "Grand sum of 1848 tensor sets is: [607.4508666992188, 1505.2529296875, -436.7131042480469, 63.43218994140625, -2770.388671875]\n",
      "\n",
      "Instance 2624 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [51]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 51: [-0.31948551535606384, 0.6329426169395447, -0.008432582020759583, 0.7299581170082092, -2.6011059284210205]\n",
      "Grand sum of 1849 tensor sets is: [607.1314086914062, 1505.8858642578125, -436.7215270996094, 64.16214752197266, -2772.98974609375]\n",
      "\n",
      "Instance 2625 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 37: [-0.08559057116508484, 0.28616732358932495, -1.5485185384750366, -1.6077601909637451, -3.895199775695801]\n",
      "Grand sum of 1850 tensor sets is: [607.0458374023438, 1506.1719970703125, -438.2700500488281, 62.554386138916016, -2776.885009765625]\n",
      "\n",
      "Instance 2626 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2627 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26, 36]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 26: [1.1294137239456177, 2.0012154579162598, 0.7162389755249023, 1.7791978120803833, -4.387946605682373]\n",
      "forever at index 36: [0.42110419273376465, 1.0108684301376343, 0.8314676284790039, -0.2685456871986389, -3.26582407951355]\n",
      "Grand sum of 1851 tensor sets is: [607.8211059570312, 1507.677978515625, -437.4961853027344, 63.30971145629883, -2780.7119140625]\n",
      "\n",
      "Instance 2628 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 8: [0.49635282158851624, 1.451690912246704, 0.117647185921669, -0.5936737060546875, -2.4969401359558105]\n",
      "Grand sum of 1852 tensor sets is: [608.3174438476562, 1509.129638671875, -437.3785400390625, 62.71603775024414, -2783.208740234375]\n",
      "\n",
      "Instance 2629 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "forever at index 30: [-0.1166355237364769, 1.3978617191314697, 0.3407113552093506, -0.055488526821136475, -1.0360134840011597]\n",
      "Grand sum of 1853 tensor sets is: [608.2008056640625, 1510.5274658203125, -437.037841796875, 62.66054916381836, -2784.244873046875]\n",
      "\n",
      "Instance 2630 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 20: [0.1544121652841568, 2.6956369876861572, -0.30628371238708496, -0.3998771011829376, -2.5857431888580322]\n",
      "Grand sum of 1854 tensor sets is: [608.355224609375, 1513.22314453125, -437.3441162109375, 62.26067352294922, -2786.83056640625]\n",
      "\n",
      "Instance 2631 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2632 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 14: [0.21717102825641632, 1.0272125005722046, 0.001334354281425476, 0.6784761548042297, 0.5428685545921326]\n",
      "Grand sum of 1855 tensor sets is: [608.5723876953125, 1514.2503662109375, -437.3427734375, 62.93914794921875, -2786.28759765625]\n",
      "\n",
      "Instance 2633 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [97]\n",
      "Size of token embeddings is torch.Size([103, 13, 768])\n",
      "Shape of summed layers is: 103 x 768\n",
      "forever at index 97: [-0.5152525901794434, 1.6571012735366821, 0.7270866632461548, -1.3622987270355225, -1.908617377281189]\n",
      "Grand sum of 1856 tensor sets is: [608.05712890625, 1515.907470703125, -436.6156921386719, 61.57685089111328, -2788.1962890625]\n",
      "\n",
      "Instance 2634 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "forever at index 27: [0.8574728965759277, 1.8817707300186157, -0.3501396179199219, -1.81563138961792, -2.766359329223633]\n",
      "Grand sum of 1857 tensor sets is: [608.9146118164062, 1517.7891845703125, -436.9658203125, 59.7612190246582, -2790.962646484375]\n",
      "\n",
      "Instance 2635 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 17: [-0.4190237820148468, 0.050488948822021484, -1.1512290239334106, -1.215449333190918, -4.23914909362793]\n",
      "Grand sum of 1858 tensor sets is: [608.49560546875, 1517.8397216796875, -438.1170349121094, 58.54576873779297, -2795.201904296875]\n",
      "\n",
      "Instance 2636 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 3: [-0.6234313249588013, 2.0174930095672607, -1.335849404335022, 2.0489115715026855, -2.6674509048461914]\n",
      "Grand sum of 1859 tensor sets is: [607.8721923828125, 1519.857177734375, -439.452880859375, 60.59468078613281, -2797.869384765625]\n",
      "\n",
      "Instance 2637 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2638 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 5: [0.543906569480896, -0.6348281502723694, -0.6495499610900879, -1.1422220468521118, 1.1213560104370117]\n",
      "Grand sum of 1860 tensor sets is: [608.4160766601562, 1519.2222900390625, -440.1024169921875, 59.452457427978516, -2796.748046875]\n",
      "\n",
      "Instance 2639 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 37: [0.21632987260818481, 0.3767281174659729, -0.6308167576789856, 1.163720965385437, -6.457067012786865]\n",
      "Grand sum of 1861 tensor sets is: [608.6323852539062, 1519.5989990234375, -440.7332458496094, 60.61617660522461, -2803.205078125]\n",
      "\n",
      "Instance 2640 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 11: [-0.04714576154947281, 1.5581154823303223, 0.622721791267395, 3.1942262649536133, 0.7621023654937744]\n",
      "Grand sum of 1862 tensor sets is: [608.5852661132812, 1521.1571044921875, -440.11053466796875, 63.810401916503906, -2802.44287109375]\n",
      "\n",
      "Instance 2641 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2642 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2643 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 65: [-0.4072004556655884, -1.165315866470337, -1.116049885749817, -1.6974555253982544, -1.7651728391647339]\n",
      "Grand sum of 1863 tensor sets is: [608.1780395507812, 1519.9918212890625, -441.2265930175781, 62.112945556640625, -2804.2080078125]\n",
      "\n",
      "Instance 2644 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "forever at index 28: [0.37363481521606445, 1.9645576477050781, 0.6712744235992432, 0.5728694796562195, -0.8423131704330444]\n",
      "Grand sum of 1864 tensor sets is: [608.5516967773438, 1521.9564208984375, -440.5553283691406, 62.685813903808594, -2805.05029296875]\n",
      "\n",
      "Instance 2645 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2646 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 22: [-0.008392930030822754, 1.7310545444488525, 0.5146083235740662, 1.5235223770141602, 1.5913927555084229]\n",
      "Grand sum of 1865 tensor sets is: [608.5432739257812, 1523.6875, -440.04071044921875, 64.20933532714844, -2803.458984375]\n",
      "\n",
      "Instance 2647 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 6: [-0.022896677255630493, 1.6134440898895264, -0.02442409098148346, -1.2658820152282715, -2.53287672996521]\n",
      "Grand sum of 1866 tensor sets is: [608.5203857421875, 1525.3009033203125, -440.06512451171875, 62.94345474243164, -2805.991943359375]\n",
      "\n",
      "Instance 2648 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2649 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 3: [-0.4204009473323822, 2.5056018829345703, -0.43993955850601196, -1.654463291168213, -1.3028905391693115]\n",
      "Grand sum of 1867 tensor sets is: [608.0999755859375, 1527.8065185546875, -440.50506591796875, 61.28899002075195, -2807.294921875]\n",
      "\n",
      "Instance 2650 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 6: [-0.8656287789344788, 1.2341161966323853, -0.012317229062318802, 2.3273112773895264, -0.19194747507572174]\n",
      "Grand sum of 1868 tensor sets is: [607.234375, 1529.0406494140625, -440.51739501953125, 63.616302490234375, -2807.48681640625]\n",
      "\n",
      "Instance 2651 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2652 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2653 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 8: [0.5235716700553894, 2.1776952743530273, 0.12706515192985535, -0.8810579776763916, -3.46952486038208]\n",
      "Grand sum of 1869 tensor sets is: [607.7579345703125, 1531.2183837890625, -440.39031982421875, 62.73524475097656, -2810.956298828125]\n",
      "\n",
      "Instance 2654 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2655 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2656 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "forever at index 46: [0.15407930314540863, -0.11646556854248047, 0.044514454901218414, -1.9048917293548584, -1.5517337322235107]\n",
      "Grand sum of 1870 tensor sets is: [607.9119873046875, 1531.1019287109375, -440.3457946777344, 60.830352783203125, -2812.508056640625]\n",
      "\n",
      "Instance 2657 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "forever at index 35: [0.46226900815963745, 0.45593515038490295, -1.2998024225234985, -0.6927465200424194, -3.984184980392456]\n",
      "Grand sum of 1871 tensor sets is: [608.374267578125, 1531.557861328125, -441.6455993652344, 60.13760757446289, -2816.4921875]\n",
      "\n",
      "Instance 2658 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2659 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2660 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 11: [-0.29996928572654724, 2.4060163497924805, 0.2397550642490387, -0.04382893443107605, -0.5868548154830933]\n",
      "Grand sum of 1872 tensor sets is: [608.0742797851562, 1533.9638671875, -441.4058532714844, 60.093780517578125, -2817.0791015625]\n",
      "\n",
      "Instance 2661 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "forever at index 7: [0.6947833895683289, 1.1333074569702148, 1.2031441926956177, -1.2693626880645752, -0.8902418613433838]\n",
      "Grand sum of 1873 tensor sets is: [608.76904296875, 1535.09716796875, -440.20269775390625, 58.82441711425781, -2817.96923828125]\n",
      "\n",
      "Instance 2662 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 4: [-0.15989167988300323, 0.6102015376091003, -0.4848882257938385, 0.1716737598180771, -0.0923786461353302]\n",
      "Grand sum of 1874 tensor sets is: [608.609130859375, 1535.7073974609375, -440.6875915527344, 58.996089935302734, -2818.0615234375]\n",
      "\n",
      "Instance 2663 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 4: [0.7895189523696899, 2.3263988494873047, -0.05623345077037811, 0.6651526689529419, 2.2280237674713135]\n",
      "Grand sum of 1875 tensor sets is: [609.3986206054688, 1538.0338134765625, -440.74383544921875, 59.6612434387207, -2815.83349609375]\n",
      "\n",
      "Instance 2664 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2665 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2666 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 1876 tensor sets is: [609.5366821289062, 1538.68798828125, -441.1844177246094, 59.363922119140625, -2818.683837890625]\n",
      "\n",
      "Instance 2667 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2668 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "forever at index 24: [0.5823067426681519, 1.839061975479126, -0.017360389232635498, -0.45592373609542847, -3.8431906700134277]\n",
      "Grand sum of 1877 tensor sets is: [610.1190185546875, 1540.527099609375, -441.2017822265625, 58.907997131347656, -2822.527099609375]\n",
      "\n",
      "Instance 2669 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2670 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2671 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "forever at index 11: [-0.07318879663944244, 0.6577862501144409, -0.7881293892860413, 1.2358450889587402, -3.76011323928833]\n",
      "Grand sum of 1878 tensor sets is: [610.0458374023438, 1541.1849365234375, -441.9898986816406, 60.14384078979492, -2826.287109375]\n",
      "\n",
      "Instance 2672 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2673 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 20: [0.5187315344810486, 2.623962879180908, 0.292629212141037, 3.6678824424743652, -0.1364719718694687]\n",
      "Grand sum of 1879 tensor sets is: [610.5645751953125, 1543.8089599609375, -441.697265625, 63.81172180175781, -2826.423583984375]\n",
      "\n",
      "Instance 2674 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 14: [0.330258309841156, 0.9127354025840759, 0.05045109987258911, -1.0715011358261108, -2.7009894847869873]\n",
      "Grand sum of 1880 tensor sets is: [610.8948364257812, 1544.7216796875, -441.6468200683594, 62.74021911621094, -2829.12451171875]\n",
      "\n",
      "Instance 2675 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 30: [1.0676368474960327, 1.828202486038208, -0.6096475720405579, -0.5763261914253235, 1.0746411085128784]\n",
      "Grand sum of 1881 tensor sets is: [611.9624633789062, 1546.5499267578125, -442.2564697265625, 62.16389465332031, -2828.0498046875]\n",
      "\n",
      "Instance 2676 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [51]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "forever at index 51: [0.7909182906150818, -0.5219452381134033, -1.5186384916305542, 1.796737551689148, -1.9444493055343628]\n",
      "Grand sum of 1882 tensor sets is: [612.7533569335938, 1546.0279541015625, -443.7751159667969, 63.96063232421875, -2829.994140625]\n",
      "\n",
      "Instance 2677 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([112, 13, 768])\n",
      "Shape of summed layers is: 112 x 768\n",
      "forever at index 88: [0.2279881238937378, 0.5516111254692078, 0.7744902968406677, -0.14299587905406952, -0.8696667551994324]\n",
      "Grand sum of 1883 tensor sets is: [612.9813232421875, 1546.57958984375, -443.0006408691406, 63.81763458251953, -2830.86376953125]\n",
      "\n",
      "Instance 2678 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2679 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 26: [0.9049415588378906, -1.4486569166183472, -0.17121872305870056, -0.8746490478515625, -0.6089862585067749]\n",
      "Grand sum of 1884 tensor sets is: [613.8862915039062, 1545.1309814453125, -443.1718444824219, 62.94298553466797, -2831.47265625]\n",
      "\n",
      "Instance 2680 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 10: [1.030529260635376, -0.937955379486084, -1.095366358757019, 0.9940557479858398, -4.031533241271973]\n",
      "Grand sum of 1885 tensor sets is: [614.9168090820312, 1544.1929931640625, -444.2672119140625, 63.937042236328125, -2835.504150390625]\n",
      "\n",
      "Instance 2681 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 10: [0.7015156149864197, 1.6863141059875488, 0.4860362410545349, -1.0204066038131714, 3.1100289821624756]\n",
      "Grand sum of 1886 tensor sets is: [615.6183471679688, 1545.8792724609375, -443.78118896484375, 62.9166374206543, -2832.39404296875]\n",
      "\n",
      "Instance 2682 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2683 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 35: [-0.2715536057949066, 0.4865565896034241, -0.8110759854316711, -2.3517632484436035, -1.0169554948806763]\n",
      "Grand sum of 1887 tensor sets is: [615.3468017578125, 1546.3658447265625, -444.5922546386719, 60.56487274169922, -2833.410888671875]\n",
      "\n",
      "Instance 2684 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [73, 306]\n",
      "Size of token embeddings is torch.Size([366, 13, 768])\n",
      "Shape of summed layers is: 366 x 768\n",
      "forever at index 73: [-0.2401716262102127, 2.2776107788085938, -0.08971258997917175, 0.43860936164855957, -0.1759650707244873]\n",
      "forever at index 306: [0.04323674738407135, 2.133942127227783, -0.3427940011024475, -0.28511229157447815, -3.5771842002868652]\n",
      "Grand sum of 1888 tensor sets is: [615.2483520507812, 1548.5716552734375, -444.8085021972656, 60.64162063598633, -2835.287353515625]\n",
      "\n",
      "Instance 2685 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 3: [0.022603385150432587, 0.4576917886734009, 0.533186674118042, -0.29273536801338196, -2.2041168212890625]\n",
      "Grand sum of 1889 tensor sets is: [615.2709350585938, 1549.029296875, -444.27532958984375, 60.34888458251953, -2837.491455078125]\n",
      "\n",
      "Instance 2686 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2687 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "forever at index 17: [0.1516321450471878, 1.2882511615753174, 0.42040008306503296, -0.8939779996871948, -6.262052059173584]\n",
      "Grand sum of 1890 tensor sets is: [615.4225463867188, 1550.3175048828125, -443.85491943359375, 59.45490646362305, -2843.75341796875]\n",
      "\n",
      "Instance 2688 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2689 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 17: [0.33422285318374634, -0.8155922293663025, -0.537778913974762, -0.07142467796802521, -3.95515775680542]\n",
      "Grand sum of 1891 tensor sets is: [615.7567749023438, 1549.501953125, -444.3927001953125, 59.383480072021484, -2847.70849609375]\n",
      "\n",
      "Instance 2690 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2691 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2692 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 7: [0.9832466840744019, 0.19009733200073242, -0.32321181893348694, 0.42473360896110535, 0.5483078360557556]\n",
      "Grand sum of 1892 tensor sets is: [616.7400512695312, 1549.6920166015625, -444.7159118652344, 59.80821228027344, -2847.16015625]\n",
      "\n",
      "Instance 2693 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "forever at index 23: [0.25613653659820557, 1.7356373071670532, -0.0017609372735023499, 1.8842601776123047, -1.2285208702087402]\n",
      "Grand sum of 1893 tensor sets is: [616.9962158203125, 1551.4276123046875, -444.7176818847656, 61.692474365234375, -2848.388671875]\n",
      "\n",
      "Instance 2694 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [0.20144517719745636, -0.6056551337242126, 0.9097043871879578, -0.8823636770248413, 3.642965793609619]\n",
      "Grand sum of 1894 tensor sets is: [617.1976318359375, 1550.8218994140625, -443.8079833984375, 60.81011199951172, -2844.74560546875]\n",
      "\n",
      "Instance 2695 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [0.36479946970939636, 0.24682025611400604, -0.4848631024360657, -0.5248885154724121, -1.1551042795181274]\n",
      "Grand sum of 1895 tensor sets is: [617.5624389648438, 1551.0687255859375, -444.2928466796875, 60.28522491455078, -2845.900634765625]\n",
      "\n",
      "Instance 2696 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2697 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2698 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [0.5741387009620667, 2.143437623977661, -0.34711724519729614, -0.5437552332878113, -0.9650851488113403]\n",
      "Grand sum of 1896 tensor sets is: [618.1365966796875, 1553.212158203125, -444.63995361328125, 59.74147033691406, -2846.86572265625]\n",
      "\n",
      "Instance 2699 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "forever at index 20: [0.6300421953201294, 1.8560326099395752, 0.22185774147510529, 0.3324981927871704, -0.6448638439178467]\n",
      "Grand sum of 1897 tensor sets is: [618.7666625976562, 1555.0682373046875, -444.4180908203125, 60.07396697998047, -2847.510498046875]\n",
      "\n",
      "Instance 2700 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2701 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [74]\n",
      "Size of token embeddings is torch.Size([79, 13, 768])\n",
      "Shape of summed layers is: 79 x 768\n",
      "forever at index 74: [0.1380610316991806, 0.6541885733604431, -0.44058239459991455, -0.29732009768486023, -2.8503072261810303]\n",
      "Grand sum of 1898 tensor sets is: [618.9047241210938, 1555.722412109375, -444.8586730957031, 59.77664566040039, -2850.36083984375]\n",
      "\n",
      "Instance 2702 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 10: [1.3355209827423096, -1.1548044681549072, -1.3995260000228882, 0.867976725101471, -4.381512641906738]\n",
      "Grand sum of 1899 tensor sets is: [620.240234375, 1554.567626953125, -446.2582092285156, 60.644622802734375, -2854.742431640625]\n",
      "\n",
      "Instance 2703 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [99]\n",
      "Size of token embeddings is torch.Size([102, 13, 768])\n",
      "Shape of summed layers is: 102 x 768\n",
      "forever at index 99: [0.8253326416015625, 0.8208157420158386, -0.32888996601104736, 0.7303957939147949, -1.320308804512024]\n",
      "Grand sum of 1900 tensor sets is: [621.0655517578125, 1555.388427734375, -446.58709716796875, 61.37501907348633, -2856.062744140625]\n",
      "\n",
      "Instance 2704 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 17: [0.4191344678401947, 1.385462760925293, -0.6033922433853149, -0.3351379632949829, -0.8355673551559448]\n",
      "Grand sum of 1901 tensor sets is: [621.4846801757812, 1556.77392578125, -447.19049072265625, 61.03988265991211, -2856.898193359375]\n",
      "\n",
      "Instance 2705 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2706 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 16: [0.2767907381057739, 1.2694132328033447, -0.4388395845890045, -0.37906399369239807, -1.1787409782409668]\n",
      "Grand sum of 1902 tensor sets is: [621.761474609375, 1558.0433349609375, -447.62933349609375, 60.66082000732422, -2858.076904296875]\n",
      "\n",
      "Instance 2707 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "forever at index 39: [0.19261828064918518, -0.3796582818031311, -0.33681121468544006, -0.2852908670902252, -3.456049680709839]\n",
      "Grand sum of 1903 tensor sets is: [621.9541015625, 1557.6636962890625, -447.9661560058594, 60.37553024291992, -2861.532958984375]\n",
      "\n",
      "Instance 2708 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2709 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "forever at index 22: [-0.05781424045562744, 1.380739450454712, 0.7748735547065735, -0.14100658893585205, -1.358765959739685]\n",
      "Grand sum of 1904 tensor sets is: [621.8963012695312, 1559.04443359375, -447.1912841796875, 60.23452377319336, -2862.891845703125]\n",
      "\n",
      "Instance 2710 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 3: [1.7913411855697632, -1.5546380281448364, -0.6822367310523987, -0.8459237813949585, 0.4530629515647888]\n",
      "Grand sum of 1905 tensor sets is: [623.6876220703125, 1557.48974609375, -447.87353515625, 59.38859939575195, -2862.438720703125]\n",
      "\n",
      "Instance 2711 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "forever at index 6: [0.36479946970939636, 0.24682025611400604, -0.4848631024360657, -0.5248885154724121, -1.1551042795181274]\n",
      "Grand sum of 1906 tensor sets is: [624.0524291992188, 1557.736572265625, -448.3583984375, 58.863712310791016, -2863.59375]\n",
      "\n",
      "Instance 2712 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2713 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 5: [0.29076969623565674, 1.0662729740142822, -1.5838046073913574, -1.260906457901001, -4.8516621589660645]\n",
      "Grand sum of 1907 tensor sets is: [624.3432006835938, 1558.8028564453125, -449.94219970703125, 57.602806091308594, -2868.4453125]\n",
      "\n",
      "Instance 2714 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [41]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 41: [0.6255550384521484, -0.15466003119945526, -0.07857590168714523, 1.2068227529525757, 2.083484649658203]\n",
      "Grand sum of 1908 tensor sets is: [624.96875, 1558.648193359375, -450.0207824707031, 58.809627532958984, -2866.36181640625]\n",
      "\n",
      "Instance 2715 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [46]\n",
      "Size of token embeddings is torch.Size([121, 13, 768])\n",
      "Shape of summed layers is: 121 x 768\n",
      "forever at index 46: [0.41324031352996826, 1.1886684894561768, -0.8633221387863159, -0.13346849381923676, -1.5512607097625732]\n",
      "Grand sum of 1909 tensor sets is: [625.3820190429688, 1559.8369140625, -450.88409423828125, 58.6761589050293, -2867.9130859375]\n",
      "\n",
      "Instance 2716 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2717 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [72]\n",
      "Size of token embeddings is torch.Size([85, 13, 768])\n",
      "Shape of summed layers is: 85 x 768\n",
      "forever at index 72: [-0.6354273557662964, 1.7217719554901123, 0.7325127124786377, 1.305780053138733, -0.17672330141067505]\n",
      "Grand sum of 1910 tensor sets is: [624.74658203125, 1561.5587158203125, -450.1515808105469, 59.981937408447266, -2868.08984375]\n",
      "\n",
      "Instance 2718 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2719 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 6: [1.6201378107070923, -0.5744689702987671, -0.7070295214653015, -0.005738990381360054, 2.0631537437438965]\n",
      "Grand sum of 1911 tensor sets is: [626.36669921875, 1560.9842529296875, -450.8586120605469, 59.976200103759766, -2866.026611328125]\n",
      "\n",
      "Instance 2720 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 22: [0.2165706604719162, 2.807011127471924, -0.6104094386100769, -1.5591027736663818, -1.324192762374878]\n",
      "Grand sum of 1912 tensor sets is: [626.583251953125, 1563.791259765625, -451.4690246582031, 58.41709899902344, -2867.350830078125]\n",
      "\n",
      "Instance 2721 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2722 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 9: [0.2615699768066406, -0.8733716607093811, -0.330057829618454, 0.8237425684928894, 2.332779884338379]\n",
      "Grand sum of 1913 tensor sets is: [626.8448486328125, 1562.9178466796875, -451.799072265625, 59.240840911865234, -2865.01806640625]\n",
      "\n",
      "Instance 2723 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "forever at index 7: [0.41262415051460266, 0.31489428877830505, -0.8894286751747131, -0.6596914529800415, 1.0049713850021362]\n",
      "Grand sum of 1914 tensor sets is: [627.2574462890625, 1563.2327880859375, -452.6885070800781, 58.58115005493164, -2864.01318359375]\n",
      "\n",
      "Instance 2724 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 9: [0.16780094802379608, 0.3673933148384094, -0.6137426495552063, 1.908109426498413, -4.417931079864502]\n",
      "Grand sum of 1915 tensor sets is: [627.4252319335938, 1563.6002197265625, -453.30224609375, 60.4892578125, -2868.43115234375]\n",
      "\n",
      "Instance 2725 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 7: [0.7759811878204346, 0.9266924858093262, -0.5942471623420715, -1.026684284210205, -1.78507399559021]\n",
      "Grand sum of 1916 tensor sets is: [628.2012329101562, 1564.52685546875, -453.896484375, 59.46257400512695, -2870.21630859375]\n",
      "\n",
      "Instance 2726 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "forever at index 27: [0.44000712037086487, -0.2536422610282898, -0.2801014184951782, -1.110748291015625, 0.5492421984672546]\n",
      "Grand sum of 1917 tensor sets is: [628.6412353515625, 1564.273193359375, -454.17657470703125, 58.35182571411133, -2869.6669921875]\n",
      "\n",
      "Instance 2727 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 11: [1.497785210609436, 2.8504326343536377, -0.6869360208511353, 2.3454999923706055, 0.24576565623283386]\n",
      "Grand sum of 1918 tensor sets is: [630.1390380859375, 1567.1236572265625, -454.863525390625, 60.69732666015625, -2869.421142578125]\n",
      "\n",
      "Instance 2728 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2729 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 3: [0.6909928321838379, 1.617631435394287, 0.18570426106452942, 0.6482849717140198, 2.116894483566284]\n",
      "Grand sum of 1919 tensor sets is: [630.8300170898438, 1568.7413330078125, -454.6778259277344, 61.345611572265625, -2867.30419921875]\n",
      "\n",
      "Instance 2730 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2731 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "forever at index 18: [0.1960027813911438, 0.2730625867843628, -0.4758768081665039, 0.21202661097049713, -5.125371932983398]\n",
      "Grand sum of 1920 tensor sets is: [631.0260009765625, 1569.014404296875, -455.1537170410156, 61.557640075683594, -2872.4296875]\n",
      "\n",
      "Instance 2732 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2733 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 15: [0.1854611039161682, -0.5264131426811218, -0.23824426531791687, 0.39875519275665283, -2.1095895767211914]\n",
      "Grand sum of 1921 tensor sets is: [631.2114868164062, 1568.488037109375, -455.3919677734375, 61.95639419555664, -2874.539306640625]\n",
      "\n",
      "Instance 2734 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9, 16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "forever at index 9: [-0.14892971515655518, 1.0169497728347778, -1.1009567975997925, 1.5566856861114502, -2.6457858085632324]\n",
      "forever at index 16: [-0.7121927738189697, -0.19312229752540588, 1.4129095077514648, -1.2704696655273438, -2.7356905937194824]\n",
      "Grand sum of 1922 tensor sets is: [630.7809448242188, 1568.89990234375, -455.2359924316406, 62.09950256347656, -2877.22998046875]\n",
      "\n",
      "Instance 2735 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 9: [0.7601019144058228, 1.7670985460281372, 0.6998885273933411, 1.0222638845443726, -0.9792816638946533]\n",
      "Grand sum of 1923 tensor sets is: [631.5410766601562, 1570.6669921875, -454.5361022949219, 63.12176513671875, -2878.209228515625]\n",
      "\n",
      "Instance 2736 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2737 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 11: [0.47724905610084534, 1.7305452823638916, -0.11443791538476944, -1.6825122833251953, -4.202861309051514]\n",
      "Grand sum of 1924 tensor sets is: [632.018310546875, 1572.3975830078125, -454.6505432128906, 61.43925476074219, -2882.412109375]\n",
      "\n",
      "Instance 2738 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 9: [1.565279245376587, 0.04754091799259186, -0.5471208691596985, -1.221991777420044, -1.8230191469192505]\n",
      "Grand sum of 1925 tensor sets is: [633.5836181640625, 1572.445068359375, -455.1976623535156, 60.217262268066406, -2884.235107421875]\n",
      "\n",
      "Instance 2739 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "forever at index 19: [0.8377693295478821, 1.0446984767913818, -0.22073562443256378, 0.8123583793640137, -0.9855910539627075]\n",
      "Grand sum of 1926 tensor sets is: [634.42138671875, 1573.48974609375, -455.41839599609375, 61.02962112426758, -2885.220703125]\n",
      "\n",
      "Instance 2740 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 5: [0.07980796694755554, 1.091821551322937, -1.3777132034301758, -0.11967724561691284, -2.6841390132904053]\n",
      "Grand sum of 1927 tensor sets is: [634.501220703125, 1574.58154296875, -456.7961120605469, 60.909942626953125, -2887.90478515625]\n",
      "\n",
      "Instance 2741 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2742 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 14: [0.601614773273468, 0.8601912260055542, -0.9699770212173462, -0.7300403714179993, -5.347193717956543]\n",
      "Grand sum of 1928 tensor sets is: [635.1028442382812, 1575.4417724609375, -457.7660827636719, 60.179901123046875, -2893.251953125]\n",
      "\n",
      "Instance 2743 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2744 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "forever at index 26: [0.8051968216896057, 1.3902976512908936, -0.26105159521102905, 1.6370011568069458, -3.1982784271240234]\n",
      "Grand sum of 1929 tensor sets is: [635.9080200195312, 1576.83203125, -458.0271301269531, 61.81690216064453, -2896.4501953125]\n",
      "\n",
      "Instance 2745 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 65: [-0.4072004556655884, -1.165315866470337, -1.116049885749817, -1.6974555253982544, -1.7651728391647339]\n",
      "Grand sum of 1930 tensor sets is: [635.5007934570312, 1575.666748046875, -459.1431884765625, 60.11944580078125, -2898.21533203125]\n",
      "\n",
      "Instance 2746 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30, 64]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "forever at index 30: [0.5711737275123596, 0.4939953088760376, 1.285870909690857, 0.41039711236953735, -0.629509449005127]\n",
      "forever at index 64: [0.479422926902771, 0.19982075691223145, 1.3869476318359375, 0.12268781661987305, -0.7783094644546509]\n",
      "Grand sum of 1931 tensor sets is: [636.0260620117188, 1576.013671875, -457.8067932128906, 60.385990142822266, -2898.919189453125]\n",
      "\n",
      "Instance 2747 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 10: [0.02371002733707428, -0.2126496583223343, 0.5296247005462646, -1.9451913833618164, -3.6411209106445312]\n",
      "Grand sum of 1932 tensor sets is: [636.0497436523438, 1575.801025390625, -457.27716064453125, 58.440799713134766, -2902.560302734375]\n",
      "\n",
      "Instance 2748 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2749 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 23: [-1.4012020826339722, 2.0466442108154297, -1.8467037677764893, 1.689387321472168, -0.9971396923065186]\n",
      "Grand sum of 1933 tensor sets is: [634.6485595703125, 1577.84765625, -459.1238708496094, 60.13018798828125, -2903.557373046875]\n",
      "\n",
      "Instance 2750 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 7: [1.1482813358306885, 0.3194982409477234, -0.6874419450759888, 0.4948996305465698, -1.9417659044265747]\n",
      "Grand sum of 1934 tensor sets is: [635.7968139648438, 1578.1671142578125, -459.8113098144531, 60.62508773803711, -2905.4990234375]\n",
      "\n",
      "Instance 2751 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "forever at index 9: [0.20841796696186066, 1.8051385879516602, -0.21306222677230835, 1.9995012283325195, -0.5898510217666626]\n",
      "Grand sum of 1935 tensor sets is: [636.0052490234375, 1579.9722900390625, -460.0243835449219, 62.62458801269531, -2906.0888671875]\n",
      "\n",
      "Instance 2752 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 3: [0.6744242906570435, -0.3577236533164978, -1.4759751558303833, -0.46071290969848633, -0.6701667308807373]\n",
      "Grand sum of 1936 tensor sets is: [636.6796875, 1579.6146240234375, -461.5003662109375, 62.163875579833984, -2906.759033203125]\n",
      "\n",
      "Instance 2753 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 28: [-0.06974990665912628, 2.3678395748138428, -0.41298791766166687, 0.5929281711578369, -0.6036150455474854]\n",
      "Grand sum of 1937 tensor sets is: [636.6099243164062, 1581.982421875, -461.9133605957031, 62.756805419921875, -2907.362548828125]\n",
      "\n",
      "Instance 2754 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [65]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "forever at index 65: [-0.4072004556655884, -1.165315866470337, -1.116049885749817, -1.6974555253982544, -1.7651728391647339]\n",
      "Grand sum of 1938 tensor sets is: [636.2026977539062, 1580.817138671875, -463.0294189453125, 61.059349060058594, -2909.127685546875]\n",
      "\n",
      "Instance 2755 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "forever at index 45: [0.15917105972766876, 1.8139777183532715, -1.5263479948043823, 0.43083229660987854, -5.921927452087402]\n",
      "Grand sum of 1939 tensor sets is: [636.3618774414062, 1582.631103515625, -464.5557556152344, 61.49018096923828, -2915.049560546875]\n",
      "\n",
      "Instance 2756 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "forever at index 15: [-0.4766021966934204, 1.74265718460083, 0.30620285868644714, 0.2167445719242096, -3.2190253734588623]\n",
      "Grand sum of 1940 tensor sets is: [635.88525390625, 1584.373779296875, -464.2495422363281, 61.70692443847656, -2918.2685546875]\n",
      "\n",
      "Instance 2757 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([165, 13, 768])\n",
      "Shape of summed layers is: 165 x 768\n",
      "forever at index 149: [0.7540604472160339, -0.5604724884033203, 0.2821454405784607, 0.6984797120094299, -2.0526654720306396]\n",
      "Grand sum of 1941 tensor sets is: [636.6393432617188, 1583.8133544921875, -463.9674072265625, 62.40540313720703, -2920.3212890625]\n",
      "\n",
      "Instance 2758 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 4: [0.36889582872390747, 2.0175302028656006, -0.3118392527103424, 1.0668292045593262, -0.3380414545536041]\n",
      "Grand sum of 1942 tensor sets is: [637.0082397460938, 1585.8309326171875, -464.27923583984375, 63.472232818603516, -2920.659423828125]\n",
      "\n",
      "Instance 2759 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2760 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 18: [0.10141880065202713, 1.7393444776535034, -0.6380208134651184, -1.1153829097747803, -4.296627044677734]\n",
      "Grand sum of 1943 tensor sets is: [637.1096801757812, 1587.5703125, -464.9172668457031, 62.356849670410156, -2924.9560546875]\n",
      "\n",
      "Instance 2761 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [0.10969367623329163, -0.8901700377464294, 0.6898185014724731, -0.22140386700630188, -2.358600378036499]\n",
      "Grand sum of 1944 tensor sets is: [637.2193603515625, 1586.68017578125, -464.2274475097656, 62.13544464111328, -2927.314697265625]\n",
      "\n",
      "Instance 2762 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 6: [1.1830281019210815, 1.9589743614196777, 0.3243868052959442, -1.808610200881958, 0.6842765808105469]\n",
      "Grand sum of 1945 tensor sets is: [638.4024047851562, 1588.63916015625, -463.9030456542969, 60.32683563232422, -2926.63037109375]\n",
      "\n",
      "Instance 2763 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2764 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [75, 175]\n",
      "Size of token embeddings is torch.Size([296, 13, 768])\n",
      "Shape of summed layers is: 296 x 768\n",
      "forever at index 75: [1.5451886653900146, -1.4026647806167603, 0.8106837272644043, -0.22570247948169708, -5.685906887054443]\n",
      "forever at index 175: [1.0839743614196777, -0.5884178280830383, 0.9411699771881104, 0.29869818687438965, -5.385408401489258]\n",
      "Grand sum of 1946 tensor sets is: [639.7169799804688, 1587.6436767578125, -463.0271301269531, 60.36333465576172, -2932.166015625]\n",
      "\n",
      "Instance 2765 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([8, 13, 768])\n",
      "Shape of summed layers is: 8 x 768\n",
      "forever at index 3: [0.32117903232574463, 0.23560544848442078, -0.000872097909450531, -0.8400305509567261, -2.1225438117980957]\n",
      "Grand sum of 1947 tensor sets is: [640.0381469726562, 1587.8792724609375, -463.02801513671875, 59.5233039855957, -2934.28857421875]\n",
      "\n",
      "Instance 2766 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [131]\n",
      "Size of token embeddings is torch.Size([207, 13, 768])\n",
      "Shape of summed layers is: 207 x 768\n",
      "forever at index 131: [0.44415682554244995, 0.3233512043952942, -0.06486895680427551, -0.16544480621814728, -1.6536425352096558]\n",
      "Grand sum of 1948 tensor sets is: [640.4822998046875, 1588.20263671875, -463.0928955078125, 59.35786056518555, -2935.942138671875]\n",
      "\n",
      "Instance 2767 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2768 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2769 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 3: [0.32994553446769714, 0.18613892793655396, -0.5860008597373962, 0.7834742069244385, 1.6237897872924805]\n",
      "Grand sum of 1949 tensor sets is: [640.812255859375, 1588.3887939453125, -463.67889404296875, 60.141334533691406, -2934.318359375]\n",
      "\n",
      "Instance 2770 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 9: [0.16780094802379608, 0.3673933148384094, -0.6137426495552063, 1.908109426498413, -4.417931079864502]\n",
      "Grand sum of 1950 tensor sets is: [640.9800415039062, 1588.7562255859375, -464.2926330566406, 62.049442291259766, -2938.736328125]\n",
      "\n",
      "Instance 2771 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [51]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "forever at index 51: [-0.12422116100788116, 1.841947078704834, 0.1956890970468521, 0.5813219547271729, 0.3082122504711151]\n",
      "Grand sum of 1951 tensor sets is: [640.8558349609375, 1590.59814453125, -464.0969543457031, 62.63076400756836, -2938.42822265625]\n",
      "\n",
      "Instance 2772 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [1.0301811695098877, 0.09672722220420837, -1.850982904434204, 0.9610074162483215, -2.2480239868164062]\n",
      "Grand sum of 1952 tensor sets is: [641.885986328125, 1590.69482421875, -465.94793701171875, 63.59177017211914, -2940.67626953125]\n",
      "\n",
      "Instance 2773 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [51]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "forever at index 51: [-0.14476311206817627, 0.22544246912002563, -0.34030580520629883, 0.2974168658256531, -1.3580951690673828]\n",
      "Grand sum of 1953 tensor sets is: [641.7412109375, 1590.9202880859375, -466.2882385253906, 63.88918685913086, -2942.034423828125]\n",
      "\n",
      "Instance 2774 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2775 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 34: [0.3250618577003479, -1.317368507385254, -0.30396685004234314, -1.2233247756958008, -1.6374026536941528]\n",
      "Grand sum of 1954 tensor sets is: [642.0662841796875, 1589.6029052734375, -466.5921936035156, 62.665863037109375, -2943.671875]\n",
      "\n",
      "Instance 2776 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2777 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "forever at index 26: [1.3161708116531372, 0.7910457253456116, 0.03404364734888077, -0.2547284960746765, -6.300019264221191]\n",
      "Grand sum of 1955 tensor sets is: [643.3824462890625, 1590.3939208984375, -466.5581359863281, 62.4111328125, -2949.971923828125]\n",
      "\n",
      "Instance 2778 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 1956 tensor sets is: [643.7781372070312, 1591.402587890625, -466.6277160644531, 61.727569580078125, -2948.675537109375]\n",
      "\n",
      "Instance 2779 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2780 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2781 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 13: [0.2610348165035248, 0.9071619510650635, -1.5011452436447144, 0.12028273940086365, -3.6704349517822266]\n",
      "Grand sum of 1957 tensor sets is: [644.0391845703125, 1592.3096923828125, -468.1288757324219, 61.84785079956055, -2952.345947265625]\n",
      "\n",
      "Instance 2782 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [47]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 47: [0.6362676620483398, 1.3238205909729004, -0.721892774105072, -1.420104742050171, -2.0686702728271484]\n",
      "Grand sum of 1958 tensor sets is: [644.6754760742188, 1593.633544921875, -468.85076904296875, 60.4277458190918, -2954.41455078125]\n",
      "\n",
      "Instance 2783 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "forever at index 32: [0.17908090353012085, 2.336167097091675, 0.1324176788330078, -1.2805347442626953, -0.7550381422042847]\n",
      "Grand sum of 1959 tensor sets is: [644.8545532226562, 1595.9697265625, -468.7183532714844, 59.14720916748047, -2955.169677734375]\n",
      "\n",
      "Instance 2784 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 8: [0.4342479109764099, 1.2866073846817017, -0.09902036190032959, -1.8417854309082031, 0.1676633208990097]\n",
      "Grand sum of 1960 tensor sets is: [645.288818359375, 1597.25634765625, -468.8173828125, 57.305423736572266, -2955.001953125]\n",
      "\n",
      "Instance 2785 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 13: [0.6654954552650452, 0.12991851568222046, 0.7434297800064087, 1.5151540040969849, 1.9550411701202393]\n",
      "Grand sum of 1961 tensor sets is: [645.9542846679688, 1597.38623046875, -468.0739440917969, 58.820579528808594, -2953.046875]\n",
      "\n",
      "Instance 2786 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2787 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 3: [0.3994351923465729, 1.5764358043670654, -0.36014407873153687, 0.9022860527038574, -4.092756748199463]\n",
      "Grand sum of 1962 tensor sets is: [646.3536987304688, 1598.962646484375, -468.43408203125, 59.72286605834961, -2957.1396484375]\n",
      "\n",
      "Instance 2788 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 9: [-0.4678524136543274, 2.043095350265503, -0.6969066262245178, 1.2929551601409912, -2.59586238861084]\n",
      "Grand sum of 1963 tensor sets is: [645.8858642578125, 1601.0057373046875, -469.1309814453125, 61.01581954956055, -2959.735595703125]\n",
      "\n",
      "Instance 2789 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "forever at index 3: [1.3427575826644897, -0.7265383005142212, 0.4534558057785034, -1.0354959964752197, 1.510983943939209]\n",
      "Grand sum of 1964 tensor sets is: [647.2286376953125, 1600.2791748046875, -468.6775207519531, 59.980323791503906, -2958.224609375]\n",
      "\n",
      "Instance 2790 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "forever at index 7: [0.10393273085355759, 1.759730577468872, -1.1455589532852173, -0.6791334748268127, -3.535442352294922]\n",
      "Grand sum of 1965 tensor sets is: [647.3325805664062, 1602.0389404296875, -469.8230895996094, 59.30118942260742, -2961.760009765625]\n",
      "\n",
      "Instance 2791 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 11: [-0.007295828312635422, 1.662345290184021, -0.03417526185512543, -0.03740984946489334, 0.3787590563297272]\n",
      "Grand sum of 1966 tensor sets is: [647.3252563476562, 1603.7012939453125, -469.8572692871094, 59.26377868652344, -2961.38134765625]\n",
      "\n",
      "Instance 2792 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 3: [0.790726363658905, 2.432720184326172, -0.383671373128891, 0.5971522331237793, -0.4804054796695709]\n",
      "Grand sum of 1967 tensor sets is: [648.115966796875, 1606.134033203125, -470.2409362792969, 59.860931396484375, -2961.86181640625]\n",
      "\n",
      "Instance 2793 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 9: [0.2437722086906433, 0.5364900827407837, 0.05812656134366989, 0.039083242416381836, 0.11338984966278076]\n",
      "Grand sum of 1968 tensor sets is: [648.3597412109375, 1606.6705322265625, -470.18280029296875, 59.9000129699707, -2961.74853515625]\n",
      "\n",
      "Instance 2794 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 23: [0.8983327746391296, 1.7763617038726807, -1.1277590990066528, 0.08003230392932892, -0.9232195615768433]\n",
      "Grand sum of 1969 tensor sets is: [649.258056640625, 1608.4468994140625, -471.310546875, 59.980045318603516, -2962.671875]\n",
      "\n",
      "Instance 2795 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2796 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 3: [1.1376326084136963, -0.5165720582008362, 0.21537081897258759, -0.1489756554365158, 0.17728225886821747]\n",
      "Grand sum of 1970 tensor sets is: [650.3956909179688, 1607.9302978515625, -471.0951843261719, 59.83106994628906, -2962.49462890625]\n",
      "\n",
      "Instance 2797 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "forever at index 22: [0.8386253118515015, 0.35598382353782654, -0.4016624987125397, 0.5136248469352722, 2.539083957672119]\n",
      "Grand sum of 1971 tensor sets is: [651.2343139648438, 1608.2862548828125, -471.4968566894531, 60.344696044921875, -2959.95556640625]\n",
      "\n",
      "Instance 2798 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2799 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2800 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "forever at index 12: [0.8131793737411499, -0.11790712177753448, -0.2667977213859558, -1.756739854812622, 1.1042990684509277]\n",
      "Grand sum of 1972 tensor sets is: [652.0474853515625, 1608.1683349609375, -471.7636413574219, 58.587955474853516, -2958.851318359375]\n",
      "\n",
      "Instance 2801 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 4: [-0.22814792394638062, -0.5612687468528748, -0.8439787030220032, -1.4865918159484863, -2.5202507972717285]\n",
      "Grand sum of 1973 tensor sets is: [651.8193359375, 1607.6070556640625, -472.60760498046875, 57.10136413574219, -2961.37158203125]\n",
      "\n",
      "Instance 2802 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "forever at index 5: [-0.38718658685684204, -0.7276805639266968, -0.4920441210269928, -0.22040963172912598, 0.40566539764404297]\n",
      "Grand sum of 1974 tensor sets is: [651.43212890625, 1606.87939453125, -473.0996398925781, 56.88095474243164, -2960.9658203125]\n",
      "\n",
      "Instance 2803 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 7: [0.29507502913475037, 1.8510929346084595, -0.01439964771270752, 0.17857331037521362, -4.131156921386719]\n",
      "Grand sum of 1975 tensor sets is: [651.7272338867188, 1608.73046875, -473.1140441894531, 57.05952835083008, -2965.096923828125]\n",
      "\n",
      "Instance 2804 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2805 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [51]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "forever at index 51: [0.06289482116699219, 0.4586185812950134, -0.5497884750366211, 1.4579877853393555, -3.7321877479553223]\n",
      "Grand sum of 1976 tensor sets is: [651.7901000976562, 1609.1890869140625, -473.663818359375, 58.51751708984375, -2968.8291015625]\n",
      "\n",
      "Instance 2806 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "forever at index 4: [0.3956987261772156, 1.0086688995361328, -0.06957342475652695, -0.6835622787475586, 1.2963086366653442]\n",
      "Grand sum of 1977 tensor sets is: [652.185791015625, 1610.19775390625, -473.7333984375, 57.833953857421875, -2967.53271484375]\n",
      "\n",
      "Instance 2807 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 7: [1.2908999919891357, 3.1583690643310547, -0.4833223521709442, -1.5544908046722412, -0.9892051815986633]\n",
      "Grand sum of 1978 tensor sets is: [653.4766845703125, 1613.3560791015625, -474.21673583984375, 56.27946472167969, -2968.52197265625]\n",
      "\n",
      "Instance 2808 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2809 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2810 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "forever at index 24: [0.9730425477027893, 0.2309516817331314, 0.7920351624488831, 0.600679874420166, -3.5121421813964844]\n",
      "Grand sum of 1979 tensor sets is: [654.44970703125, 1613.5870361328125, -473.4247131347656, 56.88014602661133, -2972.0341796875]\n",
      "\n",
      "Instance 2811 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [54]\n",
      "Size of token embeddings is torch.Size([77, 13, 768])\n",
      "Shape of summed layers is: 77 x 768\n",
      "forever at index 54: [-0.05395650863647461, 0.11204768717288971, -0.6998974680900574, 0.836296796798706, -0.08169679343700409]\n",
      "Grand sum of 1980 tensor sets is: [654.395751953125, 1613.6990966796875, -474.1246032714844, 57.7164421081543, -2972.115966796875]\n",
      "\n",
      "Instance 2812 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2813 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "forever at index 30: [0.485734224319458, 1.011696457862854, -0.3912021815776825, 0.865526556968689, -1.632388710975647]\n",
      "Grand sum of 1981 tensor sets is: [654.8814697265625, 1614.7108154296875, -474.51580810546875, 58.58197021484375, -2973.748291015625]\n",
      "\n",
      "Instance 2814 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [80]\n",
      "Size of token embeddings is torch.Size([83, 13, 768])\n",
      "Shape of summed layers is: 83 x 768\n",
      "forever at index 80: [0.990983247756958, 1.5596613883972168, -0.3734959065914154, 2.611299514770508, 4.079904556274414]\n",
      "Grand sum of 1982 tensor sets is: [655.8724365234375, 1616.2705078125, -474.8893127441406, 61.193267822265625, -2969.66845703125]\n",
      "\n",
      "Instance 2815 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2816 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([7, 13, 768])\n",
      "Shape of summed layers is: 7 x 768\n",
      "forever at index 3: [1.9713290929794312, -0.3095471262931824, -0.7405399084091187, -0.18016651272773743, 1.4263298511505127]\n",
      "Grand sum of 1983 tensor sets is: [657.84375, 1615.9609375, -475.6298522949219, 61.013099670410156, -2968.2421875]\n",
      "\n",
      "Instance 2817 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "forever at index 8: [0.14286170899868011, 0.6601489782333374, -1.9007694721221924, 0.5324844121932983, -2.996035575866699]\n",
      "Grand sum of 1984 tensor sets is: [657.9866333007812, 1616.62109375, -477.5306091308594, 61.54558563232422, -2971.23828125]\n",
      "\n",
      "Instance 2818 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [4, 12]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "forever at index 4: [0.19769474864006042, 0.9239522218704224, 0.0995880514383316, -0.08179770410060883, 0.22700658440589905]\n",
      "forever at index 12: [0.6060097217559814, 0.5739008784294128, 0.5269936323165894, -1.1561846733093262, -0.11992436647415161]\n",
      "Grand sum of 1985 tensor sets is: [658.3884887695312, 1617.3699951171875, -477.2173156738281, 60.92659378051758, -2971.184814453125]\n",
      "\n",
      "Instance 2819 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [69]\n",
      "Size of token embeddings is torch.Size([92, 13, 768])\n",
      "Shape of summed layers is: 92 x 768\n",
      "forever at index 69: [-0.16531673073768616, 1.34368097782135, -0.46323028206825256, -0.35911324620246887, 0.27292829751968384]\n",
      "Grand sum of 1986 tensor sets is: [658.22314453125, 1618.713623046875, -477.6805419921875, 60.567481994628906, -2970.911865234375]\n",
      "\n",
      "Instance 2820 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "forever at index 11: [0.30160099267959595, 0.06564424932003021, -0.24079585075378418, 0.10302742570638657, -1.6318563222885132]\n",
      "Grand sum of 1987 tensor sets is: [658.5247192382812, 1618.779296875, -477.92132568359375, 60.670509338378906, -2972.543701171875]\n",
      "\n",
      "Instance 2821 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "forever at index 5: [0.21831780672073364, -0.11492738127708435, -0.5116299986839294, 0.6557007431983948, -2.333277940750122]\n",
      "Grand sum of 1988 tensor sets is: [658.7430419921875, 1618.6644287109375, -478.4329528808594, 61.326210021972656, -2974.876953125]\n",
      "\n",
      "Instance 2822 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2823 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "forever at index 31: [0.5424565672874451, 1.5214838981628418, 0.23401018977165222, -0.9015097618103027, -0.6216533184051514]\n",
      "Grand sum of 1989 tensor sets is: [659.2855224609375, 1620.1859130859375, -478.1989440917969, 60.42470169067383, -2975.49853515625]\n",
      "\n",
      "Instance 2824 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 13: [-0.7347370982170105, 0.40485069155693054, 0.3916848301887512, -0.35705989599227905, -4.5608367919921875]\n",
      "Grand sum of 1990 tensor sets is: [658.55078125, 1620.5908203125, -477.8072509765625, 60.06764221191406, -2980.059326171875]\n",
      "\n",
      "Instance 2825 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "forever at index 8: [-0.42821425199508667, 1.714135766029358, -0.39708155393600464, -1.5474927425384521, -2.7945683002471924]\n",
      "Grand sum of 1991 tensor sets is: [658.12255859375, 1622.304931640625, -478.204345703125, 58.52014923095703, -2982.85400390625]\n",
      "\n",
      "Instance 2826 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2827 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2828 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "forever at index 11: [-0.1492677628993988, 3.0590031147003174, -0.27470752596855164, -0.5988104343414307, -1.2925117015838623]\n",
      "Grand sum of 1992 tensor sets is: [657.9732666015625, 1625.3638916015625, -478.47906494140625, 57.92133712768555, -2984.146484375]\n",
      "\n",
      "Instance 2829 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "forever at index 17: [-0.11339549720287323, 0.5605106949806213, -1.0170693397521973, -0.36234456300735474, -0.7534738183021545]\n",
      "Grand sum of 1993 tensor sets is: [657.85986328125, 1625.9244384765625, -479.4961242675781, 57.55899429321289, -2984.89990234375]\n",
      "\n",
      "Instance 2830 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "forever at index 8: [-0.582751989364624, -0.27392667531967163, -1.0126334428787231, 0.5255881547927856, -3.18249249458313]\n",
      "Grand sum of 1994 tensor sets is: [657.277099609375, 1625.6505126953125, -480.5087585449219, 58.0845832824707, -2988.082275390625]\n",
      "\n",
      "Instance 2831 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [58]\n",
      "Size of token embeddings is torch.Size([84, 13, 768])\n",
      "Shape of summed layers is: 84 x 768\n",
      "forever at index 58: [-0.7161506414413452, -0.32668977975845337, -1.289252519607544, 1.0085606575012207, -0.34214338660240173]\n",
      "Grand sum of 1995 tensor sets is: [656.5609741210938, 1625.3238525390625, -481.7980041503906, 59.093143463134766, -2988.42431640625]\n",
      "\n",
      "Instance 2832 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2833 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2834 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "forever at index 23: [-0.6013897657394409, 0.2933318614959717, -1.5900779962539673, 0.1969842165708542, -5.368989944458008]\n",
      "Grand sum of 1996 tensor sets is: [655.9595947265625, 1625.6171875, -483.3880920410156, 59.29012680053711, -2993.793212890625]\n",
      "\n",
      "Instance 2835 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2836 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2837 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2838 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "forever at index 10: [0.8496501445770264, 0.15791328251361847, -0.11639366298913956, -0.46939951181411743, -2.529433488845825]\n",
      "Grand sum of 1997 tensor sets is: [656.8092651367188, 1625.775146484375, -483.5044860839844, 58.82072830200195, -2996.32275390625]\n",
      "\n",
      "Instance 2839 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([5, 13, 768])\n",
      "Shape of summed layers is: 5 x 768\n",
      "forever at index 3: [-0.7470606565475464, 0.510550320148468, 0.0043852925300598145, -1.135771632194519, 0.26709645986557007]\n",
      "Grand sum of 1998 tensor sets is: [656.0621948242188, 1626.28564453125, -483.5000915527344, 57.68495559692383, -2996.0556640625]\n",
      "\n",
      "Instance 2840 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([9, 13, 768])\n",
      "Shape of summed layers is: 9 x 768\n",
      "forever at index 3: [0.5422123670578003, -0.37714433670043945, -1.0815092325210571, 0.34070926904678345, -2.6716675758361816]\n",
      "Grand sum of 1999 tensor sets is: [656.6044311523438, 1625.908447265625, -484.58160400390625, 58.025665283203125, -2998.727294921875]\n",
      "\n",
      "Instance 2841 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2842 of forever.\n",
      "Looking for vocab token: forever\n",
      "\n",
      "Instance 2843 of forever.\n",
      "Looking for vocab token: forever\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "forever at index 16: [-0.07950429618358612, 2.501434087753296, -0.28149083256721497, 1.8005561828613281, -2.0686068534851074]\n",
      "Grand sum of 2000 tensor sets is: [656.52490234375, 1628.409912109375, -484.86309814453125, 59.82622146606445, -3000.7958984375]\n",
      "Mean of tensors is: tensor([ 0.3283,  0.8142, -0.2424,  0.0299, -1.5004]) (768 features in tensor)\n",
      "Saved the embedding for forever.\n",
      "Saved the count of sentences used to create forever embedding\n",
      "Run time for forever was 315.0940584214404 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Process vocabulary words in the outer loop.\n",
    "for v in vocab:\n",
    "    start = timer()\n",
    "    with open(context_file, 'r') as lines:\n",
    "        v_sum = torch.zeros([1, FEATURE_COUNT])\n",
    "        v_tokens = tokenizer.encode(v)\n",
    "        print(f'\\nThere are {len(v_tokens) - 2} tokens in tokenized vocabulary word:')\n",
    "        for t in v_tokens[1:-1]:\n",
    "            print(tokenizer.decode(t).strip())\n",
    "        count_sentence = 0\n",
    "        count_tensor = 0\n",
    "        \n",
    "        # Process all lines in the context file in the inner loop.\n",
    "        for line in lines:\n",
    "            # Check for this vocab word in this line; if found, split the line into individual sentences.\n",
    "            if v in line.lower().split():\n",
    "                for sentence in line.split('.'):\n",
    "                    if v in sentence.lower():\n",
    "                        line = sentence\n",
    "                        count_sentence += 1\n",
    "                        print(f'\\nInstance {count_sentence} of {tokenizer.decode(v_tokens[1:-1]).strip()}.')\n",
    "                        break\n",
    "                # Split the new sentence-based line into tokens.\n",
    "                # Use max_length to avoid overflowing the maximum sequence length for the model.\n",
    "                tokenized_text = tokenizer.encode(line, add_special_tokens=True, max_length=512)\n",
    "#                 print(f'The decoded sentence has {len(tokenized_text)} tokens and is: {tokenizer.decode(tokenized_text)}')\n",
    "                indices = []              \n",
    "\n",
    "                # Check to see whether the vocab word is found in this particular line.\n",
    "                # Initially, some lines may have comprised multiple sentences, which were\n",
    "                # broken out individually above.\n",
    "                for t in v_tokens[1:-1]:\n",
    "                    print(f'Looking for vocab token: {tokenizer.decode(t).strip()}')\n",
    "                    for i, token_str in enumerate(tokenized_text):\n",
    "#                         print(f'Next sentence token: {tokenizer.decode(token_str).strip()}')\n",
    "#                         print(tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip())\n",
    "                        if tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip():\n",
    "                            indices.append(i)               \n",
    "\n",
    "                ###################################################################################\n",
    "                # If the vocabulary word was found, process the containing line.\n",
    "                if indices:\n",
    "\n",
    "                    # The vocab word was found in this line/sentence, at the locations in indices.\n",
    "                    print(f'Indices are {indices}')\n",
    "                    # Get the feature vectors for all tokens in the line/sentence.\n",
    "                    token_embeddings = create_token_embeddings(tokenized_text)\n",
    "                    # Sum the last four layers to get embeddings for the line/sentence.\n",
    "#                         for t in v_tokens[1:-1]:\n",
    "#                             for i, token_str in enumerate(tokenized_text):\n",
    "#                                 if (tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip()):\n",
    "#                                     print(f'{tokenizer.decode(token_str).strip()} is index {i} in the sentence and {token_str} in the vocabulary.')\n",
    "                    token_vecs_layer = sum_last_four_token_vecs(token_embeddings)\n",
    "\n",
    "                    # Get the vocab word's contextual embedding for this line.\n",
    "                    tensor_layer = torch.zeros([1, FEATURE_COUNT])\n",
    "                    for i in range(len(indices)):\n",
    "                        v_index = i % len(v_tokens[1:-1])\n",
    "                        print(f'{tokenizer.decode(v_tokens[v_index + 1]).strip()} at index {indices[i]}: {token_vecs_layer[indices[i]][:5].tolist()}')\n",
    "                        tensor_layer += token_vecs_layer[indices[i]]\n",
    "#                         print(f'Sum of tensors is: {tensor_layer[0][:5].tolist()} before taking the mean.')\n",
    "\n",
    "                    # If our vocab word is broken into more than one token, we need to get the mean of the token embeddings.\n",
    "                    tensor_layer /= len(indices)\n",
    "#                     print(f'Sum of tensors is: {tensor_layer[0][:5].tolist()} after taking the mean.')\n",
    "\n",
    "                    # Add the embedding distilled from this line to the sum of embeddings for all lines.\n",
    "                    v_sum += tensor_layer\n",
    "                    count_tensor += 1\n",
    "                    print(f'Grand sum of {count_tensor} tensor sets is: {v_sum[0][:5].tolist()}')\n",
    "                ###################################################################################\n",
    "            # Stop processing lines once we've found 2000 instances of our vocab word.\n",
    "            if count_tensor >= 2000:\n",
    "                break\n",
    "        \n",
    "        # We're done processing all lines of 512 tokens or less containing our vocab word.\n",
    "        # Get the mean embedding for the word.\n",
    "        v_mean = v_sum / count_tensor\n",
    "        print(f'Mean of tensors is: {v_mean[0][:5]} ({len(v_mean[0])} features in tensor)')\n",
    "        write_embedding(output_file, v, v_mean)\n",
    "        try:\n",
    "            with open(count_file, 'a') as counts:\n",
    "                counts.write(v + ', ' + str(count_tensor) + '\\n')\n",
    "            print(f'Saved the count of sentences used to create {v} embedding')\n",
    "        except:\n",
    "            print('Wha?! Could not write the sentence count.')\n",
    "    end = timer()\n",
    "    print(f'Run time for {v} was {end - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    vocab = []\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_token_embeddings(tokenized_text):\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)  # Batch size 1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "        encoded_layers = outputs[2]\n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        print(f'Size of token embeddings is {token_embeddings.size()}')\n",
    "        return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the last 4 layers' features\n",
    "def sum_last_four_token_vecs(token_embeddings):\n",
    "    token_vecs_sum_last_four = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_last_four.append(sum_vec)\n",
    "\n",
    "    print ('Shape of summed layers is: %d x %d' % (len(token_vecs_sum_last_four), len(token_vecs_sum_last_four[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_sum_last_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a single layer of the model.\n",
    "def get_layer_token_vecs(token_embeddings, layer_number):\n",
    "    token_vecs_layer = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        layer_vec = token[layer_number]\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_layer.append(layer_vec)\n",
    "\n",
    "    print ('Shape of summed layers is: %d x %d' % (len(token_vecs_layer), len(token_vecs_layer[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "        print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal-venv-3.6",
   "language": "python",
   "name": "crystal-venv-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
