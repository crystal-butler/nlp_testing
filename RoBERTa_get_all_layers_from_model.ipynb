{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, RobertaConfig\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/Notebooks/crystal/NLP/transformers/examples'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we're in the transformers directory with fine-tuned model output.\n",
    "os.chdir('/home/jupyter/Notebooks/crystal/NLP/transformers/examples/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# and Transformers documentation: https://huggingface.co/transformers/model_doc/roberta.html#robertaformaskedlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-034b968488b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcount_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvocab_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/data/vocab_files/FE_vocab_study.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mFEATURE_COUNT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('./roBERTa_base/')\n",
    "config = RobertaConfig.from_pretrained('./roBERTa_base/')\n",
    "model = RobertaForMaskedLM.from_pretrained('./roBERTa_base/', config=config)\n",
    "# Outputting hidden states allows direct access to hidden layers of the model.\n",
    "# Outputting hidden states must be set to \"true\" in the config file during fine-tuning.\n",
    "# config.output_hidden_states = True\n",
    "model.eval()\n",
    "\n",
    "context_file = \"/home/jupyter/Notebooks/crystal/NLP/transformers/examples/CC_WET_test_ab\"\n",
    "output_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/'\n",
    "count_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/'\n",
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/data/vocab_files/FE_vocab_study.txt'\n",
    "vocab = make_vocab(vocab_file)\n",
    "\n",
    "FEATURE_COUNT = 768\n",
    "LAYER_COUNT = 2\n",
    "MAX_LINES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_layer0_roberta_base_FEvocab.txt /home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_layer0_roberta_base_counts.txt\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "st\n",
      "upe\n",
      "f\n",
      "ied\n",
      "Mean of 10 tensors is: tensor([-0.1499,  0.3891,  0.2168, -0.3009, -0.4857]) (768 features in tensor)\n",
      "Run time for stupefied was 25.81966659601312 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "scorn\n",
      "ful\n",
      "Mean of 18 tensors is: tensor([ 0.3809, -0.0835, -0.2398, -0.0859, -0.1096]) (768 features in tensor)\n",
      "Run time for scornful was 25.50855671998579 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disbel\n",
      "ieving\n",
      "Mean of 26 tensors is: tensor([ 0.1109,  0.1336, -0.2724,  0.5597,  0.0137]) (768 features in tensor)\n",
      "Run time for disbelieving was 25.88151633902453 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disdain\n",
      "ful\n",
      "Mean of 20 tensors is: tensor([ 0.3525, -0.1842, -0.1626, -0.1342, -0.1577]) (768 features in tensor)\n",
      "Run time for disdainful was 24.820783561910503 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "revol\n",
      "ted\n",
      "Mean of 26 tensors is: tensor([ 0.0949,  0.2123, -0.1108,  0.6197, -0.3241]) (768 features in tensor)\n",
      "Run time for revolted was 25.299011729075573 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "m\n",
      "iff\n",
      "ed\n",
      "Mean of 50 tensors is: tensor([-0.0715,  0.1850,  0.1752, -0.5103,  0.0555]) (768 features in tensor)\n",
      "Run time for miffed was 30.120752632967196 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "a\n",
      "gh\n",
      "ast\n",
      "Mean of 37 tensors is: tensor([ 0.0418, -0.1208,  0.2271,  0.1027, -0.0836]) (768 features in tensor)\n",
      "Run time for aghast was 24.76510535099078 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "inc\n",
      "ensed\n",
      "Mean of 56 tensors is: tensor([-0.3533,  0.2595,  0.1571, -0.2192, -0.0022]) (768 features in tensor)\n",
      "Run time for incensed was 23.642565616988577 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "de\n",
      "jected\n",
      "Mean of 35 tensors is: tensor([0.0060, 0.2329, 0.1491, 0.1720, 0.2906]) (768 features in tensor)\n",
      "Run time for dejected was 21.230694546015002 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "rep\n",
      "uls\n",
      "ed\n",
      "Mean of 56 tensors is: tensor([ 0.0734,  0.0948,  0.0731, -0.2947, -0.3942]) (768 features in tensor)\n",
      "Run time for repulsed was 24.5038557020016 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "mourn\n",
      "ful\n",
      "Mean of 45 tensors is: tensor([ 0.3781,  0.0661, -0.2699, -0.2233,  0.0798]) (768 features in tensor)\n",
      "Run time for mournful was 21.15710990608204 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disple\n",
      "ased\n",
      "Mean of 55 tensors is: tensor([-0.2913,  0.0954, -0.1565, -0.0800,  0.1539]) (768 features in tensor)\n",
      "Run time for displeased was 21.784341137041338 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "infuri\n",
      "ated\n",
      "Mean of 66 tensors is: tensor([ 0.0600,  0.4040,  0.0254, -0.0146,  0.0636]) (768 features in tensor)\n",
      "Run time for infuriated was 22.055370558053255 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aw\n",
      "ed\n",
      "Mean of 72 tensors is: tensor([-0.0127,  0.1855,  0.2347, -0.5370,  0.0157]) (768 features in tensor)\n",
      "Run time for awed was 22.73756629298441 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "repe\n",
      "lled\n",
      "Mean of 68 tensors is: tensor([ 0.2750,  0.1359, -0.0334,  0.0241, -0.1816]) (768 features in tensor)\n",
      "Run time for repelled was 22.220208166982047 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "resent\n",
      "ful\n",
      "Mean of 74 tensors is: tensor([ 0.2867,  0.0626, -0.2209,  0.0883, -0.1252]) (768 features in tensor)\n",
      "Run time for resentful was 23.397482522996143 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sorrow\n",
      "ful\n",
      "Mean of 76 tensors is: tensor([ 0.4157,  0.1070, -0.2138,  0.1864, -0.0435]) (768 features in tensor)\n",
      "Run time for sorrowful was 23.06550959206652 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ir\n",
      "ate\n",
      "Mean of 73 tensors is: tensor([ 0.1123,  0.1873,  0.2915, -0.2350, -0.2774]) (768 features in tensor)\n",
      "Run time for irate was 22.63980561599601 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "dismay\n",
      "ed\n",
      "Mean of 95 tensors is: tensor([ 0.1681, -0.0391,  0.2456,  0.0579, -0.0199]) (768 features in tensor)\n",
      "Run time for dismayed was 24.86123348295223 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "el\n",
      "ated\n",
      "Mean of 100 tensors is: tensor([ 0.0738,  0.4203,  0.5584,  0.2777, -0.3751]) (768 features in tensor)\n",
      "Run time for elated was 20.769283130997792 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "enraged\n",
      "Mean of 100 tensors is: tensor([-0.3101,  0.0029, -0.0333,  0.1018, -0.0282]) (768 features in tensor)\n",
      "Run time for enraged was 23.16982362791896 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "apprehens\n",
      "ive\n",
      "Mean of 100 tensors is: tensor([ 0.1189,  0.2489,  0.3273,  0.2877, -0.3164]) (768 features in tensor)\n",
      "Run time for apprehensive was 19.1919221279677 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bewild\n",
      "ered\n",
      "Mean of 92 tensors is: tensor([ 0.0247,  0.0478, -0.0325, -0.1191,  0.0143]) (768 features in tensor)\n",
      "Run time for bewildered was 26.041185377980582 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ast\n",
      "ounded\n",
      "Mean of 100 tensors is: tensor([-0.4557,  0.3387,  0.1328, -0.1567, -0.0302]) (768 features in tensor)\n",
      "Run time for astounded was 18.437472259043716 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "perplex\n",
      "ed\n",
      "Mean of 100 tensors is: tensor([-0.0171,  0.2363,  0.2828, -0.5001, -0.1523]) (768 features in tensor)\n",
      "Run time for perplexed was 20.83168351103086 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "appalled\n",
      "Mean of 100 tensors is: tensor([ 0.2476,  0.0687, -0.1348,  0.1088, -0.0398]) (768 features in tensor)\n",
      "Run time for appalled was 18.424728071084246 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "startled\n",
      "Mean of 100 tensors is: tensor([-0.3847, -0.0572, -0.1063,  0.6453,  0.3263]) (768 features in tensor)\n",
      "Run time for startled was 19.871978063951246 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "astonished\n",
      "Mean of 100 tensors is: tensor([-0.3577, -0.0973, -0.0811,  0.0210, -0.2454]) (768 features in tensor)\n",
      "Run time for astonished was 17.7477191289654 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alarmed\n",
      "Mean of 100 tensors is: tensor([ 0.1677,  0.1923, -0.0614,  0.3763, -0.0253]) (768 features in tensor)\n",
      "Run time for alarmed was 17.70309028099291 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "outraged\n",
      "Mean of 100 tensors is: tensor([-0.0540, -0.2142, -0.1200, -0.0267,  0.0115]) (768 features in tensor)\n",
      "Run time for outraged was 15.51826468098443 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "disgusted\n",
      "Mean of 100 tensors is: tensor([ 0.2169, -0.4134,  0.0348, -0.0091, -0.2745]) (768 features in tensor)\n",
      "Run time for disgusted was 16.905806937022135 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "doubtful\n",
      "Mean of 100 tensors is: tensor([ 0.1528,  0.0058,  0.4638,  0.3202, -0.0716]) (768 features in tensor)\n",
      "Run time for doubtful was 14.837858731974848 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "dissatisfied\n",
      "Mean of 100 tensors is: tensor([-0.0444,  0.0713,  0.2473,  0.0892,  0.0286]) (768 features in tensor)\n",
      "Run time for dissatisfied was 12.016773752053268 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "saddened\n",
      "Mean of 100 tensors is: tensor([ 0.2740,  0.3981, -0.0959,  0.7429,  0.3920]) (768 features in tensor)\n",
      "Run time for saddened was 11.8554160509957 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "irritated\n",
      "Mean of 100 tensors is: tensor([-0.1827,  0.2183,  0.5473,  0.1434, -0.0530]) (768 features in tensor)\n",
      "Run time for irritated was 13.07542641100008 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amused\n",
      "Mean of 100 tensors is: tensor([ 0.2358,  0.2209,  0.2635,  0.0598, -0.4008]) (768 features in tensor)\n",
      "Run time for amused was 17.0655758890789 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "frightened\n",
      "Mean of 100 tensors is: tensor([0.0610, 0.1323, 0.3324, 0.4325, 0.3745]) (768 features in tensor)\n",
      "Run time for frightened was 14.015369315049611 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "discouraged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 tensors is: tensor([ 0.2524, -0.1718,  0.8378,  1.0775, -0.3389]) (768 features in tensor)\n",
      "Run time for discouraged was 12.18205466598738 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stunned\n",
      "Mean of 100 tensors is: tensor([-0.0347, -0.1143, -0.1919,  0.3307, -0.2458]) (768 features in tensor)\n",
      "Run time for stunned was 10.906711356015876 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "fearful\n",
      "Mean of 100 tensors is: tensor([0.0310, 0.3575, 0.1188, 0.2435, 0.4270]) (768 features in tensor)\n",
      "Run time for fearful was 12.896710635977797 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "pissed\n",
      "Mean of 100 tensors is: tensor([ 0.3167, -0.1452, -0.1645,  0.0293,  0.3432]) (768 features in tensor)\n",
      "Run time for pissed was 10.822908477042802 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "terrified\n",
      "Mean of 100 tensors is: tensor([-0.0674,  0.0385,  0.1501,  0.1075,  0.4392]) (768 features in tensor)\n",
      "Run time for terrified was 11.545178843080066 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "offended\n",
      "Mean of 100 tensors is: tensor([ 0.0556, -0.0693,  0.2461,  0.0895, -0.0738]) (768 features in tensor)\n",
      "Run time for offended was 11.047368222963996 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "annoyed\n",
      "Mean of 100 tensors is: tensor([ 0.0028,  0.1169,  0.4006,  0.0921, -0.1345]) (768 features in tensor)\n",
      "Run time for annoyed was 9.738359309965745 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "skeptical\n",
      "Mean of 100 tensors is: tensor([-0.4574,  0.0569,  0.1838,  0.0425,  0.3583]) (768 features in tensor)\n",
      "Run time for skeptical was 9.426771408994682 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "hostile\n",
      "Mean of 100 tensors is: tensor([-0.1001, -0.1951, -0.3319, -0.3616,  0.3050]) (768 features in tensor)\n",
      "Run time for hostile was 12.946587018086575 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "furious\n",
      "Mean of 100 tensors is: tensor([ 0.2449, -0.1094, -0.3616,  0.0781,  0.0060]) (768 features in tensor)\n",
      "Run time for furious was 11.687297608004883 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bothered\n",
      "Mean of 100 tensors is: tensor([0.1030, 0.5229, 0.4203, 0.2993, 0.0735]) (768 features in tensor)\n",
      "Run time for bothered was 9.65575869998429 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "joyful\n",
      "Mean of 100 tensors is: tensor([-0.0171,  0.6030,  0.0716,  0.0230,  0.0014]) (768 features in tensor)\n",
      "Run time for joyful was 10.480477471952327 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "uncertain\n",
      "Mean of 100 tensors is: tensor([-0.0536, -0.0498, -0.0238,  0.6707,  0.1598]) (768 features in tensor)\n",
      "Run time for uncertain was 9.88687138305977 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "cheerful\n",
      "Mean of 100 tensors is: tensor([ 0.0267,  0.2796,  0.0833, -0.0098,  0.2400]) (768 features in tensor)\n",
      "Run time for cheerful was 12.812646560952999 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "depressed\n",
      "Mean of 100 tensors is: tensor([0.0182, 0.4291, 0.7933, 0.1956, 0.0250]) (768 features in tensor)\n",
      "Run time for depressed was 9.398975463933311 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anxious\n",
      "Mean of 100 tensors is: tensor([ 0.2352,  0.3966,  0.7305, -0.1056,  0.1366]) (768 features in tensor)\n",
      "Run time for anxious was 8.811630359035917 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "frustrated\n",
      "Mean of 100 tensors is: tensor([-0.0377,  0.2160,  0.4785,  0.2439,  0.1714]) (768 features in tensor)\n",
      "Run time for frustrated was 9.084162396960892 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "distressed\n",
      "Mean of 100 tensors is: tensor([-0.3705,  0.4244,  0.2400, -0.0774,  0.2244]) (768 features in tensor)\n",
      "Run time for distressed was 8.918378297938034 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bored\n",
      "Mean of 100 tensors is: tensor([ 0.2313, -0.0326,  0.0947,  0.2662,  0.3904]) (768 features in tensor)\n",
      "Run time for bored was 9.004030260955915 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suspicious\n",
      "Mean of 100 tensors is: tensor([-0.1495,  0.1788, -0.1792,  0.0223,  0.0780]) (768 features in tensor)\n",
      "Run time for suspicious was 9.647020471049473 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shocked\n",
      "Mean of 100 tensors is: tensor([-0.1671,  0.1820, -0.1356,  0.2751, -0.0423]) (768 features in tensor)\n",
      "Run time for shocked was 8.61213269806467 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amazed\n",
      "Mean of 100 tensors is: tensor([-0.0572,  0.1156,  0.0773,  0.3952, -0.1858]) (768 features in tensor)\n",
      "Run time for amazed was 7.963791860966012 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "rejected\n",
      "Mean of 100 tensors is: tensor([ 0.0507,  0.0995, -0.0616,  0.4011,  0.1804]) (768 features in tensor)\n",
      "Run time for rejected was 8.65988136001397 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "disappointed\n",
      "Mean of 100 tensors is: tensor([ 0.0675, -0.5470,  0.0449,  0.5879,  0.2564]) (768 features in tensor)\n",
      "Run time for disappointed was 8.712150089093484 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "upset\n",
      "Mean of 100 tensors is: tensor([-0.0709,  0.5114,  0.1947, -0.2311,  0.2484]) (768 features in tensor)\n",
      "Run time for upset was 9.020836989977397 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "delighted\n",
      "Mean of 100 tensors is: tensor([-0.0493, -0.0339, -0.0178,  0.4936,  0.3003]) (768 features in tensor)\n",
      "Run time for delighted was 8.150064505054615 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scared\n",
      "Mean of 100 tensors is: tensor([-0.0655,  0.1224,  0.3531,  0.4717,  0.6315]) (768 features in tensor)\n",
      "Run time for scared was 8.504107816028409 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "worried\n",
      "Mean of 100 tensors is: tensor([-0.0490,  0.7623,  0.2964,  0.5374, -0.0199]) (768 features in tensor)\n",
      "Run time for worried was 8.223037058953196 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "confused\n",
      "Mean of 100 tensors is: tensor([-0.2533,  0.2184,  0.0535,  0.3340,  0.3364]) (768 features in tensor)\n",
      "Run time for confused was 8.667990920017473 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "neutral\n",
      "Mean of 100 tensors is: tensor([ 0.3695,  0.0052,  0.2118,  0.2327, -0.0216]) (768 features in tensor)\n",
      "Run time for neutral was 8.481435936992057 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angry\n",
      "Mean of 100 tensors is: tensor([ 0.0963, -0.0586, -0.1935,  0.4693,  0.2096]) (768 features in tensor)\n",
      "Run time for angry was 9.360678250901401 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "satisfied\n",
      "Mean of 100 tensors is: tensor([-0.1219, -0.3294,  0.2267,  0.1867, -0.1344]) (768 features in tensor)\n",
      "Run time for satisfied was 7.295275643002242 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "surprised\n",
      "Mean of 100 tensors is: tensor([-0.3534,  0.0288,  0.2314,  0.5236,  0.0237]) (768 features in tensor)\n",
      "Run time for surprised was 8.070431979023851 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "pleased\n",
      "Mean of 100 tensors is: tensor([ 0.1037,  0.1093,  0.2440,  0.3523, -0.0254]) (768 features in tensor)\n",
      "Run time for pleased was 6.647369965096004 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "concerned\n",
      "Mean of 100 tensors is: tensor([-0.3107,  0.3701,  0.2406,  0.1716,  0.2168]) (768 features in tensor)\n",
      "Run time for concerned was 7.494584586005658 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "mad\n",
      "Mean of 100 tensors is: tensor([ 0.0567, -0.1008,  0.3311,  0.1831,  0.1752]) (768 features in tensor)\n",
      "Run time for mad was 10.041805385029875 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "hurt\n",
      "Mean of 100 tensors is: tensor([ 0.0737, -0.1471,  0.6242, -0.0599,  0.0857]) (768 features in tensor)\n",
      "Run time for hurt was 8.461942353984341 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "excited\n",
      "Mean of 100 tensors is: tensor([-0.0594,  0.3188,  0.2611,  0.2767, -0.4278]) (768 features in tensor)\n",
      "Run time for excited was 7.948131470009685 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sad\n",
      "Mean of 100 tensors is: tensor([0.3800, 0.3112, 0.2893, 0.1347, 0.6188]) (768 features in tensor)\n",
      "Run time for sad was 7.766332291997969 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "interested\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 tensors is: tensor([-0.0427,  0.1519, -0.0349,  0.7986, -0.2600]) (768 features in tensor)\n",
      "Run time for interested was 6.771487821009941 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "happy\n",
      "Mean of 100 tensors is: tensor([ 0.2117,  0.2532,  0.4332,  0.4256, -0.0266]) (768 features in tensor)\n",
      "Run time for happy was 7.2554441310931 seconds.\n",
      "/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_layer1_roberta_base_FEvocab.txt /home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_layer1_roberta_base_counts.txt\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "st\n",
      "upe\n",
      "f\n",
      "ied\n",
      "Mean of 10 tensors is: tensor([-0.1586,  0.6347,  0.4507, -0.5318, -0.8145]) (768 features in tensor)\n",
      "Run time for stupefied was 17.747816493967548 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "scorn\n",
      "ful\n",
      "Mean of 18 tensors is: tensor([ 0.3017, -0.2832,  0.0431, -0.0180, -0.1338]) (768 features in tensor)\n",
      "Run time for scornful was 18.73118283506483 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disbel\n",
      "ieving\n",
      "Mean of 26 tensors is: tensor([-0.1324,  0.4512, -0.0440,  1.0108,  0.2091]) (768 features in tensor)\n",
      "Run time for disbelieving was 19.64872759906575 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disdain\n",
      "ful\n",
      "Mean of 20 tensors is: tensor([ 0.3289, -0.2042,  0.3302, -0.0908, -0.2551]) (768 features in tensor)\n",
      "Run time for disdainful was 19.14547568396665 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "revol\n",
      "ted\n",
      "Mean of 26 tensors is: tensor([-0.1501,  0.1440, -0.1414,  0.9069, -0.3982]) (768 features in tensor)\n",
      "Run time for revolted was 19.152966438094154 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "m\n",
      "iff\n",
      "ed\n",
      "Mean of 50 tensors is: tensor([ 0.1299,  0.5192,  0.2111, -0.6414,  0.2133]) (768 features in tensor)\n",
      "Run time for miffed was 22.230247072060592 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "a\n",
      "gh\n",
      "ast\n",
      "Mean of 37 tensors is: tensor([-0.0155, -0.1312,  0.3052,  0.1148,  0.1981]) (768 features in tensor)\n",
      "Run time for aghast was 20.118265530094504 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "inc\n",
      "ensed\n",
      "Mean of 56 tensors is: tensor([-0.4141,  0.1085,  0.3307,  0.0015, -0.0652]) (768 features in tensor)\n",
      "Run time for incensed was 21.614815238979645 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "de\n",
      "jected\n",
      "Mean of 35 tensors is: tensor([-0.1129,  0.4813,  0.5806,  0.3651,  0.3429]) (768 features in tensor)\n",
      "Run time for dejected was 19.629411641974002 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "rep\n",
      "uls\n",
      "ed\n",
      "Mean of 56 tensors is: tensor([ 0.0208,  0.1680,  0.0107, -0.2977, -0.4237]) (768 features in tensor)\n",
      "Run time for repulsed was 22.970188247039914 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "mourn\n",
      "ful\n",
      "Mean of 45 tensors is: tensor([ 0.3365,  0.2151, -0.0292, -0.2503,  0.2373]) (768 features in tensor)\n",
      "Run time for mournful was 20.509596437914297 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disple\n",
      "ased\n",
      "Mean of 55 tensors is: tensor([-0.4339,  0.0546,  0.2939,  0.2211,  0.2066]) (768 features in tensor)\n",
      "Run time for displeased was 21.324842559057288 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "infuri\n",
      "ated\n",
      "Mean of 66 tensors is: tensor([-0.0650,  0.6908,  0.3321,  0.4672,  0.1517]) (768 features in tensor)\n",
      "Run time for infuriated was 21.946084355004132 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aw\n",
      "ed\n",
      "Mean of 72 tensors is: tensor([ 0.0884,  0.3603,  0.3207, -0.6656,  0.0562]) (768 features in tensor)\n",
      "Run time for awed was 22.49855367199052 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "repe\n",
      "lled\n",
      "Mean of 68 tensors is: tensor([-0.0834,  0.3304,  0.2373,  0.2608, -0.2478]) (768 features in tensor)\n",
      "Run time for repelled was 22.11432266398333 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "resent\n",
      "ful\n",
      "Mean of 74 tensors is: tensor([ 0.2904, -0.1131,  0.1757,  0.3506, -0.1703]) (768 features in tensor)\n",
      "Run time for resentful was 23.18185224093031 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sorrow\n",
      "ful\n",
      "Mean of 76 tensors is: tensor([0.5531, 0.2632, 0.0488, 0.1963, 0.1863]) (768 features in tensor)\n",
      "Run time for sorrowful was 22.881399734993465 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ir\n",
      "ate\n",
      "Mean of 73 tensors is: tensor([ 0.0196,  0.4580,  0.5247, -0.0828, -0.4657]) (768 features in tensor)\n",
      "Run time for irate was 22.739562160917558 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "dismay\n",
      "ed\n",
      "Mean of 95 tensors is: tensor([0.0027, 0.2705, 0.3023, 0.1823, 0.0792]) (768 features in tensor)\n",
      "Run time for dismayed was 24.844232389936224 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "el\n",
      "ated\n",
      "Mean of 100 tensors is: tensor([ 0.0597,  0.6598,  0.9102,  0.5476, -0.5367]) (768 features in tensor)\n",
      "Run time for elated was 20.569322398980148 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "enraged\n",
      "Mean of 100 tensors is: tensor([-0.2736,  0.2241,  0.1366,  0.2842, -0.1993]) (768 features in tensor)\n",
      "Run time for enraged was 22.82242473703809 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "apprehens\n",
      "ive\n",
      "Mean of 100 tensors is: tensor([-0.1690,  0.3822,  0.7542,  0.3654, -0.3560]) (768 features in tensor)\n",
      "Run time for apprehensive was 19.20653077994939 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bewild\n",
      "ered\n",
      "Mean of 92 tensors is: tensor([-0.1873,  0.2448,  0.0746,  0.2322,  0.2084]) (768 features in tensor)\n",
      "Run time for bewildered was 25.681155077996664 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ast\n",
      "ounded\n",
      "Mean of 100 tensors is: tensor([-0.5499,  0.6377,  0.5501,  0.0308, -0.0862]) (768 features in tensor)\n",
      "Run time for astounded was 18.418128545978107 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "perplex\n",
      "ed\n",
      "Mean of 100 tensors is: tensor([-0.2034,  0.5521,  0.3600, -0.4059, -0.1288]) (768 features in tensor)\n",
      "Run time for perplexed was 20.884897356969304 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "appalled\n",
      "Mean of 100 tensors is: tensor([0.1267, 0.2893, 0.0958, 0.3295, 0.1052]) (768 features in tensor)\n",
      "Run time for appalled was 18.292816086905077 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "startled\n",
      "Mean of 100 tensors is: tensor([-0.5278,  0.2030,  0.1266,  0.6014,  0.7086]) (768 features in tensor)\n",
      "Run time for startled was 19.379253524006344 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "astonished\n",
      "Mean of 100 tensors is: tensor([-0.4335,  0.1393,  0.2290,  0.0016, -0.0910]) (768 features in tensor)\n",
      "Run time for astonished was 17.37010611104779 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alarmed\n",
      "Mean of 100 tensors is: tensor([0.0151, 0.5304, 0.1203, 0.5669, 0.0933]) (768 features in tensor)\n",
      "Run time for alarmed was 17.745897946995683 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "outraged\n",
      "Mean of 100 tensors is: tensor([ 0.0496, -0.1774,  0.2036, -0.0605,  0.0464]) (768 features in tensor)\n",
      "Run time for outraged was 15.34256205894053 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "disgusted\n",
      "Mean of 100 tensors is: tensor([ 0.1671, -0.4743,  0.4362,  0.1565, -0.2481]) (768 features in tensor)\n",
      "Run time for disgusted was 16.646031219046563 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "doubtful\n",
      "Mean of 100 tensors is: tensor([-0.0192,  0.0148,  0.5083,  0.4267, -0.0313]) (768 features in tensor)\n",
      "Run time for doubtful was 14.75379102898296 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "dissatisfied\n",
      "Mean of 100 tensors is: tensor([-0.0294,  0.1609,  0.6421,  0.3035, -0.1245]) (768 features in tensor)\n",
      "Run time for dissatisfied was 11.98219477001112 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "saddened\n",
      "Mean of 100 tensors is: tensor([0.1400, 0.8153, 0.0356, 0.7695, 0.8017]) (768 features in tensor)\n",
      "Run time for saddened was 11.532981571974233 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "irritated\n",
      "Mean of 100 tensors is: tensor([-0.3556,  0.5285,  0.8757,  0.3025, -0.0401]) (768 features in tensor)\n",
      "Run time for irritated was 12.875540629960597 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amused\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 tensors is: tensor([-0.0162,  0.6408,  0.6147,  0.2643, -0.8166]) (768 features in tensor)\n",
      "Run time for amused was 16.65588416403625 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "frightened\n",
      "Mean of 100 tensors is: tensor([-0.1136,  0.1248,  0.3792,  0.6913,  0.8156]) (768 features in tensor)\n",
      "Run time for frightened was 13.816430102917366 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "discouraged\n",
      "Mean of 100 tensors is: tensor([ 0.1375,  0.0506,  1.1548,  0.9737, -0.1447]) (768 features in tensor)\n",
      "Run time for discouraged was 11.912536459043622 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stunned\n",
      "Mean of 100 tensors is: tensor([-0.2764,  0.1920,  0.0638,  0.3221,  0.1024]) (768 features in tensor)\n",
      "Run time for stunned was 10.866584729054011 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "fearful\n",
      "Mean of 100 tensors is: tensor([-0.3102,  0.4526,  0.3339,  0.7427,  0.7396]) (768 features in tensor)\n",
      "Run time for fearful was 12.807440740987659 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "pissed\n",
      "Mean of 100 tensors is: tensor([ 0.1384, -0.1828,  0.0714,  0.1847,  0.2391]) (768 features in tensor)\n",
      "Run time for pissed was 10.698313710978255 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "terrified\n",
      "Mean of 100 tensors is: tensor([-0.2585,  0.0787,  0.3215,  0.3413,  1.0750]) (768 features in tensor)\n",
      "Run time for terrified was 11.281333222985268 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "offended\n",
      "Mean of 100 tensors is: tensor([-0.1066, -0.0041,  0.8337,  0.0406, -0.2354]) (768 features in tensor)\n",
      "Run time for offended was 11.140002375934273 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "annoyed\n",
      "Mean of 100 tensors is: tensor([-0.1788,  0.3321,  0.7654,  0.3707, -0.2600]) (768 features in tensor)\n",
      "Run time for annoyed was 9.636673367000185 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "skeptical\n",
      "Mean of 100 tensors is: tensor([-0.6837,  0.0423,  0.6193, -0.1435,  0.5914]) (768 features in tensor)\n",
      "Run time for skeptical was 9.461925377021544 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "hostile\n",
      "Mean of 100 tensors is: tensor([-3.6891e-01,  1.2492e-01, -4.9739e-04, -3.9911e-01,  6.3924e-01]) (768 features in tensor)\n",
      "Run time for hostile was 12.760645086993463 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "furious\n",
      "Mean of 100 tensors is: tensor([-0.0756,  0.0147, -0.3660,  0.5755, -0.1282]) (768 features in tensor)\n",
      "Run time for furious was 11.73627626907546 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bothered\n",
      "Mean of 100 tensors is: tensor([-0.0142,  0.7007,  0.7819,  0.3796, -0.0418]) (768 features in tensor)\n",
      "Run time for bothered was 9.771083570085466 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "joyful\n",
      "Mean of 100 tensors is: tensor([-0.2327,  1.0831,  0.4206,  0.1995,  0.0301]) (768 features in tensor)\n",
      "Run time for joyful was 10.414904307108372 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "uncertain\n",
      "Mean of 100 tensors is: tensor([-0.3480,  0.1054,  0.3390,  0.7039,  0.3801]) (768 features in tensor)\n",
      "Run time for uncertain was 9.870049015036784 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "cheerful\n",
      "Mean of 100 tensors is: tensor([0.0322, 0.5330, 0.4441, 0.0823, 0.5647]) (768 features in tensor)\n",
      "Run time for cheerful was 12.66918249509763 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "depressed\n",
      "Mean of 100 tensors is: tensor([-0.1073,  0.6178,  1.0787,  0.1537,  0.2577]) (768 features in tensor)\n",
      "Run time for depressed was 9.460941353929229 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anxious\n",
      "Mean of 100 tensors is: tensor([ 0.2793,  0.3620,  1.0810, -0.1115,  0.2701]) (768 features in tensor)\n",
      "Run time for anxious was 8.81844145199284 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "frustrated\n",
      "Mean of 100 tensors is: tensor([-0.1907,  0.4995,  0.9029,  0.7387,  0.3029]) (768 features in tensor)\n",
      "Run time for frustrated was 9.021126817096956 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "distressed\n",
      "Mean of 100 tensors is: tensor([-0.0905,  0.8191,  0.3988,  0.1753,  0.7244]) (768 features in tensor)\n",
      "Run time for distressed was 8.942702401080169 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bored\n",
      "Mean of 100 tensors is: tensor([ 0.2084, -0.0333,  0.5298,  0.3786,  0.9025]) (768 features in tensor)\n",
      "Run time for bored was 9.029323789058253 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suspicious\n",
      "Mean of 100 tensors is: tensor([-0.3737,  0.3860,  0.1336,  0.0121,  0.2221]) (768 features in tensor)\n",
      "Run time for suspicious was 9.529134253971279 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shocked\n",
      "Mean of 100 tensors is: tensor([-0.3486,  0.6849,  0.1506,  0.2435,  0.2631]) (768 features in tensor)\n",
      "Run time for shocked was 8.662782053928822 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amazed\n",
      "Mean of 100 tensors is: tensor([-0.1544,  0.5751,  0.1839,  0.4741, -0.1826]) (768 features in tensor)\n",
      "Run time for amazed was 7.971746010007337 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "rejected\n",
      "Mean of 100 tensors is: tensor([0.0962, 0.1336, 0.0809, 0.7740, 0.6940]) (768 features in tensor)\n",
      "Run time for rejected was 8.599267742945813 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "disappointed\n",
      "Mean of 100 tensors is: tensor([ 0.1415, -0.6122,  0.3020,  0.9318,  0.2377]) (768 features in tensor)\n",
      "Run time for disappointed was 8.645274189999327 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "upset\n",
      "Mean of 100 tensors is: tensor([-0.1383,  0.6968,  0.4550,  0.2229, -0.0073]) (768 features in tensor)\n",
      "Run time for upset was 8.907317979028448 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "delighted\n",
      "Mean of 100 tensors is: tensor([-0.1412,  0.2326,  0.2499,  0.7243,  0.4997]) (768 features in tensor)\n",
      "Run time for delighted was 7.943111513974145 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scared\n",
      "Mean of 100 tensors is: tensor([-0.3258,  0.0937,  0.5635,  0.6074,  1.1387]) (768 features in tensor)\n",
      "Run time for scared was 8.458386350073852 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "worried\n",
      "Mean of 100 tensors is: tensor([-0.1099,  1.0532,  0.6819,  0.8837,  0.1048]) (768 features in tensor)\n",
      "Run time for worried was 8.1219665260287 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "confused\n",
      "Mean of 100 tensors is: tensor([-0.3913,  0.3415,  0.4361,  0.8857,  0.6325]) (768 features in tensor)\n",
      "Run time for confused was 8.623201278038323 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "neutral\n",
      "Mean of 100 tensors is: tensor([ 1.4175e-01, -5.6669e-02,  7.5939e-01,  3.5618e-01,  2.3648e-04]) (768 features in tensor)\n",
      "Run time for neutral was 8.383734144968912 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angry\n",
      "Mean of 100 tensors is: tensor([ 0.0426, -0.1933,  0.0317,  1.1211,  0.4665]) (768 features in tensor)\n",
      "Run time for angry was 9.25924316700548 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "satisfied\n",
      "Mean of 100 tensors is: tensor([-0.0420, -0.4000,  0.6539,  0.4240, -0.4481]) (768 features in tensor)\n",
      "Run time for satisfied was 6.876021898002364 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "surprised\n",
      "Mean of 100 tensors is: tensor([-0.7138,  0.4254,  0.3816,  0.7118,  0.1432]) (768 features in tensor)\n",
      "Run time for surprised was 7.765892999013886 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "pleased\n",
      "Mean of 100 tensors is: tensor([-0.2300,  0.3442,  0.5591,  0.6452, -0.1878]) (768 features in tensor)\n",
      "Run time for pleased was 6.621742984978482 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "concerned\n",
      "Mean of 100 tensors is: tensor([-0.3127,  0.7621,  0.4001,  0.4496,  0.1352]) (768 features in tensor)\n",
      "Run time for concerned was 7.249977731029503 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "mad\n",
      "Mean of 100 tensors is: tensor([-0.2668,  0.2276,  0.6265,  0.7546,  0.6267]) (768 features in tensor)\n",
      "Run time for mad was 10.111195120029151 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "hurt\n",
      "Mean of 100 tensors is: tensor([ 0.0268, -0.0508,  1.0101, -0.2489,  0.2562]) (768 features in tensor)\n",
      "Run time for hurt was 8.020282599958591 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "excited\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 tensors is: tensor([-0.0188,  0.7806,  0.5708,  0.3402, -0.5025]) (768 features in tensor)\n",
      "Run time for excited was 7.4258560690796 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sad\n",
      "Mean of 100 tensors is: tensor([0.4739, 0.5487, 0.3463, 0.4823, 1.3078]) (768 features in tensor)\n",
      "Run time for sad was 7.526232686010189 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "interested\n",
      "Mean of 100 tensors is: tensor([-0.2166,  0.3488,  0.1476,  1.2446, -0.4377]) (768 features in tensor)\n",
      "Run time for interested was 6.498790754005313 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "happy\n",
      "Mean of 100 tensors is: tensor([-0.2037,  0.6813,  0.6732,  0.8802,  0.0304]) (768 features in tensor)\n",
      "Run time for happy was 6.763832855038345 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Test all layers of the model in the outer loop.\n",
    "for l in range(0, LAYER_COUNT):\n",
    "    l_output_file = ''\n",
    "    l_count_file = ''\n",
    "    l_output_file = os.path.join(output_file, 'roberta_layer' + str(l) + '_roberta_base_FEvocab.txt')\n",
    "    l_count_file = os.path.join(count_file, 'roberta_layer'+ str(l) + '_roberta_base_counts.txt')\n",
    "    print(l_output_file, l_count_file)\n",
    "    # Process vocabulary words in the middle loop.\n",
    "    for v in vocab:\n",
    "        start = timer()\n",
    "        with open(context_file, 'r') as lines:\n",
    "            v_sum = torch.zeros([1, FEATURE_COUNT])\n",
    "            v_tokens = tokenizer.encode(v)\n",
    "            print(f'\\nThere are {len(v_tokens) - 2} tokens in tokenized vocabulary word:')\n",
    "            for t in v_tokens[1:-1]:\n",
    "                print(tokenizer.decode(t).strip())\n",
    "            count_sentence = 0\n",
    "            count_tensor = 0\n",
    "\n",
    "            # Process all lines in the context file in the inner loop.\n",
    "            for line in lines:\n",
    "                # Check for this vocab word in this line; if found, split the line into individual sentences.\n",
    "                if v in line.lower().split():\n",
    "                    for sentence in line.split('.'):\n",
    "                        if v in sentence.lower():\n",
    "                            line = sentence\n",
    "                            count_sentence += 1\n",
    "                            break\n",
    "                    # Split the new sentence-based line into tokens.\n",
    "                    # Use max_length to avoid overflowing the maximum sequence length for the model.\n",
    "                    tokenized_text = tokenizer.encode(line, add_special_tokens=True, max_length=512)\n",
    "                    indices = []              \n",
    "\n",
    "                    # Check to see whether the vocab word is found in this particular line.\n",
    "                    # Initially, some lines may have comprised multiple sentences, which were\n",
    "                    # broken out individually above.\n",
    "                    for t in v_tokens[1:-1]:\n",
    "                        for i, token_str in enumerate(tokenized_text):\n",
    "                            if tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip():\n",
    "                                indices.append(i)               \n",
    "\n",
    "                    ###################################################################################\n",
    "                    # If the vocabulary word was found, process the containing line.\n",
    "                    if indices:\n",
    "\n",
    "                        # The vocab word was found in this line/sentence, at the locations in indices.\n",
    "                        # Get the feature vectors for all tokens in the line/sentence.\n",
    "                        token_embeddings = create_token_embeddings(tokenized_text)\n",
    "                        token_vecs_layer = get_layer_token_vecs(token_embeddings, l)\n",
    "\n",
    "                        # Get the vocab word's contextual embedding for this line.\n",
    "                        tensor_layer = torch.zeros([1, FEATURE_COUNT])\n",
    "                        for i in range(len(indices)):\n",
    "                            v_index = i % len(v_tokens[1:-1])\n",
    "                            tensor_layer += token_vecs_layer[indices[i]]\n",
    "\n",
    "                        # If our vocab word is broken into more than one token, we need to get the mean of the token embeddings.\n",
    "                        tensor_layer /= len(indices)\n",
    "\n",
    "                        # Add the embedding distilled from this line to the sum of embeddings for all lines.\n",
    "                        v_sum += tensor_layer\n",
    "                        count_tensor += 1\n",
    "                    ###################################################################################\n",
    "                # Stop processing lines once we've found 2000 instances of our vocab word.\n",
    "                if count_tensor >= MAX_LINES:\n",
    "                    break\n",
    "\n",
    "            # We're done processing all lines of 512 tokens or less containing our vocab word.\n",
    "            # Get the mean embedding for the word.\n",
    "            v_mean = v_sum / count_tensor\n",
    "            print(f'Mean of {count_tensor} tensors is: {v_mean[0][:5]} ({len(v_mean[0])} features in tensor)')\n",
    "            write_embedding(l_output_file, v, v_mean)\n",
    "            try:\n",
    "                with open(l_count_file, 'a') as counts:\n",
    "                    counts.write(v + ', ' + str(count_tensor) + '\\n')\n",
    "            except:\n",
    "                print('Wha?! Could not write the sentence count.')\n",
    "        end = timer()\n",
    "        print(f'Run time for {v} was {end - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    vocab = []\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_token_embeddings(tokenized_text):\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)  # Batch size 1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "        encoded_layers = outputs[2]\n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "#         print(f'Size of token embeddings is {token_embeddings.size()}')\n",
    "        return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the last 4 layers' features\n",
    "def sum_last_four_token_vecs(token_embeddings):\n",
    "    token_vecs_sum_last_four = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_last_four.append(sum_vec)\n",
    "\n",
    "#     print ('Shape of summed layers is: %d x %d' % (len(token_vecs_sum_last_four), len(token_vecs_sum_last_four[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_sum_last_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a single layer of the model.\n",
    "def get_layer_token_vecs(token_embeddings, layer_number):\n",
    "    token_vecs_layer = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        layer_vec = token[layer_number]\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_layer.append(layer_vec)\n",
    "\n",
    "#     print ('Shape of summed layers is: %d x %d' % (len(token_vecs_layer), len(token_vecs_layer[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "#         print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal-venv-3.6",
   "language": "python",
   "name": "crystal-venv-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
