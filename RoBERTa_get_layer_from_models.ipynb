{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, RobertaConfig\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# and Transformers documentation: https://huggingface.co/transformers/model_doc/roberta.html#robertaformaskedlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/checkpoint-2000_layer_8_aef_FEvocab.txt\n",
      "/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/final_layer_8_aef_FEvocab.txt\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/jupyter/Notebooks/crystal/NLP/transformers/examples/')\n",
    "os.getcwd()\n",
    "\n",
    "model_root_dir = './output_CC-aef/'\n",
    "\n",
    "for dir, subdirs, files in os.walk(model_root_dir):\n",
    "    for name in subdirs:\n",
    "        print(os.path.join(output_file, name + '_layer_8_aef_FEvocab.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('./output_CC-aef/')\n",
    "config = RobertaConfig.from_pretrained('./output_CC-aef/')\n",
    "model_root_dir = './output_CC-aef/'\n",
    "# model = RobertaForMaskedLM.from_pretrained('./output_CC-aef/', config=config)\n",
    "\n",
    "context_file = \"/home/jupyter/Notebooks/crystal/NLP/transformers/examples/CC_WET_test_ae\"\n",
    "output_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/'\n",
    "count_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/'\n",
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/data/vocab_files/FE_vocab_study.txt'\n",
    "vocab = make_vocab(vocab_file)\n",
    "\n",
    "FEATURE_COUNT = 768\n",
    "LAYER_COUNT = 13\n",
    "LAYER = 8\n",
    "MAX_LINES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/checkpoint-2000_layer_8_aef_10lines.txt /home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/checkpoint-2000_layer_8_aef_10lines_counts.txt\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "st\n",
      "upe\n",
      "f\n",
      "ied\n",
      "Mean of 10 tensors is: tensor([ 0.0010,  0.1839, -0.1286,  0.0957, -0.7192]) (768 features in tensor)\n",
      "Run time for stupefied was 16.109985283925198 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "scorn\n",
      "ful\n",
      "Mean of 10 tensors is: tensor([ 0.2185,  0.0789, -0.2566,  0.0634, -0.0715]) (768 features in tensor)\n",
      "Run time for scornful was 9.741986316046678 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disbel\n",
      "ieving\n",
      "Mean of 10 tensors is: tensor([ 0.1155,  0.2178, -0.3281,  0.7425,  0.3196]) (768 features in tensor)\n",
      "Run time for disbelieving was 5.578524191048928 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disdain\n",
      "ful\n",
      "Mean of 10 tensors is: tensor([ 0.1078, -0.0287, -0.0487, -0.0458,  0.1650]) (768 features in tensor)\n",
      "Run time for disdainful was 8.759855229058303 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "revol\n",
      "ted\n",
      "Mean of 10 tensors is: tensor([ 0.2138,  0.3466,  0.0965,  0.8424, -0.1517]) (768 features in tensor)\n",
      "Run time for revolted was 8.102811874938197 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "m\n",
      "iff\n",
      "ed\n",
      "Mean of 10 tensors is: tensor([ 0.0531,  0.5290, -0.0499,  0.6108,  0.2484]) (768 features in tensor)\n",
      "Run time for miffed was 2.7337759459624067 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "a\n",
      "gh\n",
      "ast\n",
      "Mean of 10 tensors is: tensor([ 0.0192,  0.0526, -0.1286,  0.2819,  0.1162]) (768 features in tensor)\n",
      "Run time for aghast was 5.510698114987463 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "inc\n",
      "ensed\n",
      "Mean of 10 tensors is: tensor([ 0.2001,  0.4355,  0.0463,  0.3301, -0.2771]) (768 features in tensor)\n",
      "Run time for incensed was 3.1397912820102647 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "de\n",
      "jected\n",
      "Mean of 10 tensors is: tensor([ 0.1732,  0.4583, -0.2425, -0.1464,  0.0277]) (768 features in tensor)\n",
      "Run time for dejected was 4.678581280051731 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "rep\n",
      "uls\n",
      "ed\n",
      "Mean of 10 tensors is: tensor([ 0.1209,  0.3184, -0.0143,  0.2160, -0.2797]) (768 features in tensor)\n",
      "Run time for repulsed was 3.3251000170130283 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "mourn\n",
      "ful\n",
      "Mean of 10 tensors is: tensor([ 0.2499,  0.7484, -0.1381,  0.3768,  0.8751]) (768 features in tensor)\n",
      "Run time for mournful was 3.341316523961723 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disple\n",
      "ased\n",
      "Mean of 10 tensors is: tensor([0.0258, 0.4873, 0.0337, 0.5296, 0.0526]) (768 features in tensor)\n",
      "Run time for displeased was 3.4037430320167914 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "infuri\n",
      "ated\n",
      "Mean of 10 tensors is: tensor([ 0.2628,  0.4465,  0.0660,  0.4813, -0.0260]) (768 features in tensor)\n",
      "Run time for infuriated was 2.250171128078364 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aw\n",
      "ed\n",
      "Mean of 10 tensors is: tensor([ 0.1508,  0.2739, -0.0223, -0.5353, -0.0903]) (768 features in tensor)\n",
      "Run time for awed was 3.59712886903435 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "repe\n",
      "lled\n",
      "Mean of 10 tensors is: tensor([ 0.0858,  0.4246, -0.0262,  0.5010, -0.2344]) (768 features in tensor)\n",
      "Run time for repelled was 1.7008916030172259 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "resent\n",
      "ful\n",
      "Mean of 10 tensors is: tensor([0.1241, 0.1141, 0.0366, 0.4105, 0.1681]) (768 features in tensor)\n",
      "Run time for resentful was 3.404163811937906 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sorrow\n",
      "ful\n",
      "Mean of 10 tensors is: tensor([ 0.1869,  0.5310, -0.2104,  0.7761,  1.2090]) (768 features in tensor)\n",
      "Run time for sorrowful was 2.586285474942997 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ir\n",
      "ate\n",
      "Mean of 10 tensors is: tensor([ 0.1132,  0.8309, -0.0261,  0.4915, -0.3085]) (768 features in tensor)\n",
      "Run time for irate was 3.1461269200081006 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "dismay\n",
      "ed\n",
      "Mean of 10 tensors is: tensor([ 0.0291,  0.2509,  0.0856,  0.4506, -0.2190]) (768 features in tensor)\n",
      "Run time for dismayed was 1.8676705879624933 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "el\n",
      "ated\n",
      "Mean of 10 tensors is: tensor([ 0.2735,  0.7736,  0.0161,  0.7265, -0.6286]) (768 features in tensor)\n",
      "Run time for elated was 1.6140184390824288 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "enraged\n",
      "Mean of 10 tensors is: tensor([-0.0207,  0.3885,  0.0458,  0.1600,  0.1242]) (768 features in tensor)\n",
      "Run time for enraged was 3.2432969510555267 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "apprehens\n",
      "ive\n",
      "Mean of 10 tensors is: tensor([ 0.1652,  0.1405,  0.0349,  1.1585, -0.5460]) (768 features in tensor)\n",
      "Run time for apprehensive was 1.3057967409258708 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bewild\n",
      "ered\n",
      "Mean of 10 tensors is: tensor([-0.1377,  0.2853, -0.0748,  0.3598,  0.0371]) (768 features in tensor)\n",
      "Run time for bewildered was 2.1169065000722185 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ast\n",
      "ounded\n",
      "Mean of 10 tensors is: tensor([ 0.1993,  0.3213, -0.0045,  0.5585, -0.1580]) (768 features in tensor)\n",
      "Run time for astounded was 2.332509508007206 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "perplex\n",
      "ed\n",
      "Mean of 10 tensors is: tensor([-0.0340,  0.2750,  0.0260,  0.6645, -0.0936]) (768 features in tensor)\n",
      "Run time for perplexed was 2.089053816976957 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "appalled\n",
      "Mean of 10 tensors is: tensor([-0.0256,  0.2326,  0.0496,  0.1734, -0.0041]) (768 features in tensor)\n",
      "Run time for appalled was 1.9683937870431691 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "startled\n",
      "Mean of 10 tensors is: tensor([0.0968, 0.2056, 0.1008, 0.7523, 0.4398]) (768 features in tensor)\n",
      "Run time for startled was 1.6083286079810932 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "astonished\n",
      "Mean of 10 tensors is: tensor([ 0.1397,  0.2761, -0.1378,  0.1600, -0.2441]) (768 features in tensor)\n",
      "Run time for astonished was 1.9401827480178326 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alarmed\n",
      "Mean of 10 tensors is: tensor([ 0.1417,  0.3006,  0.1323,  0.2876, -0.0744]) (768 features in tensor)\n",
      "Run time for alarmed was 1.7086752349277958 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "outraged\n",
      "Mean of 10 tensors is: tensor([-0.0156, -0.0115, -0.0699,  0.0355, -0.0971]) (768 features in tensor)\n",
      "Run time for outraged was 1.0632255889941007 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "disgusted\n",
      "Mean of 10 tensors is: tensor([ 0.1147, -0.1469,  0.1144, -0.1363, -0.2261]) (768 features in tensor)\n",
      "Run time for disgusted was 2.3188850389560685 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "doubtful\n",
      "Mean of 10 tensors is: tensor([ 0.2585,  0.0044, -0.0108,  0.4671,  0.6178]) (768 features in tensor)\n",
      "Run time for doubtful was 1.2284404779784381 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "dissatisfied\n",
      "Mean of 10 tensors is: tensor([-0.2810,  0.3292,  0.1168,  0.4138, -0.5570]) (768 features in tensor)\n",
      "Run time for dissatisfied was 1.1792144160717726 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "saddened\n",
      "Mean of 10 tensors is: tensor([0.0838, 0.7183, 0.1871, 0.6856, 1.1982]) (768 features in tensor)\n",
      "Run time for saddened was 1.325991567922756 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "irritated\n",
      "Mean of 10 tensors is: tensor([0.2120, 0.2117, 0.2143, 0.4240, 0.1069]) (768 features in tensor)\n",
      "Run time for irritated was 1.3101443930063397 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amused\n",
      "Mean of 10 tensors is: tensor([ 0.0114,  0.7433,  0.2673,  0.6725, -1.0079]) (768 features in tensor)\n",
      "Run time for amused was 1.6057536259759218 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "frightened\n",
      "Mean of 10 tensors is: tensor([ 0.2381, -0.1504,  0.1994,  0.6951,  0.8666]) (768 features in tensor)\n",
      "Run time for frightened was 1.042239253059961 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "discouraged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 10 tensors is: tensor([-0.0303,  0.2995,  0.0016,  0.0480, -0.2184]) (768 features in tensor)\n",
      "Run time for discouraged was 0.9497430030023679 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stunned\n",
      "Mean of 10 tensors is: tensor([ 0.0160,  0.0309, -0.0828,  0.1190,  0.1482]) (768 features in tensor)\n",
      "Run time for stunned was 1.0764770018868148 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "fearful\n",
      "Mean of 10 tensors is: tensor([-0.1429, -0.1920,  0.0814,  0.8578,  0.9421]) (768 features in tensor)\n",
      "Run time for fearful was 1.3835058159893379 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "pissed\n",
      "Mean of 10 tensors is: tensor([0.0189, 0.1696, 0.2592, 0.5493, 0.1464]) (768 features in tensor)\n",
      "Run time for pissed was 1.0124489470617846 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "terrified\n",
      "Mean of 10 tensors is: tensor([ 0.1405, -0.4664,  0.1915,  0.6460,  0.9292]) (768 features in tensor)\n",
      "Run time for terrified was 1.170761766959913 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "offended\n",
      "Mean of 10 tensors is: tensor([-0.0655,  0.1223,  0.1667,  0.0354, -0.8100]) (768 features in tensor)\n",
      "Run time for offended was 1.1278336129616946 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "annoyed\n",
      "Mean of 10 tensors is: tensor([ 0.0221,  0.4864,  0.1502,  0.3203, -0.3454]) (768 features in tensor)\n",
      "Run time for annoyed was 0.9451606600778177 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "skeptical\n",
      "Mean of 10 tensors is: tensor([0.1650, 0.3596, 0.1350, 0.2018, 0.3711]) (768 features in tensor)\n",
      "Run time for skeptical was 0.8491270049707964 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "hostile\n",
      "Mean of 10 tensors is: tensor([ 0.0548,  0.1272, -0.1390, -0.1841,  0.9971]) (768 features in tensor)\n",
      "Run time for hostile was 1.5284405159763992 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "furious\n",
      "Mean of 10 tensors is: tensor([ 0.0202,  0.4502, -0.1745,  0.4657,  0.5416]) (768 features in tensor)\n",
      "Run time for furious was 1.2367083720164374 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bothered\n",
      "Mean of 10 tensors is: tensor([ 0.0046,  0.7649, -0.0081,  0.5356, -0.6325]) (768 features in tensor)\n",
      "Run time for bothered was 0.8591109630651772 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "joyful\n",
      "Mean of 10 tensors is: tensor([0.0505, 0.9018, 0.1778, 0.1685, 0.6276]) (768 features in tensor)\n",
      "Run time for joyful was 1.248079615063034 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "uncertain\n",
      "Mean of 10 tensors is: tensor([-0.0313,  0.1196, -0.2751,  0.4403, -0.1710]) (768 features in tensor)\n",
      "Run time for uncertain was 1.0339006589492783 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "cheerful\n",
      "Mean of 10 tensors is: tensor([-0.0157,  1.0259,  0.1302, -0.2382,  0.9299]) (768 features in tensor)\n",
      "Run time for cheerful was 1.4597449690336362 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "depressed\n",
      "Mean of 10 tensors is: tensor([0.2245, 0.4579, 0.2841, 0.1243, 0.7329]) (768 features in tensor)\n",
      "Run time for depressed was 0.9067006279947236 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anxious\n",
      "Mean of 10 tensors is: tensor([0.1918, 0.3129, 0.2201, 0.5230, 0.5096]) (768 features in tensor)\n",
      "Run time for anxious was 0.7296629520133138 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "frustrated\n",
      "Mean of 10 tensors is: tensor([-0.0548,  0.3550,  0.2249,  0.7194,  0.1065]) (768 features in tensor)\n",
      "Run time for frustrated was 0.9787433129968122 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "distressed\n",
      "Mean of 10 tensors is: tensor([ 0.1661,  0.4219, -0.0124,  0.1644,  0.7849]) (768 features in tensor)\n",
      "Run time for distressed was 0.7811389040434733 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bored\n",
      "Mean of 10 tensors is: tensor([-0.1409, -0.0023,  0.4641,  0.3760,  0.5752]) (768 features in tensor)\n",
      "Run time for bored was 1.0064447140321136 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suspicious\n",
      "Mean of 10 tensors is: tensor([ 0.1954,  0.4560,  0.1539, -0.1854,  0.3380]) (768 features in tensor)\n",
      "Run time for suspicious was 1.082762103062123 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shocked\n",
      "Mean of 10 tensors is: tensor([ 0.0644, -0.0384, -0.0929,  0.9493,  0.0695]) (768 features in tensor)\n",
      "Run time for shocked was 0.8123836750164628 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amazed\n",
      "Mean of 10 tensors is: tensor([ 0.0380,  0.3873, -0.2581,  0.5469, -0.3325]) (768 features in tensor)\n",
      "Run time for amazed was 0.7817964319838211 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "rejected\n",
      "Mean of 10 tensors is: tensor([-0.1212, -0.0538, -0.0575,  0.2822,  0.5838]) (768 features in tensor)\n",
      "Run time for rejected was 0.9189965120749548 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "disappointed\n",
      "Mean of 10 tensors is: tensor([-0.0564,  0.0946, -0.0470,  1.1105,  0.0293]) (768 features in tensor)\n",
      "Run time for disappointed was 0.7443980169482529 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "upset\n",
      "Mean of 10 tensors is: tensor([ 0.0699,  0.4774, -0.0096,  0.6656, -0.1779]) (768 features in tensor)\n",
      "Run time for upset was 0.889853514963761 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "delighted\n",
      "Mean of 10 tensors is: tensor([0.0429, 0.3151, 0.0087, 0.9146, 0.1549]) (768 features in tensor)\n",
      "Run time for delighted was 0.7999051460064948 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scared\n",
      "Mean of 10 tensors is: tensor([ 0.1468, -0.3577,  0.0322,  0.6637,  0.8749]) (768 features in tensor)\n",
      "Run time for scared was 0.7324216830311343 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "worried\n",
      "Mean of 10 tensors is: tensor([0.1741, 0.5009, 0.1544, 0.7984, 0.0976]) (768 features in tensor)\n",
      "Run time for worried was 0.939183801994659 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "confused\n",
      "Mean of 10 tensors is: tensor([-0.2360,  0.2358, -0.0240,  1.1303,  0.3073]) (768 features in tensor)\n",
      "Run time for confused was 0.9215691590216011 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "neutral\n",
      "Mean of 10 tensors is: tensor([ 0.0585,  0.1725,  0.3239, -0.0028,  0.2992]) (768 features in tensor)\n",
      "Run time for neutral was 0.801224160939455 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angry\n",
      "Mean of 10 tensors is: tensor([-0.2015, -0.0790, -0.0535,  0.7927,  0.3822]) (768 features in tensor)\n",
      "Run time for angry was 0.7862260869005695 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "satisfied\n",
      "Mean of 10 tensors is: tensor([ 0.0337,  0.4901,  0.0093,  0.3546, -0.5026]) (768 features in tensor)\n",
      "Run time for satisfied was 0.7835762640461326 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "surprised\n",
      "Mean of 10 tensors is: tensor([-0.0077,  0.4779,  0.0401,  0.9564, -0.3635]) (768 features in tensor)\n",
      "Run time for surprised was 0.8177205449901521 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "pleased\n",
      "Mean of 10 tensors is: tensor([0.1020, 0.2530, 0.1555, 0.8330, 0.0218]) (768 features in tensor)\n",
      "Run time for pleased was 0.6539761800086126 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "concerned\n",
      "Mean of 10 tensors is: tensor([-0.0829,  0.5516,  0.3907,  0.5855, -0.3947]) (768 features in tensor)\n",
      "Run time for concerned was 0.8254475189605728 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "mad\n",
      "Mean of 10 tensors is: tensor([ 0.0791,  0.2047, -0.1878,  0.5360,  1.1639]) (768 features in tensor)\n",
      "Run time for mad was 0.738707322976552 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "hurt\n",
      "Mean of 10 tensors is: tensor([ 0.3365,  0.1044, -0.1620, -0.0115, -0.0607]) (768 features in tensor)\n",
      "Run time for hurt was 0.9906283039599657 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "excited\n",
      "Mean of 10 tensors is: tensor([ 0.0480,  0.5860,  0.3118,  0.6763, -0.7028]) (768 features in tensor)\n",
      "Run time for excited was 0.6300752769457176 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sad\n",
      "Mean of 10 tensors is: tensor([ 0.3052,  0.4146, -0.0042,  0.5150,  1.7901]) (768 features in tensor)\n",
      "Run time for sad was 0.8616048420080915 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "interested\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 10 tensors is: tensor([-0.1336,  0.3943,  0.3404,  1.3790, -0.6872]) (768 features in tensor)\n",
      "Run time for interested was 0.6357301060343161 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "happy\n",
      "Mean of 10 tensors is: tensor([0.1192, 0.2869, 0.1430, 1.0610, 0.2167]) (768 features in tensor)\n",
      "Run time for happy was 1.010699694044888 seconds.\n",
      "/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/final_layer_8_aef_10lines.txt /home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/final_layer_8_aef_10lines_counts.txt\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "st\n",
      "upe\n",
      "f\n",
      "ied\n",
      "Mean of 10 tensors is: tensor([ 0.0038,  0.3654, -0.0125, -0.0978, -0.5446]) (768 features in tensor)\n",
      "Run time for stupefied was 16.145301024080254 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "scorn\n",
      "ful\n",
      "Mean of 10 tensors is: tensor([ 0.1378,  0.0449,  0.0543, -0.0913, -0.1127]) (768 features in tensor)\n",
      "Run time for scornful was 9.631033264915459 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disbel\n",
      "ieving\n",
      "Mean of 10 tensors is: tensor([ 0.1134,  0.3276, -0.1688,  0.4073,  0.2847]) (768 features in tensor)\n",
      "Run time for disbelieving was 5.599477916955948 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disdain\n",
      "ful\n",
      "Mean of 10 tensors is: tensor([ 0.1476, -0.0461,  0.1804, -0.2002,  0.0359]) (768 features in tensor)\n",
      "Run time for disdainful was 8.933261608006433 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "revol\n",
      "ted\n",
      "Mean of 10 tensors is: tensor([ 0.0952,  0.3614,  0.1091,  0.8073, -0.0822]) (768 features in tensor)\n",
      "Run time for revolted was 8.17035011597909 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "m\n",
      "iff\n",
      "ed\n",
      "Mean of 10 tensors is: tensor([ 0.0229,  0.5724, -0.0148,  0.3857,  0.2856]) (768 features in tensor)\n",
      "Run time for miffed was 2.8008261809591204 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "a\n",
      "gh\n",
      "ast\n",
      "Mean of 10 tensors is: tensor([0.1026, 0.0820, 0.0148, 0.1001, 0.1249]) (768 features in tensor)\n",
      "Run time for aghast was 5.457188609987497 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "inc\n",
      "ensed\n",
      "Mean of 10 tensors is: tensor([ 0.1519,  0.7366,  0.0692,  0.1276, -0.3625]) (768 features in tensor)\n",
      "Run time for incensed was 3.1101061139488593 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "de\n",
      "jected\n",
      "Mean of 10 tensors is: tensor([ 0.2527,  0.3458,  0.0560, -0.3853,  0.2175]) (768 features in tensor)\n",
      "Run time for dejected was 4.726723995991051 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "rep\n",
      "uls\n",
      "ed\n",
      "Mean of 10 tensors is: tensor([ 0.2028,  0.5209,  0.1086,  0.2106, -0.3446]) (768 features in tensor)\n",
      "Run time for repulsed was 3.316176688997075 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "mourn\n",
      "ful\n",
      "Mean of 10 tensors is: tensor([ 0.3490,  0.7566, -0.1104,  0.2441,  0.6971]) (768 features in tensor)\n",
      "Run time for mournful was 3.305463013937697 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disple\n",
      "ased\n",
      "Mean of 10 tensors is: tensor([ 0.0753,  0.6593,  0.1848,  0.2215, -0.0075]) (768 features in tensor)\n",
      "Run time for displeased was 3.265282294014469 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "infuri\n",
      "ated\n",
      "Mean of 10 tensors is: tensor([ 0.2492,  0.6164,  0.1782,  0.4677, -0.1140]) (768 features in tensor)\n",
      "Run time for infuriated was 2.1974783600308 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aw\n",
      "ed\n",
      "Mean of 10 tensors is: tensor([ 0.0389,  0.5766,  0.0667, -0.3894, -0.1080]) (768 features in tensor)\n",
      "Run time for awed was 3.484780612983741 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "repe\n",
      "lled\n",
      "Mean of 10 tensors is: tensor([ 0.1174,  0.3929,  0.0790,  0.4431, -0.1906]) (768 features in tensor)\n",
      "Run time for repelled was 1.689769243937917 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "resent\n",
      "ful\n",
      "Mean of 10 tensors is: tensor([0.0802, 0.1908, 0.1509, 0.2303, 0.0033]) (768 features in tensor)\n",
      "Run time for resentful was 3.2672269620234147 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sorrow\n",
      "ful\n",
      "Mean of 10 tensors is: tensor([0.2390, 0.6339, 0.0206, 0.6046, 1.0465]) (768 features in tensor)\n",
      "Run time for sorrowful was 2.5349472099915147 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ir\n",
      "ate\n",
      "Mean of 10 tensors is: tensor([ 0.1396,  1.0089,  0.1044,  0.3574, -0.3441]) (768 features in tensor)\n",
      "Run time for irate was 3.124813327915035 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "dismay\n",
      "ed\n",
      "Mean of 10 tensors is: tensor([ 0.0911,  0.2995,  0.2911,  0.2832, -0.0885]) (768 features in tensor)\n",
      "Run time for dismayed was 1.8619210639735684 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "el\n",
      "ated\n",
      "Mean of 10 tensors is: tensor([ 0.2294,  0.8665,  0.0107,  0.6469, -0.6517]) (768 features in tensor)\n",
      "Run time for elated was 1.596788930008188 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "enraged\n",
      "Mean of 10 tensors is: tensor([0.1415, 0.4706, 0.1638, 0.2164, 0.1089]) (768 features in tensor)\n",
      "Run time for enraged was 3.1138050090521574 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "apprehens\n",
      "ive\n",
      "Mean of 10 tensors is: tensor([ 0.3721,  0.1194,  0.1238,  0.8994, -0.3661]) (768 features in tensor)\n",
      "Run time for apprehensive was 1.2704052700428292 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bewild\n",
      "ered\n",
      "Mean of 10 tensors is: tensor([-0.0340,  0.4922,  0.0486,  0.0946,  0.1348]) (768 features in tensor)\n",
      "Run time for bewildered was 2.0448696709936485 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ast\n",
      "ounded\n",
      "Mean of 10 tensors is: tensor([ 0.1985,  0.5476, -0.0031,  0.2404, -0.1155]) (768 features in tensor)\n",
      "Run time for astounded was 2.2613491429947317 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "perplex\n",
      "ed\n",
      "Mean of 10 tensors is: tensor([ 0.1074,  0.3726,  0.0896,  0.1117, -0.1300]) (768 features in tensor)\n",
      "Run time for perplexed was 2.0594377289526165 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "appalled\n",
      "Mean of 10 tensors is: tensor([ 0.1094,  0.3457,  0.2027,  0.0639, -0.1367]) (768 features in tensor)\n",
      "Run time for appalled was 1.9552914400119334 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "startled\n",
      "Mean of 10 tensors is: tensor([0.1572, 0.3822, 0.1014, 0.5839, 0.5565]) (768 features in tensor)\n",
      "Run time for startled was 1.5836890520295128 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "astonished\n",
      "Mean of 10 tensors is: tensor([ 0.2607,  0.7035, -0.1007,  0.1452, -0.2079]) (768 features in tensor)\n",
      "Run time for astonished was 1.92558849893976 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alarmed\n",
      "Mean of 10 tensors is: tensor([ 0.2736,  0.3942,  0.2709,  0.3789, -0.1736]) (768 features in tensor)\n",
      "Run time for alarmed was 1.6074974100338295 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "outraged\n",
      "Mean of 10 tensors is: tensor([ 0.1007,  0.1156,  0.0830,  0.1532, -0.2168]) (768 features in tensor)\n",
      "Run time for outraged was 1.0662168379640207 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "disgusted\n",
      "Mean of 10 tensors is: tensor([ 0.1058,  0.1478,  0.2787, -0.0940, -0.1725]) (768 features in tensor)\n",
      "Run time for disgusted was 2.348254333017394 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "doubtful\n",
      "Mean of 10 tensors is: tensor([0.2231, 0.1888, 0.0462, 0.1935, 0.4298]) (768 features in tensor)\n",
      "Run time for doubtful was 1.1527581410482526 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "dissatisfied\n",
      "Mean of 10 tensors is: tensor([-0.0558,  0.7307,  0.2471,  0.1627, -0.3104]) (768 features in tensor)\n",
      "Run time for dissatisfied was 1.127740875002928 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "saddened\n",
      "Mean of 10 tensors is: tensor([0.1303, 0.8097, 0.3591, 0.7087, 1.3262]) (768 features in tensor)\n",
      "Run time for saddened was 1.2210034830495715 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "irritated\n",
      "Mean of 10 tensors is: tensor([0.2407, 0.5255, 0.3520, 0.0922, 0.0734]) (768 features in tensor)\n",
      "Run time for irritated was 1.3321601749630645 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amused\n",
      "Mean of 10 tensors is: tensor([-0.0175,  0.7807,  0.2730,  0.7524, -0.9138]) (768 features in tensor)\n",
      "Run time for amused was 1.6366685410030186 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "frightened\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 10 tensors is: tensor([0.2206, 0.0311, 0.3946, 0.3962, 0.8169]) (768 features in tensor)\n",
      "Run time for frightened was 1.0588906650664285 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "discouraged\n",
      "Mean of 10 tensors is: tensor([ 0.0420,  0.6553,  0.0315,  0.2672, -0.2772]) (768 features in tensor)\n",
      "Run time for discouraged was 0.9182816169923171 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stunned\n",
      "Mean of 10 tensors is: tensor([ 0.1848,  0.0811,  0.1562, -0.0572,  0.1774]) (768 features in tensor)\n",
      "Run time for stunned was 1.0392984639620408 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "fearful\n",
      "Mean of 10 tensors is: tensor([ 0.0849, -0.1793,  0.2823,  0.5878,  0.9629]) (768 features in tensor)\n",
      "Run time for fearful was 1.4171893609454855 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "pissed\n",
      "Mean of 10 tensors is: tensor([-0.0827,  0.2701,  0.3774,  0.6046,  0.3168]) (768 features in tensor)\n",
      "Run time for pissed was 1.0293510750634596 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "terrified\n",
      "Mean of 10 tensors is: tensor([ 0.2154, -0.3349,  0.3222,  0.2644,  0.8322]) (768 features in tensor)\n",
      "Run time for terrified was 1.1706385979196057 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "offended\n",
      "Mean of 10 tensors is: tensor([-0.1354,  0.2388,  0.1508,  0.1078, -0.6579]) (768 features in tensor)\n",
      "Run time for offended was 1.1476641159970313 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "annoyed\n",
      "Mean of 10 tensors is: tensor([-0.0667,  0.5487,  0.3479,  0.1198, -0.3069]) (768 features in tensor)\n",
      "Run time for annoyed was 0.9628102380083874 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "skeptical\n",
      "Mean of 10 tensors is: tensor([ 0.1656,  0.5009,  0.2343, -0.0346,  0.4788]) (768 features in tensor)\n",
      "Run time for skeptical was 0.8370541939511895 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "hostile\n",
      "Mean of 10 tensors is: tensor([ 0.1422,  0.3417,  0.1316, -0.2374,  0.8528]) (768 features in tensor)\n",
      "Run time for hostile was 1.5711040140595287 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "furious\n",
      "Mean of 10 tensors is: tensor([-0.0238,  0.4702, -0.0164,  0.6120,  0.3918]) (768 features in tensor)\n",
      "Run time for furious was 1.2273430490167812 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bothered\n",
      "Mean of 10 tensors is: tensor([ 0.0870,  0.6061,  0.1043,  0.4565, -0.3377]) (768 features in tensor)\n",
      "Run time for bothered was 0.847034112084657 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "joyful\n",
      "Mean of 10 tensors is: tensor([0.0429, 0.9178, 0.3306, 0.1674, 0.4594]) (768 features in tensor)\n",
      "Run time for joyful was 1.1877956619719043 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "uncertain\n",
      "Mean of 10 tensors is: tensor([ 0.1875,  0.2709, -0.0611,  0.0564,  0.0880]) (768 features in tensor)\n",
      "Run time for uncertain was 1.0047434900188819 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "cheerful\n",
      "Mean of 10 tensors is: tensor([ 0.0173,  1.2422,  0.1882, -0.2110,  0.6127]) (768 features in tensor)\n",
      "Run time for cheerful was 1.455544266034849 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "depressed\n",
      "Mean of 10 tensors is: tensor([0.1969, 0.5829, 0.3869, 0.0730, 0.4108]) (768 features in tensor)\n",
      "Run time for depressed was 0.878875246969983 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anxious\n",
      "Mean of 10 tensors is: tensor([0.2639, 0.3169, 0.2713, 0.4113, 0.4126]) (768 features in tensor)\n",
      "Run time for anxious was 0.7355975180398673 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "frustrated\n",
      "Mean of 10 tensors is: tensor([-0.1058,  0.4916,  0.3194,  0.6421,  0.0611]) (768 features in tensor)\n",
      "Run time for frustrated was 0.9667829689569771 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "distressed\n",
      "Mean of 10 tensors is: tensor([0.3754, 0.6631, 0.3475, 0.3374, 0.6257]) (768 features in tensor)\n",
      "Run time for distressed was 0.7873798930086195 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bored\n",
      "Mean of 10 tensors is: tensor([-0.1649,  0.1263,  0.6231,  0.1327,  0.5921]) (768 features in tensor)\n",
      "Run time for bored was 0.9348868679953739 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suspicious\n",
      "Mean of 10 tensors is: tensor([ 0.1351,  0.6090,  0.1790, -0.1555,  0.2397]) (768 features in tensor)\n",
      "Run time for suspicious was 1.0809225830016658 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shocked\n",
      "Mean of 10 tensors is: tensor([0.0879, 0.3108, 0.0161, 0.7276, 0.1381]) (768 features in tensor)\n",
      "Run time for shocked was 0.7936608650488779 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amazed\n",
      "Mean of 10 tensors is: tensor([ 0.1603,  0.7006, -0.0847,  0.6176, -0.0227]) (768 features in tensor)\n",
      "Run time for amazed was 0.7452334919944406 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "rejected\n",
      "Mean of 10 tensors is: tensor([-0.1270,  0.1331,  0.0835,  0.2956,  0.5466]) (768 features in tensor)\n",
      "Run time for rejected was 0.9090856099501252 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "disappointed\n",
      "Mean of 10 tensors is: tensor([-0.0866,  0.2907,  0.2143,  0.9707,  0.1081]) (768 features in tensor)\n",
      "Run time for disappointed was 0.7463332919869572 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "upset\n",
      "Mean of 10 tensors is: tensor([ 0.0035,  0.5926,  0.1102,  0.5074, -0.3677]) (768 features in tensor)\n",
      "Run time for upset was 0.8718134791124612 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "delighted\n",
      "Mean of 10 tensors is: tensor([ 0.0409,  0.6674, -0.0325,  0.8841,  0.2948]) (768 features in tensor)\n",
      "Run time for delighted was 0.7539061279967427 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scared\n",
      "Mean of 10 tensors is: tensor([ 0.2321, -0.4473,  0.2283,  0.5441,  0.9093]) (768 features in tensor)\n",
      "Run time for scared was 0.7412780949380249 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "worried\n",
      "Mean of 10 tensors is: tensor([0.2427, 0.3313, 0.2175, 0.4475, 0.1039]) (768 features in tensor)\n",
      "Run time for worried was 0.9494539099978283 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "confused\n",
      "Mean of 10 tensors is: tensor([-0.0284,  0.4174,  0.1196,  0.7062,  0.2363]) (768 features in tensor)\n",
      "Run time for confused was 0.9349445630796254 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "neutral\n",
      "Mean of 10 tensors is: tensor([0.1676, 0.3643, 0.4947, 0.0640, 0.1513]) (768 features in tensor)\n",
      "Run time for neutral was 0.8154550839681178 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angry\n",
      "Mean of 10 tensors is: tensor([-0.2339,  0.1168,  0.0936,  0.6571,  0.3515]) (768 features in tensor)\n",
      "Run time for angry was 0.7967904480174184 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "satisfied\n",
      "Mean of 10 tensors is: tensor([ 0.0659,  0.8015,  0.1691,  0.3678, -0.5254]) (768 features in tensor)\n",
      "Run time for satisfied was 0.8015189319849014 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "surprised\n",
      "Mean of 10 tensors is: tensor([ 0.0135,  0.6599,  0.1110,  0.8857, -0.3416]) (768 features in tensor)\n",
      "Run time for surprised was 0.8071150099858642 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "pleased\n",
      "Mean of 10 tensors is: tensor([0.1790, 0.7439, 0.3627, 0.7339, 0.1637]) (768 features in tensor)\n",
      "Run time for pleased was 0.6766182529972866 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "concerned\n",
      "Mean of 10 tensors is: tensor([ 0.0038,  0.4454,  0.4731,  0.2416, -0.2216]) (768 features in tensor)\n",
      "Run time for concerned was 0.8458048629108816 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "mad\n",
      "Mean of 10 tensors is: tensor([0.0932, 0.3361, 0.0850, 0.4285, 1.0614]) (768 features in tensor)\n",
      "Run time for mad was 0.7514328710967675 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "hurt\n",
      "Mean of 10 tensors is: tensor([ 0.2070,  0.1113, -0.0409, -0.1111,  0.0127]) (768 features in tensor)\n",
      "Run time for hurt was 0.9899457620922476 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "excited\n",
      "Mean of 10 tensors is: tensor([ 0.1573,  0.5952,  0.3200,  0.6427, -0.4304]) (768 features in tensor)\n",
      "Run time for excited was 0.6445704279467463 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 10 tensors is: tensor([0.2132, 0.4743, 0.2141, 0.3428, 1.7060]) (768 features in tensor)\n",
      "Run time for sad was 0.8410435459809378 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "interested\n",
      "Mean of 10 tensors is: tensor([-0.1126,  0.4254,  0.3858,  0.9725, -0.7285]) (768 features in tensor)\n",
      "Run time for interested was 0.6348282570252195 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "happy\n",
      "Mean of 10 tensors is: tensor([2.4114e-04, 4.8459e-01, 3.1010e-01, 9.0218e-01, 2.4891e-01]) (768 features in tensor)\n",
      "Run time for happy was 0.9784655469702557 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Make sure we're in the transformers directory with fine-tuned model output.\n",
    "os.chdir('/home/jupyter/Notebooks/crystal/NLP/transformers/examples/')\n",
    "os.getcwd()\n",
    "# Get the 8th layer of the model for all checkpoints in a fine-tuning output directory.\n",
    "for dir, subdirs, files in os.walk(model_root_dir):\n",
    "    for subdir in subdirs:\n",
    "        model = RobertaForMaskedLM.from_pretrained(os.path.join(dir, subdir), config=config)\n",
    "        model.eval()\n",
    "        subdir_output_file = ''\n",
    "        subdir_count_file = ''\n",
    "        subdir_output_file = os.path.join(output_file, subdir + '_layer_8_aef_10lines.txt')\n",
    "        subdir_count_file = os.path.join(count_file, subdir + '_layer_8_aef_10lines_counts.txt')\n",
    "        print(subdir_output_file, subdir_count_file)\n",
    "        # Process vocabulary words in the middle loop.\n",
    "        for v in vocab:\n",
    "            start = timer()\n",
    "            with open(context_file, 'r') as lines:\n",
    "                v_sum = torch.zeros([1, FEATURE_COUNT])\n",
    "                v_tokens = tokenizer.encode(v)\n",
    "                print(f'\\nThere are {len(v_tokens) - 2} tokens in tokenized vocabulary word:')\n",
    "                for t in v_tokens[1:-1]:\n",
    "                    print(tokenizer.decode(t).strip())\n",
    "                count_sentence = 0\n",
    "                count_tensor = 0\n",
    "\n",
    "                # Process all lines in the context file in the inner loop.\n",
    "                for line in lines:\n",
    "                    # Check for this vocab word in this line; if found, split the line into individual sentences.\n",
    "                    if v in line.lower().split():\n",
    "                        for sentence in line.split('.'):\n",
    "                            if v in sentence.lower():\n",
    "                                line = sentence\n",
    "                                count_sentence += 1\n",
    "                                break\n",
    "                        # Split the new sentence-based line into tokens.\n",
    "                        # Use max_length to avoid overflowing the maximum sequence length for the model.\n",
    "                        tokenized_text = tokenizer.encode(line, add_special_tokens=True, max_length=512)\n",
    "                        indices = []              \n",
    "\n",
    "                        # Check to see whether the vocab word is found in this particular line.\n",
    "                        # Initially, some lines may have comprised multiple sentences, which were\n",
    "                        # broken out individually above.\n",
    "                        for t in v_tokens[1:-1]:\n",
    "                            for i, token_str in enumerate(tokenized_text):\n",
    "                                if tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip():\n",
    "                                    indices.append(i)               \n",
    "\n",
    "                        # If the vocabulary word was found, process the containing line.\n",
    "                        if indices:\n",
    "\n",
    "                            # The vocab word was found in this line/sentence, at the locations in indices.\n",
    "                            # Get the feature vectors for all tokens in the line/sentence.\n",
    "                            token_embeddings = create_token_embeddings(tokenized_text)\n",
    "                            token_vecs_layer = get_token_vecs_layer(token_embeddings, LAYER)\n",
    "\n",
    "                            # Get the vocab word's contextual embedding for this line.\n",
    "                            tensor_layer = torch.zeros([1, FEATURE_COUNT])\n",
    "                            for i in range(len(indices)):\n",
    "                                v_index = i % len(v_tokens[1:-1])\n",
    "                                tensor_layer += token_vecs_layer[indices[i]]\n",
    "\n",
    "                            # If our vocab word is broken into more than one token, we need to get the mean of the token embeddings.\n",
    "                            tensor_layer /= len(indices)\n",
    "\n",
    "                            # Add the embedding distilled from this line to the sum of embeddings for all lines.\n",
    "                            v_sum += tensor_layer\n",
    "                            count_tensor += 1\n",
    "\n",
    "                    # Stop processing lines once we've found MAX_LINES instances of our vocab word.\n",
    "                    if count_tensor >= MAX_LINES:\n",
    "                        break\n",
    "\n",
    "                # We're done processing all lines of 512 tokens or less containing our vocab word.\n",
    "                # Get the mean embedding for the word.\n",
    "                v_mean = v_sum / count_tensor\n",
    "                print(f'Mean of {count_tensor} tensors is: {v_mean[0][:5]} ({len(v_mean[0])} features in tensor)')\n",
    "                write_embedding(subdir_output_file, v, v_mean)\n",
    "                try:\n",
    "                    with open(subdir_count_file, 'a') as counts:\n",
    "                        counts.write(v + ', ' + str(count_tensor) + '\\n')\n",
    "                except:\n",
    "                    print('Wha?! Could not write the sentence count.')\n",
    "            end = timer()\n",
    "            print(f'Run time for {v} was {end - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    vocab = []\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_token_embeddings(tokenized_text):\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)  # Batch size 1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "        encoded_layers = outputs[2]\n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "#         print(f'Size of token embeddings is {token_embeddings.size()}')\n",
    "        return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the last 4 layers' features\n",
    "def sum_last_four_token_vecs(token_embeddings):\n",
    "    token_vecs_sum_last_four = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_last_four.append(sum_vec)\n",
    "\n",
    "#     print ('Shape of summed layers is: %d x %d' % (len(token_vecs_sum_last_four), len(token_vecs_sum_last_four[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_sum_last_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a single layer of the model.\n",
    "def get_token_vecs_layer(token_embeddings, layer_number):\n",
    "    token_vecs_layer = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        layer_vec = token[layer_number]\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_layer.append(layer_vec)\n",
    "\n",
    "#     print ('Shape of summed layers is: %d x %d' % (len(token_vecs_layer), len(token_vecs_layer[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "#         print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal-venv-3.6",
   "language": "python",
   "name": "crystal-venv-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
