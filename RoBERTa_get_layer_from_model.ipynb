{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, RobertaConfig\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/Notebooks/crystal/NLP/transformers/examples'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we're in the transformers directory with fine-tuned model output.\n",
    "os.chdir('/home/jupyter/Notebooks/crystal/NLP/transformers/examples/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# and Transformers documentation: https://huggingface.co/transformers/model_doc/roberta.html#robertaformaskedlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('./roBERTa_base')\n",
    "config = RobertaConfig.from_pretrained('./roBERTa_base')\n",
    "model = RobertaForMaskedLM.from_pretrained('./roBERTa_base')\n",
    "model.eval()\n",
    "\n",
    "context_file = \"/home/jupyter/Notebooks/crystal/NLP/transformers/examples/CC_WET_train_aefg\"\n",
    "output_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/'\n",
    "count_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/'\n",
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/data/vocab_files/SimLex_MC_MiFace_vocab.txt'\n",
    "vocab = make_vocab(vocab_file)\n",
    "\n",
    "FEATURE_COUNT = 768\n",
    "LAYER = 8\n",
    "MAX_LINES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_base_layer-8_maxlines-50_aefg.txt /home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_base_layer-8_maxlines-50_aefg_counts.txt\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aback\n",
      "Mean of 50 tensors is: tensor([0.0678, 0.2718, 0.1138, 0.4885, 0.0466]) (768 features in tensor)\n",
      "Run time for aback was 14.45582143124193 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ab\n",
      "ashed\n",
      "Mean of 24 tensors is: tensor([ 0.1307,  0.3149, -0.0467,  0.1518,  0.4097]) (768 features in tensor)\n",
      "Run time for abashed was 102.83370731212199 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "abdomen\n",
      "Mean of 50 tensors is: tensor([ 0.4638, -0.1044,  0.0831,  0.3742, -1.0691]) (768 features in tensor)\n",
      "Run time for abdomen was 9.522613850422204 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "abhor\n",
      "Mean of 50 tensors is: tensor([0.1734, 0.3307, 0.1492, 0.3795, 0.1214]) (768 features in tensor)\n",
      "Run time for abhor was 19.292531684041023 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "abhor\n",
      "red\n",
      "Mean of 50 tensors is: tensor([ 0.1157,  0.2951,  0.1425,  0.5447, -0.5053]) (768 features in tensor)\n",
      "Run time for abhorred was 65.57425657939166 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "abhor\n",
      "rence\n",
      "Mean of 50 tensors is: tensor([ 0.2156,  0.0787,  0.0241,  0.7425, -0.3401]) (768 features in tensor)\n",
      "Run time for abhorrence was 53.64422489795834 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "abhor\n",
      "rent\n",
      "Mean of 50 tensors is: tensor([ 0.2969,  0.3797,  0.0526,  0.2800, -0.5709]) (768 features in tensor)\n",
      "Run time for abhorrent was 22.457104422152042 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "ability\n",
      "Mean of 50 tensors is: tensor([ 0.3216,  0.6134,  0.0648,  0.7001, -1.2258]) (768 features in tensor)\n",
      "Run time for ability was 3.954466924071312 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "ab\n",
      "omin\n",
      "able\n",
      "Mean of 50 tensors is: tensor([ 0.1948,  0.2728, -0.2044,  0.2463,  0.1853]) (768 features in tensor)\n",
      "Run time for abominable was 13.386720870621502 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "abound\n",
      "Mean of 50 tensors is: tensor([-0.0249, -0.0616, -0.1286, -0.7683, -0.5020]) (768 features in tensor)\n",
      "Run time for abound was 7.59056010004133 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "absence\n",
      "Mean of 50 tensors is: tensor([ 0.1175,  0.0799, -0.2256,  0.4930,  0.0106]) (768 features in tensor)\n",
      "Run time for absence was 4.98190791439265 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "absent\n",
      "Mean of 50 tensors is: tensor([ 0.0720, -0.0766, -0.0015,  0.1626,  0.0616]) (768 features in tensor)\n",
      "Run time for absent was 6.413146082311869 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "absorb\n",
      "Mean of 50 tensors is: tensor([ 0.2037, -0.0064,  0.1260,  0.0245, -0.6154]) (768 features in tensor)\n",
      "Run time for absorb was 5.378364499658346 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "absorbed\n",
      "Mean of 50 tensors is: tensor([-0.0173, -0.0565,  0.2302,  0.4847, -0.1840]) (768 features in tensor)\n",
      "Run time for absorbed was 4.394212568178773 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "abundance\n",
      "Mean of 50 tensors is: tensor([ 0.3239,  0.1088,  0.0135, -0.4918, -0.5869]) (768 features in tensor)\n",
      "Run time for abundance was 5.590776169672608 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accept\n",
      "Mean of 50 tensors is: tensor([-0.1054, -0.0041, -0.1646,  0.3682, -0.3196]) (768 features in tensor)\n",
      "Run time for accept was 3.816022456623614 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "acceptance\n",
      "Mean of 50 tensors is: tensor([-0.0535, -0.2029, -0.1664,  0.2688, -0.2386]) (768 features in tensor)\n",
      "Run time for acceptance was 4.554144315421581 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accepted\n",
      "Mean of 50 tensors is: tensor([ 0.0662, -0.0607, -0.0052,  0.5873, -0.1972]) (768 features in tensor)\n",
      "Run time for accepted was 4.450689125806093 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accepting\n",
      "Mean of 50 tensors is: tensor([-0.0555, -0.0522, -0.0127,  0.3402,  0.1772]) (768 features in tensor)\n",
      "Run time for accepting was 4.252683455124497 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accident\n",
      "Mean of 50 tensors is: tensor([ 0.2257,  0.7847,  0.2109, -0.0739, -1.1010]) (768 features in tensor)\n",
      "Run time for accident was 4.589586168527603 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accommodating\n",
      "Mean of 50 tensors is: tensor([ 0.0758,  0.2129,  0.1780,  0.3810, -0.1854]) (768 features in tensor)\n",
      "Run time for accommodating was 6.49236567877233 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accomplish\n",
      "Mean of 50 tensors is: tensor([ 0.2477,  0.1279,  0.1147, -0.1574, -0.9452]) (768 features in tensor)\n",
      "Run time for accomplish was 4.604057221673429 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accomplished\n",
      "Mean of 50 tensors is: tensor([ 0.1708,  0.3116,  0.1804,  0.0065, -0.6944]) (768 features in tensor)\n",
      "Run time for accomplished was 4.6892635729163885 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "accord\n",
      "ant\n",
      "Mean of 50 tensors is: tensor([ 0.2614, -0.6719,  0.2517,  0.5693, -0.1940]) (768 features in tensor)\n",
      "Run time for accordant was 32.42762288916856 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "acc\n",
      "ursed\n",
      "Mean of 50 tensors is: tensor([0.1245, 0.6611, 0.0488, 0.6385, 0.4535]) (768 features in tensor)\n",
      "Run time for accursed was 28.289583620615304 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "accus\n",
      "atory\n",
      "Mean of 50 tensors is: tensor([ 0.1090,  0.1839, -0.0815,  0.2645, -0.6020]) (768 features in tensor)\n",
      "Run time for accusatory was 58.893895723856986 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accused\n",
      "Mean of 50 tensors is: tensor([-0.1593, -0.2619,  0.1106,  0.1522,  0.1836]) (768 features in tensor)\n",
      "Run time for accused was 4.443344091065228 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accusing\n",
      "Mean of 50 tensors is: tensor([-0.3553, -0.0579,  0.0244,  0.0680,  0.7652]) (768 features in tensor)\n",
      "Run time for accusing was 4.623309265822172 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "ac\n",
      "erb\n",
      "ic\n",
      "Mean of 50 tensors is: tensor([ 0.2010,  0.8423, -0.2029, -0.0983, -0.1486]) (768 features in tensor)\n",
      "Run time for acerbic was 45.24803360551596 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "achieve\n",
      "Mean of 50 tensors is: tensor([ 0.3173,  0.0137,  0.0698,  0.0232, -1.3096]) (768 features in tensor)\n",
      "Run time for achieve was 4.183149547316134 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "acidic\n",
      "Mean of 50 tensors is: tensor([ 0.1613,  0.4289,  0.0254,  0.3507, -0.1164]) (768 features in tensor)\n",
      "Run time for acidic was 6.280620298348367 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "acknowledge\n",
      "Mean of 50 tensors is: tensor([ 0.0919, -0.3524, -0.1874, -0.3830,  0.1426]) (768 features in tensor)\n",
      "Run time for acknowledge was 4.780343416146934 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "acquire\n",
      "Mean of 50 tensors is: tensor([ 0.3109, -0.2378,  0.0190,  0.0956, -0.9231]) (768 features in tensor)\n",
      "Run time for acquire was 5.241491335444152 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "action\n",
      "Mean of 50 tensors is: tensor([ 0.1048, -0.1253, -0.0207,  1.0798, -0.7005]) (768 features in tensor)\n",
      "Run time for action was 5.3860103487968445 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "active\n",
      "Mean of 50 tensors is: tensor([-0.0887, -0.2151, -0.0914,  1.0331, -0.5639]) (768 features in tensor)\n",
      "Run time for active was 3.90459637157619 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "activity\n",
      "Mean of 50 tensors is: tensor([ 0.0278,  0.0477,  0.0931,  0.5309, -0.4681]) (768 features in tensor)\n",
      "Run time for activity was 3.953613016754389 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "actor\n",
      "Mean of 50 tensors is: tensor([ 0.0670, -0.4481,  0.1012, -1.0188, -1.1520]) (768 features in tensor)\n",
      "Run time for actor was 6.978329984471202 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "actress\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.1961, -0.2198,  0.0692, -0.6464, -0.5064]) (768 features in tensor)\n",
      "Run time for actress was 10.233919771388173 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "acute\n",
      "Mean of 50 tensors is: tensor([ 0.1484,  0.1825, -0.2688,  0.3861,  0.5295]) (768 features in tensor)\n",
      "Run time for acute was 4.487680409103632 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "adamant\n",
      "Mean of 50 tensors is: tensor([-0.0148,  0.3897, -0.0857, -0.2592, -0.3916]) (768 features in tensor)\n",
      "Run time for adamant was 9.49912191182375 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "add\n",
      "Mean of 50 tensors is: tensor([ 0.3383,  0.5262,  0.1888, -0.1093, -1.2622]) (768 features in tensor)\n",
      "Run time for add was 3.2420229418203235 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "add\n",
      "led\n",
      "Mean of 50 tensors is: tensor([0.1653, 0.3888, 0.0771, 0.2757, 0.0293]) (768 features in tensor)\n",
      "Run time for addled was 40.88772522006184 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "administration\n",
      "Mean of 50 tensors is: tensor([-0.0639,  0.6139,  0.2314, -0.2894, -0.9271]) (768 features in tensor)\n",
      "Run time for administration was 4.572853853926063 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "admiration\n",
      "Mean of 50 tensors is: tensor([ 0.0635, -0.1183,  0.1077, -0.4694, -1.0226]) (768 features in tensor)\n",
      "Run time for admiration was 6.140476318076253 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "admit\n",
      "Mean of 50 tensors is: tensor([ 0.1157, -0.4311, -0.1657, -0.3476,  0.1800]) (768 features in tensor)\n",
      "Run time for admit was 3.9847921319305897 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ad\n",
      "oration\n",
      "Mean of 50 tensors is: tensor([ 0.1846,  0.1524, -0.0562, -0.3413, -0.4428]) (768 features in tensor)\n",
      "Run time for adoration was 11.700245387852192 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ad\n",
      "oring\n",
      "Mean of 50 tensors is: tensor([-0.0688,  0.5953, -0.1531, -0.0013,  0.2022]) (768 features in tensor)\n",
      "Run time for adoring was 11.909059731289744 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ad\n",
      "rift\n",
      "Mean of 50 tensors is: tensor([ 0.0890, -0.2274,  0.0169, -0.0581, -0.1572]) (768 features in tensor)\n",
      "Run time for adrift was 14.429873742163181 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "adult\n",
      "Mean of 50 tensors is: tensor([ 0.0864,  0.0598, -0.0395,  0.2062, -0.3546]) (768 features in tensor)\n",
      "Run time for adult was 9.049212976358831 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "advers\n",
      "arial\n",
      "Mean of 50 tensors is: tensor([ 0.1366,  0.5135, -0.0870,  0.0419, -0.1389]) (768 features in tensor)\n",
      "Run time for adversarial was 24.320596110075712 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "adversary\n",
      "Mean of 50 tensors is: tensor([-0.1806,  0.2852,  0.0808,  0.1522, -0.6115]) (768 features in tensor)\n",
      "Run time for adversary was 13.330536232329905 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "advise\n",
      "Mean of 50 tensors is: tensor([ 8.4304e-04,  4.6828e-01, -7.0393e-02,  7.9063e-01, -8.5641e-01]) (768 features in tensor)\n",
      "Run time for advise was 4.3190057668834925 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aff\n",
      "ability\n",
      "Mean of 20 tensors is: tensor([ 0.2060,  0.6052,  0.1200,  0.4865, -1.1945]) (768 features in tensor)\n",
      "Run time for affability was 115.47646656259894 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "affected\n",
      "Mean of 50 tensors is: tensor([ 0.0126,  0.8354, -0.0138,  1.0353, -0.4583]) (768 features in tensor)\n",
      "Run time for affected was 4.370649398304522 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "affection\n",
      "ate\n",
      "Mean of 50 tensors is: tensor([ 0.1475,  0.7052,  0.1112,  0.0652, -0.9643]) (768 features in tensor)\n",
      "Run time for affectionate was 7.500293682329357 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "afflicted\n",
      "Mean of 50 tensors is: tensor([ 0.3492,  0.2175, -0.0203, -0.0196,  0.1629]) (768 features in tensor)\n",
      "Run time for afflicted was 8.617787542752922 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "aff\n",
      "ront\n",
      "ed\n",
      "Mean of 37 tensors is: tensor([ 0.1248,  0.2003,  0.0490, -0.0114, -0.9539]) (768 features in tensor)\n",
      "Run time for affronted was 108.01632359530777 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "a\n",
      "fl\n",
      "utter\n",
      "Mean of 50 tensors is: tensor([ 0.1932,  0.0483, -0.0061, -0.0690, -0.5032]) (768 features in tensor)\n",
      "Run time for aflutter was 54.11788835097104 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "afraid\n",
      "Mean of 50 tensors is: tensor([ 0.1697, -0.2557,  0.0858,  0.8498,  0.4115]) (768 features in tensor)\n",
      "Run time for afraid was 4.680080458521843 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ag\n",
      "ape\n",
      "Mean of 50 tensors is: tensor([ 0.1654,  0.1208,  0.0140, -0.1661,  0.5888]) (768 features in tensor)\n",
      "Run time for agape was 16.159897211939096 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "age\n",
      "Mean of 50 tensors is: tensor([ 0.1530, -0.3331, -0.2738, -0.9351, -0.0423]) (768 features in tensor)\n",
      "Run time for age was 6.081644851714373 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aggravated\n",
      "Mean of 50 tensors is: tensor([-0.0379,  0.1037,  0.2095,  0.7784, -0.2038]) (768 features in tensor)\n",
      "Run time for aggravated was 11.47121827211231 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aggrav\n",
      "ation\n",
      "Mean of 50 tensors is: tensor([ 0.1114,  0.9082, -0.0872,  0.5979, -0.9597]) (768 features in tensor)\n",
      "Run time for aggravation was 13.302562799304724 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aggression\n",
      "Mean of 50 tensors is: tensor([ 0.1216, -0.2473,  0.0868,  0.3184, -0.6039]) (768 features in tensor)\n",
      "Run time for aggression was 6.858123631216586 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aggressive\n",
      "Mean of 50 tensors is: tensor([ 0.0267,  0.1312, -0.0700,  0.8250, -0.4131]) (768 features in tensor)\n",
      "Run time for aggressive was 4.365510372444987 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "agg\n",
      "rieve\n",
      "Mean of 2 tensors is: tensor([ 0.1643,  0.5284,  0.0622,  0.2465, -0.8153]) (768 features in tensor)\n",
      "Run time for aggrieve was 101.03609250113368 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "agg\n",
      "rieved\n",
      "Mean of 50 tensors is: tensor([ 0.1517,  0.4819,  0.1320,  0.8508, -0.6564]) (768 features in tensor)\n",
      "Run time for aggrieved was 18.842772751115263 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "a\n",
      "gh\n",
      "ast\n",
      "Mean of 50 tensors is: tensor([ 0.0414,  0.1056, -0.0036,  0.2696,  0.0713]) (768 features in tensor)\n",
      "Run time for aghast was 29.25280108023435 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "agitated\n",
      "Mean of 50 tensors is: tensor([ 0.2113,  0.3725,  0.0057,  0.3703, -0.1136]) (768 features in tensor)\n",
      "Run time for agitated was 9.22640771791339 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ag\n",
      "og\n",
      "Mean of 50 tensors is: tensor([ 0.2029, -0.2036, -0.1152, -0.0667, -0.0950]) (768 features in tensor)\n",
      "Run time for agog was 77.78102146927267 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "agon\n",
      "ized\n",
      "Mean of 50 tensors is: tensor([ 0.2285,  0.5500, -0.0568,  0.3388, -0.4645]) (768 features in tensor)\n",
      "Run time for agonized was 45.25592742860317 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "agony\n",
      "Mean of 50 tensors is: tensor([ 3.6249e-01,  4.5518e-01,  5.9729e-03,  3.8794e-04, -3.8125e-01]) (768 features in tensor)\n",
      "Run time for agony was 8.602289944887161 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "agree\n",
      "Mean of 50 tensors is: tensor([ 0.2616,  0.1767,  0.0499,  0.0111, -0.8204]) (768 features in tensor)\n",
      "Run time for agree was 3.9739797562360764 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "agreeable\n",
      "Mean of 50 tensors is: tensor([ 0.2038,  0.6774,  0.1775,  0.2902, -0.3799]) (768 features in tensor)\n",
      "Run time for agreeable was 5.913765958510339 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "agreement\n",
      "Mean of 50 tensors is: tensor([ 0.1433,  0.4279,  0.0610,  0.2092, -1.2339]) (768 features in tensor)\n",
      "Run time for agreement was 4.449381075799465 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ag\n",
      "ressive\n",
      "Mean of 50 tensors is: tensor([ 0.0712,  0.0029,  0.0273,  0.1761, -0.8093]) (768 features in tensor)\n",
      "Run time for agressive was 18.83535980153829 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "air\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.3920, -0.0249,  0.0183, -0.9810, -0.3431]) (768 features in tensor)\n",
      "Run time for air was 3.8291665110737085 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "air\n",
      "head\n",
      "Mean of 50 tensors is: tensor([ 0.1502, -0.1447, -0.0410, -0.6845,  0.1570]) (768 features in tensor)\n",
      "Run time for airhead was 54.85662148613483 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "airport\n",
      "Mean of 50 tensors is: tensor([-0.0867,  0.8159,  0.0223, -0.7761, -0.6384]) (768 features in tensor)\n",
      "Run time for airport was 4.5908777276054025 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aisle\n",
      "Mean of 50 tensors is: tensor([ 0.1025,  0.5465, -0.0319, -0.4098, -0.8816]) (768 features in tensor)\n",
      "Run time for aisle was 7.818837974220514 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alarm\n",
      "Mean of 50 tensors is: tensor([ 0.1356,  0.2249, -0.1112, -0.5541, -0.3347]) (768 features in tensor)\n",
      "Run time for alarm was 4.900718811899424 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alarmed\n",
      "Mean of 50 tensors is: tensor([ 0.1126,  0.3581,  0.1278,  0.2449, -0.1796]) (768 features in tensor)\n",
      "Run time for alarmed was 9.258540775626898 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alarming\n",
      "Mean of 50 tensors is: tensor([ 0.1733,  0.3804, -0.0145, -0.0552,  0.0454]) (768 features in tensor)\n",
      "Run time for alarming was 6.913088668137789 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alcohol\n",
      "Mean of 50 tensors is: tensor([-0.0070,  0.2358,  0.1101, -0.1917, -0.3117]) (768 features in tensor)\n",
      "Run time for alcohol was 5.933129302226007 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alert\n",
      "Mean of 50 tensors is: tensor([ 0.1078, -0.2291,  0.0312, -0.0004, -0.1454]) (768 features in tensor)\n",
      "Run time for alert was 4.2881513787433505 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alerted\n",
      "Mean of 50 tensors is: tensor([ 0.0522, -0.1738, -0.0671,  0.3338, -0.5051]) (768 features in tensor)\n",
      "Run time for alerted was 7.059551725164056 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alienated\n",
      "Mean of 50 tensors is: tensor([ 0.2070,  0.0643,  0.1805,  0.2709, -0.2594]) (768 features in tensor)\n",
      "Run time for alienated was 10.194913916289806 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "allergic\n",
      "Mean of 50 tensors is: tensor([ 0.3689,  0.8840,  0.2568, -0.0920,  0.1200]) (768 features in tensor)\n",
      "Run time for allergic was 4.77399670984596 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "allev\n",
      "iated\n",
      "Mean of 50 tensors is: tensor([ 0.4071,  0.6629,  0.2174,  0.3472, -0.5317]) (768 features in tensor)\n",
      "Run time for alleviated was 15.987238829955459 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alley\n",
      "Mean of 50 tensors is: tensor([ 0.1105, -0.0263, -0.0667,  0.3130, -0.2034]) (768 features in tensor)\n",
      "Run time for alley was 9.06446130387485 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "allow\n",
      "Mean of 50 tensors is: tensor([ 0.2293,  0.2557,  0.1411,  0.0543, -0.7180]) (768 features in tensor)\n",
      "Run time for allow was 4.086421113461256 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "all\n",
      "uring\n",
      "Mean of 50 tensors is: tensor([ 0.2117,  0.5972, -0.1434, -0.4350, -0.6155]) (768 features in tensor)\n",
      "Run time for alluring was 7.100407727062702 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "al\n",
      "oof\n",
      "Mean of 50 tensors is: tensor([-0.0060,  0.4705, -0.0672, -0.1397, -0.0550]) (768 features in tensor)\n",
      "Run time for aloof was 17.28248308505863 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aluminum\n",
      "Mean of 50 tensors is: tensor([ 0.2084,  0.4071, -0.0332, -0.3193, -0.2616]) (768 features in tensor)\n",
      "Run time for aluminum was 7.594898038543761 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "am\n",
      "atory\n",
      "Mean of 50 tensors is: tensor([ 0.1737,  0.0641, -0.0330,  0.1286, -0.1398]) (768 features in tensor)\n",
      "Run time for amatory was 106.04512845631689 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amazed\n",
      "Mean of 50 tensors is: tensor([ 0.0956,  0.4006, -0.0321,  0.2458, -0.4289]) (768 features in tensor)\n",
      "Run time for amazed was 4.169138074852526 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "amaz\n",
      "ement\n",
      "Mean of 50 tensors is: tensor([ 0.1845,  0.4659,  0.0536,  0.5377, -0.6998]) (768 features in tensor)\n",
      "Run time for amazement was 10.60866869147867 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amazing\n",
      "Mean of 50 tensors is: tensor([ 0.0969,  0.6247, -0.0415, -0.4032, -0.3443]) (768 features in tensor)\n",
      "Run time for amazing was 3.56779167894274 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "ambition\n",
      "Mean of 50 tensors is: tensor([ 0.1425, -0.3508, -0.0241,  0.4952, -0.7718]) (768 features in tensor)\n",
      "Run time for ambition was 5.492460888810456 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "ambitious\n",
      "Mean of 50 tensors is: tensor([ 0.0698, -0.0320, -0.0662,  0.0589,  0.1124]) (768 features in tensor)\n",
      "Run time for ambitious was 5.194111414253712 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "amb\n",
      "ival\n",
      "ence\n",
      "Mean of 50 tensors is: tensor([-0.0086,  0.3862, -0.0447, -0.0568, -1.0087]) (768 features in tensor)\n",
      "Run time for ambivalence was 26.899061198346317 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "amb\n",
      "ivalent\n",
      "Mean of 50 tensors is: tensor([-0.0496,  0.6134, -0.0594,  0.0080, -0.9004]) (768 features in tensor)\n",
      "Run time for ambivalent was 13.094389245845377 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "am\n",
      "enable\n",
      "Mean of 50 tensors is: tensor([-0.0962,  0.5492,  0.1847,  0.5407, -0.8218]) (768 features in tensor)\n",
      "Run time for amenable was 15.417025360278785 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "am\n",
      "iable\n",
      "Mean of 50 tensors is: tensor([ 0.1395,  0.8436,  0.0286,  0.4340, -0.4515]) (768 features in tensor)\n",
      "Run time for amiable was 14.052364280447364 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "am\n",
      "icable\n",
      "Mean of 50 tensors is: tensor([ 0.0222,  1.3548,  0.2603,  0.4951, -0.3121]) (768 features in tensor)\n",
      "Run time for amicable was 20.355108394287527 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amused\n",
      "Mean of 50 tensors is: tensor([-0.0453,  0.8712,  0.1793,  0.4929, -1.2299]) (768 features in tensor)\n",
      "Run time for amused was 7.305273925885558 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amusement\n",
      "Mean of 50 tensors is: tensor([ 0.0876,  0.8850,  0.1566, -0.4245, -0.8725]) (768 features in tensor)\n",
      "Run time for amusement was 9.252642358653247 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "analytical\n",
      "Mean of 50 tensors is: tensor([ 0.1035,  0.3973,  0.0119,  0.6428, -0.2996]) (768 features in tensor)\n",
      "Run time for analytical was 5.560619221068919 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "analyze\n",
      "Mean of 50 tensors is: tensor([ 0.3174, -0.1343, -0.3660,  0.7784, -0.3206]) (768 features in tensor)\n",
      "Run time for analyze was 3.4936485439538956 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "analyzing\n",
      "Mean of 50 tensors is: tensor([ 0.1366, -0.0908,  0.1257,  0.7032,  0.4618]) (768 features in tensor)\n",
      "Run time for analyzing was 5.1542570581659675 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anarchy\n",
      "Mean of 50 tensors is: tensor([ 0.1005, -0.2594,  0.3075,  0.1096, -0.3884]) (768 features in tensor)\n",
      "Run time for anarchy was 30.502467829734087 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anatomy\n",
      "Mean of 50 tensors is: tensor([ 0.2940,  0.2746,  0.0851,  0.9111, -0.8953]) (768 features in tensor)\n",
      "Run time for anatomy was 10.322523551993072 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anchor\n",
      "Mean of 50 tensors is: tensor([ 0.0502,  0.0301,  0.1441, -0.4482,  0.0137]) (768 features in tensor)\n",
      "Run time for anchor was 8.888883874751627 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "ancient\n",
      "Mean of 50 tensors is: tensor([ 0.0263,  0.1410,  0.0492, -0.4349,  0.1703]) (768 features in tensor)\n",
      "Run time for ancient was 5.863348552025855 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anger\n",
      "Mean of 50 tensors is: tensor([ 0.1857, -0.2767,  0.1784,  0.4349, -0.2641]) (768 features in tensor)\n",
      "Run time for anger was 6.777068138122559 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([0.1501, 0.2755, 0.2214, 0.2111, 0.1267]) (768 features in tensor)\n",
      "Run time for angered was 12.460219966247678 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angle\n",
      "Mean of 50 tensors is: tensor([ 0.1606,  0.7239,  0.1291,  0.5848, -0.1953]) (768 features in tensor)\n",
      "Run time for angle was 5.274487386457622 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angrily\n",
      "Mean of 50 tensors is: tensor([ 0.4645,  0.3924, -0.1596,  0.0620,  0.2278]) (768 features in tensor)\n",
      "Run time for angrily was 14.52509154099971 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angry\n",
      "Mean of 50 tensors is: tensor([0.0321, 0.1823, 0.0149, 0.6952, 0.1702]) (768 features in tensor)\n",
      "Run time for angry was 4.628210744820535 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angst\n",
      "Mean of 50 tensors is: tensor([ 0.2315,  0.1470,  0.0293, -0.0009, -0.0442]) (768 features in tensor)\n",
      "Run time for angst was 9.81400201190263 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anguish\n",
      "Mean of 50 tensors is: tensor([ 0.3624,  0.5392, -0.0236,  0.2774, -0.0825]) (768 features in tensor)\n",
      "Run time for anguish was 7.4662881987169385 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "ang\n",
      "u\n",
      "ished\n",
      "Mean of 50 tensors is: tensor([ 0.0563,  0.2596, -0.1694,  0.6337, -0.1670]) (768 features in tensor)\n",
      "Run time for anguished was 28.138040268793702 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "animal\n",
      "Mean of 50 tensors is: tensor([ 0.0259,  0.4062,  0.0505, -0.7202, -0.2552]) (768 features in tensor)\n",
      "Run time for animal was 6.496272996068001 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "animated\n",
      "Mean of 50 tensors is: tensor([ 0.2288,  0.2041,  0.0965, -0.0261, -0.6621]) (768 features in tensor)\n",
      "Run time for animated was 7.474777980707586 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "animosity\n",
      "Mean of 50 tensors is: tensor([-0.0443,  0.3729, -0.0973, -0.1370, -0.1307]) (768 features in tensor)\n",
      "Run time for animosity was 24.139151461422443 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "ankle\n",
      "Mean of 50 tensors is: tensor([ 0.2614, -0.1588,  0.0217,  0.4522, -0.2844]) (768 features in tensor)\n",
      "Run time for ankle was 6.172299169935286 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "announce\n",
      "Mean of 50 tensors is: tensor([ 0.1554, -0.1286, -0.1520,  0.6649, -0.4767]) (768 features in tensor)\n",
      "Run time for announce was 4.229204976931214 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "annoyance\n",
      "Mean of 50 tensors is: tensor([-0.0172,  1.0605,  0.1039,  0.1772, -0.9934]) (768 features in tensor)\n",
      "Run time for annoyance was 11.284129739739 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "annoyed\n",
      "Mean of 50 tensors is: tensor([ 0.1008,  0.5678,  0.1831,  0.4572, -0.5569]) (768 features in tensor)\n",
      "Run time for annoyed was 5.138909951783717 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "annoying\n",
      "Mean of 50 tensors is: tensor([ 0.0778,  1.0842, -0.0698,  0.1615,  0.0673]) (768 features in tensor)\n",
      "Run time for annoying was 5.267192989587784 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "ant\n",
      "Mean of 50 tensors is: tensor([ 0.0780, -0.2267, -0.0064,  0.0812,  0.1333]) (768 features in tensor)\n",
      "Run time for ant was 14.503426192328334 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "antagon\n",
      "istic\n",
      "Mean of 50 tensors is: tensor([ 0.0672,  0.4162, -0.1796,  0.2936, -0.2121]) (768 features in tensor)\n",
      "Run time for antagonistic was 15.539071531035006 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "antagon\n",
      "ized\n",
      "Mean of 50 tensors is: tensor([ 0.1449,  0.3290,  0.1403,  0.5450, -0.0386]) (768 features in tensor)\n",
      "Run time for antagonized was 80.04281698539853 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anticipated\n",
      "Mean of 50 tensors is: tensor([ 0.1456,  0.3686,  0.0695, -0.1207, -0.5945]) (768 features in tensor)\n",
      "Run time for anticipated was 5.385613405145705 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anticipating\n",
      "Mean of 50 tensors is: tensor([ 0.2047, -0.0440,  0.1064,  0.2313,  0.0801]) (768 features in tensor)\n",
      "Run time for anticipating was 6.8664581486955285 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anticipation\n",
      "Mean of 50 tensors is: tensor([ 0.2198,  0.0369,  0.1332, -0.1260, -0.7548]) (768 features in tensor)\n",
      "Run time for anticipation was 5.508608273230493 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "anticip\n",
      "ative\n",
      "Mean of 10 tensors is: tensor([ 0.2878, -0.0865, -0.0810,  1.0063, -0.8442]) (768 features in tensor)\n",
      "Run time for anticipative was 103.85063551738858 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "anticip\n",
      "atory\n",
      "Mean of 50 tensors is: tensor([ 0.3113, -0.0560, -0.0031,  0.3172, -0.7889]) (768 features in tensor)\n",
      "Run time for anticipatory was 18.766861972399056 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "antip\n",
      "athy\n",
      "Mean of 50 tensors is: tensor([ 0.1733,  0.2168, -0.0471, -0.0163, -0.5235]) (768 features in tensor)\n",
      "Run time for antipathy was 27.34583557397127 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ants\n",
      "y\n",
      "Mean of 50 tensors is: tensor([ 0.0969, -0.0813,  0.2608,  0.2642,  0.2276]) (768 features in tensor)\n",
      "Run time for antsy was 17.732787643559277 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anxiety\n",
      "Mean of 50 tensors is: tensor([0.2670, 0.4379, 0.2989, 0.0762, 0.2389]) (768 features in tensor)\n",
      "Run time for anxiety was 5.725577652454376 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anxious\n",
      "Mean of 50 tensors is: tensor([0.1367, 0.2596, 0.1128, 0.3483, 0.0187]) (768 features in tensor)\n",
      "Run time for anxious was 4.889125742018223 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "anx\n",
      "iously\n",
      "Mean of 50 tensors is: tensor([ 0.1660,  0.7274, -0.2155,  0.0539, -0.4653]) (768 features in tensor)\n",
      "Run time for anxiously was 9.61057632509619 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "apartment\n",
      "Mean of 50 tensors is: tensor([-0.0664, -0.1508,  0.2501, -0.6039, -0.5574]) (768 features in tensor)\n",
      "Run time for apartment was 4.097811181098223 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ap\n",
      "athetic\n",
      "Mean of 50 tensors is: tensor([-0.0124,  0.2351,  0.0891, -0.4094,  0.1886]) (768 features in tensor)\n",
      "Run time for apathetic was 29.667883107438684 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ap\n",
      "athy\n",
      "Mean of 50 tensors is: tensor([ 0.0563, -0.1132,  0.1472, -0.6061,  0.0569]) (768 features in tensor)\n",
      "Run time for apathy was 21.716837869025767 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "apolog\n",
      "etic\n",
      "Mean of 50 tensors is: tensor([ 0.0850,  0.1526,  0.1196,  0.3883, -0.2454]) (768 features in tensor)\n",
      "Run time for apologetic was 11.358038942329586 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "appalled\n",
      "Mean of 50 tensors is: tensor([0.0956, 0.1514, 0.0936, 0.1001, 0.0455]) (768 features in tensor)\n",
      "Run time for appalled was 9.401625148952007 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "app\n",
      "all\n",
      "ingly\n",
      "Mean of 50 tensors is: tensor([ 0.1248,  0.8074, -0.2056, -0.5206, -0.6418]) (768 features in tensor)\n",
      "Run time for appallingly was 63.01045390404761 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "apparent\n",
      "Mean of 50 tensors is: tensor([ 0.0867, -0.2022,  0.0138, -0.1858, -0.1134]) (768 features in tensor)\n",
      "Run time for apparent was 4.9126013442873955 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "appear\n",
      "Mean of 50 tensors is: tensor([ 0.0552, -0.1701,  0.0527, -0.6256, -1.0841]) (768 features in tensor)\n",
      "Run time for appear was 4.36865464784205 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "appe\n",
      "ased\n",
      "Mean of 50 tensors is: tensor([ 0.1440,  0.1392, -0.0315,  0.1966, -0.5332]) (768 features in tensor)\n",
      "Run time for appeased was 60.39339389465749 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "appe\n",
      "asing\n",
      "Mean of 50 tensors is: tensor([ 0.0130,  0.2391, -0.0301,  0.2311, -0.0714]) (768 features in tensor)\n",
      "Run time for appeasing was 33.23824317846447 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "apple\n",
      "Mean of 50 tensors is: tensor([ 0.0884, -0.2207,  0.2339,  0.2779, -0.5346]) (768 features in tensor)\n",
      "Run time for apple was 7.30636240541935 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "appliance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.2321,  0.3407, -0.0770, -0.8135, -0.8890]) (768 features in tensor)\n",
      "Run time for appliance was 10.992344048805535 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "appoint\n",
      "Mean of 50 tensors is: tensor([-0.0145,  0.2525,  0.0441,  0.1105, -1.5270]) (768 features in tensor)\n",
      "Run time for appoint was 8.04832282755524 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "appointment\n",
      "Mean of 50 tensors is: tensor([ 0.2094,  0.4981,  0.2017,  0.0540, -0.6496]) (768 features in tensor)\n",
      "Run time for appointment was 4.962004272267222 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "appreci\n",
      "ative\n",
      "Mean of 50 tensors is: tensor([-0.0136,  0.2793, -0.0094,  0.7884, -1.0356]) (768 features in tensor)\n",
      "Run time for appreciative was 5.797259575687349 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "apprehension\n",
      "Mean of 50 tensors is: tensor([ 0.1369,  0.0151,  0.0641,  0.0508, -0.7101]) (768 features in tensor)\n",
      "Run time for apprehension was 15.43090220913291 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "apprehens\n",
      "ive\n",
      "Mean of 50 tensors is: tensor([ 0.1801,  0.2262,  0.1021,  0.9616, -0.7798]) (768 features in tensor)\n",
      "Run time for apprehensive was 10.70721438806504 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "approve\n",
      "Mean of 50 tensors is: tensor([ 0.1279,  0.5169, -0.0599,  0.0036, -1.2214]) (768 features in tensor)\n",
      "Run time for approve was 4.338226391002536 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "approved\n",
      "Mean of 50 tensors is: tensor([ 0.1118,  0.1948, -0.0774,  0.5831, -0.6620]) (768 features in tensor)\n",
      "Run time for approved was 3.9410230023786426 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "approving\n",
      "Mean of 50 tensors is: tensor([ 0.2186,  0.2759, -0.0201,  0.1609, -0.4179]) (768 features in tensor)\n",
      "Run time for approving was 7.574765356257558 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "arch\n",
      "bishop\n",
      "Mean of 50 tensors is: tensor([-0.0215, -0.1844,  0.2273, -0.3340, -0.4322]) (768 features in tensor)\n",
      "Run time for archbishop was 31.527035321108997 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "area\n",
      "Mean of 50 tensors is: tensor([ 0.0809,  0.4159,  0.1034,  0.0345, -0.0198]) (768 features in tensor)\n",
      "Run time for area was 4.143676660954952 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "argue\n",
      "Mean of 50 tensors is: tensor([ 0.1073,  0.4695,  0.0903,  0.0838, -1.1430]) (768 features in tensor)\n",
      "Run time for argue was 4.262103395536542 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "argument\n",
      "Mean of 50 tensors is: tensor([ 0.1133,  0.6722,  0.0716,  0.1044, -1.0130]) (768 features in tensor)\n",
      "Run time for argument was 4.355084104463458 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "argument\n",
      "ative\n",
      "Mean of 50 tensors is: tensor([ 0.2306,  0.0401, -0.0092,  1.5342, -0.5945]) (768 features in tensor)\n",
      "Run time for argumentative was 5.03332281857729 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "arithmetic\n",
      "Mean of 50 tensors is: tensor([ 0.1977, -0.2392,  0.0021,  0.4878, -0.2913]) (768 features in tensor)\n",
      "Run time for arithmetic was 20.638783186674118 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "arm\n",
      "Mean of 50 tensors is: tensor([ 0.1992,  0.1761,  0.1397,  0.1011, -0.6537]) (768 features in tensor)\n",
      "Run time for arm was 4.482731922529638 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "army\n",
      "Mean of 50 tensors is: tensor([ 0.0923,  0.1418, -0.0095, -0.4699, -0.5577]) (768 features in tensor)\n",
      "Run time for army was 7.38181546330452 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aroused\n",
      "Mean of 50 tensors is: tensor([ 0.4154, -0.0311,  0.1002,  0.2011, -0.2229]) (768 features in tensor)\n",
      "Run time for aroused was 10.58809329289943 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "arrange\n",
      "Mean of 50 tensors is: tensor([ 0.2404, -0.1454,  0.1181,  0.3813, -1.0200]) (768 features in tensor)\n",
      "Run time for arrange was 4.7876704428344965 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "arrive\n",
      "Mean of 50 tensors is: tensor([ 0.1100, -0.0554, -0.2154, -0.5810, -1.7048]) (768 features in tensor)\n",
      "Run time for arrive was 4.569122423417866 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "arrogance\n",
      "Mean of 50 tensors is: tensor([ 0.0603, -0.2054,  0.1759, -0.7118, -0.6110]) (768 features in tensor)\n",
      "Run time for arrogance was 9.7260075872764 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "arrogant\n",
      "Mean of 50 tensors is: tensor([ 0.0934, -0.2649,  0.0344, -0.6384, -0.1590]) (768 features in tensor)\n",
      "Run time for arrogant was 5.682246237061918 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "arrog\n",
      "antly\n",
      "Mean of 50 tensors is: tensor([ 0.2650,  0.4051, -0.3522, -0.5071, -0.8469]) (768 features in tensor)\n",
      "Run time for arrogantly was 36.60612615291029 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "arrow\n",
      "Mean of 50 tensors is: tensor([ 0.0619, -0.4917,  0.1752, -0.3270, -0.8054]) (768 features in tensor)\n",
      "Run time for arrow was 7.25502554513514 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "article\n",
      "Mean of 50 tensors is: tensor([ 0.0205,  0.1635,  0.3048, -0.0972, -1.1394]) (768 features in tensor)\n",
      "Run time for article was 3.51822448335588 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "artificial\n",
      "Mean of 50 tensors is: tensor([ 0.1493,  0.2862,  0.0551, -0.4832,  0.3276]) (768 features in tensor)\n",
      "Run time for artificial was 4.279416324570775 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "ashamed\n",
      "Mean of 50 tensors is: tensor([ 0.1989, -0.4977,  0.1452,  0.4642, -0.6043]) (768 features in tensor)\n",
      "Run time for ashamed was 4.614574429579079 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "ask\n",
      "Mean of 50 tensors is: tensor([ 0.1538, -0.4411,  0.0955,  0.3172, -0.3448]) (768 features in tensor)\n",
      "Run time for ask was 4.500006299465895 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aspiring\n",
      "Mean of 50 tensors is: tensor([ 0.2322,  0.0593, -0.0436,  0.2869, -0.4704]) (768 features in tensor)\n",
      "Run time for aspiring was 4.452092730440199 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "assert\n",
      "ive\n",
      "Mean of 50 tensors is: tensor([ 0.2694,  0.3769,  0.1917,  0.2655, -0.6357]) (768 features in tensor)\n",
      "Run time for assertive was 10.73579697869718 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "assert\n",
      "ively\n",
      "Mean of 50 tensors is: tensor([ 0.4596,  0.5526,  0.1555, -0.2055, -0.7366]) (768 features in tensor)\n",
      "Run time for assertively was 51.85017593204975 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "assessing\n",
      "Mean of 50 tensors is: tensor([ 0.2738, -0.1169,  0.1381,  0.4795, -0.0728]) (768 features in tensor)\n",
      "Run time for assessing was 5.081292066723108 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "assignment\n",
      "Mean of 50 tensors is: tensor([ 0.2204, -0.1133,  0.0995,  0.6089, -0.0868]) (768 features in tensor)\n",
      "Run time for assignment was 5.334620086476207 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "assume\n",
      "Mean of 50 tensors is: tensor([ 0.2224, -0.4362, -0.0461,  0.5608, -0.3921]) (768 features in tensor)\n",
      "Run time for assume was 3.997465867549181 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "assured\n",
      "Mean of 50 tensors is: tensor([ 0.3063, -0.0168,  0.0687,  0.1853, -0.3207]) (768 features in tensor)\n",
      "Run time for assured was 3.9406720148399472 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "astonished\n",
      "Mean of 50 tensors is: tensor([ 0.1010,  0.2392,  0.1732,  0.0883, -0.2634]) (768 features in tensor)\n",
      "Run time for astonished was 8.380741445347667 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aston\n",
      "ishment\n",
      "Mean of 50 tensors is: tensor([ 0.2798,  0.5982,  0.1406,  0.9624, -0.9778]) (768 features in tensor)\n",
      "Run time for astonishment was 21.441190572455525 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ast\n",
      "ounded\n",
      "Mean of 50 tensors is: tensor([ 0.1375,  0.4668,  0.0165,  0.3109, -0.2862]) (768 features in tensor)\n",
      "Run time for astounded was 9.51045816205442 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "asylum\n",
      "Mean of 50 tensors is: tensor([-0.0907, -0.2936,  0.0263, -0.6075,  0.0060]) (768 features in tensor)\n",
      "Run time for asylum was 9.680172340013087 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "atmosphere\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.1165,  0.1672,  0.2404, -0.5144,  0.3720]) (768 features in tensor)\n",
      "Run time for atmosphere was 5.114715863950551 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "atom\n",
      "Mean of 50 tensors is: tensor([ 0.1494,  0.2631,  0.3219, -0.4035,  0.0687]) (768 features in tensor)\n",
      "Run time for atom was 103.2143241846934 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "attach\n",
      "Mean of 50 tensors is: tensor([ 0.2901,  0.4358,  0.0806,  0.0979, -1.4896]) (768 features in tensor)\n",
      "Run time for attach was 4.342141373082995 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "attempting\n",
      "Mean of 50 tensors is: tensor([0.0810, 0.3266, 0.0580, 0.5391, 0.1896]) (768 features in tensor)\n",
      "Run time for attempting was 4.456236680969596 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "attend\n",
      "Mean of 50 tensors is: tensor([ 0.1576, -0.0490, -0.2086,  0.1007, -0.9227]) (768 features in tensor)\n",
      "Run time for attend was 4.0578291565179825 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "attention\n",
      "Mean of 50 tensors is: tensor([ 0.1744,  0.5145,  0.1901,  1.0983, -1.3069]) (768 features in tensor)\n",
      "Run time for attention was 4.398865940980613 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "attentive\n",
      "Mean of 50 tensors is: tensor([ 0.0755,  0.2591,  0.2293,  0.7005, -0.7185]) (768 features in tensor)\n",
      "Run time for attentive was 5.6574542876333 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "attent\n",
      "iveness\n",
      "Mean of 50 tensors is: tensor([ 0.2385,  0.2997,  0.2473,  0.5000, -1.2505]) (768 features in tensor)\n",
      "Run time for attentiveness was 25.86500513833016 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "attitude\n",
      "Mean of 50 tensors is: tensor([0.1676, 0.0377, 0.0784, 0.2259, 0.1460]) (768 features in tensor)\n",
      "Run time for attitude was 4.682753941044211 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "attorney\n",
      "Mean of 50 tensors is: tensor([ 0.0465,  0.3687,  0.3117, -0.7454, -0.6934]) (768 features in tensor)\n",
      "Run time for attorney was 7.52710791118443 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "attracted\n",
      "Mean of 50 tensors is: tensor([ 0.3041,  0.3673,  0.2313,  0.1881, -0.7041]) (768 features in tensor)\n",
      "Run time for attracted was 4.067623450420797 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aug\n",
      "ust\n",
      "Mean of 50 tensors is: tensor([-0.0380,  0.1549,  0.0014,  0.0628, -0.4027]) (768 features in tensor)\n",
      "Run time for august was 12.391312976367772 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aunt\n",
      "Mean of 50 tensors is: tensor([ 0.2737,  0.4405,  0.1358, -0.1083, -0.9137]) (768 features in tensor)\n",
      "Run time for aunt was 7.233226543292403 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "author\n",
      "Mean of 50 tensors is: tensor([ 0.3531, -0.1719,  0.3237, -0.1279, -1.5548]) (768 features in tensor)\n",
      "Run time for author was 4.609708573669195 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "automobile\n",
      "Mean of 50 tensors is: tensor([-0.0874,  0.5185, -0.0036,  0.0386, -1.1090]) (768 features in tensor)\n",
      "Run time for automobile was 8.9840148948133 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aven\n",
      "ging\n",
      "Mean of 50 tensors is: tensor([ 0.2262,  0.3269, -0.1394,  0.6439,  0.9653]) (768 features in tensor)\n",
      "Run time for avenging was 18.48332087881863 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "a\n",
      "verse\n",
      "Mean of 50 tensors is: tensor([ 0.2442,  0.1369,  0.0960,  0.1801, -0.6506]) (768 features in tensor)\n",
      "Run time for averse was 13.059760401025414 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aversion\n",
      "Mean of 50 tensors is: tensor([ 0.1548,  0.2180,  0.1219,  0.3914, -0.6582]) (768 features in tensor)\n",
      "Run time for aversion was 6.5864955596625805 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "a\n",
      "versive\n",
      "Mean of 50 tensors is: tensor([ 0.2110,  0.2265, -0.0813,  0.4287, -0.1260]) (768 features in tensor)\n",
      "Run time for aversive was 36.78851808048785 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aviation\n",
      "Mean of 50 tensors is: tensor([-0.0515,  0.7699, -0.0753, -0.0325, -0.5635]) (768 features in tensor)\n",
      "Run time for aviation was 8.816576696932316 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "avid\n",
      "Mean of 50 tensors is: tensor([ 0.0891,  0.5540, -0.0853,  0.4300, -0.6645]) (768 features in tensor)\n",
      "Run time for avid was 4.930518641136587 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "avoid\n",
      "Mean of 50 tensors is: tensor([ 0.4966,  0.0317,  0.1481,  0.1573, -1.0175]) (768 features in tensor)\n",
      "Run time for avoid was 4.320679225958884 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "avoiding\n",
      "Mean of 50 tensors is: tensor([ 0.3486,  0.0319,  0.0300,  0.1993, -0.1830]) (768 features in tensor)\n",
      "Run time for avoiding was 4.222752284258604 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "awaiting\n",
      "Mean of 50 tensors is: tensor([ 0.1188,  0.1018,  0.0965,  0.7083, -0.2942]) (768 features in tensor)\n",
      "Run time for awaiting was 2.52047214563936 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "awakened\n",
      "Mean of 50 tensors is: tensor([ 0.1794,  0.0258,  0.1883, -0.2886,  0.2482]) (768 features in tensor)\n",
      "Run time for awakened was 8.717534409835935 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aware\n",
      "Mean of 50 tensors is: tensor([ 0.0551, -0.1819,  0.0593,  0.3908, -0.1989]) (768 features in tensor)\n",
      "Run time for aware was 3.97828249912709 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "awareness\n",
      "Mean of 50 tensors is: tensor([ 0.0826, -0.3203,  0.1772, -0.3174, -0.4588]) (768 features in tensor)\n",
      "Run time for awareness was 4.811087862588465 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "awe\n",
      "Mean of 50 tensors is: tensor([ 0.1833, -0.0958,  0.1432, -0.4965, -0.7257]) (768 features in tensor)\n",
      "Run time for awe was 6.193617698736489 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aw\n",
      "ed\n",
      "Mean of 50 tensors is: tensor([ 0.2057,  0.3580,  0.0081, -0.5730, -0.3152]) (768 features in tensor)\n",
      "Run time for awed was 13.827767451293766 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "aw\n",
      "est\n",
      "ruck\n",
      "Mean of 50 tensors is: tensor([ 0.0132, -0.0726, -0.0397,  0.0725, -0.4396]) (768 features in tensor)\n",
      "Run time for awestruck was 26.71138790436089 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "awful\n",
      "Mean of 50 tensors is: tensor([ 0.1617,  0.2509, -0.1127, -0.1972,  0.3067]) (768 features in tensor)\n",
      "Run time for awful was 4.565513669513166 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "awkward\n",
      "Mean of 50 tensors is: tensor([ 0.1719,  0.0377,  0.0512,  0.0158, -0.1776]) (768 features in tensor)\n",
      "Run time for awkward was 5.276848223991692 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "awkward\n",
      "ness\n",
      "Mean of 50 tensors is: tensor([ 0.2650,  0.2441,  0.2390, -0.0139, -0.7096]) (768 features in tensor)\n",
      "Run time for awkwardness was 15.280623183585703 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ax\n",
      "ed\n",
      "Mean of 50 tensors is: tensor([ 0.0593,  0.4700,  0.0817, -0.5704, -0.6584]) (768 features in tensor)\n",
      "Run time for axed was 23.064824705012143 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "baby\n",
      "Mean of 50 tensors is: tensor([ 0.1030,  0.3477,  0.1182, -0.2332, -0.5475]) (768 features in tensor)\n",
      "Run time for baby was 6.231762370094657 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "back\n",
      "handed\n",
      "Mean of 50 tensors is: tensor([ 0.1837,  0.3244, -0.1343,  0.4267, -0.1502]) (768 features in tensor)\n",
      "Run time for backhanded was 71.30366088449955 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bacon\n",
      "Mean of 50 tensors is: tensor([ 0.0711, -0.3638,  0.1704, -0.2203, -0.2800]) (768 features in tensor)\n",
      "Run time for bacon was 8.12621085345745 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bad\n",
      "Mean of 50 tensors is: tensor([ 0.2112,  0.5248, -0.0349,  0.2015,  0.5018]) (768 features in tensor)\n",
      "Run time for bad was 3.785773550160229 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "badly\n",
      "Mean of 50 tensors is: tensor([ 0.1759,  0.6774, -0.1375, -0.2320,  0.8437]) (768 features in tensor)\n",
      "Run time for badly was 4.38353565428406 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "baff\n",
      "le\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.1315,  0.2180, -0.0339, -0.2252, -0.5190]) (768 features in tensor)\n",
      "Run time for baffle was 18.051189319230616 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "baffled\n",
      "Mean of 50 tensors is: tensor([-0.0488,  0.3646,  0.0945,  0.3340,  0.2618]) (768 features in tensor)\n",
      "Run time for baffled was 9.600227010436356 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "baff\n",
      "ling\n",
      "Mean of 50 tensors is: tensor([ 0.0548,  0.3666, -0.0071,  0.3269,  0.1301]) (768 features in tensor)\n",
      "Run time for baffling was 12.391212930902839 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bag\n",
      "Mean of 50 tensors is: tensor([ 0.1554, -0.1191,  0.1651,  0.3748,  0.4960]) (768 features in tensor)\n",
      "Run time for bag was 4.7575295977294445 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "baked\n",
      "Mean of 50 tensors is: tensor([ 0.2239, -0.0821,  0.1297,  0.3686,  0.7931]) (768 features in tensor)\n",
      "Run time for baked was 5.849575004540384 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "ball\n",
      "Mean of 50 tensors is: tensor([ 0.1425, -0.1859, -0.0053, -0.1388,  0.3643]) (768 features in tensor)\n",
      "Run time for ball was 5.073435932397842 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ball\n",
      "ad\n",
      "Mean of 50 tensors is: tensor([ 0.5622,  0.7164, -0.1620, -0.3300,  0.1949]) (768 features in tensor)\n",
      "Run time for ballad was 23.57219872623682 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "balloon\n",
      "Mean of 50 tensors is: tensor([ 0.1581,  0.0025,  0.3733, -1.0087, -0.0267]) (768 features in tensor)\n",
      "Run time for balloon was 9.095691693015397 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ban\n",
      "al\n",
      "Mean of 50 tensors is: tensor([ 0.0344,  0.5993, -0.1191,  0.6891, -0.1573]) (768 features in tensor)\n",
      "Run time for banal was 11.401450146920979 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "band\n",
      "Mean of 50 tensors is: tensor([ 0.2572,  0.3963, -0.1224, -0.2607, -0.3674]) (768 features in tensor)\n",
      "Run time for band was 5.3917796071618795 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "banker\n",
      "Mean of 50 tensors is: tensor([-0.0275,  0.7487,  0.0588,  0.0931, -0.7093]) (768 features in tensor)\n",
      "Run time for banker was 11.897033325396478 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bar\n",
      "Mean of 50 tensors is: tensor([-0.0449,  0.4045,  0.0365, -0.7630, -0.3825]) (768 features in tensor)\n",
      "Run time for bar was 4.228300095535815 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "barking\n",
      "Mean of 50 tensors is: tensor([ 0.0096,  0.0834, -0.2400,  0.0167,  1.2221]) (768 features in tensor)\n",
      "Run time for barking was 7.952033259905875 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "barn\n",
      "Mean of 50 tensors is: tensor([ 0.2097,  0.1401,  0.0746, -0.3348,  0.0182]) (768 features in tensor)\n",
      "Run time for barn was 6.5985992308706045 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "baseball\n",
      "Mean of 50 tensors is: tensor([-0.0448, -0.1074, -0.2926,  0.6212,  0.2761]) (768 features in tensor)\n",
      "Run time for baseball was 5.13421927113086 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bash\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.3710,  0.4000, -0.3414,  0.3418,  0.4840]) (768 features in tensor)\n",
      "Run time for bashful was 15.023998158052564 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "basket\n",
      "Mean of 50 tensors is: tensor([ 0.0935, -0.0443,  0.0679,  0.0579,  0.3722]) (768 features in tensor)\n",
      "Run time for basket was 5.755515303462744 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "basketball\n",
      "Mean of 50 tensors is: tensor([ 0.3076, -0.0424, -0.1099, -0.0035, -0.9078]) (768 features in tensor)\n",
      "Run time for basketball was 7.883240434341133 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bath\n",
      "Mean of 50 tensors is: tensor([ 0.1809,  0.1049,  0.1268, -0.2888,  0.2117]) (768 features in tensor)\n",
      "Run time for bath was 4.1539801843464375 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bathroom\n",
      "Mean of 50 tensors is: tensor([ 0.1106,  0.3947,  0.1226, -0.2585, -0.2453]) (768 features in tensor)\n",
      "Run time for bathroom was 3.8515859311446548 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "battle\n",
      "Mean of 50 tensors is: tensor([ 0.0450,  0.1700,  0.1547, -0.0770, -0.2769]) (768 features in tensor)\n",
      "Run time for battle was 5.751382169313729 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "beach\n",
      "Mean of 50 tensors is: tensor([ 0.1681,  0.8460,  0.1610, -0.1037, -0.3454]) (768 features in tensor)\n",
      "Run time for beach was 8.453342198394239 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "be\n",
      "aming\n",
      "Mean of 50 tensors is: tensor([ 0.0910, -0.0188, -0.4193, -0.3136,  0.2441]) (768 features in tensor)\n",
      "Run time for beaming was 8.727151508443058 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bean\n",
      "Mean of 50 tensors is: tensor([ 0.1843, -0.1866,  0.0707,  0.0938, -0.0128]) (768 features in tensor)\n",
      "Run time for bean was 16.000109468586743 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bear\n",
      "ish\n",
      "Mean of 50 tensors is: tensor([0.1140, 0.3747, 0.0535, 1.1601, 0.0898]) (768 features in tensor)\n",
      "Run time for bearish was 7.7599894823506474 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "beast\n",
      "Mean of 50 tensors is: tensor([ 0.0133,  0.0260,  0.1607, -0.6625,  0.0510]) (768 features in tensor)\n",
      "Run time for beast was 8.629856882616878 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "beat\n",
      "Mean of 50 tensors is: tensor([ 0.1173,  0.2238, -0.0087, -0.3758, -0.6827]) (768 features in tensor)\n",
      "Run time for beat was 3.8567768037319183 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "beaten\n",
      "Mean of 50 tensors is: tensor([ 0.0210, -0.0390,  0.1660, -0.3326, -0.2621]) (768 features in tensor)\n",
      "Run time for beaten was 4.8464781837537885 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "beautiful\n",
      "Mean of 50 tensors is: tensor([ 0.1729,  1.0971, -0.1244, -0.1789,  0.2062]) (768 features in tensor)\n",
      "Run time for beautiful was 4.093257629312575 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "beauty\n",
      "Mean of 50 tensors is: tensor([ 0.2199,  0.9582,  0.0464, -0.2539, -0.5857]) (768 features in tensor)\n",
      "Run time for beauty was 4.4730031955987215 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "become\n",
      "Mean of 50 tensors is: tensor([-0.0295, -0.0047,  0.5106,  0.0515, -1.3685]) (768 features in tensor)\n",
      "Run time for become was 3.784285904839635 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bed\n",
      "Mean of 50 tensors is: tensor([ 0.1276,  0.0249, -0.0040, -0.0011, -0.0750]) (768 features in tensor)\n",
      "Run time for bed was 5.223357479088008 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bed\n",
      "ev\n",
      "iled\n",
      "Mean of 50 tensors is: tensor([ 0.2973,  0.2038, -0.1609,  0.1466, -0.3642]) (768 features in tensor)\n",
      "Run time for bedeviled was 76.05874519608915 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bedroom\n",
      "Mean of 50 tensors is: tensor([ 0.2868,  0.5283,  0.0161, -0.1897,  0.5996]) (768 features in tensor)\n",
      "Run time for bedroom was 4.210458193905652 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bee\n",
      "Mean of 50 tensors is: tensor([ 0.0827,  0.3542, -0.0040, -0.3014,  0.1627]) (768 features in tensor)\n",
      "Run time for bee was 9.322121613658965 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "beef\n",
      "Mean of 50 tensors is: tensor([ 0.1505, -0.2787,  0.0051, -1.1069, -0.3426]) (768 features in tensor)\n",
      "Run time for beef was 8.04947362281382 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "beer\n",
      "Mean of 50 tensors is: tensor([ 0.0306, -0.2018,  0.0863, -0.3907, -0.6830]) (768 features in tensor)\n",
      "Run time for beer was 5.533785266801715 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "be\n",
      "f\n",
      "uddled\n",
      "Mean of 50 tensors is: tensor([ 5.8391e-02,  4.0094e-01, -1.0315e-04,  3.1635e-02, -1.7634e-01]) (768 features in tensor)\n",
      "Run time for befuddled was 21.871406987309456 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "beg\n",
      "Mean of 50 tensors is: tensor([-0.1266, -0.2670, -0.0829,  0.1739,  0.2158]) (768 features in tensor)\n",
      "Run time for beg was 7.164216264151037 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "begging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([-0.1983, -0.3551, -0.0834,  0.1298,  0.8533]) (768 features in tensor)\n",
      "Run time for begging was 6.114422778598964 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "begin\n",
      "Mean of 50 tensors is: tensor([ 0.1446,  0.2559, -0.1971,  0.2509, -1.6240]) (768 features in tensor)\n",
      "Run time for begin was 3.8584720101207495 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "be\n",
      "gr\n",
      "udge\n",
      "Mean of 50 tensors is: tensor([-0.0452, -0.3495, -0.1547,  0.2001, -0.5327]) (768 features in tensor)\n",
      "Run time for begrudge was 18.475448158569634 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "be\n",
      "gr\n",
      "udging\n",
      "Mean of 43 tensors is: tensor([ 0.0250, -0.1415, -0.2545,  0.2014,  0.2047]) (768 features in tensor)\n",
      "Run time for begrudging was 108.53741673659533 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "be\n",
      "gr\n",
      "udging\n",
      "ly\n",
      "Mean of 50 tensors is: tensor([ 0.1700,  0.1237, -0.2849,  0.2099, -0.1499]) (768 features in tensor)\n",
      "Run time for begrudgingly was 28.698909149505198 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "beg\n",
      "u\n",
      "iled\n",
      "Mean of 50 tensors is: tensor([ 0.2603,  0.0379, -0.0233, -0.0295, -0.1500]) (768 features in tensor)\n",
      "Run time for beguiled was 24.332311811856925 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "behave\n",
      "Mean of 50 tensors is: tensor([ 0.1564, -0.1176, -0.1761,  0.6425, -0.9496]) (768 features in tensor)\n",
      "Run time for behave was 5.220857908017933 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bel\n",
      "ated\n",
      "Mean of 50 tensors is: tensor([ 0.1633,  0.8445, -0.3332,  0.7457,  0.5502]) (768 features in tensor)\n",
      "Run time for belated was 5.89552157279104 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "belief\n",
      "Mean of 50 tensors is: tensor([ 0.1599,  0.0257, -0.1674,  0.3393, -0.1485]) (768 features in tensor)\n",
      "Run time for belief was 5.12622841168195 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "believe\n",
      "Mean of 50 tensors is: tensor([ 0.0055,  0.1127, -0.0325,  0.5955,  0.0839]) (768 features in tensor)\n",
      "Run time for believe was 3.6015435038134456 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bel\n",
      "itt\n",
      "ling\n",
      "Mean of 50 tensors is: tensor([ 0.1563,  0.1793, -0.2217,  0.5781, -0.3869]) (768 features in tensor)\n",
      "Run time for belittling was 15.461004498414695 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bell\n",
      "Mean of 50 tensors is: tensor([ 0.0268, -0.1176, -0.1540, -0.5102,  0.0221]) (768 features in tensor)\n",
      "Run time for bell was 7.12649064976722 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bellig\n",
      "erence\n",
      "Mean of 50 tensors is: tensor([ 0.1112,  0.0618, -0.0193,  0.5546, -0.4554]) (768 features in tensor)\n",
      "Run time for belligerence was 61.11359609849751 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bellig\n",
      "erent\n",
      "Mean of 50 tensors is: tensor([ 0.0870,  0.5276,  0.0641,  0.8425, -0.5561]) (768 features in tensor)\n",
      "Run time for belligerent was 18.274639438837767 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "belly\n",
      "Mean of 50 tensors is: tensor([ 0.3754, -0.3173,  0.1900,  0.6211, -0.0485]) (768 features in tensor)\n",
      "Run time for belly was 5.546979082748294 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "belonging\n",
      "Mean of 50 tensors is: tensor([ 0.1165,  0.3518, -0.0183, -0.3642,  0.1941]) (768 features in tensor)\n",
      "Run time for belonging was 5.004580564796925 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "belt\n",
      "Mean of 50 tensors is: tensor([ 0.1153, -0.2621, -0.0810,  0.8430, -0.2290]) (768 features in tensor)\n",
      "Run time for belt was 5.9858249351382256 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "be\n",
      "m\n",
      "used\n",
      "Mean of 50 tensors is: tensor([ 0.0074,  0.6169, -0.0157,  0.3392, -0.1500]) (768 features in tensor)\n",
      "Run time for bemused was 24.22082227934152 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "be\n",
      "m\n",
      "use\n",
      "ment\n",
      "Mean of 50 tensors is: tensor([-0.0096,  0.6691, -0.0737,  0.3049, -0.5958]) (768 features in tensor)\n",
      "Run time for bemusement was 89.40998046845198 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bench\n",
      "Mean of 50 tensors is: tensor([ 0.0246,  0.2115,  0.1884, -0.4633, -0.1015]) (768 features in tensor)\n",
      "Run time for bench was 6.363023026846349 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bene\n",
      "vol\n",
      "ence\n",
      "Mean of 50 tensors is: tensor([ 0.1758,  0.4576, -0.1181,  0.0082, -0.8598]) (768 features in tensor)\n",
      "Run time for benevolence was 21.565634136088192 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "benevolent\n",
      "Mean of 50 tensors is: tensor([ 0.1013,  0.3643,  0.0078, -0.2381,  0.4633]) (768 features in tensor)\n",
      "Run time for benevolent was 9.356382706202567 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "ben\n",
      "umb\n",
      "ed\n",
      "Mean of 12 tensors is: tensor([ 0.3381, -0.0730,  0.0907, -0.2951, -0.1457]) (768 features in tensor)\n",
      "Run time for benumbed was 105.04013811796904 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ber\n",
      "ate\n",
      "Mean of 50 tensors is: tensor([-0.0101,  0.0793,  0.0156,  0.6506, -0.8248]) (768 features in tensor)\n",
      "Run time for berate was 15.631351470947266 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ber\n",
      "ating\n",
      "Mean of 50 tensors is: tensor([-0.0415,  0.3426, -0.2408,  0.5125, -0.1339]) (768 features in tensor)\n",
      "Run time for berating was 27.82344689592719 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bere\n",
      "aved\n",
      "Mean of 50 tensors is: tensor([ 0.2681,  0.8262,  0.1701, -0.1797, -0.0469]) (768 features in tensor)\n",
      "Run time for bereaved was 11.958707716315985 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bere\n",
      "ft\n",
      "Mean of 50 tensors is: tensor([-0.1566,  0.1125, -0.2145,  0.0118,  0.1772]) (768 features in tensor)\n",
      "Run time for bereft was 22.641962154768407 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bes\n",
      "ee\n",
      "ching\n",
      "Mean of 50 tensors is: tensor([ 0.1951,  0.1306, -0.0970,  0.3664,  0.5012]) (768 features in tensor)\n",
      "Run time for beseeching was 99.78849333152175 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "best\n",
      "ed\n",
      "Mean of 50 tensors is: tensor([ 0.0523,  0.6517,  0.1204, -0.6312, -0.8209]) (768 features in tensor)\n",
      "Run time for bested was 32.2938363943249 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "betrayal\n",
      "Mean of 50 tensors is: tensor([ 0.1073,  0.1665,  0.1793, -0.4173, -0.1142]) (768 features in tensor)\n",
      "Run time for betrayal was 8.952742200344801 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "betrayed\n",
      "Mean of 50 tensors is: tensor([ 0.0868, -0.2183,  0.1434, -0.4227,  0.1468]) (768 features in tensor)\n",
      "Run time for betrayed was 7.131272468715906 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "beverage\n",
      "Mean of 50 tensors is: tensor([ 0.0117,  0.1071, -0.0527, -0.4595, -0.4063]) (768 features in tensor)\n",
      "Run time for beverage was 7.952621773816645 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bewild\n",
      "ered\n",
      "Mean of 50 tensors is: tensor([-0.0148,  0.3224,  0.0664,  0.3503, -0.0494]) (768 features in tensor)\n",
      "Run time for bewildered was 23.45083315949887 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bewild\n",
      "er\n",
      "ment\n",
      "Mean of 50 tensors is: tensor([ 3.6411e-04,  4.6472e-01,  7.9453e-02,  3.2003e-01, -8.3885e-01]) (768 features in tensor)\n",
      "Run time for bewilderment was 27.641686596907675 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bi\n",
      "Mean of 50 tensors is: tensor([0.3169, 0.3622, 0.3524, 0.7694, 0.2700]) (768 features in tensor)\n",
      "Run time for bi was 8.864286748692393 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bias\n",
      "Mean of 50 tensors is: tensor([-0.0203,  0.0525, -0.0018,  0.8240,  0.1409]) (768 features in tensor)\n",
      "Run time for bias was 5.620131383650005 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bible\n",
      "Mean of 50 tensors is: tensor([ 0.2232,  0.1903,  0.2942,  0.4278, -0.3288]) (768 features in tensor)\n",
      "Run time for bible was 10.139377693645656 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bicycle\n",
      "Mean of 50 tensors is: tensor([-0.2254,  0.6888,  0.2054, -0.2171, -0.2797]) (768 features in tensor)\n",
      "Run time for bicycle was 9.42330944351852 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "big\n",
      "Mean of 50 tensors is: tensor([ 0.1984, -0.0292, -0.2427,  0.3639,  1.0815]) (768 features in tensor)\n",
      "Run time for big was 9.11203171312809 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bil\n",
      "ious\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.1424,  0.5495, -0.1097, -0.2243,  0.2043]) (768 features in tensor)\n",
      "Run time for bilious was 77.83089735079557 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "biography\n",
      "Mean of 50 tensors is: tensor([ 0.2731,  0.0879,  0.2423,  0.2050, -0.3211]) (768 features in tensor)\n",
      "Run time for biography was 6.592969001270831 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "biology\n",
      "Mean of 50 tensors is: tensor([ 0.1209,  0.5662, -0.0900,  1.2592, -0.4936]) (768 features in tensor)\n",
      "Run time for biology was 8.478996598161757 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bird\n",
      "Mean of 50 tensors is: tensor([ 0.0285,  0.2871,  0.2494, -0.6381,  0.0720]) (768 features in tensor)\n",
      "Run time for bird was 9.549430299550295 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "birthday\n",
      "Mean of 50 tensors is: tensor([-0.0208,  0.3260, -0.1618,  0.3403,  0.9125]) (768 features in tensor)\n",
      "Run time for birthday was 4.806927410885692 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bishop\n",
      "Mean of 50 tensors is: tensor([ 0.0149, -0.0456,  0.1890, -0.1833, -0.2283]) (768 features in tensor)\n",
      "Run time for bishop was 25.30937488656491 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bit\n",
      "Mean of 50 tensors is: tensor([-0.1171,  1.1286,  0.2325,  0.6313,  0.8674]) (768 features in tensor)\n",
      "Run time for bit was 3.3849291373044252 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "biting\n",
      "Mean of 50 tensors is: tensor([ 0.0981,  0.3005, -0.0916,  0.1680,  1.2314]) (768 features in tensor)\n",
      "Run time for biting was 7.333491791039705 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bitter\n",
      "Mean of 50 tensors is: tensor([ 0.1127,  0.3435, -0.0193,  0.3821,  0.6315]) (768 features in tensor)\n",
      "Run time for bitter was 4.672860560938716 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bitters\n",
      "weet\n",
      "Mean of 50 tensors is: tensor([ 0.1369,  1.0260, -0.0337,  0.6047,  0.8170]) (768 features in tensor)\n",
      "Run time for bittersweet was 6.929703918285668 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bizarre\n",
      "Mean of 50 tensors is: tensor([0.0846, 0.1573, 0.0333, 0.5514, 0.7547]) (768 features in tensor)\n",
      "Run time for bizarre was 7.99731033295393 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blade\n",
      "Mean of 50 tensors is: tensor([ 0.0312,  0.1046,  0.0644,  0.2211, -0.4512]) (768 features in tensor)\n",
      "Run time for blade was 7.01695541292429 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blaming\n",
      "Mean of 50 tensors is: tensor([-0.0812, -0.0839,  0.0098,  0.6321,  0.7713]) (768 features in tensor)\n",
      "Run time for blaming was 4.491100378334522 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bland\n",
      "Mean of 50 tensors is: tensor([ 0.1453, -0.5583,  0.1023,  0.8898, -0.1174]) (768 features in tensor)\n",
      "Run time for bland was 5.529608090408146 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blank\n",
      "Mean of 50 tensors is: tensor([ 0.0601, -0.7381,  0.0912, -0.6603,  0.7276]) (768 features in tensor)\n",
      "Run time for blank was 3.664888324216008 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blanket\n",
      "Mean of 50 tensors is: tensor([ 0.1008, -0.1464,  0.1828, -1.1601,  1.1039]) (768 features in tensor)\n",
      "Run time for blanket was 5.420123641379178 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bl\n",
      "ase\n",
      "Mean of 50 tensors is: tensor([ 0.0661,  0.0459, -0.2469,  0.2986, -0.4165]) (768 features in tensor)\n",
      "Run time for blase was 19.16781726665795 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bl\n",
      "azed\n",
      "Mean of 50 tensors is: tensor([ 0.3146, -0.0042, -0.2160, -0.5056, -0.6302]) (768 features in tensor)\n",
      "Run time for blazed was 16.284505509771407 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bleak\n",
      "Mean of 50 tensors is: tensor([ 0.2270,  0.2481, -0.1091, -0.8109,  1.3870]) (768 features in tensor)\n",
      "Run time for bleak was 7.900920753367245 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ble\n",
      "ary\n",
      "Mean of 50 tensors is: tensor([ 0.2155, -0.0368, -0.1022,  0.4892,  1.0363]) (768 features in tensor)\n",
      "Run time for bleary was 59.99928041175008 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blessed\n",
      "Mean of 50 tensors is: tensor([0.2277, 0.2104, 0.0652, 0.6737, 0.2842]) (768 features in tensor)\n",
      "Run time for blessed was 3.912485391832888 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blew\n",
      "Mean of 50 tensors is: tensor([ 0.2447,  0.4498,  0.0842, -0.0351,  0.1736]) (768 features in tensor)\n",
      "Run time for blew was 5.321897320449352 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blinded\n",
      "Mean of 50 tensors is: tensor([ 0.2525,  0.0340,  0.0616, -0.4705,  0.0048]) (768 features in tensor)\n",
      "Run time for blinded was 8.34372746758163 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "blind\n",
      "sided\n",
      "Mean of 50 tensors is: tensor([ 0.0871,  0.2102, -0.0844,  0.0702, -0.2044]) (768 features in tensor)\n",
      "Run time for blindsided was 19.190796189010143 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bliss\n",
      "Mean of 50 tensors is: tensor([ 0.3850,  0.4670,  0.1990,  0.1265, -0.1153]) (768 features in tensor)\n",
      "Run time for bliss was 7.328116234391928 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bliss\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.3610,  0.7027, -0.0706,  0.4092,  0.3793]) (768 features in tensor)\n",
      "Run time for blissful was 6.975669885054231 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bliss\n",
      "fully\n",
      "Mean of 50 tensors is: tensor([0.1552, 0.6613, 0.0408, 0.0652, 0.3559]) (768 features in tensor)\n",
      "Run time for blissfully was 12.350283689796925 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bl\n",
      "ithe\n",
      "Mean of 50 tensors is: tensor([ 0.1617,  0.1324, -0.0797, -0.8169,  0.2374]) (768 features in tensor)\n",
      "Run time for blithe was 31.351322889328003 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blood\n",
      "Mean of 50 tensors is: tensor([ 0.0560, -0.2066,  0.3260, -0.2258, -0.0620]) (768 features in tensor)\n",
      "Run time for blood was 4.4071831395849586 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blown\n",
      "Mean of 50 tensors is: tensor([ 0.0961,  0.1248,  0.1821, -0.3409,  0.1200]) (768 features in tensor)\n",
      "Run time for blown was 4.0780853079631925 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blue\n",
      "Mean of 50 tensors is: tensor([ 0.4727, -0.4887, -0.0944,  0.1410,  0.7004]) (768 features in tensor)\n",
      "Run time for blue was 3.756898078136146 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blues\n",
      "Mean of 50 tensors is: tensor([0.2612, 0.4204, 0.1134, 0.2417, 0.5097]) (768 features in tensor)\n",
      "Run time for blues was 5.496505091898143 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bluff\n",
      "ing\n",
      "Mean of 50 tensors is: tensor([ 0.1166,  0.2271, -0.0538, -0.2032, -0.3674]) (768 features in tensor)\n",
      "Run time for bluffing was 27.839634254574776 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blunt\n",
      "Mean of 50 tensors is: tensor([-0.0047,  0.2942, -0.1292,  0.0383,  0.2100]) (768 features in tensor)\n",
      "Run time for blunt was 6.393534826114774 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bl\n",
      "ushing\n",
      "Mean of 50 tensors is: tensor([ 0.2665,  0.3391, -0.1419, -0.0718,  0.0382]) (768 features in tensor)\n",
      "Run time for blushing was 11.480981934815645 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bl\n",
      "ust\n",
      "ering\n",
      "Mean of 50 tensors is: tensor([ 0.0815,  0.2750, -0.3513, -0.1664, -0.1268]) (768 features in tensor)\n",
      "Run time for blustering was 81.03579579573125 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "boast\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.2085,  0.3044, -0.0369, -0.0906, -0.0891]) (768 features in tensor)\n",
      "Run time for boastful was 36.17027341295034 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "boat\n",
      "Mean of 50 tensors is: tensor([0.0321, 0.7080, 0.2667, 0.0140, 0.1306]) (768 features in tensor)\n",
      "Run time for boat was 4.346385980956256 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "body\n",
      "Mean of 50 tensors is: tensor([ 0.0863,  0.3266,  0.2927,  0.2277, -0.8666]) (768 features in tensor)\n",
      "Run time for body was 4.2057797173038125 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "b\n",
      "ogg\n",
      "led\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.1851,  0.3703, -0.2834,  0.5476,  0.3055]) (768 features in tensor)\n",
      "Run time for boggled was 70.85265291947871 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "boiling\n",
      "Mean of 50 tensors is: tensor([ 0.2103, -0.4597,  0.1178, -0.1645,  0.1718]) (768 features in tensor)\n",
      "Run time for boiling was 5.008052486926317 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bo\n",
      "ister\n",
      "ous\n",
      "Mean of 50 tensors is: tensor([ 0.2388,  0.5260, -0.1681,  0.4812, -0.2737]) (768 features in tensor)\n",
      "Run time for boisterous was 16.29658151604235 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bold\n",
      "Mean of 50 tensors is: tensor([ 0.2837, -0.2842, -0.1205,  0.0040,  0.8247]) (768 features in tensor)\n",
      "Run time for bold was 5.847631849348545 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bone\n",
      "Mean of 50 tensors is: tensor([ 0.0698,  0.0814,  0.1916, -0.5118,  0.7937]) (768 features in tensor)\n",
      "Run time for bone was 6.149317551404238 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "book\n",
      "Mean of 50 tensors is: tensor([ 0.1976, -0.1059,  0.2952, -0.0575, -0.2011]) (768 features in tensor)\n",
      "Run time for book was 4.0322212520986795 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "booth\n",
      "Mean of 50 tensors is: tensor([-0.0164,  0.4318,  0.3041, -0.9984, -0.4626]) (768 features in tensor)\n",
      "Run time for booth was 5.541107280179858 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "border\n",
      "Mean of 50 tensors is: tensor([ 0.1812,  0.9659, -0.0564,  0.0558, -0.4306]) (768 features in tensor)\n",
      "Run time for border was 5.132076578214765 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bored\n",
      "Mean of 50 tensors is: tensor([-0.0197, -0.0433,  0.4117,  0.1916,  0.3911]) (768 features in tensor)\n",
      "Run time for bored was 4.584624346345663 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "boredom\n",
      "Mean of 50 tensors is: tensor([ 0.1203,  0.0084,  0.3096,  0.0113, -0.1150]) (768 features in tensor)\n",
      "Run time for boredom was 7.852426792494953 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "boring\n",
      "Mean of 50 tensors is: tensor([-0.0045,  0.3824,  0.1873,  0.6504,  0.4348]) (768 features in tensor)\n",
      "Run time for boring was 5.871608710847795 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "borrow\n",
      "Mean of 50 tensors is: tensor([ 0.0860, -0.1575,  0.0832, -0.1579, -0.2808]) (768 features in tensor)\n",
      "Run time for borrow was 5.2601652424782515 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bothered\n",
      "Mean of 50 tensors is: tensor([ 0.2617,  0.7508, -0.0121,  0.2217, -0.4747]) (768 features in tensor)\n",
      "Run time for bothered was 4.65803481079638 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bottle\n",
      "Mean of 50 tensors is: tensor([-0.1058, -0.2573,  0.0654, -0.7956,  0.2449]) (768 features in tensor)\n",
      "Run time for bottle was 4.6178823271766305 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bottom\n",
      "Mean of 50 tensors is: tensor([ 0.3539, -0.5828, -0.0266,  1.0400, -0.0461]) (768 features in tensor)\n",
      "Run time for bottom was 3.725476619787514 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "boundary\n",
      "Mean of 50 tensors is: tensor([ 0.1790,  0.1436, -0.1901,  0.3002, -0.4497]) (768 features in tensor)\n",
      "Run time for boundary was 6.600311656482518 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bound\n",
      "er\n",
      "Mean of 50 tensors is: tensor([ 0.0332,  0.4677, -0.1775, -0.0979,  0.2998]) (768 features in tensor)\n",
      "Run time for bounder was 54.28038707282394 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bowl\n",
      "Mean of 50 tensors is: tensor([-0.0259, -0.4614,  0.0939,  0.4512, -0.1068]) (768 features in tensor)\n",
      "Run time for bowl was 4.436996617354453 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "box\n",
      "Mean of 50 tensors is: tensor([ 0.0852, -0.0913,  0.1415, -0.5243,  0.0212]) (768 features in tensor)\n",
      "Run time for box was 4.533187936991453 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "boy\n",
      "Mean of 50 tensors is: tensor([ 0.2323, -0.1165, -0.0784,  0.2192, -0.2899]) (768 features in tensor)\n",
      "Run time for boy was 10.329129082150757 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "brain\n",
      "Mean of 50 tensors is: tensor([ 0.1471,  0.4209,  0.0986,  0.6262, -0.3095]) (768 features in tensor)\n",
      "Run time for brain was 5.2572966972365975 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "brand\n",
      "y\n",
      "Mean of 50 tensors is: tensor([-0.0091, -0.2971,  0.1875, -0.8144,  0.0902]) (768 features in tensor)\n",
      "Run time for brandy was 35.611047432757914 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "br\n",
      "ash\n",
      "ness\n",
      "Mean of 36 tensors is: tensor([ 0.3986,  0.1131, -0.0620,  0.4223, -0.2210]) (768 features in tensor)\n",
      "Run time for brashness was 105.99477290175855 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "br\n",
      "at\n",
      "ty\n",
      "Mean of 50 tensors is: tensor([ 0.2118,  0.4544, -0.2156,  0.6820,  0.3803]) (768 features in tensor)\n",
      "Run time for bratty was 34.94305625278503 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "brave\n",
      "Mean of 50 tensors is: tensor([ 0.2197,  0.2875,  0.1312, -0.2429,  0.4403]) (768 features in tensor)\n",
      "Run time for brave was 4.974355003796518 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bread\n",
      "Mean of 50 tensors is: tensor([ 0.1801, -0.5452,  0.1045,  0.6422,  0.7031]) (768 features in tensor)\n",
      "Run time for bread was 7.041010562330484 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "breakfast\n",
      "Mean of 50 tensors is: tensor([ 0.0634, -0.0317, -0.0708,  0.3120,  0.4335]) (768 features in tensor)\n",
      "Run time for breakfast was 4.7146815694868565 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "breathe\n",
      "Mean of 50 tensors is: tensor([ 0.2708, -0.0141,  0.3345, -0.0192, -0.0736]) (768 features in tensor)\n",
      "Run time for breathe was 6.831380862742662 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "brick\n",
      "Mean of 50 tensors is: tensor([ 0.4317, -0.0508,  0.0835, -0.2764,  0.0440]) (768 features in tensor)\n",
      "Run time for brick was 8.143593966960907 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bride\n",
      "Mean of 50 tensors is: tensor([ 0.1460,  0.6105,  0.3006,  0.0873, -0.3269]) (768 features in tensor)\n",
      "Run time for bride was 8.524470074102283 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bridge\n",
      "Mean of 50 tensors is: tensor([ 0.1258,  0.4700,  0.0550, -0.3968,  0.6838]) (768 features in tensor)\n",
      "Run time for bridge was 5.716985734179616 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bright\n",
      "Mean of 50 tensors is: tensor([ 0.2356,  0.1493, -0.1088, -0.0856,  1.1867]) (768 features in tensor)\n",
      "Run time for bright was 4.234577911905944 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bring\n",
      "Mean of 50 tensors is: tensor([ 0.2822,  0.3458,  0.1867,  0.1331, -0.8985]) (768 features in tensor)\n",
      "Run time for bring was 3.9229049775749445 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "brist\n",
      "ling\n",
      "Mean of 50 tensors is: tensor([ 0.2836,  0.1939, -0.3815,  0.3039,  0.0870]) (768 features in tensor)\n",
      "Run time for bristling was 54.36760716326535 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "broad\n",
      "Mean of 50 tensors is: tensor([ 0.0744,  0.0407, -0.3243,  0.3093,  0.9592]) (768 features in tensor)\n",
      "Run time for broad was 4.354030534625053 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "broken\n",
      "Mean of 50 tensors is: tensor([-0.0347,  0.1462,  0.1112, -0.2322,  0.5572]) (768 features in tensor)\n",
      "Run time for broken was 3.9785347981378436 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "broken\n",
      "hearted\n",
      "Mean of 50 tensors is: tensor([ 0.1832,  0.5042,  0.3843, -0.2837,  0.6940]) (768 features in tensor)\n",
      "Run time for brokenhearted was 58.407033709809184 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "broken\n",
      "heartedly\n",
      "Mean of 0 tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Run time for brokenheartedly was 106.08963326178491 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bro\n",
      "oding\n",
      "Mean of 50 tensors is: tensor([ 0.2316,  0.4346, -0.2679,  0.3085,  0.3527]) (768 features in tensor)\n",
      "Run time for brooding was 12.942435516044497 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bro\n",
      "ody\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.0853,  0.1068, -0.0993,  0.1551,  0.0593]) (768 features in tensor)\n",
      "Run time for bus was 8.648526129312813 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bush\n",
      "Mean of 40 tensors is: tensor([ 0.1429,  0.1505,  0.0081,  0.0785, -0.5852]) (768 features in tensor)\n",
      "Run time for repugnance was 105.28884508647025 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "rep\n",
      "ug\n",
      "nant\n",
      "Mean of 50 tensors is: tensor([ 0.3886,  0.3943,  0.0998, -0.0360, -0.4274]) (768 features in tensor)\n",
      "Run time for repugnant was 14.951754060573876 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "rep\n",
      "uls\n",
      "ed\n",
      "Mean of 50 tensors is: tensor([ 0.1177,  0.4046,  0.0569,  0.1201, -0.4265]) (768 features in tensor)\n",
      "Run time for repulsed was 18.596346581354737 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "rep\n",
      "ulsion\n",
      "Mean of 50 tensors is: tensor([ 0.2312,  0.1610,  0.0622,  0.5016, -0.4384]) (768 features in tensor)\n",
      "Run time for repulsion was 36.428989538922906 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "require\n",
      "Mean of 50 tensors is: tensor([ 0.1040,  0.1740, -0.0942, -0.4779, -0.0879]) (768 features in tensor)\n",
      "Run time for require was 3.9692839412018657 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "resent\n",
      "Mean of 50 tensors is: tensor([-0.0121,  0.0561, -0.0401,  0.2365, -0.1744]) (768 features in tensor)\n",
      "Run time for resent was 8.808427545242012 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "resent\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.1282,  0.1577,  0.0416,  0.4465, -0.2396]) (768 features in tensor)\n",
      "Run time for resentful was 15.45872953440994 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "resent\n",
      "ing\n",
      "Mean of 50 tensors is: tensor([0.0666, 0.3289, 0.0666, 0.5012, 0.0611]) (768 features in tensor)\n",
      "Run time for resenting was 29.51241869945079 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "resentment\n",
      "Mean of 50 tensors is: tensor([ 0.0208, -0.0588,  0.0039, -0.0434, -0.1989]) (768 features in tensor)\n",
      "Run time for resentment was 8.656643014401197 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "reserved\n",
      "Mean of 50 tensors is: tensor([ 0.2090, -0.5330,  0.3854,  0.8166, -0.3238]) (768 features in tensor)\n",
      "Run time for reserved was 2.671239083632827 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "resignation\n",
      "Mean of 50 tensors is: tensor([-0.0449,  0.1003,  0.2604, -0.2252, -0.4505]) (768 features in tensor)\n",
      "Run time for resignation was 6.992698439396918 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "resigned\n",
      "Mean of 50 tensors is: tensor([ 0.0071,  0.0402,  0.2623, -0.2625, -0.5213]) (768 features in tensor)\n",
      "Run time for resigned was 5.885246803052723 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "resilience\n",
      "Mean of 50 tensors is: tensor([ 0.2471,  0.2095,  0.4336,  0.5071, -0.5871]) (768 features in tensor)\n",
      "Run time for resilience was 5.032267469912767 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "resistance\n",
      "Mean of 50 tensors is: tensor([ 0.0483,  0.2569,  0.1253,  0.5251, -0.1933]) (768 features in tensor)\n",
      "Run time for resistance was 3.9557353937998414 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "resistant\n",
      "Mean of 50 tensors is: tensor([-0.0142,  0.1518,  0.4180,  0.6419,  0.1346]) (768 features in tensor)\n",
      "Run time for resistant was 4.030951699241996 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "resist\n",
      "ent\n",
      "Mean of 50 tensors is: tensor([ 0.0677, -0.1182,  0.0810,  0.6012, -1.3935]) (768 features in tensor)\n",
      "Run time for resistent was 25.496373392641544 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "resisting\n",
      "Mean of 50 tensors is: tensor([ 0.1893, -0.0265,  0.1957,  0.3527,  0.1275]) (768 features in tensor)\n",
      "Run time for resisting was 8.045957516878843 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "res\n",
      "olute\n",
      "Mean of 50 tensors is: tensor([-0.0047,  0.5981, -0.1257,  0.4014, -0.7478]) (768 features in tensor)\n",
      "Run time for resolute was 10.306174130178988 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "resolved\n",
      "Mean of 50 tensors is: tensor([0.2589, 0.1600, 0.2641, 0.8641, 0.0427]) (768 features in tensor)\n",
      "Run time for resolved was 4.6489316411316395 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "responsive\n",
      "Mean of 50 tensors is: tensor([ 0.1223,  0.3876,  0.1503,  0.3079, -0.4819]) (768 features in tensor)\n",
      "Run time for responsive was 4.403011047281325 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "rest\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.2374,  0.3510, -0.1108,  0.2049, -0.0178]) (768 features in tensor)\n",
      "Run time for restful was 8.685383760370314 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "resting\n",
      "Mean of 50 tensors is: tensor([ 0.1811,  0.3413, -0.0191,  0.0415, -0.1191]) (768 features in tensor)\n",
      "Run time for resting was 5.158765255473554 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "restless\n",
      "Mean of 50 tensors is: tensor([ 0.1494, -0.1780,  0.1002,  0.3209,  0.2139]) (768 features in tensor)\n",
      "Run time for restless was 7.765080441720784 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "rest\n",
      "lessness\n",
      "Mean of 50 tensors is: tensor([ 0.2426, -0.3136,  0.2528, -0.0261, -0.4769]) (768 features in tensor)\n",
      "Run time for restlessness was 21.485392345115542 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "restore\n",
      "Mean of 50 tensors is: tensor([ 0.2173,  0.8091,  0.2558,  0.3777, -1.0455]) (768 features in tensor)\n",
      "Run time for restore was 4.813455764204264 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "restrained\n",
      "Mean of 50 tensors is: tensor([ 0.1327,  0.4601,  0.1348,  0.4875, -0.3841]) (768 features in tensor)\n",
      "Run time for restrained was 8.616768956184387 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "restraint\n",
      "Mean of 50 tensors is: tensor([ 0.1499,  0.1183, -0.0391,  0.0047, -0.7820]) (768 features in tensor)\n",
      "Run time for restraint was 7.212311756797135 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "retain\n",
      "Mean of 50 tensors is: tensor([ 0.0500,  0.0713, -0.2036,  0.2260, -1.2214]) (768 features in tensor)\n",
      "Run time for retain was 5.2288575526326895 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "retali\n",
      "ating\n",
      "Mean of 50 tensors is: tensor([ 0.0229,  0.0377, -0.0909,  1.1016, -0.6588]) (768 features in tensor)\n",
      "Run time for retaliating was 51.71123135089874 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "retali\n",
      "atory\n",
      "Mean of 50 tensors is: tensor([ 0.2428,  0.3274, -0.1145,  1.3675, -0.7719]) (768 features in tensor)\n",
      "Run time for retaliatory was 13.374938303604722 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "re\n",
      "thinking\n",
      "Mean of 50 tensors is: tensor([0.0560, 0.1899, 0.5407, 0.8363, 0.3715]) (768 features in tensor)\n",
      "Run time for rethinking was 7.848907603882253 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "ret\n",
      "ic\n",
      "ence\n",
      "Mean of 50 tensors is: tensor([ 0.1252,  0.5690, -0.3819,  0.6004, -0.3839]) (768 features in tensor)\n",
      "Run time for reticence was 39.90781903360039 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ret\n",
      "icent\n",
      "Mean of 50 tensors is: tensor([ 0.0774,  0.3528, -0.2684,  0.8570, -0.2990]) (768 features in tensor)\n",
      "Run time for reticent was 24.523200068622828 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "revenge\n",
      "ful\n",
      "Mean of 48 tensors is: tensor([0.2260, 0.3882, 0.1417, 0.6544, 0.3943]) (768 features in tensor)\n",
      "Run time for revengeful was 108.04220894537866 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "reve\n",
      "rent\n",
      "Mean of 50 tensors is: tensor([ 0.2223,  0.2492,  0.0037,  0.2741, -0.9346]) (768 features in tensor)\n",
      "Run time for reverent was 19.768199083395302 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "revol\n",
      "ted\n",
      "Mean of 50 tensors is: tensor([ 0.2223,  0.2582,  0.0744,  0.7296, -0.5561]) (768 features in tensor)\n",
      "Run time for revolted was 33.16254399996251 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "rev\n",
      "ulsion\n",
      "Mean of 50 tensors is: tensor([ 0.2515,  0.1707, -0.0741,  0.3661, -0.7016]) (768 features in tensor)\n",
      "Run time for revulsion was 33.149352801032364 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "rhythm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.2175,  0.4354,  0.0855,  0.3084, -0.5854]) (768 features in tensor)\n",
      "Run time for rhythm was 6.9810764994472265 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "rice\n",
      "Mean of 50 tensors is: tensor([ 0.0912, -0.3714,  0.1003,  0.6848,  0.2731]) (768 features in tensor)\n",
      "Run time for rice was 7.67865617480129 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "right\n",
      "Mean of 50 tensors is: tensor([ 0.0421,  0.1954,  0.0175, -0.2625, -0.3330]) (768 features in tensor)\n",
      "Run time for right was 4.57079956587404 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "righteous\n",
      "Mean of 50 tensors is: tensor([ 0.1618,  0.3963,  0.2928, -0.2005,  0.1304]) (768 features in tensor)\n",
      "Run time for righteous was 6.002139080315828 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "rigid\n",
      "Mean of 50 tensors is: tensor([ 0.1494,  0.1172, -0.3146,  0.1428,  0.1857]) (768 features in tensor)\n",
      "Run time for rigid was 5.06232781149447 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "r\n",
      "iled\n",
      "Mean of 50 tensors is: tensor([ 0.2185,  0.5592, -0.1808,  0.4242, -0.0819]) (768 features in tensor)\n",
      "Run time for riled was 15.671255633234978 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "riot\n",
      "ous\n",
      "Mean of 50 tensors is: tensor([ 0.1206,  0.4199, -0.0799,  0.4265,  0.5092]) (768 features in tensor)\n",
      "Run time for riotous was 28.242737260647118 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "river\n",
      "Mean of 50 tensors is: tensor([ 0.0248,  0.2405,  0.2366, -0.5935,  0.1879]) (768 features in tensor)\n",
      "Run time for river was 5.506797460839152 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "rive\n",
      "ted\n",
      "Mean of 50 tensors is: tensor([ 0.2749,  0.3031, -0.0024,  0.4657, -0.5684]) (768 features in tensor)\n",
      "Run time for riveted was 17.126740909181535 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "roam\n",
      "Mean of 50 tensors is: tensor([ 0.2525, -0.3278,  0.0370,  0.1511, -1.1005]) (768 features in tensor)\n",
      "Run time for roam was 7.5680452436208725 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "roar\n",
      "Mean of 50 tensors is: tensor([ 0.2318,  0.3079, -0.0587,  0.0263,  0.1508]) (768 features in tensor)\n",
      "Run time for roar was 10.489512923173606 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "rod\n",
      "Mean of 50 tensors is: tensor([ 0.1514, -0.3179, -0.0238, -0.0772,  0.0456]) (768 features in tensor)\n",
      "Run time for rod was 7.516935401596129 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "ro\n",
      "gu\n",
      "ish\n",
      "Mean of 50 tensors is: tensor([ 0.0314,  0.2246, -0.2297,  0.4327, -0.4735]) (768 features in tensor)\n",
      "Run time for roguish was 74.82951832562685 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ro\n",
      "iled\n",
      "Mean of 50 tensors is: tensor([ 0.2901,  0.1251, -0.0940,  0.2611, -0.0664]) (768 features in tensor)\n",
      "Run time for roiled was 46.451185043901205 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "roof\n",
      "Mean of 50 tensors is: tensor([ 0.2523, -0.1376,  0.0395, -0.7434,  0.7962]) (768 features in tensor)\n",
      "Run time for roof was 5.53462359495461 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "room\n",
      "Mean of 50 tensors is: tensor([ 0.2819,  0.1085,  0.0432, -0.4614,  0.0810]) (768 features in tensor)\n",
      "Run time for room was 3.6201244173571467 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ro\n",
      "oster\n",
      "Mean of 50 tensors is: tensor([ 0.0475,  0.0842,  0.1438, -1.2267, -0.0704]) (768 features in tensor)\n",
      "Run time for rooster was 14.881585228256881 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "root\n",
      "Mean of 50 tensors is: tensor([ 0.2450,  0.1515,  0.0800,  0.9636, -0.2366]) (768 features in tensor)\n",
      "Run time for root was 5.3994596330448985 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "rough\n",
      "Mean of 50 tensors is: tensor([ 0.1663,  0.4483, -0.0204,  0.1667,  0.3895]) (768 features in tensor)\n",
      "Run time for rough was 5.131034943275154 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "r\n",
      "oused\n",
      "Mean of 50 tensors is: tensor([0.2726, 0.1253, 0.0812, 0.2760, 0.3321]) (768 features in tensor)\n",
      "Run time for roused was 25.30046594236046 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "rude\n",
      "Mean of 50 tensors is: tensor([ 0.1999,  0.4401,  0.0523, -0.1174,  0.1056]) (768 features in tensor)\n",
      "Run time for rude was 5.24430460575968 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "r\n",
      "ue\n",
      "ful\n",
      "Mean of 46 tensors is: tensor([ 0.1859,  0.5127, -0.2248,  0.3297,  0.2932]) (768 features in tensor)\n",
      "Run time for rueful was 109.77915586717427 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "r\n",
      "uff\n",
      "led\n",
      "Mean of 50 tensors is: tensor([ 0.4165,  0.3362, -0.2006,  0.5141,  0.1530]) (768 features in tensor)\n",
      "Run time for ruffled was 6.373543348163366 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "rum\n",
      "inating\n",
      "Mean of 50 tensors is: tensor([ 0.1102,  0.2137, -0.1546,  0.8903, -0.1528]) (768 features in tensor)\n",
      "Run time for ruminating was 38.718641768209636 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "rust\n",
      "led\n",
      "Mean of 50 tensors is: tensor([ 0.1711,  0.1305, -0.1492,  0.4310, -0.2691]) (768 features in tensor)\n",
      "Run time for rustled was 64.28412040881813 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "ruthless\n",
      "Mean of 50 tensors is: tensor([0.0983, 0.2449, 0.0212, 0.0308, 0.0170]) (768 features in tensor)\n",
      "Run time for ruthless was 9.392411914654076 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sad\n",
      "Mean of 50 tensors is: tensor([0.2985, 0.5734, 0.1453, 0.3317, 1.3904]) (768 features in tensor)\n",
      "Run time for sad was 4.039839141070843 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sadd\n",
      "en\n",
      "Mean of 50 tensors is: tensor([ 0.2765,  0.2703,  0.1625, -0.1430,  0.3112]) (768 features in tensor)\n",
      "Run time for sadden was 53.56012921966612 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "saddened\n",
      "Mean of 50 tensors is: tensor([0.1541, 0.5072, 0.2470, 0.8899, 0.8005]) (768 features in tensor)\n",
      "Run time for saddened was 5.757237889803946 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sad\n",
      "istic\n",
      "Mean of 50 tensors is: tensor([ 0.1514,  0.3304, -0.0045,  0.0802,  0.8758]) (768 features in tensor)\n",
      "Run time for sadistic was 13.094727960415184 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sadness\n",
      "Mean of 50 tensors is: tensor([0.1942, 0.4828, 0.4208, 0.1582, 0.9032]) (768 features in tensor)\n",
      "Run time for sadness was 5.417745008133352 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "safety\n",
      "Mean of 50 tensors is: tensor([ 0.2059,  0.3946,  0.2120,  0.0775, -1.0340]) (768 features in tensor)\n",
      "Run time for safety was 3.9169273106381297 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sailor\n",
      "Mean of 50 tensors is: tensor([ 0.0030,  0.1433,  0.1760, -0.2519,  0.1592]) (768 features in tensor)\n",
      "Run time for sailor was 14.29173072334379 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "saint\n",
      "Mean of 50 tensors is: tensor([ 0.2275, -0.0539,  0.2755, -0.1997, -0.0028]) (768 features in tensor)\n",
      "Run time for saint was 21.122621754184365 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sal\n",
      "acious\n",
      "Mean of 50 tensors is: tensor([ 0.2418,  0.2468, -0.2547, -0.0626,  0.3796]) (768 features in tensor)\n",
      "Run time for salacious was 27.168043105863035 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "salad\n",
      "Mean of 50 tensors is: tensor([ 0.1712,  0.0581,  0.1820,  0.6830, -0.1294]) (768 features in tensor)\n",
      "Run time for salad was 6.392866152338684 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "salary\n",
      "Mean of 50 tensors is: tensor([ 0.1561, -0.1333,  0.0125, -0.9171,  0.3331]) (768 features in tensor)\n",
      "Run time for salary was 6.131703072227538 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sal\n",
      "ivating\n",
      "Mean of 50 tensors is: tensor([ 0.1467, -0.1242, -0.4027,  0.0763, -0.2577]) (768 features in tensor)\n",
      "Run time for salivating was 27.462951307184994 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "salt\n",
      "Mean of 50 tensors is: tensor([-0.0297, -0.2255,  0.2485,  0.5219,  0.0033]) (768 features in tensor)\n",
      "Run time for salt was 5.73086008336395 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sanct\n",
      "imon\n",
      "ious\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.0157,  0.5711, -0.0568, -0.1375, -0.2635]) (768 features in tensor)\n",
      "Run time for sanctimonious was 18.637435520999134 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sandwich\n",
      "Mean of 50 tensors is: tensor([-0.0485, -0.1481,  0.0167,  0.0277,  0.8861]) (768 features in tensor)\n",
      "Run time for sandwich was 8.469160439446568 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sane\n",
      "Mean of 50 tensors is: tensor([0.1689, 0.7080, 0.1967, 0.4002, 0.1618]) (768 features in tensor)\n",
      "Run time for sane was 5.4593188324943185 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "s\n",
      "angu\n",
      "ine\n",
      "Mean of 50 tensors is: tensor([ 0.1058,  0.3473, -0.0336,  0.7217, -0.4541]) (768 features in tensor)\n",
      "Run time for sanguine was 12.013386879116297 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sa\n",
      "ppy\n",
      "Mean of 50 tensors is: tensor([ 0.3160,  0.9188, -0.0802,  0.1658,  0.2669]) (768 features in tensor)\n",
      "Run time for sappy was 17.991487846709788 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sarc\n",
      "asm\n",
      "Mean of 50 tensors is: tensor([ 0.0543, -0.1644, -0.0916, -0.4161,  0.3928]) (768 features in tensor)\n",
      "Run time for sarcasm was 10.026371777057648 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sarcastic\n",
      "Mean of 50 tensors is: tensor([ 0.0260,  0.5111, -0.0320, -0.1603,  0.2093]) (768 features in tensor)\n",
      "Run time for sarcastic was 7.235408757813275 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "s\n",
      "ard\n",
      "onic\n",
      "Mean of 50 tensors is: tensor([ 0.2692,  0.4728, -0.1842,  0.2512,  0.0543]) (768 features in tensor)\n",
      "Run time for sardonic was 28.238283568061888 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "s\n",
      "assy\n",
      "Mean of 50 tensors is: tensor([ 0.1911,  0.7286, -0.1008,  0.3349,  0.2935]) (768 features in tensor)\n",
      "Run time for sassy was 7.463046550750732 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "s\n",
      "ated\n",
      "Mean of 50 tensors is: tensor([ 0.2344,  0.4128,  0.1012,  0.4841, -0.0012]) (768 features in tensor)\n",
      "Run time for sated was 45.419579304754734 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sat\n",
      "iated\n",
      "Mean of 50 tensors is: tensor([ 0.1626,  0.1710,  0.1375,  0.2361, -0.2479]) (768 features in tensor)\n",
      "Run time for satiated was 45.327035867609084 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "satirical\n",
      "Mean of 50 tensors is: tensor([ 0.0845,  0.4328,  0.1079,  0.2500, -0.2254]) (768 features in tensor)\n",
      "Run time for satirical was 8.932538683526218 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "satisfaction\n",
      "Mean of 50 tensors is: tensor([ 0.1745,  0.3637,  0.0487,  0.2470, -0.8698]) (768 features in tensor)\n",
      "Run time for satisfaction was 4.537131008692086 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "satisfied\n",
      "Mean of 50 tensors is: tensor([ 0.0897,  0.2377, -0.0275,  0.1118, -0.8860]) (768 features in tensor)\n",
      "Run time for satisfied was 3.7112574242055416 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "satisfy\n",
      "Mean of 50 tensors is: tensor([ 0.3385,  0.2354, -0.2881, -0.3997, -0.8997]) (768 features in tensor)\n",
      "Run time for satisfy was 4.308656060136855 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sat\n",
      "urn\n",
      "ine\n",
      "Mean of 22 tensors is: tensor([ 0.1201,  0.5260, -0.1468,  0.4424,  0.1570]) (768 features in tensor)\n",
      "Run time for saturnine was 104.88686389476061 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sauce\n",
      "Mean of 50 tensors is: tensor([ 0.1733, -0.1434,  0.2367, -0.2050,  0.2124]) (768 features in tensor)\n",
      "Run time for sauce was 6.165516491048038 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sa\n",
      "u\n",
      "cy\n",
      "Mean of 50 tensors is: tensor([ 0.2190,  0.7276, -0.2022,  0.2607,  0.9602]) (768 features in tensor)\n",
      "Run time for saucy was 9.073151198215783 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "savage\n",
      "Mean of 50 tensors is: tensor([ 0.1122,  0.2517, -0.0340, -0.0300,  0.4766]) (768 features in tensor)\n",
      "Run time for savage was 9.416824450716376 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "save\n",
      "Mean of 50 tensors is: tensor([ 0.3603, -0.1237,  0.3364,  0.0921, -0.3824]) (768 features in tensor)\n",
      "Run time for save was 4.6751217087730765 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "say\n",
      "Mean of 50 tensors is: tensor([ 0.1071,  0.0402, -0.0820, -0.6830,  0.1190]) (768 features in tensor)\n",
      "Run time for say was 3.8185109738260508 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "scandal\n",
      "ized\n",
      "Mean of 50 tensors is: tensor([0.0546, 0.4351, 0.1505, 0.2872, 0.3836]) (768 features in tensor)\n",
      "Run time for scandalized was 66.47699909843504 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scarce\n",
      "Mean of 50 tensors is: tensor([-0.0316, -0.1582, -0.1173, -0.2062,  0.2777]) (768 features in tensor)\n",
      "Run time for scarce was 8.697277050465345 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scare\n",
      "Mean of 50 tensors is: tensor([ 0.2624,  0.4562,  0.1385, -0.3237,  0.1297]) (768 features in tensor)\n",
      "Run time for scare was 4.5926969442516565 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scared\n",
      "Mean of 50 tensors is: tensor([ 0.2428, -0.0817,  0.1618,  0.3946,  0.5151]) (768 features in tensor)\n",
      "Run time for scared was 4.921902048401535 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scary\n",
      "Mean of 50 tensors is: tensor([0.1981, 0.2707, 0.1028, 0.0269, 0.8501]) (768 features in tensor)\n",
      "Run time for scary was 4.956976481713355 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scattered\n",
      "Mean of 50 tensors is: tensor([0.0111, 0.1517, 0.0491, 0.5589, 0.2745]) (768 features in tensor)\n",
      "Run time for scattered was 5.445458448491991 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "sch\n",
      "aden\n",
      "fre\n",
      "ude\n",
      "Mean of 50 tensors is: tensor([ 0.0501, -0.0072,  0.0029,  0.2086, -0.4026]) (768 features in tensor)\n",
      "Run time for schadenfreude was 32.83126377314329 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scheme\n",
      "Mean of 50 tensors is: tensor([ 0.1543, -0.1148,  0.0052,  0.3526,  0.2567]) (768 features in tensor)\n",
      "Run time for scheme was 4.619331295602024 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sche\n",
      "ming\n",
      "Mean of 50 tensors is: tensor([-0.0043,  0.1962, -0.1787,  0.7931,  0.1157]) (768 features in tensor)\n",
      "Run time for scheming was 12.670904224738479 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "school\n",
      "Mean of 50 tensors is: tensor([ 0.2156, -0.3052, -0.2231, -0.0063, -0.1049]) (768 features in tensor)\n",
      "Run time for school was 6.755844550207257 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "science\n",
      "Mean of 50 tensors is: tensor([ 0.1160,  0.1875, -0.1398,  0.2008, -0.2628]) (768 features in tensor)\n",
      "Run time for science was 5.17076640855521 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scientist\n",
      "Mean of 50 tensors is: tensor([ 0.1722,  0.3412, -0.0692, -0.2290, -0.5637]) (768 features in tensor)\n",
      "Run time for scientist was 6.930278420448303 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "scoff\n",
      "er\n",
      "Mean of 11 tensors is: tensor([ 0.0436,  0.2571,  0.0271, -0.6155, -0.8139]) (768 features in tensor)\n",
      "Run time for scoffer was 103.61016991920769 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "scoff\n",
      "ing\n",
      "Mean of 50 tensors is: tensor([ 0.0342,  0.1315, -0.2323, -0.1934, -0.0356]) (768 features in tensor)\n",
      "Run time for scoffing was 47.98045335430652 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scorn\n",
      "Mean of 50 tensors is: tensor([ 0.1985,  0.0433, -0.0237,  0.1364, -0.7451]) (768 features in tensor)\n",
      "Run time for scorn was 25.245067235082388 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sc\n",
      "orned\n",
      "Mean of 50 tensors is: tensor([ 0.0461,  0.1243, -0.0018,  0.1716,  0.0065]) (768 features in tensor)\n",
      "Run time for scorned was 23.648869588039815 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "scorn\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.1198,  0.2292, -0.0315, -0.0434, -0.2914]) (768 features in tensor)\n",
      "Run time for scornful was 31.709109066985548 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sc\n",
      "owl\n",
      "Mean of 50 tensors is: tensor([ 0.1356, -0.2579, -0.1491, -0.0833,  0.1295]) (768 features in tensor)\n",
      "Run time for scowl was 37.298213514499366 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sc\n",
      "ow\n",
      "ling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.1527, -0.1380, -0.3502,  0.1324, -0.0775]) (768 features in tensor)\n",
      "Run time for scowling was 64.24277968332171 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scream\n",
      "Mean of 50 tensors is: tensor([ 0.1531, -0.1513, -0.2262, -0.2417,  0.4294]) (768 features in tensor)\n",
      "Run time for scream was 8.497401289641857 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "screaming\n",
      "Mean of 50 tensors is: tensor([ 0.0225, -0.2171, -0.3071,  0.2097,  1.2949]) (768 features in tensor)\n",
      "Run time for screaming was 5.271194022148848 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "scrutin\n",
      "izing\n",
      "Mean of 50 tensors is: tensor([0.2034, 0.1565, 0.1596, 0.5626, 0.0885]) (768 features in tensor)\n",
      "Run time for scrutinizing was 19.929841240867972 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sea\n",
      "Mean of 50 tensors is: tensor([-0.1055,  0.3430,  0.4930, -0.4630,  0.3602]) (768 features in tensor)\n",
      "Run time for sea was 5.406393333338201 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sealed\n",
      "Mean of 50 tensors is: tensor([ 0.2705, -0.0087, -0.0547, -0.3743,  0.2261]) (768 features in tensor)\n",
      "Run time for sealed was 5.399061664007604 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "searching\n",
      "Mean of 50 tensors is: tensor([ 0.0373, -0.4965,  0.1658,  0.5183, -0.1899]) (768 features in tensor)\n",
      "Run time for searching was 4.35485780145973 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "se\n",
      "ash\n",
      "ore\n",
      "Mean of 50 tensors is: tensor([ 0.2364,  0.0398, -0.1595, -0.1539,  0.0740]) (768 features in tensor)\n",
      "Run time for seashore was 20.064155604690313 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "season\n",
      "Mean of 50 tensors is: tensor([ 0.0611,  0.4831,  0.0287, -0.5441, -0.5793]) (768 features in tensor)\n",
      "Run time for season was 5.113861113786697 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "second\n",
      "Mean of 50 tensors is: tensor([0.2427, 0.1275, 0.1738, 1.2765, 0.1078]) (768 features in tensor)\n",
      "Run time for second was 4.111310192383826 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "secretary\n",
      "Mean of 50 tensors is: tensor([ 0.0881,  0.1683,  0.0238,  0.0213, -0.5939]) (768 features in tensor)\n",
      "Run time for secretary was 7.168137821368873 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "secretive\n",
      "Mean of 50 tensors is: tensor([ 0.1975,  0.1776, -0.0389,  0.1773, -0.1165]) (768 features in tensor)\n",
      "Run time for secretive was 10.919288841076195 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "secret\n",
      "ively\n",
      "Mean of 25 tensors is: tensor([ 0.3247,  0.3689, -0.0939, -0.3462, -0.1409]) (768 features in tensor)\n",
      "Run time for secretively was 106.53441846650094 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "secure\n",
      "Mean of 50 tensors is: tensor([ 0.2595,  0.1690, -0.0069,  0.1099, -0.4539]) (768 features in tensor)\n",
      "Run time for secure was 4.288335035555065 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sed\n",
      "ate\n",
      "Mean of 50 tensors is: tensor([ 0.2378,  0.4224,  0.1215,  0.0641, -0.6223]) (768 features in tensor)\n",
      "Run time for sedate was 22.431704728864133 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sed\n",
      "uction\n",
      "Mean of 50 tensors is: tensor([ 0.1722, -0.0924,  0.0961, -0.7474,  0.0246]) (768 features in tensor)\n",
      "Run time for seduction was 14.578876411542296 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sed\n",
      "uctive\n",
      "Mean of 50 tensors is: tensor([ 0.2339,  0.5459, -0.1202,  0.1076, -0.0460]) (768 features in tensor)\n",
      "Run time for seductive was 6.1632762756198645 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "seed\n",
      "Mean of 50 tensors is: tensor([ 0.2703,  0.1190,  0.0344, -0.0767,  0.0703]) (768 features in tensor)\n",
      "Run time for seed was 5.500320074148476 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "seem\n",
      "Mean of 50 tensors is: tensor([ 0.0535, -0.0388,  0.2764, -0.3629, -0.2832]) (768 features in tensor)\n",
      "Run time for seem was 4.337344993837178 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "se\n",
      "et\n",
      "hing\n",
      "Mean of 50 tensors is: tensor([ 0.2840,  0.0634, -0.3985, -0.1042,  0.4129]) (768 features in tensor)\n",
      "Run time for seething was 13.200207808986306 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "self\n",
      "Mean of 50 tensors is: tensor([ 0.4419, -0.3397,  0.2966,  0.4134, -0.6568]) (768 features in tensor)\n",
      "Run time for self was 4.15752761438489 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sell\n",
      "Mean of 50 tensors is: tensor([-0.2199,  0.0219,  0.0337, -0.6537, -0.6696]) (768 features in tensor)\n",
      "Run time for sell was 4.118717424571514 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "send\n",
      "Mean of 50 tensors is: tensor([-0.0532, -0.1803,  0.0488,  0.1792, -0.3413]) (768 features in tensor)\n",
      "Run time for send was 4.146934701129794 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sense\n",
      "Mean of 50 tensors is: tensor([ 0.1307,  0.3724,  0.2093,  0.4800, -0.3094]) (768 features in tensor)\n",
      "Run time for sense was 4.025265893898904 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sens\n",
      "ual\n",
      "Mean of 50 tensors is: tensor([ 0.2686,  0.2138,  0.0218,  0.1699, -0.8372]) (768 features in tensor)\n",
      "Run time for sensual was 10.765892297960818 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sentimental\n",
      "Mean of 50 tensors is: tensor([ 0.1087,  0.6018,  0.0807, -0.1110, -0.0081]) (768 features in tensor)\n",
      "Run time for sentimental was 7.650456260889769 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sent\n",
      "ry\n",
      "Mean of 50 tensors is: tensor([ 0.0812,  0.0952, -0.2125,  0.0693,  0.7347]) (768 features in tensor)\n",
      "Run time for sentry was 12.188121491111815 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "se\n",
      "rene\n",
      "Mean of 50 tensors is: tensor([0.3818, 0.7274, 0.1827, 0.2129, 0.0080]) (768 features in tensor)\n",
      "Run time for serene was 6.7742009945213795 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "serious\n",
      "Mean of 50 tensors is: tensor([ 0.1232,  0.8700, -0.0831,  0.4040,  0.3946]) (768 features in tensor)\n",
      "Run time for serious was 3.6758393039926887 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "seriousness\n",
      "Mean of 50 tensors is: tensor([ 0.0762,  0.6899,  0.1474, -0.0164,  0.0676]) (768 features in tensor)\n",
      "Run time for seriousness was 8.260868126526475 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "serv\n",
      "ile\n",
      "Mean of 50 tensors is: tensor([ 0.0837,  0.0592, -0.0015,  0.2460, -0.0937]) (768 features in tensor)\n",
      "Run time for servile was 32.413468771614134 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "set\n",
      "Mean of 50 tensors is: tensor([ 0.1541, -0.0286, -0.2085, -0.0963,  0.4717]) (768 features in tensor)\n",
      "Run time for set was 3.141707031056285 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "severe\n",
      "Mean of 50 tensors is: tensor([ 0.0931,  0.6804, -0.2819,  0.8884,  0.6089]) (768 features in tensor)\n",
      "Run time for severe was 3.7815125212073326 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sh\n",
      "abby\n",
      "Mean of 50 tensors is: tensor([0.1871, 0.8110, 0.1429, 0.1382, 0.3778]) (768 features in tensor)\n",
      "Run time for shabby was 5.004411100409925 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shady\n",
      "Mean of 50 tensors is: tensor([ 0.0948,  0.4671,  0.0270, -0.3180,  0.7320]) (768 features in tensor)\n",
      "Run time for shady was 7.195925461128354 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shaken\n",
      "Mean of 50 tensors is: tensor([0.2296, 0.2595, 0.0977, 0.4169, 0.1031]) (768 features in tensor)\n",
      "Run time for shaken was 7.790193380787969 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shaky\n",
      "Mean of 50 tensors is: tensor([-0.0090,  0.3940, -0.2300,  0.4228,  0.7200]) (768 features in tensor)\n",
      "Run time for shaky was 8.690550366416574 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shame\n",
      "Mean of 50 tensors is: tensor([ 0.2387, -0.0221,  0.2552, -0.0025, -0.2541]) (768 features in tensor)\n",
      "Run time for shame was 4.008620332926512 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sh\n",
      "amed\n",
      "Mean of 50 tensors is: tensor([ 0.0727, -0.3630, -0.0543,  0.3832, -0.0143]) (768 features in tensor)\n",
      "Run time for shamed was 11.829346744343638 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "shame\n",
      "faced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 14 tensors is: tensor([ 0.2143, -0.0037,  0.0717, -0.4472,  0.5075]) (768 features in tensor)\n",
      "Run time for shamefaced was 105.05533922091126 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shameful\n",
      "Mean of 50 tensors is: tensor([ 0.2490,  0.0084,  0.1173, -0.2750,  0.5903]) (768 features in tensor)\n",
      "Run time for shameful was 8.545755396597087 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shameless\n",
      "Mean of 50 tensors is: tensor([ 0.1259, -0.0997, -0.0236, -0.6157,  0.6918]) (768 features in tensor)\n",
      "Run time for shameless was 9.617279299534857 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sharp\n",
      "Mean of 50 tensors is: tensor([ 0.0149,  0.3286, -0.1919,  0.8434,  0.5373]) (768 features in tensor)\n",
      "Run time for sharp was 4.540487539023161 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sheep\n",
      "Mean of 50 tensors is: tensor([ 0.0132,  0.1504,  0.0558, -0.9046,  0.3543]) (768 features in tensor)\n",
      "Run time for sheep was 5.626982742920518 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sheep\n",
      "ish\n",
      "Mean of 50 tensors is: tensor([ 0.1191, -0.0112, -0.1710,  0.4315,  0.2513]) (768 features in tensor)\n",
      "Run time for sheepish was 36.136466338299215 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sheep\n",
      "ish\n",
      "ness\n",
      "Mean of 1 tensors is: tensor([ 7.2663e-05, -1.8948e-01, -1.0198e-01,  4.8906e-01,  1.2000e-01]) (768 features in tensor)\n",
      "Run time for sheepishness was 100.66792524885386 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "she\n",
      "lled\n",
      "Mean of 50 tensors is: tensor([ 0.0108,  0.0445, -0.1402, -0.3296,  0.2732]) (768 features in tensor)\n",
      "Run time for shelled was 11.614732539281249 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shelter\n",
      "Mean of 50 tensors is: tensor([ 0.1669, -0.0965, -0.0017, -0.4614,  0.5883]) (768 features in tensor)\n",
      "Run time for shelter was 6.884042349644005 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sheriff\n",
      "Mean of 50 tensors is: tensor([-0.1360,  0.3463,  0.1730, -0.1403, -0.1234]) (768 features in tensor)\n",
      "Run time for sheriff was 11.029062720946968 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "shif\n",
      "ty\n",
      "Mean of 50 tensors is: tensor([ 0.2381,  0.1386, -0.2890,  0.7311, -0.1317]) (768 features in tensor)\n",
      "Run time for shifty was 47.03268946520984 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shock\n",
      "Mean of 50 tensors is: tensor([ 0.1903,  0.5594,  0.1640, -0.0674,  0.0304]) (768 features in tensor)\n",
      "Run time for shock was 5.065069294534624 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shocked\n",
      "Mean of 50 tensors is: tensor([0.1007, 0.2148, 0.2065, 0.4834, 0.1949]) (768 features in tensor)\n",
      "Run time for shocked was 4.287108080461621 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shocking\n",
      "Mean of 50 tensors is: tensor([ 0.1510,  0.4338, -0.0829,  0.0339,  0.3955]) (768 features in tensor)\n",
      "Run time for shocking was 4.830618775449693 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shockingly\n",
      "Mean of 50 tensors is: tensor([ 0.1487,  0.2134,  0.1488, -0.4422,  0.3891]) (768 features in tensor)\n",
      "Run time for shockingly was 10.228196324780583 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shook\n",
      "Mean of 50 tensors is: tensor([ 0.1796,  0.2047, -0.3125,  0.7176,  0.1777]) (768 features in tensor)\n",
      "Run time for shook was 4.543607281520963 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shore\n",
      "Mean of 50 tensors is: tensor([ 0.2224,  0.9007, -0.0068,  0.4469,  0.2671]) (768 features in tensor)\n",
      "Run time for shore was 6.169483305886388 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "short\n",
      "Mean of 50 tensors is: tensor([-0.0233,  0.4455, -0.1267,  0.5314,  0.1821]) (768 features in tensor)\n",
      "Run time for short was 4.519957473501563 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shoulder\n",
      "Mean of 50 tensors is: tensor([0.2412, 0.0728, 0.0589, 0.3538, 0.2056]) (768 features in tensor)\n",
      "Run time for shoulder was 5.02526041213423 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shout\n",
      "Mean of 50 tensors is: tensor([ 0.0618, -0.0243, -0.1329,  0.0231, -0.1197]) (768 features in tensor)\n",
      "Run time for shout was 4.683531489223242 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shouting\n",
      "Mean of 50 tensors is: tensor([ 0.0230, -0.2348, -0.3418,  0.1533,  0.0698]) (768 features in tensor)\n",
      "Run time for shouting was 6.511007119901478 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "shrew\n",
      "d\n",
      "Mean of 50 tensors is: tensor([ 0.3351,  0.8346,  0.1206,  0.6434, -0.2427]) (768 features in tensor)\n",
      "Run time for shrewd was 13.19698475394398 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shrink\n",
      "Mean of 50 tensors is: tensor([ 0.0305, -0.1575,  0.0223, -0.0663, -0.5010]) (768 features in tensor)\n",
      "Run time for shrink was 6.785599410533905 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shy\n",
      "Mean of 50 tensors is: tensor([ 0.3112,  0.1629, -0.1458,  0.1003,  0.1901]) (768 features in tensor)\n",
      "Run time for shy was 4.517201663926244 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "shy\n",
      "ness\n",
      "Mean of 50 tensors is: tensor([ 0.2787, -0.0373,  0.0807, -0.2575,  0.1021]) (768 features in tensor)\n",
      "Run time for shyness was 16.572403700090945 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sick\n",
      "Mean of 50 tensors is: tensor([ 0.0796,  0.0219,  0.2365, -0.2449,  0.5089]) (768 features in tensor)\n",
      "Run time for sick was 4.11641747225076 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "s\n",
      "icken\n",
      "Mean of 50 tensors is: tensor([ 0.2039,  0.2933, -0.0062, -0.3216, -0.4084]) (768 features in tensor)\n",
      "Run time for sicken was 53.00193957891315 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sick\n",
      "ened\n",
      "Mean of 50 tensors is: tensor([ 0.3175, -0.0334,  0.0449, -0.0281,  0.0166]) (768 features in tensor)\n",
      "Run time for sickened was 13.804032100364566 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sickness\n",
      "Mean of 50 tensors is: tensor([ 0.3703,  0.3819,  0.0383, -0.6540,  0.2447]) (768 features in tensor)\n",
      "Run time for sickness was 8.497273595072329 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "side\n",
      "Mean of 50 tensors is: tensor([ 0.1853,  0.7737,  0.0376,  0.9139, -0.5369]) (768 features in tensor)\n",
      "Run time for side was 4.547000702470541 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sigh\n",
      "Mean of 50 tensors is: tensor([ 0.1922,  0.4426,  0.3566, -0.0743,  0.8485]) (768 features in tensor)\n",
      "Run time for sigh was 6.269263084046543 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "silenced\n",
      "Mean of 50 tensors is: tensor([ 0.1503, -0.0174,  0.2770,  0.5134,  0.0623]) (768 features in tensor)\n",
      "Run time for silenced was 8.494717551395297 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "silent\n",
      "Mean of 50 tensors is: tensor([0.1713, 0.0094, 0.2178, 0.1263, 0.4617]) (768 features in tensor)\n",
      "Run time for silent was 4.658366026356816 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sill\n",
      "iness\n",
      "Mean of 50 tensors is: tensor([ 0.0727,  0.4004,  0.0450, -0.1028,  0.1774]) (768 features in tensor)\n",
      "Run time for silliness was 13.441541307605803 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "silly\n",
      "Mean of 50 tensors is: tensor([-0.0391,  0.3840, -0.1106, -0.3042,  0.4155]) (768 features in tensor)\n",
      "Run time for silly was 4.827118745073676 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "simmer\n",
      "ing\n",
      "Mean of 50 tensors is: tensor([ 0.1557, -0.0782, -0.1983, -0.1419, -0.1331]) (768 features in tensor)\n",
      "Run time for simmering was 8.70580785907805 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sim\n",
      "per\n",
      "Mean of 25 tensors is: tensor([ 0.1920,  1.0342, -0.0575, -0.5139, -0.6789]) (768 features in tensor)\n",
      "Run time for simper was 106.24098809342831 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sim\n",
      "pering\n",
      "Mean of 39 tensors is: tensor([-0.0094,  0.5875, -0.4063,  0.1128,  0.3448]) (768 features in tensor)\n",
      "Run time for simpering was 108.46109935455024 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "simple\n",
      "Mean of 50 tensors is: tensor([0.0625, 0.4504, 0.0323, 0.4731, 0.6482]) (768 features in tensor)\n",
      "Run time for simple was 3.9831268852576613 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "simplicity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([0.1737, 0.1365, 0.3535, 0.3430, 0.1890]) (768 features in tensor)\n",
      "Run time for simplicity was 5.44970319699496 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sincere\n",
      "Mean of 50 tensors is: tensor([ 0.1817,  0.4411, -0.3436, -0.1294,  0.8024]) (768 features in tensor)\n",
      "Run time for sincere was 4.417014051228762 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sinful\n",
      "Mean of 50 tensors is: tensor([ 0.1617,  0.2393, -0.0562,  0.3180,  0.7245]) (768 features in tensor)\n",
      "Run time for sinful was 8.497060235589743 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "singer\n",
      "Mean of 50 tensors is: tensor([ 0.2616,  0.2236, -0.0300, -0.3807,  0.3566]) (768 features in tensor)\n",
      "Run time for singer was 6.667961156927049 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "singing\n",
      "Mean of 50 tensors is: tensor([-0.0092,  0.0089, -0.1888, -0.0768,  0.5551]) (768 features in tensor)\n",
      "Run time for singing was 4.427406111732125 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sinister\n",
      "Mean of 50 tensors is: tensor([ 0.0056,  0.4685, -0.2044,  0.2661,  0.2985]) (768 features in tensor)\n",
      "Run time for sinister was 6.919147630222142 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sinister\n",
      "ly\n",
      "Mean of 15 tensors is: tensor([ 0.3107,  0.8376, -0.2070, -0.1246,  0.1903]) (768 features in tensor)\n",
      "Run time for sinisterly was 106.13935695309192 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "s\n",
      "inner\n",
      "Mean of 50 tensors is: tensor([ 0.1792,  0.0971,  0.0309,  0.2090, -0.0182]) (768 features in tensor)\n",
      "Run time for sinner was 12.960084790363908 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "situation\n",
      "Mean of 50 tensors is: tensor([ 0.1704,  0.2954, -0.1808,  0.2045,  0.2048]) (768 features in tensor)\n",
      "Run time for situation was 4.517711532302201 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "size\n",
      "Mean of 50 tensors is: tensor([ 0.1051, -0.1719, -0.0838,  0.3130, -0.3199]) (768 features in tensor)\n",
      "Run time for size was 4.120983900502324 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sizing\n",
      "Mean of 50 tensors is: tensor([ 0.2743, -0.4408,  0.0119, -0.3263,  0.0647]) (768 features in tensor)\n",
      "Run time for sizing was 4.4124267948791385 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "skept\n",
      "ic\n",
      "Mean of 50 tensors is: tensor([ 0.0053,  0.3149,  0.0958, -0.1882, -0.1951]) (768 features in tensor)\n",
      "Run time for skeptic was 21.52492986060679 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "skeptical\n",
      "Mean of 50 tensors is: tensor([ 0.0551,  0.2849,  0.1714, -0.1076,  0.1544]) (768 features in tensor)\n",
      "Run time for skeptical was 5.6916795913130045 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "skept\n",
      "ically\n",
      "Mean of 50 tensors is: tensor([ 0.2045,  0.8460, -0.2088,  0.0324, -0.0047]) (768 features in tensor)\n",
      "Run time for skeptically was 99.72557627595961 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "skepticism\n",
      "Mean of 50 tensors is: tensor([ 0.0623,  0.1957,  0.1753, -0.2882, -0.2463]) (768 features in tensor)\n",
      "Run time for skepticism was 8.308479079976678 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sketch\n",
      "y\n",
      "Mean of 50 tensors is: tensor([ 0.0319,  0.3054, -0.0747,  0.0176,  0.7957]) (768 features in tensor)\n",
      "Run time for sketchy was 10.409933052025735 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "skill\n",
      "Mean of 50 tensors is: tensor([ 0.2639,  0.3304,  0.2016,  0.4265, -0.8247]) (768 features in tensor)\n",
      "Run time for skill was 4.548805157653987 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sk\n",
      "itt\n",
      "ish\n",
      "Mean of 50 tensors is: tensor([ 0.1832,  0.5315,  0.1092,  0.6694, -0.3197]) (768 features in tensor)\n",
      "Run time for skittish was 31.47530305478722 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sky\n",
      "Mean of 50 tensors is: tensor([ 0.1795,  0.0238,  0.1911, -0.4670,  0.8811]) (768 features in tensor)\n",
      "Run time for sky was 6.113112570717931 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "slack\n",
      "Mean of 50 tensors is: tensor([ 0.1945, -0.0184, -0.0384, -0.4394,  0.2043]) (768 features in tensor)\n",
      "Run time for slack was 9.126389999873936 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "slave\n",
      "Mean of 50 tensors is: tensor([ 0.2444,  0.0808,  0.3132, -0.3576,  0.0250]) (768 features in tensor)\n",
      "Run time for slave was 9.609567722305655 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sle\n",
      "azy\n",
      "Mean of 50 tensors is: tensor([ 0.1357,  0.2406, -0.1983,  0.2551,  0.3922]) (768 features in tensor)\n",
      "Run time for sleazy was 8.633685045875609 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sleepy\n",
      "Mean of 50 tensors is: tensor([0.1976, 0.1501, 0.1380, 0.4481, 1.1376]) (768 features in tensor)\n",
      "Run time for sleepy was 7.4365967605262995 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "slick\n",
      "Mean of 50 tensors is: tensor([ 0.1172,  0.5377, -0.0308, -0.9298,  0.6369]) (768 features in tensor)\n",
      "Run time for slick was 7.108787999488413 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sl\n",
      "oth\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.1224,  0.0707,  0.1093, -0.1433,  0.2258]) (768 features in tensor)\n",
      "Run time for slothful was 67.32246028911322 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "slow\n",
      "Mean of 50 tensors is: tensor([0.0859, 0.1567, 0.0352, 0.0029, 0.2827]) (768 features in tensor)\n",
      "Run time for slow was 4.424802411347628 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sluggish\n",
      "Mean of 50 tensors is: tensor([-0.0656,  0.0480, -0.0927, -0.0571,  0.5787]) (768 features in tensor)\n",
      "Run time for sluggish was 9.761326435953379 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sly\n",
      "Mean of 50 tensors is: tensor([ 0.1322,  0.1055, -0.3530,  0.1970,  0.6260]) (768 features in tensor)\n",
      "Run time for sly was 23.119094108231366 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sm\n",
      "arm\n",
      "y\n",
      "Mean of 50 tensors is: tensor([ 0.0672,  0.7680, -0.0994, -0.6169,  0.9058]) (768 features in tensor)\n",
      "Run time for smarmy was 59.415055996738374 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "smart\n",
      "Mean of 50 tensors is: tensor([ 0.2196, -0.1869,  0.0548,  0.4597, -0.1026]) (768 features in tensor)\n",
      "Run time for smart was 4.922348658554256 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "smashed\n",
      "Mean of 50 tensors is: tensor([ 0.1686, -0.1108,  0.1491, -0.1433,  0.6408]) (768 features in tensor)\n",
      "Run time for smashed was 6.3370525483042 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "smile\n",
      "Mean of 50 tensors is: tensor([ 0.1982,  0.2718,  0.1958, -0.4306, -0.4186]) (768 features in tensor)\n",
      "Run time for smile was 3.833494123071432 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "smile\n",
      "y\n",
      "Mean of 50 tensors is: tensor([-0.0523,  0.0525,  0.1251, -0.6709,  0.3083]) (768 features in tensor)\n",
      "Run time for smiley was 10.495768724940717 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "smiling\n",
      "Mean of 50 tensors is: tensor([ 0.1276,  0.2634, -0.0231, -0.2293,  0.5305]) (768 features in tensor)\n",
      "Run time for smiling was 4.005438530817628 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sm\n",
      "irk\n",
      "Mean of 50 tensors is: tensor([-0.0449, -0.2950, -0.1913,  0.0797, -0.4087]) (768 features in tensor)\n",
      "Run time for smirk was 16.999886345118284 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "smir\n",
      "king\n",
      "Mean of 50 tensors is: tensor([ 0.0995, -0.0873, -0.3964,  0.2355, -0.1779]) (768 features in tensor)\n",
      "Run time for smirking was 21.859691665507853 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sm\n",
      "old\n",
      "ering\n",
      "Mean of 50 tensors is: tensor([ 0.0861, -0.0055, -0.4062, -0.7742,  0.3036]) (768 features in tensor)\n",
      "Run time for smoldering was 14.764934071339667 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sm\n",
      "oo\n",
      "ching\n",
      "Mean of 50 tensors is: tensor([ 0.1768,  0.0529, -0.1823, -0.4937, -0.0634]) (768 features in tensor)\n",
      "Run time for smooching was 75.46505165752023 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "smooth\n",
      "Mean of 50 tensors is: tensor([ 0.1836,  0.6894,  0.2649,  0.1523, -0.0304]) (768 features in tensor)\n",
      "Run time for smooth was 3.5280785262584686 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "smug\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.1289, -0.0844, -0.0584, -0.6081, -0.2166]) (768 features in tensor)\n",
      "Run time for smug was 12.498377126641572 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "smug\n",
      "ness\n",
      "Mean of 50 tensors is: tensor([ 0.1386,  0.0237,  0.1457, -0.5125, -0.5792]) (768 features in tensor)\n",
      "Run time for smugness was 49.525522761046886 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "snake\n",
      "Mean of 50 tensors is: tensor([ 0.0660, -0.1634,  0.1250, -0.7206,  0.1538]) (768 features in tensor)\n",
      "Run time for snake was 6.011407991871238 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "appy\n",
      "Mean of 50 tensors is: tensor([ 0.2709,  1.0989, -0.0300, -0.1659,  0.2115]) (768 features in tensor)\n",
      "Run time for snappy was 11.014679787680507 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "ark\n",
      "y\n",
      "Mean of 50 tensors is: tensor([ 0.1449,  0.4240, -0.0725,  0.1665,  0.3464]) (768 features in tensor)\n",
      "Run time for snarky was 11.118055501952767 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "arl\n",
      "Mean of 50 tensors is: tensor([ 0.2489, -0.0743, -0.3198,  0.2535, -0.1001]) (768 features in tensor)\n",
      "Run time for snarl was 25.39640810713172 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "ar\n",
      "led\n",
      "Mean of 50 tensors is: tensor([ 0.2546,  0.1971, -0.2846,  0.4387,  0.1551]) (768 features in tensor)\n",
      "Run time for snarled was 45.80805979296565 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "arling\n",
      "Mean of 50 tensors is: tensor([ 0.1768,  0.4702, -0.4618,  0.0286,  0.8513]) (768 features in tensor)\n",
      "Run time for snarling was 28.721794717013836 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "ar\n",
      "ly\n",
      "Mean of 17 tensors is: tensor([ 0.2575,  0.7974, -0.1816,  0.5282,  0.5728]) (768 features in tensor)\n",
      "Run time for snarly was 111.40952906850725 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sneaky\n",
      "Mean of 50 tensors is: tensor([ 0.1223,  0.2056, -0.2746,  0.2345,  0.2583]) (768 features in tensor)\n",
      "Run time for sneaky was 10.230785752646625 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sne\n",
      "er\n",
      "Mean of 50 tensors is: tensor([ 0.1305,  0.1743, -0.3573, -0.5057, -0.5166]) (768 features in tensor)\n",
      "Run time for sneer was 18.294136902317405 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sne\n",
      "ering\n",
      "Mean of 50 tensors is: tensor([ 0.0238, -0.0083, -0.3013,  0.2055, -0.2709]) (768 features in tensor)\n",
      "Run time for sneering was 25.38965435605496 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sne\n",
      "e\n",
      "ze\n",
      "Mean of 50 tensors is: tensor([ 0.2544, -0.0651, -0.1881, -0.6714, -0.1001]) (768 features in tensor)\n",
      "Run time for sneeze was 11.695865077897906 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sne\n",
      "ez\n",
      "ing\n",
      "Mean of 50 tensors is: tensor([ 0.2792,  0.0494, -0.0536, -0.2537,  0.2116]) (768 features in tensor)\n",
      "Run time for sneezing was 25.007815753109753 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "icker\n",
      "Mean of 50 tensors is: tensor([ 0.0622,  0.2933, -0.4777, -0.2579, -0.6214]) (768 features in tensor)\n",
      "Run time for snicker was 37.89531716518104 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "ick\n",
      "ering\n",
      "Mean of 50 tensors is: tensor([ 0.0259,  0.1552, -0.4480, -0.0024, -0.1892]) (768 features in tensor)\n",
      "Run time for snickering was 75.10259579773992 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "ide\n",
      "Mean of 50 tensors is: tensor([ 0.0244,  0.5871, -0.4687,  0.2708, -0.2756]) (768 features in tensor)\n",
      "Run time for snide was 19.250030019320548 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "igger\n",
      "ing\n",
      "Mean of 38 tensors is: tensor([ 0.0953, -0.0801, -0.3843,  0.1864, -0.2891]) (768 features in tensor)\n",
      "Run time for sniggering was 105.70854789577425 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "ive\n",
      "ling\n",
      "Mean of 50 tensors is: tensor([ 0.1080,  0.0803, -0.3010,  0.4770, -0.0542]) (768 features in tensor)\n",
      "Run time for sniveling was 94.36733663734049 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "obb\n",
      "ish\n",
      "Mean of 50 tensors is: tensor([ 0.0672,  0.4740, -0.2052,  0.2191, -0.4025]) (768 features in tensor)\n",
      "Run time for snobbish was 44.981944725848734 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "obby\n",
      "Mean of 50 tensors is: tensor([ 0.1974,  0.6359, -0.2410,  0.3627, -0.2793]) (768 features in tensor)\n",
      "Run time for snobby was 26.865293647162616 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sn\n",
      "oot\n",
      "y\n",
      "Mean of 50 tensors is: tensor([ 0.0234,  0.4409, -0.1374,  0.0536,  0.4593]) (768 features in tensor)\n",
      "Run time for snooty was 35.84702787455171 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "s\n",
      "not\n",
      "ty\n",
      "Mean of 50 tensors is: tensor([ 0.2737,  0.1488, -0.0896,  0.1834,  0.3575]) (768 features in tensor)\n",
      "Run time for snotty was 34.46138800494373 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "snow\n",
      "Mean of 50 tensors is: tensor([ 0.1155,  0.0660,  0.1926, -0.7184,  1.6172]) (768 features in tensor)\n",
      "Run time for snow was 4.958379683084786 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "soc\n",
      "iable\n",
      "Mean of 50 tensors is: tensor([ 0.1804,  0.8319,  0.3097,  0.3527, -0.5839]) (768 features in tensor)\n",
      "Run time for sociable was 10.738449521362782 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "society\n",
      "Mean of 50 tensors is: tensor([-0.0033,  0.3829, -0.1234, -0.4526, -0.5294]) (768 features in tensor)\n",
      "Run time for society was 5.3618218172341585 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sofa\n",
      "Mean of 50 tensors is: tensor([ 0.1274, -0.0676,  0.0335, -0.7760,  0.1657]) (768 features in tensor)\n",
      "Run time for sofa was 4.631380188278854 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "soft\n",
      "Mean of 50 tensors is: tensor([0.2519, 0.1595, 0.0590, 0.5034, 1.0281]) (768 features in tensor)\n",
      "Run time for soft was 3.963410126976669 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "soldier\n",
      "Mean of 50 tensors is: tensor([-0.0232,  0.0480, -0.0101, -0.2756,  0.0647]) (768 features in tensor)\n",
      "Run time for soldier was 6.593286765739322 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "solemn\n",
      "Mean of 50 tensors is: tensor([ 0.2229,  0.2859, -0.0876, -0.0801,  0.7367]) (768 features in tensor)\n",
      "Run time for solemn was 8.657799395732582 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "solicit\n",
      "ous\n",
      "Mean of 50 tensors is: tensor([ 0.0770,  0.2904,  0.0606,  0.1858, -0.3049]) (768 features in tensor)\n",
      "Run time for solicitous was 59.23886832408607 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "solitary\n",
      "Mean of 50 tensors is: tensor([0.1643, 0.0233, 0.0973, 0.3503, 1.0702]) (768 features in tensor)\n",
      "Run time for solitary was 6.010895851068199 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "solitude\n",
      "Mean of 50 tensors is: tensor([ 0.2192, -0.1002,  0.3807,  0.2788,  0.2806]) (768 features in tensor)\n",
      "Run time for solitude was 7.77367952093482 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "som\n",
      "ber\n",
      "Mean of 50 tensors is: tensor([0.1694, 0.6347, 0.0512, 0.1695, 0.5752]) (768 features in tensor)\n",
      "Run time for somber was 25.400766392238438 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "som\n",
      "ber\n",
      "ly\n",
      "Mean of 18 tensors is: tensor([ 0.3193,  0.8756,  0.0129, -0.0366,  0.5079]) (768 features in tensor)\n",
      "Run time for somberly was 104.23966170009226 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "som\n",
      "n\n",
      "ol\n",
      "ent\n",
      "Mean of 31 tensors is: tensor([-0.0512,  0.1573,  0.0528, -0.0936, -0.2931]) (768 features in tensor)\n",
      "Run time for somnolent was 107.2942442111671 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "son\n",
      "Mean of 50 tensors is: tensor([ 0.1445,  0.1280,  0.0577,  0.0511, -0.1928]) (768 features in tensor)\n",
      "Run time for son was 8.787328775040805 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "song\n",
      "Mean of 50 tensors is: tensor([ 0.2705,  0.6223,  0.0593, -0.5364,  0.3203]) (768 features in tensor)\n",
      "Run time for song was 4.95282746758312 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "so\n",
      "ot\n",
      "hed\n",
      "Mean of 50 tensors is: tensor([0.4514, 0.5883, 0.2640, 0.0728, 0.2219]) (768 features in tensor)\n",
      "Run time for soothed was 21.819965900853276 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.0762, -0.0891,  0.1242,  0.2336,  0.7677]) (768 features in tensor)\n",
      "Run time for sore was 4.288731937296689 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sorrow\n",
      "Mean of 50 tensors is: tensor([ 0.3339,  0.2837,  0.2275,  0.4591, -0.0664]) (768 features in tensor)\n",
      "Run time for sorrow was 5.836686476133764 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sorrow\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([0.2469, 0.4835, 0.1118, 0.4147, 0.4776]) (768 features in tensor)\n",
      "Run time for sorrowful was 11.334347808733582 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sorry\n",
      "Mean of 50 tensors is: tensor([ 0.2184, -0.0899, -0.1186,  0.9474,  0.8494]) (768 features in tensor)\n",
      "Run time for sorry was 3.6227898309007287 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "soul\n",
      "Mean of 50 tensors is: tensor([ 0.1336,  0.2078,  0.3663,  0.0318, -0.0995]) (768 features in tensor)\n",
      "Run time for soul was 4.515641861595213 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "soup\n",
      "Mean of 50 tensors is: tensor([ 0.2503, -0.2137,  0.2078, -0.1627,  0.5996]) (768 features in tensor)\n",
      "Run time for soup was 6.241048209369183 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sour\n",
      "Mean of 50 tensors is: tensor([-0.0085,  0.2520,  0.3208,  0.9870, -0.0235]) (768 features in tensor)\n",
      "Run time for sour was 5.610244560986757 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "south\n",
      "Mean of 50 tensors is: tensor([ 0.0358, -0.5670,  0.1602,  0.5401,  0.6189]) (768 features in tensor)\n",
      "Run time for south was 4.980700275860727 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "spaced\n",
      "Mean of 50 tensors is: tensor([ 0.0046,  0.7098,  0.0168,  0.9386, -0.3839]) (768 features in tensor)\n",
      "Run time for spaced was 9.41400833800435 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "spacing\n",
      "Mean of 50 tensors is: tensor([ 0.2019,  0.2889, -0.1717,  0.4487, -0.2571]) (768 features in tensor)\n",
      "Run time for spacing was 7.1131845554336905 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sp\n",
      "astic\n",
      "Mean of 50 tensors is: tensor([ 0.0462,  1.0110, -0.1561,  0.3962,  0.9312]) (768 features in tensor)\n",
      "Run time for spastic was 19.410922321490943 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "speak\n",
      "Mean of 50 tensors is: tensor([ 0.2136,  0.1182, -0.1415, -0.3351, -0.7330]) (768 features in tensor)\n",
      "Run time for speak was 3.639462170191109 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "speaking\n",
      "Mean of 50 tensors is: tensor([-0.0379,  0.0313, -0.2095, -0.0261, -0.0654]) (768 features in tensor)\n",
      "Run time for speaking was 4.373881958425045 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "spec\n",
      "ious\n",
      "Mean of 50 tensors is: tensor([ 0.0298,  0.5941, -0.2342,  0.4519, -0.1991]) (768 features in tensor)\n",
      "Run time for specious was 26.789717842824757 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "speculative\n",
      "Mean of 50 tensors is: tensor([ 0.0157,  0.2156,  0.0579,  0.5718, -0.2672]) (768 features in tensor)\n",
      "Run time for speculative was 8.16308306157589 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "speech\n",
      "less\n",
      "Mean of 50 tensors is: tensor([ 0.0953, -0.5000, -0.0540, -0.1614,  0.1585]) (768 features in tensor)\n",
      "Run time for speechless was 12.184437302872539 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "spend\n",
      "Mean of 50 tensors is: tensor([ 0.2867,  0.2144,  0.1312, -0.1212, -0.6963]) (768 features in tensor)\n",
      "Run time for spend was 3.9651321601122618 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "spent\n",
      "Mean of 50 tensors is: tensor([ 0.3212,  0.3262,  0.2365,  0.2832, -0.2965]) (768 features in tensor)\n",
      "Run time for spent was 3.9052519844844937 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "spine\n",
      "Mean of 50 tensors is: tensor([ 0.3364,  0.5357, -0.0637,  0.8253, -0.6630]) (768 features in tensor)\n",
      "Run time for spine was 5.511727958917618 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "spirit\n",
      "Mean of 50 tensors is: tensor([ 0.1939,  0.3354,  0.3276, -0.0976,  0.3138]) (768 features in tensor)\n",
      "Run time for spirit was 4.387729100883007 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "spirited\n",
      "Mean of 50 tensors is: tensor([ 0.1564,  0.6928, -0.1986,  0.1711,  0.6412]) (768 features in tensor)\n",
      "Run time for spirited was 7.867708034813404 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "spirit\n",
      "less\n",
      "Mean of 9 tensors is: tensor([ 0.3243, -0.2073,  0.1748, -0.2487,  0.5519]) (768 features in tensor)\n",
      "Run time for spiritless was 104.12967443931848 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "spite\n",
      "Mean of 50 tensors is: tensor([ 0.2504,  0.2761, -0.1298, -0.4281, -0.1830]) (768 features in tensor)\n",
      "Run time for spite was 4.676781050860882 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "spite\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.3149,  0.1485, -0.1578, -0.0392, -0.0608]) (768 features in tensor)\n",
      "Run time for spiteful was 16.43745031580329 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "spoiled\n",
      "Mean of 50 tensors is: tensor([ 0.2126,  0.2543,  0.0920, -0.3739,  0.6065]) (768 features in tensor)\n",
      "Run time for spoiled was 5.791919133625925 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sp\n",
      "ooked\n",
      "Mean of 50 tensors is: tensor([0.2136, 0.7107, 0.0335, 0.6229, 0.7547]) (768 features in tensor)\n",
      "Run time for spooked was 17.91998211480677 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "spoon\n",
      "Mean of 50 tensors is: tensor([-0.1079, -0.5427,  0.1128, -0.4294,  0.1197]) (768 features in tensor)\n",
      "Run time for spoon was 6.9361840868368745 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "squad\n",
      "Mean of 50 tensors is: tensor([ 0.2244,  0.5065, -0.0650,  0.0329, -0.1549]) (768 features in tensor)\n",
      "Run time for squad was 6.896738801151514 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sque\n",
      "am\n",
      "ish\n",
      "Mean of 50 tensors is: tensor([ 0.1929,  0.5654, -0.0439,  0.6324, -0.5954]) (768 features in tensor)\n",
      "Run time for squeamish was 26.30439070146531 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "staggered\n",
      "Mean of 50 tensors is: tensor([ 0.0144,  0.2317, -0.0075,  0.2732, -0.1018]) (768 features in tensor)\n",
      "Run time for staggered was 9.413150686770678 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "st\n",
      "alker\n",
      "Mean of 50 tensors is: tensor([-0.0828, -0.2472, -0.0972,  0.0896,  0.1914]) (768 features in tensor)\n",
      "Run time for stalker was 8.464411822147667 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stare\n",
      "Mean of 50 tensors is: tensor([ 0.1993, -0.2795,  0.1580, -0.2084,  0.0768]) (768 features in tensor)\n",
      "Run time for stare was 6.890957009978592 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "staring\n",
      "Mean of 50 tensors is: tensor([-0.0054, -0.7043, -0.1095, -0.5074,  0.5356]) (768 features in tensor)\n",
      "Run time for staring was 4.862517109140754 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "star\n",
      "stru\n",
      "ck\n",
      "Mean of 50 tensors is: tensor([ 0.2339, -0.2318,  0.1187,  0.0948,  0.0284]) (768 features in tensor)\n",
      "Run time for starstruck was 38.2814461318776 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "started\n",
      "Mean of 50 tensors is: tensor([ 0.0961,  0.1543, -0.0837,  0.3765, -0.6991]) (768 features in tensor)\n",
      "Run time for started was 3.83594751637429 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "startled\n",
      "Mean of 50 tensors is: tensor([0.1865, 0.2936, 0.1660, 0.4696, 0.3723]) (768 features in tensor)\n",
      "Run time for startled was 9.41296898573637 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "state\n",
      "Mean of 50 tensors is: tensor([ 0.1316,  0.2567, -0.1086,  0.1227,  0.1102]) (768 features in tensor)\n",
      "Run time for state was 4.269773976877332 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "state\n",
      "ly\n",
      "Mean of 50 tensors is: tensor([ 0.2544,  1.2093, -0.3304, -0.3211,  0.1452]) (768 features in tensor)\n",
      "Run time for stately was 10.412027842365205 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "steadfast\n",
      "Mean of 50 tensors is: tensor([-0.0266,  0.1940, -0.0960, -0.3312, -0.3717]) (768 features in tensor)\n",
      "Run time for steadfast was 8.87771472800523 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "steady\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([-0.1370,  0.4947, -0.1143, -0.0831,  0.0868]) (768 features in tensor)\n",
      "Run time for steady was 5.422767129726708 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "steak\n",
      "Mean of 50 tensors is: tensor([ 0.2017, -0.5837,  0.0153, -0.5228, -0.3745]) (768 features in tensor)\n",
      "Run time for steak was 7.276515596546233 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "steal\n",
      "Mean of 50 tensors is: tensor([ 0.2128,  0.2417, -0.0409, -0.3770, -0.0080]) (768 features in tensor)\n",
      "Run time for steal was 5.143182043917477 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "stealth\n",
      "y\n",
      "Mean of 50 tensors is: tensor([ 0.0380,  0.3387, -0.0052,  0.2261,  0.3141]) (768 features in tensor)\n",
      "Run time for stealthy was 14.22631679289043 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ste\n",
      "amed\n",
      "Mean of 50 tensors is: tensor([ 0.0976, -0.1244,  0.0402, -0.3474,  0.3430]) (768 features in tensor)\n",
      "Run time for steamed was 6.460868627764285 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ste\n",
      "aming\n",
      "Mean of 50 tensors is: tensor([ 0.1146, -0.3019, -0.1432, -0.5489,  0.0123]) (768 features in tensor)\n",
      "Run time for steaming was 6.860900150611997 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "steel\n",
      "ing\n",
      "Mean of 42 tensors is: tensor([ 0.2585,  0.0856,  0.0365, -0.0817, -0.0231]) (768 features in tensor)\n",
      "Run time for steeling was 108.4104952448979 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "st\n",
      "eely\n",
      "Mean of 50 tensors is: tensor([-0.1057,  0.3227, -0.0550, -0.2003, -0.0124]) (768 features in tensor)\n",
      "Run time for steely was 27.137942649424076 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "steep\n",
      "le\n",
      "Mean of 50 tensors is: tensor([ 0.1208,  0.1527, -0.0529, -0.3783,  0.1166]) (768 features in tensor)\n",
      "Run time for steeple was 52.560275027528405 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stern\n",
      "Mean of 50 tensors is: tensor([ 0.1747,  0.2340, -0.0426,  0.3058, -0.1657]) (768 features in tensor)\n",
      "Run time for stern was 9.554798187687993 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stiff\n",
      "Mean of 50 tensors is: tensor([ 0.0391,  0.1103, -0.2126, -0.3211,  1.1596]) (768 features in tensor)\n",
      "Run time for stiff was 4.806994850747287 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "st\n",
      "ifled\n",
      "Mean of 50 tensors is: tensor([ 0.1022,  0.1541, -0.0797, -0.1234, -0.2514]) (768 features in tensor)\n",
      "Run time for stifled was 14.901425370946527 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "st\n",
      "ifling\n",
      "Mean of 50 tensors is: tensor([ 0.1481,  0.2825, -0.2207, -0.3143,  0.1723]) (768 features in tensor)\n",
      "Run time for stifling was 13.38019538205117 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "still\n",
      "Mean of 50 tensors is: tensor([ 0.0630,  0.2531, -0.0579,  0.0835, -0.2846]) (768 features in tensor)\n",
      "Run time for still was 3.8018802497535944 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "still\n",
      "ness\n",
      "Mean of 50 tensors is: tensor([ 0.4065,  0.0209,  0.3041, -0.4043,  0.2179]) (768 features in tensor)\n",
      "Run time for stillness was 9.712778054177761 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stimulated\n",
      "Mean of 50 tensors is: tensor([ 0.2707,  0.1776,  0.1357, -0.0052,  0.0252]) (768 features in tensor)\n",
      "Run time for stimulated was 8.766045122407377 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "st\n",
      "inky\n",
      "Mean of 50 tensors is: tensor([ 0.1659,  0.2495, -0.2140, -0.1066,  0.4164]) (768 features in tensor)\n",
      "Run time for stinky was 15.795169997029006 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stirred\n",
      "Mean of 50 tensors is: tensor([ 0.2831, -0.0230, -0.0113,  0.1291,  0.6113]) (768 features in tensor)\n",
      "Run time for stirred was 7.823914815671742 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sto\n",
      "ic\n",
      "Mean of 50 tensors is: tensor([ 0.1330,  0.6243, -0.1686, -0.1228,  0.5770]) (768 features in tensor)\n",
      "Run time for stoic was 10.382447035051882 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sto\n",
      "ical\n",
      "Mean of 21 tensors is: tensor([ 0.1338,  0.8654,  0.0421,  0.1117, -0.0687]) (768 features in tensor)\n",
      "Run time for stoical was 105.36169809382409 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "st\n",
      "olid\n",
      "Mean of 44 tensors is: tensor([ 0.0802,  0.3935, -0.1741, -0.3777, -0.1074]) (768 features in tensor)\n",
      "Run time for stolid was 109.02654499467462 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stomach\n",
      "Mean of 50 tensors is: tensor([ 0.4250, -0.6296,  0.3586,  0.2651, -0.1273]) (768 features in tensor)\n",
      "Run time for stomach was 4.744985454715788 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "st\n",
      "oned\n",
      "Mean of 50 tensors is: tensor([ 0.1351,  0.0347,  0.1709, -0.9455, -0.1098]) (768 features in tensor)\n",
      "Run time for stoned was 22.493337660096586 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "storm\n",
      "Mean of 50 tensors is: tensor([0.1440, 0.3091, 0.3421, 0.1585, 0.6279]) (768 features in tensor)\n",
      "Run time for storm was 7.698684721253812 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "storm\n",
      "ing\n",
      "Mean of 50 tensors is: tensor([ 0.1949,  0.1291, -0.1137,  0.0529,  0.3027]) (768 features in tensor)\n",
      "Run time for storming was 11.17696472350508 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "storm\n",
      "y\n",
      "Mean of 50 tensors is: tensor([ 0.1673,  0.4537, -0.1467,  0.1177,  0.9912]) (768 features in tensor)\n",
      "Run time for stormy was 7.120413505472243 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "story\n",
      "Mean of 50 tensors is: tensor([0.1014, 0.3758, 0.3411, 1.0124, 0.1310]) (768 features in tensor)\n",
      "Run time for story was 5.583486722782254 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stout\n",
      "Mean of 50 tensors is: tensor([ 0.1731,  0.1276, -0.0869, -0.0543,  0.2611]) (768 features in tensor)\n",
      "Run time for stout was 11.309550150297582 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stove\n",
      "Mean of 50 tensors is: tensor([ 0.1244, -0.0740,  0.1089, -0.7052,  0.1090]) (768 features in tensor)\n",
      "Run time for stove was 5.421270147897303 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "straight\n",
      "Mean of 50 tensors is: tensor([ 0.2126, -0.4338,  0.0078,  0.2175,  0.2145]) (768 features in tensor)\n",
      "Run time for straight was 4.047345639206469 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "strained\n",
      "Mean of 50 tensors is: tensor([ 0.1764,  0.1175, -0.0245,  0.4521,  0.1775]) (768 features in tensor)\n",
      "Run time for strained was 7.392972404137254 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "strange\n",
      "Mean of 50 tensors is: tensor([0.1621, 0.1095, 0.0777, 0.7351, 0.7085]) (768 features in tensor)\n",
      "Run time for strange was 3.813395312987268 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "straw\n",
      "Mean of 50 tensors is: tensor([ 0.0250, -0.0377, -0.0639, -0.5347, -0.1612]) (768 features in tensor)\n",
      "Run time for straw was 6.945007114671171 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stream\n",
      "Mean of 50 tensors is: tensor([ 0.1862, -0.1560,  0.0436, -0.8742,  0.0171]) (768 features in tensor)\n",
      "Run time for stream was 7.817463306710124 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "street\n",
      "Mean of 50 tensors is: tensor([-0.0095,  0.0960, -0.1056, -0.5173, -0.0409]) (768 features in tensor)\n",
      "Run time for street was 5.904436768963933 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "strength\n",
      "Mean of 50 tensors is: tensor([ 0.1351, -0.0558,  0.1262,  0.0770,  0.0766]) (768 features in tensor)\n",
      "Run time for strength was 4.971238669008017 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stressed\n",
      "Mean of 50 tensors is: tensor([0.1466, 0.4490, 0.0994, 0.3469, 0.3121]) (768 features in tensor)\n",
      "Run time for stressed was 4.948859554715455 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stricken\n",
      "Mean of 50 tensors is: tensor([ 0.2287, -0.0035,  0.0178,  0.1209,  0.3167]) (768 features in tensor)\n",
      "Run time for stricken was 10.024012396112084 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "strict\n",
      "Mean of 50 tensors is: tensor([ 0.0998,  0.0666, -0.4435,  0.8565,  0.0169]) (768 features in tensor)\n",
      "Run time for strict was 4.230845872312784 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.3335, -0.1825, -0.0976,  0.0525, -0.6514]) (768 features in tensor)\n",
      "Run time for string was 10.707802216522396 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "strong\n",
      "Mean of 50 tensors is: tensor([-0.0348,  0.2866, -0.2739,  0.4294,  0.5991]) (768 features in tensor)\n",
      "Run time for strong was 3.7875961316749454 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "struck\n",
      "Mean of 50 tensors is: tensor([0.2079, 0.1348, 0.0569, 0.4265, 0.4203]) (768 features in tensor)\n",
      "Run time for struck was 3.945714076049626 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stubborn\n",
      "Mean of 50 tensors is: tensor([ 1.6569e-01,  1.4984e-01, -9.3765e-02, -2.0025e-04,  5.6999e-01]) (768 features in tensor)\n",
      "Run time for stubborn was 5.981166071258485 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "stubborn\n",
      "ness\n",
      "Mean of 50 tensors is: tensor([ 0.2102, -0.0504,  0.0045, -0.0398, -0.2409]) (768 features in tensor)\n",
      "Run time for stubbornness was 18.69037992693484 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stud\n",
      "Mean of 50 tensors is: tensor([ 0.1999, -0.0725, -0.0263,  0.0876, -0.9817]) (768 features in tensor)\n",
      "Run time for stud was 8.555130288004875 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "student\n",
      "Mean of 50 tensors is: tensor([-0.0097, -0.3998, -0.0956,  0.2598,  0.2859]) (768 features in tensor)\n",
      "Run time for student was 4.120166857726872 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "stud\n",
      "ious\n",
      "Mean of 50 tensors is: tensor([ 0.2658,  0.3607,  0.0097,  0.5547, -0.5533]) (768 features in tensor)\n",
      "Run time for studious was 35.78563782945275 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "studying\n",
      "Mean of 50 tensors is: tensor([ 0.2467, -0.5199,  0.0657,  0.6658,  0.7279]) (768 features in tensor)\n",
      "Run time for studying was 3.5234014466404915 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "st\n",
      "umped\n",
      "Mean of 50 tensors is: tensor([ 0.1377, -0.0056, -0.0023,  0.0102, -0.1964]) (768 features in tensor)\n",
      "Run time for stumped was 11.249362085014582 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "st\n",
      "ung\n",
      "Mean of 50 tensors is: tensor([ 0.1878,  0.0322,  0.0870, -0.3385, -0.3527]) (768 features in tensor)\n",
      "Run time for stung was 13.384921624325216 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stunned\n",
      "Mean of 50 tensors is: tensor([ 0.0775, -0.0312,  0.1422,  0.1843,  0.1127]) (768 features in tensor)\n",
      "Run time for stunned was 5.861952931620181 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "st\n",
      "upe\n",
      "f\n",
      "action\n",
      "Mean of 13 tensors is: tensor([ 0.0354,  0.6419, -0.0060, -0.1891, -0.9520]) (768 features in tensor)\n",
      "Run time for stupefaction was 105.3377286316827 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "st\n",
      "upe\n",
      "f\n",
      "ied\n",
      "Mean of 48 tensors is: tensor([ 0.0870,  0.2853, -0.0390, -0.1759, -0.7111]) (768 features in tensor)\n",
      "Run time for stupefied was 111.57914565317333 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "st\n",
      "upe\n",
      "fy\n",
      "Mean of 47 tensors is: tensor([ 0.2011,  0.5656, -0.0187, -0.1221, -0.4437]) (768 features in tensor)\n",
      "Run time for stupefy was 111.29287236742675 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stupid\n",
      "Mean of 50 tensors is: tensor([ 0.1160,  0.0114, -0.2606, -0.2289,  0.4736]) (768 features in tensor)\n",
      "Run time for stupid was 3.929954274557531 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "st\n",
      "up\n",
      "orous\n",
      "Mean of 4 tensors is: tensor([-0.0459,  0.4536,  0.1003, -0.6172,  0.2066]) (768 features in tensor)\n",
      "Run time for stuporous was 104.17415634263307 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "style\n",
      "Mean of 50 tensors is: tensor([ 0.3324,  0.0022,  0.1242,  0.7183, -0.8170]) (768 features in tensor)\n",
      "Run time for style was 3.8807250009849668 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "su\n",
      "ave\n",
      "Mean of 50 tensors is: tensor([ 0.2776,  0.4087,  0.1041,  0.2249, -0.2429]) (768 features in tensor)\n",
      "Run time for suave was 5.945695035159588 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "subdued\n",
      "Mean of 50 tensors is: tensor([ 0.1085,  0.2492,  0.1040,  0.3681, -0.0738]) (768 features in tensor)\n",
      "Run time for subdued was 8.759371077641845 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sublime\n",
      "Mean of 50 tensors is: tensor([-0.0443,  0.6830,  0.0939,  0.2821,  0.0316]) (768 features in tensor)\n",
      "Run time for sublime was 8.472273553721607 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sub\n",
      "missive\n",
      "Mean of 50 tensors is: tensor([0.2035, 0.2123, 0.1025, 0.6090, 0.2834]) (768 features in tensor)\n",
      "Run time for submissive was 10.402264766395092 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "succeed\n",
      "Mean of 50 tensors is: tensor([ 0.2442,  0.5275,  0.1029,  0.2164, -1.3960]) (768 features in tensor)\n",
      "Run time for succeed was 4.7260619988664985 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "su\n",
      "ds\n",
      "Mean of 50 tensors is: tensor([ 0.1782, -0.2076, -0.0908, -0.2030, -0.1076]) (768 features in tensor)\n",
      "Run time for suds was 36.14877763483673 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suffering\n",
      "Mean of 50 tensors is: tensor([ 0.2049,  0.1393, -0.0875,  0.1287,  0.7700]) (768 features in tensor)\n",
      "Run time for suffering was 4.2129838252440095 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sugar\n",
      "Mean of 50 tensors is: tensor([0.2418, 0.0866, 0.1385, 0.5702, 0.5671]) (768 features in tensor)\n",
      "Run time for sugar was 5.548594114370644 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suggestive\n",
      "Mean of 50 tensors is: tensor([-0.0491, -0.0353, -0.0802,  0.4937,  0.1907]) (768 features in tensor)\n",
      "Run time for suggestive was 10.862529820762575 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sul\n",
      "king\n",
      "Mean of 50 tensors is: tensor([ 1.2234e-01,  3.0256e-01, -3.1798e-01,  4.1958e-01,  2.4373e-04]) (768 features in tensor)\n",
      "Run time for sulking was 36.734859517775476 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sul\n",
      "ky\n",
      "Mean of 50 tensors is: tensor([ 0.1089,  0.7493, -0.1574, -0.1022,  0.2558]) (768 features in tensor)\n",
      "Run time for sulky was 25.154623192735016 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "su\n",
      "ll\n",
      "en\n",
      "Mean of 50 tensors is: tensor([ 0.1926, -0.0744, -0.1550, -0.1849,  0.4228]) (768 features in tensor)\n",
      "Run time for sullen was 21.70932525023818 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "su\n",
      "ll\n",
      "en\n",
      "ness\n",
      "Mean of 4 tensors is: tensor([ 0.2454, -0.1002,  0.0411,  0.0981,  0.0952]) (768 features in tensor)\n",
      "Run time for sullenness was 105.59775552898645 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "summer\n",
      "Mean of 50 tensors is: tensor([ 0.2774,  0.3883, -0.0121,  0.2221, -0.0591]) (768 features in tensor)\n",
      "Run time for summer was 5.318949306383729 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sun\n",
      "Mean of 50 tensors is: tensor([ 0.0077,  0.2223,  0.3580, -0.8589, -0.5728]) (768 features in tensor)\n",
      "Run time for sun was 4.6041386015713215 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sunny\n",
      "Mean of 50 tensors is: tensor([ 0.4109,  0.5582,  0.1556, -0.5778,  0.7355]) (768 features in tensor)\n",
      "Run time for sunny was 4.095491710118949 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sunrise\n",
      "Mean of 50 tensors is: tensor([ 0.1015,  0.2828, -0.0395, -0.4224, -0.2121]) (768 features in tensor)\n",
      "Run time for sunrise was 8.25679890345782 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sunset\n",
      "Mean of 50 tensors is: tensor([ 0.1622,  0.2657,  0.1496,  0.1979, -0.4661]) (768 features in tensor)\n",
      "Run time for sunset was 9.171076835133135 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sunshine\n",
      "Mean of 50 tensors is: tensor([ 0.4170,  0.3705,  0.4205, -0.4974, -0.4253]) (768 features in tensor)\n",
      "Run time for sunshine was 5.984091826714575 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "superior\n",
      "Mean of 50 tensors is: tensor([ 0.1196,  0.8558,  0.0572, -0.0074, -0.4955]) (768 features in tensor)\n",
      "Run time for superior was 5.145203929394484 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "superiority\n",
      "Mean of 50 tensors is: tensor([ 0.2521,  0.4555,  0.0858, -0.9669, -0.5067]) (768 features in tensor)\n",
      "Run time for superiority was 7.24055895768106 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "supper\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.1629, -0.2952,  0.0308, -0.0048,  0.8970]) (768 features in tensor)\n",
      "Run time for supper was 10.417173785157502 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suppressed\n",
      "Mean of 50 tensors is: tensor([ 0.2438,  0.0693,  0.0747,  0.6441, -0.5797]) (768 features in tensor)\n",
      "Run time for suppressed was 6.306872768327594 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suppressing\n",
      "Mean of 50 tensors is: tensor([ 0.3257,  0.1040,  0.1186,  0.5082, -0.1200]) (768 features in tensor)\n",
      "Run time for suppressing was 9.284252973273396 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suppression\n",
      "Mean of 50 tensors is: tensor([ 0.2387,  0.1842,  0.0550,  0.3170, -0.9246]) (768 features in tensor)\n",
      "Run time for suppression was 7.2574780732393265 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sure\n",
      "Mean of 50 tensors is: tensor([ 0.1414, -0.2865,  0.0411,  0.6315,  0.1842]) (768 features in tensor)\n",
      "Run time for sure was 3.725906051695347 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "surgery\n",
      "Mean of 50 tensors is: tensor([ 0.0613,  0.8070,  0.2210, -0.4399,  0.2332]) (768 features in tensor)\n",
      "Run time for surgery was 4.124191803857684 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sur\n",
      "ly\n",
      "Mean of 50 tensors is: tensor([ 0.1841,  0.6678, -0.0033, -0.1873, -0.0605]) (768 features in tensor)\n",
      "Run time for surly was 13.121836241334677 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "surprise\n",
      "Mean of 50 tensors is: tensor([ 0.2940,  0.6813,  0.1545,  0.7574, -0.1711]) (768 features in tensor)\n",
      "Run time for surprise was 4.9950582925230265 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "surprised\n",
      "Mean of 50 tensors is: tensor([ 0.0411,  0.4818,  0.1830,  0.9921, -0.4342]) (768 features in tensor)\n",
      "Run time for surprised was 3.946523758582771 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "surprising\n",
      "Mean of 50 tensors is: tensor([ 0.1799,  0.5242,  0.1978,  0.8462, -0.2289]) (768 features in tensor)\n",
      "Run time for surprising was 4.152979637496173 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "surprisingly\n",
      "Mean of 50 tensors is: tensor([0.1941, 0.3310, 0.1387, 0.4319, 0.0486]) (768 features in tensor)\n",
      "Run time for surprisingly was 4.810507723130286 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "sur\n",
      "re\n",
      "pt\n",
      "itious\n",
      "Mean of 50 tensors is: tensor([ 0.1559,  0.6671,  0.0490, -0.0587, -0.3074]) (768 features in tensor)\n",
      "Run time for surreptitious was 37.50913555454463 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suspect\n",
      "Mean of 50 tensors is: tensor([ 0.2003, -0.0773,  0.0185,  0.8888,  0.6816]) (768 features in tensor)\n",
      "Run time for suspect was 3.664634572342038 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "suspect\n",
      "ing\n",
      "Mean of 50 tensors is: tensor([ 0.0979, -0.0445, -0.0909,  0.6337,  0.2497]) (768 features in tensor)\n",
      "Run time for suspecting was 20.85325222555548 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suspense\n",
      "Mean of 50 tensors is: tensor([ 0.2410,  0.2245,  0.0647,  0.4223, -0.3782]) (768 features in tensor)\n",
      "Run time for suspense was 7.234243265353143 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suspicion\n",
      "Mean of 50 tensors is: tensor([ 0.0319,  0.1666, -0.0397,  0.2358,  0.4519]) (768 features in tensor)\n",
      "Run time for suspicion was 5.318316080607474 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suspicious\n",
      "Mean of 50 tensors is: tensor([ 0.0732,  0.4832,  0.1044, -0.0842,  0.1212]) (768 features in tensor)\n",
      "Run time for suspicious was 4.73059471603483 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "suspicious\n",
      "ly\n",
      "Mean of 50 tensors is: tensor([ 0.2198,  0.5483, -0.0617, -0.0290, -0.0665]) (768 features in tensor)\n",
      "Run time for suspiciously was 13.140528327785432 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "suspicious\n",
      "ness\n",
      "Mean of 23 tensors is: tensor([ 0.2003,  0.0463,  0.1308, -0.2081, -0.4619]) (768 features in tensor)\n",
      "Run time for suspiciousness was 107.09225513786077 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "sw\n",
      "agger\n",
      "ing\n",
      "Mean of 50 tensors is: tensor([ 0.3162,  0.1893, -0.4163,  0.1940, -0.1033]) (768 features in tensor)\n",
      "Run time for swaggering was 56.97839686367661 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "swamp\n",
      "Mean of 50 tensors is: tensor([ 0.2097, -0.2079,  0.2326, -0.6574,  0.2807]) (768 features in tensor)\n",
      "Run time for swamp was 10.647779184393585 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "swearing\n",
      "Mean of 50 tensors is: tensor([ 0.0427, -0.1255, -0.2303,  0.2073,  0.3266]) (768 features in tensor)\n",
      "Run time for swearing was 10.605295957997441 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sweater\n",
      "Mean of 50 tensors is: tensor([ 0.0126, -0.2747,  0.2841, -0.6483,  0.5231]) (768 features in tensor)\n",
      "Run time for sweater was 12.979230078868568 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "symbol\n",
      "Mean of 50 tensors is: tensor([ 0.1390, -0.1374,  0.1515, -0.8765, -0.2167]) (768 features in tensor)\n",
      "Run time for symbol was 7.128631331026554 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sympathetic\n",
      "Mean of 50 tensors is: tensor([0.1114, 0.4474, 0.2498, 0.2581, 0.0367]) (768 features in tensor)\n",
      "Run time for sympathetic was 5.875093366019428 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sympath\n",
      "izing\n",
      "Mean of 50 tensors is: tensor([ 0.2379,  0.1799,  0.0300,  0.5232, -0.5456]) (768 features in tensor)\n",
      "Run time for sympathizing was 59.00867559015751 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sympathy\n",
      "Mean of 50 tensors is: tensor([ 0.1761,  0.1227,  0.1209,  0.4928, -0.5705]) (768 features in tensor)\n",
      "Run time for sympathy was 4.942826217971742 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tacit\n",
      "urn\n",
      "Mean of 50 tensors is: tensor([ 0.3391,  0.2095, -0.2228,  0.0149, -0.1968]) (768 features in tensor)\n",
      "Run time for taciturn was 87.3193025952205 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tail\n",
      "Mean of 50 tensors is: tensor([ 0.0955, -0.1617, -0.0539,  0.9440, -0.2217]) (768 features in tensor)\n",
      "Run time for tail was 6.8437119629234076 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "take\n",
      "Mean of 50 tensors is: tensor([ 0.1467, -0.0660, -0.0570,  0.4182, -0.7178]) (768 features in tensor)\n",
      "Run time for take was 3.9811771288514137 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "talk\n",
      "ative\n",
      "Mean of 50 tensors is: tensor([ 0.1649,  0.4226,  0.1930,  0.7213, -0.8010]) (768 features in tensor)\n",
      "Run time for talkative was 17.862198062241077 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "talking\n",
      "Mean of 50 tensors is: tensor([-0.1149, -0.0143,  0.0348, -0.0440, -0.3780]) (768 features in tensor)\n",
      "Run time for talking was 4.255335902795196 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tantal\n",
      "ized\n",
      "Mean of 23 tensors is: tensor([ 0.2956,  0.1889, -0.0806,  0.0566, -0.1851]) (768 features in tensor)\n",
      "Run time for tantalized was 105.60204815119505 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "target\n",
      "Mean of 50 tensors is: tensor([ 0.0247, -0.3791, -0.0390,  1.1022, -0.0129]) (768 features in tensor)\n",
      "Run time for target was 4.338402954861522 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tart\n",
      "Mean of 50 tensors is: tensor([ 0.1494,  0.1582, -0.0517,  0.3591,  0.3700]) (768 features in tensor)\n",
      "Run time for tart was 7.369424468837678 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "task\n",
      "Mean of 50 tensors is: tensor([ 0.1260,  0.2890,  0.2950,  0.2244, -0.1130]) (768 features in tensor)\n",
      "Run time for task was 4.790939651429653 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tast\n",
      "eful\n",
      "Mean of 50 tensors is: tensor([ 0.2274,  1.0044, -0.0016, -0.2779, -0.6127]) (768 features in tensor)\n",
      "Run time for tasteful was 8.475133582949638 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tatt\n",
      "ling\n",
      "Mean of 33 tensors is: tensor([ 0.1292, -0.1798, -0.2931,  0.3416, -0.1410]) (768 features in tensor)\n",
      "Run time for tattling was 106.16256567463279 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "taunt\n",
      "Mean of 50 tensors is: tensor([ 0.0646, -0.3304, -0.2590, -0.0506, -0.6631]) (768 features in tensor)\n",
      "Run time for taunt was 15.606800204142928 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "t\n",
      "aunting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([-0.0525, -0.1593, -0.3610,  0.1299, -0.2902]) (768 features in tensor)\n",
      "Run time for taunting was 13.644758977927268 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "t\n",
      "aut\n",
      "Mean of 50 tensors is: tensor([ 0.2620,  0.0971, -0.1219,  0.4308, -0.3476]) (768 features in tensor)\n",
      "Run time for taut was 13.306375736370683 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tax\n",
      "Mean of 50 tensors is: tensor([ 0.0496,  0.5874, -0.2282, -1.0033,  0.5259]) (768 features in tensor)\n",
      "Run time for tax was 4.367366285994649 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "taxi\n",
      "Mean of 50 tensors is: tensor([-0.0059,  0.1544,  0.0726, -0.1829, -0.3423]) (768 features in tensor)\n",
      "Run time for taxi was 14.359437874518335 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tea\n",
      "Mean of 50 tensors is: tensor([ 0.0755, -0.1876, -0.0544,  0.0487,  0.9660]) (768 features in tensor)\n",
      "Run time for tea was 4.896377225406468 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "teacher\n",
      "Mean of 50 tensors is: tensor([ 0.0926, -0.3322,  0.1030,  0.3958, -0.5203]) (768 features in tensor)\n",
      "Run time for teacher was 7.2059923112392426 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tear\n",
      "Mean of 50 tensors is: tensor([ 0.2551,  0.1491,  0.2272, -0.0767, -0.6526]) (768 features in tensor)\n",
      "Run time for tear was 4.940711662173271 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tear\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.2940,  0.3049, -0.0835,  0.2471,  0.4488]) (768 features in tensor)\n",
      "Run time for tearful was 15.954735803417861 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tear\n",
      "y\n",
      "Mean of 50 tensors is: tensor([ 0.3196,  0.1297,  0.1157, -0.0200,  0.4046]) (768 features in tensor)\n",
      "Run time for teary was 15.554384652525187 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tease\n",
      "Mean of 50 tensors is: tensor([ 0.2327, -0.1102, -0.1477, -0.0897, -0.3449]) (768 features in tensor)\n",
      "Run time for tease was 11.442263289354742 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "teasing\n",
      "Mean of 50 tensors is: tensor([-0.0082,  0.1368, -0.2379,  0.6364,  0.1952]) (768 features in tensor)\n",
      "Run time for teasing was 7.230519525706768 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "teeth\n",
      "Mean of 50 tensors is: tensor([ 0.2362,  0.1345,  0.5676, -0.7760,  0.4641]) (768 features in tensor)\n",
      "Run time for teeth was 3.9562519285827875 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "telephone\n",
      "Mean of 50 tensors is: tensor([ 0.0641,  0.2564,  0.0752, -0.3307, -0.0654]) (768 features in tensor)\n",
      "Run time for telephone was 7.550769575871527 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "television\n",
      "Mean of 50 tensors is: tensor([ 0.3021, -0.2460, -0.2712, -0.7555,  0.3284]) (768 features in tensor)\n",
      "Run time for television was 5.345791682600975 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "temper\n",
      "Mean of 50 tensors is: tensor([ 0.2573,  0.0052, -0.0237,  0.6078, -0.6764]) (768 features in tensor)\n",
      "Run time for temper was 7.425599194131792 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tempered\n",
      "Mean of 50 tensors is: tensor([ 0.0851,  0.3223,  0.0014,  0.2623, -0.0990]) (768 features in tensor)\n",
      "Run time for tempered was 5.0477162934839725 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "temp\n",
      "est\n",
      "Mean of 50 tensors is: tensor([ 0.0399,  0.1612, -0.0470, -0.2443,  0.1816]) (768 features in tensor)\n",
      "Run time for tempest was 98.38385128043592 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "temp\n",
      "est\n",
      "uous\n",
      "Mean of 50 tensors is: tensor([ 0.1645,  0.5745, -0.0537, -0.0942,  0.3290]) (768 features in tensor)\n",
      "Run time for tempestuous was 50.212629130110145 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tempted\n",
      "Mean of 50 tensors is: tensor([ 0.0085,  0.1882, -0.1038,  0.6646,  0.0643]) (768 features in tensor)\n",
      "Run time for tempted was 5.172461628913879 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ten\n",
      "acious\n",
      "Mean of 50 tensors is: tensor([ 0.0776,  0.0542,  0.0337, -0.1671,  0.2751]) (768 features in tensor)\n",
      "Run time for tenacious was 21.34033215790987 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tender\n",
      "Mean of 50 tensors is: tensor([ 0.0788,  0.0185,  0.0168,  0.0295, -0.0728]) (768 features in tensor)\n",
      "Run time for tender was 4.504048976115882 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tender\n",
      "ness\n",
      "Mean of 50 tensors is: tensor([ 0.2997,  0.1843,  0.2511,  0.0283, -0.4302]) (768 features in tensor)\n",
      "Run time for tenderness was 8.93936775252223 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tense\n",
      "Mean of 50 tensors is: tensor([ 0.1591, -0.2874,  0.0127,  0.2578,  0.8033]) (768 features in tensor)\n",
      "Run time for tense was 4.605930347926915 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tens\n",
      "ed\n",
      "Mean of 50 tensors is: tensor([ 0.2139,  0.0563,  0.0411, -0.2479, -0.2606]) (768 features in tensor)\n",
      "Run time for tensed was 24.58507197909057 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tension\n",
      "Mean of 50 tensors is: tensor([ 0.1993,  0.3984,  0.1416,  0.2321, -0.2147]) (768 features in tensor)\n",
      "Run time for tension was 4.4018338760361075 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tentative\n",
      "Mean of 50 tensors is: tensor([-0.0566, -0.1000,  0.0733,  0.6531, -0.6743]) (768 features in tensor)\n",
      "Run time for tentative was 5.9439542181789875 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "terrible\n",
      "Mean of 50 tensors is: tensor([ 0.0813,  0.5066, -0.1733,  0.3507,  0.7759]) (768 features in tensor)\n",
      "Run time for terrible was 3.8667803397402167 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "terrific\n",
      "Mean of 50 tensors is: tensor([ 0.0280,  1.0852, -0.2140, -0.1058, -0.0761]) (768 features in tensor)\n",
      "Run time for terrific was 4.613377396948636 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "terrified\n",
      "Mean of 50 tensors is: tensor([ 0.2285, -0.1851,  0.2043,  0.5509,  0.4878]) (768 features in tensor)\n",
      "Run time for terrified was 6.076169080100954 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "terror\n",
      "Mean of 50 tensors is: tensor([ 0.1343,  0.0303, -0.0779,  0.3136,  0.4212]) (768 features in tensor)\n",
      "Run time for terror was 5.125428688712418 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "terror\n",
      "ized\n",
      "Mean of 50 tensors is: tensor([ 0.2780, -0.0335,  0.1085,  0.3695,  0.4109]) (768 features in tensor)\n",
      "Run time for terrorized was 11.32075678743422 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "terror\n",
      "izing\n",
      "Mean of 50 tensors is: tensor([0.1873, 0.0032, 0.1717, 0.2938, 0.3320]) (768 features in tensor)\n",
      "Run time for terrorizing was 13.566205278970301 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ter\n",
      "se\n",
      "Mean of 50 tensors is: tensor([ 0.2030,  0.4010, -0.2281,  0.7066,  0.0489]) (768 features in tensor)\n",
      "Run time for terse was 21.311908662319183 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "test\n",
      "y\n",
      "Mean of 50 tensors is: tensor([ 0.0537, -0.0968,  0.3251, -0.2251,  0.0948]) (768 features in tensor)\n",
      "Run time for testy was 10.872804670594633 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "t\n",
      "etch\n",
      "y\n",
      "Mean of 22 tensors is: tensor([ 0.2636,  0.4727, -0.0521,  0.2745,  0.1475]) (768 features in tensor)\n",
      "Run time for tetchy was 106.85007314849645 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "text\n",
      "Mean of 50 tensors is: tensor([ 0.2038, -0.2975, -0.1255,  0.2503, -0.7775]) (768 features in tensor)\n",
      "Run time for text was 3.9104139916598797 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "thankful\n",
      "Mean of 50 tensors is: tensor([ 0.1602, -0.3374,  0.2644,  0.4274,  0.2125]) (768 features in tensor)\n",
      "Run time for thankful was 3.857336145825684 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "theft\n",
      "Mean of 50 tensors is: tensor([ 0.0051,  0.2642,  0.0869, -0.8911,  0.0119]) (768 features in tensor)\n",
      "Run time for theft was 8.555882075801492 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "theme\n",
      "Mean of 50 tensors is: tensor([0.2303, 0.0973, 0.1809, 0.2702, 0.5633]) (768 features in tensor)\n",
      "Run time for theme was 4.441256359219551 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "theory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.1990, -0.1766,  0.0606,  0.8303,  0.5146]) (768 features in tensor)\n",
      "Run time for theory was 4.538937377743423 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "think\n",
      "Mean of 50 tensors is: tensor([0.1301, 0.0977, 0.2782, 0.7909, 0.2500]) (768 features in tensor)\n",
      "Run time for think was 3.715042868629098 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "thinking\n",
      "Mean of 50 tensors is: tensor([ 0.1177, -0.1317,  0.1013,  0.9410,  0.0174]) (768 features in tensor)\n",
      "Run time for thinking was 3.5086542312055826 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "thought\n",
      "Mean of 50 tensors is: tensor([0.0670, 0.2716, 0.2611, 0.7132, 0.2315]) (768 features in tensor)\n",
      "Run time for thought was 3.390676049515605 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "thoughtful\n",
      "Mean of 50 tensors is: tensor([ 0.1204,  0.2914,  0.0328,  0.5192, -0.1390]) (768 features in tensor)\n",
      "Run time for thoughtful was 4.125216426327825 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "thought\n",
      "fulness\n",
      "Mean of 50 tensors is: tensor([ 0.1768,  0.1142,  0.2207,  0.3848, -0.5762]) (768 features in tensor)\n",
      "Run time for thoughtfulness was 16.391393435187638 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "threat\n",
      "Mean of 50 tensors is: tensor([-0.0929,  0.3807,  0.0542, -0.0947, -0.0323]) (768 features in tensor)\n",
      "Run time for threat was 3.8517769621685147 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "threatened\n",
      "Mean of 50 tensors is: tensor([ 0.0632, -0.3121,  0.0986,  0.1858, -0.3414]) (768 features in tensor)\n",
      "Run time for threatened was 5.325749768875539 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "threatening\n",
      "Mean of 50 tensors is: tensor([ 0.0706, -0.0826, -0.0798,  0.2673, -0.1245]) (768 features in tensor)\n",
      "Run time for threatening was 5.476581014692783 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "thrilled\n",
      "Mean of 50 tensors is: tensor([ 0.2850,  0.6813,  0.0348,  0.2148, -0.1603]) (768 features in tensor)\n",
      "Run time for thrilled was 3.8459775717929006 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "throat\n",
      "Mean of 50 tensors is: tensor([ 0.1482, -0.4859,  0.2505, -0.1948,  0.4108]) (768 features in tensor)\n",
      "Run time for throat was 8.886526939459145 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "thrown\n",
      "Mean of 50 tensors is: tensor([ 0.1119, -0.1708,  0.1595,  0.0491, -0.5193]) (768 features in tensor)\n",
      "Run time for thrown was 4.39673862978816 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "thumb\n",
      "Mean of 50 tensors is: tensor([ 0.2145, -0.3945, -0.0820,  0.4785,  0.4710]) (768 features in tensor)\n",
      "Run time for thumb was 5.413519686087966 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "thunder\n",
      "stru\n",
      "ck\n",
      "Mean of 50 tensors is: tensor([ 0.2246, -0.0877, -0.0816,  0.0455, -0.0204]) (768 features in tensor)\n",
      "Run time for thunderstruck was 54.45377655234188 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "thwarted\n",
      "Mean of 50 tensors is: tensor([-0.0250,  0.0982, -0.0179,  0.2524,  0.2520]) (768 features in tensor)\n",
      "Run time for thwarted was 11.883931297808886 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tick\n",
      "ed\n",
      "Mean of 50 tensors is: tensor([ 0.2887,  0.4150, -0.0801, -0.2294, -0.1978]) (768 features in tensor)\n",
      "Run time for ticked was 8.139710140414536 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tick\n",
      "led\n",
      "Mean of 50 tensors is: tensor([0.5064, 0.4791, 0.0154, 0.2866, 0.2732]) (768 features in tensor)\n",
      "Run time for tickled was 6.9747927244752645 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tied\n",
      "Mean of 50 tensors is: tensor([ 0.1633, -0.0790,  0.0331, -0.0537,  0.2283]) (768 features in tensor)\n",
      "Run time for tied was 4.120496617630124 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tie\n",
      "red\n",
      "Mean of 50 tensors is: tensor([ 0.2485,  0.2691, -0.1861,  0.5907, -0.0058]) (768 features in tensor)\n",
      "Run time for tiered was 6.9410098269581795 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tight\n",
      "Mean of 50 tensors is: tensor([ 0.2779, -0.3298, -0.2609,  0.1512,  0.3493]) (768 features in tensor)\n",
      "Run time for tight was 6.507991925813258 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "tight\n",
      "l\n",
      "ipped\n",
      "Mean of 7 tensors is: tensor([-0.0178,  0.4314, -0.3606, -0.0471, -0.5565]) (768 features in tensor)\n",
      "Run time for tightlipped was 103.77766769286245 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "timber\n",
      "Mean of 50 tensors is: tensor([ 0.0726,  0.1020,  0.0977, -1.0302,  0.3234]) (768 features in tensor)\n",
      "Run time for timber was 13.792168444953859 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "timid\n",
      "Mean of 50 tensors is: tensor([ 0.2241, -0.0492, -0.0965,  0.6922,  0.0367]) (768 features in tensor)\n",
      "Run time for timid was 11.491139047779143 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "timid\n",
      "ly\n",
      "Mean of 50 tensors is: tensor([ 0.3636,  0.3452, -0.1984,  0.5216, -0.0999]) (768 features in tensor)\n",
      "Run time for timidly was 57.28695639781654 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "timid\n",
      "ness\n",
      "Mean of 5 tensors is: tensor([ 0.2478,  0.1225,  0.0802, -0.1974, -0.1819]) (768 features in tensor)\n",
      "Run time for timidness was 103.03727658651769 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tin\n",
      "Mean of 50 tensors is: tensor([ 0.3354, -0.2002,  0.0383,  0.2450,  0.5705]) (768 features in tensor)\n",
      "Run time for tin was 4.497350071556866 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tiny\n",
      "Mean of 50 tensors is: tensor([ 0.0500,  0.1592, -0.0791,  1.0452,  0.7958]) (768 features in tensor)\n",
      "Run time for tiny was 4.945319425314665 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tired\n",
      "Mean of 50 tensors is: tensor([-0.0046, -0.0589,  0.4169, -0.1428,  0.5702]) (768 features in tensor)\n",
      "Run time for tired was 3.862722766585648 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tired\n",
      "ly\n",
      "Mean of 50 tensors is: tensor([ 0.2337,  0.6113, -0.0827, -0.2546,  0.3429]) (768 features in tensor)\n",
      "Run time for tiredly was 101.1734608700499 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tired\n",
      "ness\n",
      "Mean of 50 tensors is: tensor([ 0.2666, -0.1857,  0.2586, -0.3677,  0.1997]) (768 features in tensor)\n",
      "Run time for tiredness was 13.080961465835571 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "tit\n",
      "ill\n",
      "ated\n",
      "Mean of 18 tensors is: tensor([ 0.2445,  0.3479,  0.1615,  0.0894, -0.6089]) (768 features in tensor)\n",
      "Run time for titillated was 102.35180152580142 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tobacco\n",
      "Mean of 50 tensors is: tensor([-0.0989,  0.1330,  0.0210, -0.3102, -0.4651]) (768 features in tensor)\n",
      "Run time for tobacco was 8.033821902237833 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "toe\n",
      "Mean of 50 tensors is: tensor([ 0.1887, -0.1769,  0.0624, -0.1426,  0.0212]) (768 features in tensor)\n",
      "Run time for toe was 7.833038628101349 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tolerant\n",
      "Mean of 50 tensors is: tensor([ 0.0528,  0.1329,  0.2729,  0.5987, -0.3667]) (768 features in tensor)\n",
      "Run time for tolerant was 6.697152627632022 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tongue\n",
      "Mean of 50 tensors is: tensor([ 0.0940, -0.0466,  0.2915,  0.1843, -0.7370]) (768 features in tensor)\n",
      "Run time for tongue was 5.0134606417268515 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tool\n",
      "Mean of 50 tensors is: tensor([ 0.1502,  0.0176,  0.1371,  0.2325, -0.2750]) (768 features in tensor)\n",
      "Run time for tool was 4.5001153983175755 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tooth\n",
      "Mean of 50 tensors is: tensor([ 0.3064,  0.0485,  0.4996, -0.9287,  0.0157]) (768 features in tensor)\n",
      "Run time for tooth was 5.028495272621512 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "top\n",
      "Mean of 50 tensors is: tensor([ 0.1995,  0.0512, -0.0763, -0.0598,  0.0743]) (768 features in tensor)\n",
      "Run time for top was 5.757907879538834 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "topic\n",
      "Mean of 50 tensors is: tensor([ 0.1245,  0.1207,  0.0779,  0.5726, -0.0380]) (768 features in tensor)\n",
      "Run time for topic was 4.083244372159243 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tor\n",
      "mented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.2336,  0.4131, -0.0780,  0.6265,  0.5361]) (768 features in tensor)\n",
      "Run time for tormented was 8.370955761522055 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "touched\n",
      "Mean of 50 tensors is: tensor([0.3166, 0.6957, 0.1809, 0.0959, 0.1699]) (768 features in tensor)\n",
      "Run time for touched was 3.6102425707504153 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tough\n",
      "Mean of 50 tensors is: tensor([0.1593, 0.2854, 0.0238, 0.7162, 0.5009]) (768 features in tensor)\n",
      "Run time for tough was 4.078823951072991 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tower\n",
      "Mean of 50 tensors is: tensor([ 0.0403,  0.1603,  0.1568, -0.8576, -0.4872]) (768 features in tensor)\n",
      "Run time for tower was 9.139184662140906 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "to\n",
      "ying\n",
      "Mean of 50 tensors is: tensor([ 0.0418,  0.1908, -0.2636,  0.4079, -0.6861]) (768 features in tensor)\n",
      "Run time for toying was 9.52711639739573 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tragedy\n",
      "Mean of 50 tensors is: tensor([ 0.2999,  0.8541,  0.2558, -0.1336,  0.2526]) (768 features in tensor)\n",
      "Run time for tragedy was 6.042219092138112 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tragic\n",
      "Mean of 50 tensors is: tensor([ 0.3455,  0.7528, -0.0302,  0.2594,  0.8930]) (768 features in tensor)\n",
      "Run time for tragic was 4.392001805827022 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "tr\n",
      "ag\n",
      "ical\n",
      "Mean of 39 tensors is: tensor([-0.0285,  0.4817, -0.2638,  0.3288, -0.2934]) (768 features in tensor)\n",
      "Run time for tragical was 107.43501636665314 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tranquil\n",
      "Mean of 50 tensors is: tensor([0.2355, 1.0810, 0.0971, 0.3311, 0.1866]) (768 features in tensor)\n",
      "Run time for tranquil was 5.015929032117128 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tranqu\n",
      "ility\n",
      "Mean of 50 tensors is: tensor([ 0.3357,  0.4253,  0.1897,  0.5501, -0.8449]) (768 features in tensor)\n",
      "Run time for tranquility was 7.894172932952642 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "transf\n",
      "ixed\n",
      "Mean of 50 tensors is: tensor([ 0.1569,  0.4741,  0.0635,  0.3170, -0.2225]) (768 features in tensor)\n",
      "Run time for transfixed was 34.01453105546534 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "traumat\n",
      "ized\n",
      "Mean of 50 tensors is: tensor([ 0.2493,  0.2314, -0.0140,  0.5309,  0.7592]) (768 features in tensor)\n",
      "Run time for traumatized was 6.9826563419774175 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tree\n",
      "Mean of 50 tensors is: tensor([ 0.0594,  0.3233,  0.0323, -0.5945, -0.0907]) (768 features in tensor)\n",
      "Run time for tree was 4.884007343091071 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "trembling\n",
      "Mean of 50 tensors is: tensor([ 0.1565, -0.1026, -0.2294,  0.2642,  0.6565]) (768 features in tensor)\n",
      "Run time for trembling was 10.1684833727777 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "tre\n",
      "pid\n",
      "Mean of 50 tensors is: tensor([ 0.1604,  0.6367, -0.1971,  0.0983, -0.6480]) (768 features in tensor)\n",
      "Run time for trepid was 106.04523965716362 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "tre\n",
      "p\n",
      "idation\n",
      "Mean of 50 tensors is: tensor([ 0.0772,  0.3330, -0.0691,  0.4572, -0.8741]) (768 features in tensor)\n",
      "Run time for trepidation was 21.789755732752383 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "trial\n",
      "Mean of 50 tensors is: tensor([0.0477, 0.2006, 0.2488, 0.4429, 0.2868]) (768 features in tensor)\n",
      "Run time for trial was 4.910323257558048 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "trick\n",
      "Mean of 50 tensors is: tensor([0.2687, 0.3446, 0.0273, 0.0687, 0.1904]) (768 features in tensor)\n",
      "Run time for trick was 5.093943268060684 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "trick\n",
      "ster\n",
      "Mean of 50 tensors is: tensor([ 0.0188, -0.0159,  0.0156, -0.2906,  0.1504]) (768 features in tensor)\n",
      "Run time for trickster was 16.589335249736905 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tricky\n",
      "Mean of 50 tensors is: tensor([ 0.3008,  0.7133, -0.1580,  0.8073,  0.2998]) (768 features in tensor)\n",
      "Run time for tricky was 5.249674633145332 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "trip\n",
      "Mean of 50 tensors is: tensor([0.2393, 0.7696, 0.1031, 0.3121, 0.3854]) (768 features in tensor)\n",
      "Run time for trip was 4.745799810625613 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "triumph\n",
      "Mean of 50 tensors is: tensor([ 0.3451,  0.5179,  0.2147, -0.4939, -1.2175]) (768 features in tensor)\n",
      "Run time for triumph was 7.472787149250507 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "triumphant\n",
      "Mean of 50 tensors is: tensor([ 0.1119,  0.7850, -0.0015, -0.0278, -0.1215]) (768 features in tensor)\n",
      "Run time for triumphant was 9.603613850660622 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "troubled\n",
      "Mean of 50 tensors is: tensor([0.2926, 0.6931, 0.0241, 0.6329, 1.0900]) (768 features in tensor)\n",
      "Run time for troubled was 4.985521453432739 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "troublesome\n",
      "Mean of 50 tensors is: tensor([ 0.1762,  0.7731, -0.0287,  0.7707,  0.3443]) (768 features in tensor)\n",
      "Run time for troublesome was 6.53532575443387 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "troubling\n",
      "Mean of 50 tensors is: tensor([ 0.1448,  0.6122, -0.0150,  0.2534,  1.2030]) (768 features in tensor)\n",
      "Run time for troubling was 5.704908042214811 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "trusting\n",
      "Mean of 50 tensors is: tensor([ 0.1449, -0.0160,  0.1834,  0.4343,  0.2107]) (768 features in tensor)\n",
      "Run time for trusting was 5.3146821381524205 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "trustworthy\n",
      "Mean of 50 tensors is: tensor([ 0.1621,  0.6213,  0.1750, -0.1319, -0.3045]) (768 features in tensor)\n",
      "Run time for trustworthy was 4.4885605946183205 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "try\n",
      "Mean of 50 tensors is: tensor([ 0.1096, -0.1403, -0.0676,  1.0512, -0.2590]) (768 features in tensor)\n",
      "Run time for try was 3.7123868819326162 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "tumultuous\n",
      "Mean of 50 tensors is: tensor([ 0.1864,  0.7707, -0.1148,  0.8170,  0.5565]) (768 features in tensor)\n",
      "Run time for tumultuous was 9.531920311972499 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "turbulent\n",
      "Mean of 50 tensors is: tensor([ 0.1881,  0.5922, -0.2208,  0.7232,  0.1646]) (768 features in tensor)\n",
      "Run time for turbulent was 6.778557641431689 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "turkey\n",
      "Mean of 50 tensors is: tensor([ 0.1183, -0.0119,  0.1607, -0.3614,  0.2254]) (768 features in tensor)\n",
      "Run time for turkey was 7.221579294651747 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "tw\n",
      "ink\n",
      "ly\n",
      "Mean of 50 tensors is: tensor([ 0.2059,  0.4348, -0.3348,  0.1528, -0.1194]) (768 features in tensor)\n",
      "Run time for twinkly was 64.78877711016685 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "um\n",
      "br\n",
      "age\n",
      "Mean of 50 tensors is: tensor([ 0.1498,  0.4797, -0.1751,  0.4719, -0.5264]) (768 features in tensor)\n",
      "Run time for umbrage was 48.316360547207296 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "um\n",
      "br\n",
      "age\n",
      "ous\n",
      "Mean of 6 tensors is: tensor([0.1499, 0.3499, 0.0505, 0.1183, 0.0905]) (768 features in tensor)\n",
      "Run time for umbrageous was 103.88121030200273 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unaffected\n",
      "Mean of 50 tensors is: tensor([-0.0264,  0.5892,  0.1166,  0.2642, -1.0640]) (768 features in tensor)\n",
      "Run time for unaffected was 7.633806679397821 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "un\n",
      "ag\n",
      "itated\n",
      "Mean of 6 tensors is: tensor([ 0.0468,  0.2680,  0.2223, -0.2724, -0.9083]) (768 features in tensor)\n",
      "Run time for unagitated was 100.8497056402266 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "un\n",
      "am\n",
      "used\n",
      "Mean of 30 tensors is: tensor([-0.0534,  0.5157,  0.1303,  0.1679, -0.2765]) (768 features in tensor)\n",
      "Run time for unamused was 109.13102681655437 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "un\n",
      "app\n",
      "reci\n",
      "ative\n",
      "Mean of 50 tensors is: tensor([ 0.0078,  0.3066, -0.0656, -0.2277, -0.8978]) (768 features in tensor)\n",
      "Run time for unappreciative was 106.6273178011179 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "un\n",
      "appro\n",
      "achable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.0909,  0.5968, -0.0062,  0.2303, -0.8255]) (768 features in tensor)\n",
      "Run time for unapproachable was 75.59699642751366 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "un\n",
      "assert\n",
      "ive\n",
      "Mean of 9 tensors is: tensor([ 0.2600,  0.2168,  0.0571, -0.0399, -0.8956]) (768 features in tensor)\n",
      "Run time for unassertive was 104.04399540554732 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "un\n",
      "assuming\n",
      "Mean of 50 tensors is: tensor([ 0.1450,  0.4886,  0.0984, -0.0343, -0.4233]) (768 features in tensor)\n",
      "Run time for unassuming was 12.556199456565082 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unaware\n",
      "Mean of 50 tensors is: tensor([ 0.0178, -0.1809, -0.0598, -0.0148, -0.2604]) (768 features in tensor)\n",
      "Run time for unaware was 5.066211238503456 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unbel\n",
      "ief\n",
      "Mean of 50 tensors is: tensor([ 0.2750, -0.1620,  0.0725,  0.1076, -0.8738]) (768 features in tensor)\n",
      "Run time for unbelief was 16.806563169695437 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unbelievable\n",
      "Mean of 50 tensors is: tensor([ 0.1295,  0.4322,  0.0843, -0.2474,  0.0078]) (768 features in tensor)\n",
      "Run time for unbelievable was 5.511758838780224 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unbel\n",
      "ieving\n",
      "Mean of 50 tensors is: tensor([ 0.1286,  0.0472,  0.1708,  0.1470, -0.4115]) (768 features in tensor)\n",
      "Run time for unbelieving was 22.275940496474504 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "un\n",
      "bot\n",
      "hered\n",
      "Mean of 50 tensors is: tensor([ 0.1190,  0.5012, -0.0281,  0.1416, -0.7638]) (768 features in tensor)\n",
      "Run time for unbothered was 92.47786379326135 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unc\n",
      "aring\n",
      "Mean of 50 tensors is: tensor([ 0.1978,  0.4609, -0.0245, -0.6113,  0.9425]) (768 features in tensor)\n",
      "Run time for uncaring was 13.580738548189402 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "uncertain\n",
      "Mean of 50 tensors is: tensor([ 0.0974,  0.0987, -0.1013,  0.1383,  0.0684]) (768 features in tensor)\n",
      "Run time for uncertain was 5.469036684371531 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "uncertain\n",
      "ly\n",
      "Mean of 50 tensors is: tensor([ 0.3043,  0.5066, -0.0654,  0.0018, -0.5249]) (768 features in tensor)\n",
      "Run time for uncertainly was 92.88052830472589 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "uncertainty\n",
      "Mean of 50 tensors is: tensor([ 0.1730,  0.1511,  0.1732,  0.2973, -0.9315]) (768 features in tensor)\n",
      "Run time for uncertainty was 5.716461880132556 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unc\n",
      "ivil\n",
      "Mean of 50 tensors is: tensor([ 0.2446,  0.3945,  0.0008, -0.5646, -0.0860]) (768 features in tensor)\n",
      "Run time for uncivil was 30.849075945094228 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "uncle\n",
      "Mean of 50 tensors is: tensor([ 0.1772, -0.0389,  0.1658, -0.3658, -0.7256]) (768 features in tensor)\n",
      "Run time for uncle was 6.907388641498983 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "uncomfortable\n",
      "Mean of 50 tensors is: tensor([ 0.1342, -0.0519,  0.0418,  0.3492, -0.5830]) (768 features in tensor)\n",
      "Run time for uncomfortable was 5.194476083852351 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "uncom\n",
      "mitted\n",
      "Mean of 50 tensors is: tensor([ 0.3114,  0.2297, -0.0412,  0.0658, -0.1500]) (768 features in tensor)\n",
      "Run time for uncommitted was 41.23748493939638 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "uncom\n",
      "mun\n",
      "icative\n",
      "Mean of 32 tensors is: tensor([ 0.0615,  0.6793,  0.1484,  0.1097, -0.4468]) (768 features in tensor)\n",
      "Run time for uncommunicative was 108.26457082107663 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "uncom\n",
      "pre\n",
      "hend\n",
      "ing\n",
      "Mean of 40 tensors is: tensor([ 0.2181,  0.2125, -0.0133, -0.0977, -0.4915]) (768 features in tensor)\n",
      "Run time for uncomprehending was 110.3438725490123 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "uncomp\n",
      "romising\n",
      "Mean of 50 tensors is: tensor([ 0.1359,  0.3442, -0.0438, -0.2510, -0.4957]) (768 features in tensor)\n",
      "Run time for uncompromising was 10.151073697023094 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "uncon\n",
      "cerned\n",
      "Mean of 50 tensors is: tensor([-0.0974,  0.3435, -0.0466,  0.3813, -0.8135]) (768 features in tensor)\n",
      "Run time for unconcerned was 21.638363827951252 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "un\n",
      "conf\n",
      "ident\n",
      "Mean of 26 tensors is: tensor([ 0.1963,  0.1831,  0.0322,  0.2206, -0.8593]) (768 features in tensor)\n",
      "Run time for unconfident was 107.30103179439902 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "uncon\n",
      "vin\n",
      "ced\n",
      "Mean of 50 tensors is: tensor([ 0.0360,  0.2009, -0.0064,  0.2839, -0.8016]) (768 features in tensor)\n",
      "Run time for unconvinced was 29.58081973902881 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "un\n",
      "co\n",
      "operative\n",
      "Mean of 50 tensors is: tensor([ 0.0032,  0.6327,  0.1058,  0.0433, -0.8509]) (768 features in tensor)\n",
      "Run time for uncooperative was 32.966872641816735 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unc\n",
      "urious\n",
      "Mean of 4 tensors is: tensor([ 0.0908,  0.0373,  0.0687, -0.2056, -0.0805]) (768 features in tensor)\n",
      "Run time for uncurious was 105.3423740202561 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "undecided\n",
      "Mean of 50 tensors is: tensor([ 0.1165,  0.1605, -0.0726,  0.0767, -0.7579]) (768 features in tensor)\n",
      "Run time for undecided was 12.3308649584651 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "under\n",
      "handed\n",
      "Mean of 50 tensors is: tensor([ 0.1134,  0.5134, -0.1455, -0.1912,  0.0546]) (768 features in tensor)\n",
      "Run time for underhanded was 36.4022316550836 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "understand\n",
      "Mean of 50 tensors is: tensor([ 0.0881, -0.0930,  0.0389,  0.3298, -0.5416]) (768 features in tensor)\n",
      "Run time for understand was 3.7592304488644004 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "understanding\n",
      "Mean of 50 tensors is: tensor([-0.0187, -0.0671,  0.0951,  0.3616, -0.2278]) (768 features in tensor)\n",
      "Run time for understanding was 4.145608318969607 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "undesirable\n",
      "Mean of 50 tensors is: tensor([ 0.2749,  0.7544, -0.0454,  0.2705, -0.3033]) (768 features in tensor)\n",
      "Run time for undesirable was 6.764089402742684 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "une\n",
      "ase\n",
      "Mean of 50 tensors is: tensor([ 0.1518,  0.0180, -0.0379,  0.3829, -0.6485]) (768 features in tensor)\n",
      "Run time for unease was 18.65051605552435 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "une\n",
      "as\n",
      "ily\n",
      "Mean of 50 tensors is: tensor([ 0.2094,  0.5921, -0.2002,  0.3131,  0.0523]) (768 features in tensor)\n",
      "Run time for uneasily was 69.54040062706918 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "une\n",
      "as\n",
      "iness\n",
      "Mean of 50 tensors is: tensor([ 0.1406,  0.1625, -0.0013,  0.1948,  0.0742]) (768 features in tensor)\n",
      "Run time for uneasiness was 14.129498994909227 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "uneasy\n",
      "Mean of 50 tensors is: tensor([ 0.1025, -0.0137, -0.0207,  0.3469, -0.0283]) (768 features in tensor)\n",
      "Run time for uneasy was 7.645517855882645 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "un\n",
      "em\n",
      "otional\n",
      "Mean of 50 tensors is: tensor([ 0.1772,  0.3703,  0.3004, -0.2826, -0.4838]) (768 features in tensor)\n",
      "Run time for unemotional was 60.70112814195454 seconds.\n",
      "\n",
      "There are 5 tokens in tokenized vocabulary word:\n",
      "un\n",
      "ent\n",
      "hus\n",
      "i\n",
      "astic\n",
      "Mean of 50 tensors is: tensor([ 0.0378,  0.2660, -0.0251, -0.3889, -0.9656]) (768 features in tensor)\n",
      "Run time for unenthusiastic was 102.95199348218739 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "unex\n",
      "c\n",
      "ited\n",
      "Mean of 29 tensors is: tensor([ 0.3049,  0.5244, -0.1113, -0.1301, -0.3869]) (768 features in tensor)\n",
      "Run time for unexcited was 106.45194823015481 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unexpected\n",
      "Mean of 50 tensors is: tensor([ 0.2682,  0.7737,  0.1696,  0.4925, -0.7854]) (768 features in tensor)\n",
      "Run time for unexpected was 3.696153972297907 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unfamiliar\n",
      "Mean of 50 tensors is: tensor([ 0.1980,  0.0217,  0.2804,  0.0916, -0.8778]) (768 features in tensor)\n",
      "Run time for unfamiliar was 6.96492937579751 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "unf\n",
      "athom\n",
      "able\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.2710, -0.0633, -0.0418,  0.2245, -0.8050]) (768 features in tensor)\n",
      "Run time for unfathomable was 12.82919054850936 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unf\n",
      "azed\n",
      "Mean of 50 tensors is: tensor([-0.1625,  0.4347, -0.0863,  0.4552, -0.8524]) (768 features in tensor)\n",
      "Run time for unfazed was 43.455021288245916 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "unf\n",
      "e\n",
      "eling\n",
      "Mean of 50 tensors is: tensor([ 0.1025,  0.2263,  0.0785, -0.1535, -0.3278]) (768 features in tensor)\n",
      "Run time for unfeeling was 48.62709602154791 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unfocused\n",
      "Mean of 50 tensors is: tensor([ 0.0889,  0.0582, -0.0279,  0.2229,  0.1918]) (768 features in tensor)\n",
      "Run time for unfocused was 33.14468964654952 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unforeseen\n",
      "Mean of 50 tensors is: tensor([ 0.1651,  0.5512, -0.0823,  0.3107, -0.3689]) (768 features in tensor)\n",
      "Run time for unforeseen was 7.702970045618713 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unfor\n",
      "giving\n",
      "Mean of 50 tensors is: tensor([ 0.3105,  0.0160, -0.0609,  0.2773, -0.7101]) (768 features in tensor)\n",
      "Run time for unforgiving was 10.43448480591178 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "unf\n",
      "orth\n",
      "coming\n",
      "Mean of 2 tensors is: tensor([-0.0919,  0.5604,  0.1021,  0.1655, -1.0235]) (768 features in tensor)\n",
      "Run time for unforthcoming was 104.62492235191166 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unfortunate\n",
      "Mean of 50 tensors is: tensor([ 0.2271,  0.4505, -0.0605,  0.4174,  0.6369]) (768 features in tensor)\n",
      "Run time for unfortunate was 5.478525341488421 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "unf\n",
      "riend\n",
      "ly\n",
      "Mean of 50 tensors is: tensor([ 0.0779,  0.4250, -0.0911, -0.0554, -0.6978]) (768 features in tensor)\n",
      "Run time for unfriendly was 13.830844863317907 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unhappy\n",
      "Mean of 50 tensors is: tensor([0.1129, 0.6187, 0.0608, 0.5583, 0.2556]) (768 features in tensor)\n",
      "Run time for unhappy was 5.738090638071299 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "un\n",
      "hing\n",
      "ed\n",
      "Mean of 50 tensors is: tensor([ 0.1486,  0.3463, -0.0152, -0.2755, -0.2854]) (768 features in tensor)\n",
      "Run time for unhinged was 11.871253489516675 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "un\n",
      "imp\n",
      "ressed\n",
      "Mean of 50 tensors is: tensor([-0.1320,  0.6109,  0.1239, -0.0457, -1.1705]) (768 features in tensor)\n",
      "Run time for unimpressed was 20.552429682575166 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unin\n",
      "formed\n",
      "Mean of 50 tensors is: tensor([ 0.2217,  0.2038, -0.0437,  0.4410, -1.0170]) (768 features in tensor)\n",
      "Run time for uninformed was 13.761346406303346 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "un\n",
      "inspired\n",
      "Mean of 50 tensors is: tensor([ 0.0123,  0.1977,  0.1331, -0.5680, -0.2388]) (768 features in tensor)\n",
      "Run time for uninspired was 15.539036895148456 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "un\n",
      "interested\n",
      "Mean of 50 tensors is: tensor([-0.0689,  0.0972,  0.1328,  0.3011, -0.7475]) (768 features in tensor)\n",
      "Run time for uninterested was 13.057460486888885 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "un\n",
      "involved\n",
      "Mean of 50 tensors is: tensor([-0.0259,  0.1324,  0.0605,  0.1699, -0.9011]) (768 features in tensor)\n",
      "Run time for uninvolved was 61.72610132023692 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unique\n",
      "Mean of 50 tensors is: tensor([ 0.2235,  0.6053, -0.0791,  0.8104,  0.4802]) (768 features in tensor)\n",
      "Run time for unique was 3.7668509176000953 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "universe\n",
      "Mean of 50 tensors is: tensor([ 0.0535, -0.1989,  0.1780,  0.2334, -0.9659]) (768 features in tensor)\n",
      "Run time for universe was 5.237535182386637 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unlike\n",
      "able\n",
      "Mean of 50 tensors is: tensor([ 0.0785,  0.2430,  0.1516,  0.4815, -0.1826]) (768 features in tensor)\n",
      "Run time for unlikeable was 75.81425867043436 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unm\n",
      "oved\n",
      "Mean of 50 tensors is: tensor([-0.1645,  0.3062,  0.0397,  0.2623, -0.5170]) (768 features in tensor)\n",
      "Run time for unmoved was 21.53352107293904 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unnecessary\n",
      "Mean of 50 tensors is: tensor([ 0.2166,  0.9291,  0.1147, -0.9135, -0.6135]) (768 features in tensor)\n",
      "Run time for unnecessary was 5.043602244928479 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unn\n",
      "erved\n",
      "Mean of 50 tensors is: tensor([ 0.3145,  0.3371,  0.1204,  0.5085, -0.4339]) (768 features in tensor)\n",
      "Run time for unnerved was 48.04090202972293 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unpleasant\n",
      "Mean of 50 tensors is: tensor([0.1828, 0.6775, 0.0037, 0.1627, 0.0068]) (768 features in tensor)\n",
      "Run time for unpleasant was 6.207387830130756 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unprepared\n",
      "Mean of 50 tensors is: tensor([ 0.1789, -0.2844,  0.1751, -0.2059, -0.1763]) (768 features in tensor)\n",
      "Run time for unprepared was 10.06303675007075 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "un\n",
      "quiet\n",
      "Mean of 50 tensors is: tensor([ 0.3585, -0.0315, -0.2141,  0.0553,  0.9155]) (768 features in tensor)\n",
      "Run time for unquiet was 22.407555582001805 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unre\n",
      "active\n",
      "Mean of 24 tensors is: tensor([-0.0345,  0.1749, -0.0623,  0.5403, -0.9030]) (768 features in tensor)\n",
      "Run time for unreactive was 107.87389373220503 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unresolved\n",
      "Mean of 50 tensors is: tensor([ 0.0581, -0.1638,  0.1148,  0.2789,  0.6046]) (768 features in tensor)\n",
      "Run time for unresolved was 9.856340221129358 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unrest\n",
      "rained\n",
      "Mean of 50 tensors is: tensor([ 0.3174,  0.0667,  0.0490,  0.0874, -0.2558]) (768 features in tensor)\n",
      "Run time for unrestrained was 20.62814956717193 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "un\n",
      "ruff\n",
      "led\n",
      "Mean of 50 tensors is: tensor([ 0.1075,  0.1536, -0.0069,  0.3680, -0.3425]) (768 features in tensor)\n",
      "Run time for unruffled was 46.37751523125917 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unsatisf\n",
      "ied\n",
      "Mean of 50 tensors is: tensor([-0.0208,  0.3848,  0.0433,  0.1998, -0.9554]) (768 features in tensor)\n",
      "Run time for unsatisfied was 10.13830156903714 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unsett\n",
      "led\n",
      "Mean of 50 tensors is: tensor([0.1787, 0.2326, 0.0376, 0.8784, 0.0555]) (768 features in tensor)\n",
      "Run time for unsettled was 12.148095610551536 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "uns\n",
      "oci\n",
      "able\n",
      "Mean of 36 tensors is: tensor([ 0.1196,  0.5896,  0.1114, -0.0431, -0.3945]) (768 features in tensor)\n",
      "Run time for unsociable was 107.21273925341666 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "un\n",
      "speaking\n",
      "Mean of 0 tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Run time for unspeaking was 103.7400517379865 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "un\n",
      "spoken\n",
      "Mean of 50 tensors is: tensor([ 0.3110, -0.1886, -0.2176,  0.0943, -0.1739]) (768 features in tensor)\n",
      "Run time for unspoken was 7.43154984805733 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "un\n",
      "str\n",
      "ung\n",
      "Mean of 50 tensors is: tensor([ 0.1195, -0.1940, -0.0182, -0.0468, -0.6538]) (768 features in tensor)\n",
      "Run time for unstrung was 80.97829900216311 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unsuccessful\n",
      "Mean of 50 tensors is: tensor([ 0.0930,  0.6833,  0.1080,  0.5936, -0.6795]) (768 features in tensor)\n",
      "Run time for unsuccessful was 7.884905982762575 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unsure\n",
      "Mean of 50 tensors is: tensor([-0.0784,  0.1628, -0.1586,  0.5726, -0.5115]) (768 features in tensor)\n",
      "Run time for unsure was 4.827257500961423 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "unsur\n",
      "pr\n",
      "ised\n",
      "Mean of 50 tensors is: tensor([-0.0896,  0.6962,  0.2474,  0.6491, -0.3544]) (768 features in tensor)\n",
      "Run time for unsurprised was 101.42523067072034 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unsuspecting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([-0.0319,  0.3065,  0.2233, -0.1792, -0.3809]) (768 features in tensor)\n",
      "Run time for unsuspecting was 8.170786353759468 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "uns\n",
      "way\n",
      "ed\n",
      "Mean of 7 tensors is: tensor([-0.0750,  0.2926,  0.0680, -0.0693, -0.2748]) (768 features in tensor)\n",
      "Run time for unswayed was 107.37240052036941 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "un\n",
      "sy\n",
      "mp\n",
      "athetic\n",
      "Mean of 50 tensors is: tensor([ 0.1180,  0.4604,  0.0340, -0.1777, -0.3811]) (768 features in tensor)\n",
      "Run time for unsympathetic was 20.451607468537986 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "untouched\n",
      "Mean of 50 tensors is: tensor([ 0.1229,  0.0123,  0.0778, -0.3082, -0.5517]) (768 features in tensor)\n",
      "Run time for untouched was 7.1679805014282465 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "unt\n",
      "rou\n",
      "bled\n",
      "Mean of 50 tensors is: tensor([ 0.1530,  0.2844,  0.1118,  0.9100, -0.1374]) (768 features in tensor)\n",
      "Run time for untroubled was 60.395166581496596 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "unt\n",
      "rust\n",
      "ing\n",
      "Mean of 33 tensors is: tensor([ 0.2260,  0.0305, -0.0159,  0.0868, -0.3607]) (768 features in tensor)\n",
      "Run time for untrusting was 108.39341190550476 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unwanted\n",
      "Mean of 50 tensors is: tensor([ 0.2105,  0.3127, -0.0660, -0.1737, -0.0158]) (768 features in tensor)\n",
      "Run time for unwanted was 4.008063950575888 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "unw\n",
      "avering\n",
      "Mean of 50 tensors is: tensor([ 0.0664,  0.2109, -0.2454, -0.5113, -0.7104]) (768 features in tensor)\n",
      "Run time for unwavering was 8.869666797108948 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "unw\n",
      "el\n",
      "coming\n",
      "Mean of 50 tensors is: tensor([-0.0630,  0.5954,  0.0544, -0.5235, -0.3549]) (768 features in tensor)\n",
      "Run time for unwelcoming was 70.4700950216502 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "un\n",
      "well\n",
      "Mean of 50 tensors is: tensor([ 0.2044,  0.1019,  0.2718, -1.0331,  0.2818]) (768 features in tensor)\n",
      "Run time for unwell was 8.170501814223826 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "unwilling\n",
      "Mean of 50 tensors is: tensor([-0.0666, -0.0650, -0.1316,  0.1380, -0.4542]) (768 features in tensor)\n",
      "Run time for unwilling was 6.259686640463769 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "un\n",
      "y\n",
      "ielding\n",
      "Mean of 50 tensors is: tensor([-0.0039,  0.3469, -0.0771, -0.0353, -0.2540]) (768 features in tensor)\n",
      "Run time for unyielding was 17.929423477500677 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "up\n",
      "Mean of 50 tensors is: tensor([ 0.1662, -0.1281, -0.0629, -0.5054,  0.1557]) (768 features in tensor)\n",
      "Run time for up was 3.4488072926178575 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "upbeat\n",
      "Mean of 50 tensors is: tensor([ 0.2794,  1.0258,  0.0713, -0.2371, -0.2472]) (768 features in tensor)\n",
      "Run time for upbeat was 6.791927917860448 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "upl\n",
      "ifting\n",
      "Mean of 50 tensors is: tensor([0.2785, 0.8386, 0.1211, 0.0180, 0.2656]) (768 features in tensor)\n",
      "Run time for uplifting was 5.4210929069668055 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "u\n",
      "pp\n",
      "ity\n",
      "Mean of 50 tensors is: tensor([ 0.2433,  0.2318, -0.1960, -0.0373, -0.3832]) (768 features in tensor)\n",
      "Run time for uppity was 35.867744179442525 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "upset\n",
      "Mean of 50 tensors is: tensor([ 0.1025,  0.6003,  0.1222,  0.4833, -0.6603]) (768 features in tensor)\n",
      "Run time for upset was 4.688799920491874 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "upt\n",
      "ight\n",
      "Mean of 50 tensors is: tensor([ 0.2681,  0.2131, -0.0451, -0.1127, -0.2275]) (768 features in tensor)\n",
      "Run time for uptight was 17.666711022146046 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "useless\n",
      "Mean of 50 tensors is: tensor([-0.0570,  0.3668,  0.0268, -0.4161, -0.4065]) (768 features in tensor)\n",
      "Run time for useless was 4.527107598260045 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vacant\n",
      "Mean of 50 tensors is: tensor([-0.0235, -0.8661,  0.2397, -0.6583, -0.3402]) (768 features in tensor)\n",
      "Run time for vacant was 5.917592872865498 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "vac\n",
      "uous\n",
      "Mean of 50 tensors is: tensor([ 0.0411,  0.1433, -0.2617, -0.5797, -0.1921]) (768 features in tensor)\n",
      "Run time for vacuous was 25.998367480002344 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "valley\n",
      "Mean of 50 tensors is: tensor([ 0.0810,  0.5401, -0.0156,  0.0785,  0.4306]) (768 features in tensor)\n",
      "Run time for valley was 7.4411152973771095 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "value\n",
      "Mean of 50 tensors is: tensor([ 0.1006,  0.3338,  0.0256,  0.2688, -0.7585]) (768 features in tensor)\n",
      "Run time for value was 3.9587053302675486 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vanish\n",
      "Mean of 50 tensors is: tensor([ 0.0026, -0.0965,  0.0810, -0.1926, -0.0568]) (768 features in tensor)\n",
      "Run time for vanish was 11.665790821425617 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vanquished\n",
      "Mean of 50 tensors is: tensor([ 0.1324, -0.0907,  0.0958, -0.0678, -0.0474]) (768 features in tensor)\n",
      "Run time for vanquished was 27.717145213857293 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vehement\n",
      "Mean of 50 tensors is: tensor([ 0.0853,  0.2604, -0.0642,  0.3576, -0.0825]) (768 features in tensor)\n",
      "Run time for vehement was 17.975291814655066 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vehicle\n",
      "Mean of 50 tensors is: tensor([-0.2706,  0.5946,  0.1104, -0.5012, -0.5152]) (768 features in tensor)\n",
      "Run time for vehicle was 4.211616494692862 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vein\n",
      "Mean of 50 tensors is: tensor([ 0.1698, -0.0480, -0.0075, -0.5617,  0.3236]) (768 features in tensor)\n",
      "Run time for vein was 7.463824863545597 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "v\n",
      "enge\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.1104,  0.0809, -0.0366,  0.8780,  0.3812]) (768 features in tensor)\n",
      "Run time for vengeful was 14.547648180276155 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "venom\n",
      "ous\n",
      "Mean of 50 tensors is: tensor([ 0.1451,  0.3568, -0.2882,  0.2126,  0.5357]) (768 features in tensor)\n",
      "Run time for venomous was 13.452268990688026 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "verdict\n",
      "Mean of 50 tensors is: tensor([-0.0571,  0.5750, -0.2132,  0.7616, -0.3681]) (768 features in tensor)\n",
      "Run time for verdict was 5.875080469995737 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "verify\n",
      "Mean of 50 tensors is: tensor([ 0.3340, -0.1280,  0.2341,  0.4512, -0.6377]) (768 features in tensor)\n",
      "Run time for verify was 4.174017689190805 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vessel\n",
      "Mean of 50 tensors is: tensor([ 0.0592,  0.3632,  0.1759, -0.6350, -0.5931]) (768 features in tensor)\n",
      "Run time for vessel was 6.503148389980197 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vex\n",
      "Mean of 50 tensors is: tensor([ 0.1828,  0.1144, -0.0209, -0.2678, -0.1221]) (768 features in tensor)\n",
      "Run time for vex was 38.17193647753447 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "vex\n",
      "ation\n",
      "Mean of 50 tensors is: tensor([ 0.2133,  0.7110, -0.0650,  0.1453, -0.3758]) (768 features in tensor)\n",
      "Run time for vexation was 66.84655108116567 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "vex\n",
      "ed\n",
      "Mean of 50 tensors is: tensor([ 0.1757,  0.7598, -0.0541,  0.0467, -0.1374]) (768 features in tensor)\n",
      "Run time for vexed was 21.836537459865212 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vicious\n",
      "Mean of 50 tensors is: tensor([ 0.1030,  0.6347, -0.3585,  0.2079,  0.2470]) (768 features in tensor)\n",
      "Run time for vicious was 5.771602314896882 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "vict\n",
      "or\n",
      "Mean of 50 tensors is: tensor([ 0.0375,  0.3021,  0.1150,  0.0951, -0.2745]) (768 features in tensor)\n",
      "Run time for victor was 35.57651159539819 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "victorious\n",
      "Mean of 50 tensors is: tensor([ 0.2514,  0.4481,  0.0501, -0.0071, -0.5264]) (768 features in tensor)\n",
      "Run time for victorious was 6.441802678629756 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "victory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.2368,  0.4744,  0.0737, -0.1311, -1.0963]) (768 features in tensor)\n",
      "Run time for victory was 8.255805813707411 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vigilant\n",
      "Mean of 50 tensors is: tensor([0.0120, 0.0240, 0.1904, 0.4588, 0.1117]) (768 features in tensor)\n",
      "Run time for vigilant was 8.708358452655375 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vile\n",
      "Mean of 50 tensors is: tensor([ 0.1226, -0.0963, -0.3048, -0.1104,  0.3570]) (768 features in tensor)\n",
      "Run time for vile was 7.957913567312062 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "villain\n",
      "ous\n",
      "Mean of 50 tensors is: tensor([ 0.0360,  0.3529, -0.0610,  0.5068,  0.5855]) (768 features in tensor)\n",
      "Run time for villainous was 15.106521492823958 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "vind\n",
      "ictive\n",
      "Mean of 50 tensors is: tensor([ 0.2095,  0.4280,  0.0255,  0.5463, -0.1792]) (768 features in tensor)\n",
      "Run time for vindictive was 16.90362366102636 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vinegar\n",
      "Mean of 50 tensors is: tensor([ 0.1061, -0.3357,  0.3287,  0.4201,  0.5890]) (768 features in tensor)\n",
      "Run time for vinegar was 7.245410471223295 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "violation\n",
      "Mean of 50 tensors is: tensor([-0.0286,  0.1269, -0.0835, -0.1518, -0.9239]) (768 features in tensor)\n",
      "Run time for violation was 6.596258212812245 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "violence\n",
      "Mean of 50 tensors is: tensor([ 0.0862,  0.1188,  0.0795,  0.1589, -0.3146]) (768 features in tensor)\n",
      "Run time for violence was 4.542069660499692 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "violent\n",
      "Mean of 50 tensors is: tensor([0.1496, 0.1318, 0.0007, 0.5642, 0.0918]) (768 features in tensor)\n",
      "Run time for violent was 5.216151000000536 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "violet\n",
      "Mean of 50 tensors is: tensor([ 0.2998, -0.5525,  0.2422,  1.0991, -0.0232]) (768 features in tensor)\n",
      "Run time for violet was 12.603793647140265 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "violin\n",
      "Mean of 50 tensors is: tensor([ 0.0316, -0.1293, -0.2531,  1.3264, -0.4111]) (768 features in tensor)\n",
      "Run time for violin was 11.302815753035247 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "v\n",
      "iper\n",
      "ous\n",
      "Mean of 6 tensors is: tensor([ 0.1296,  0.2903, -0.0146,  0.0930,  0.2506]) (768 features in tensor)\n",
      "Run time for viperous was 108.67869450710714 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vision\n",
      "Mean of 50 tensors is: tensor([ 0.0363, -0.0565,  0.1584,  0.4098,  0.2871]) (768 features in tensor)\n",
      "Run time for vision was 4.492263640277088 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vitamin\n",
      "Mean of 50 tensors is: tensor([-0.0729,  0.2187,  0.0781,  0.2171,  0.5700]) (768 features in tensor)\n",
      "Run time for vitamin was 6.151639129035175 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "vit\n",
      "uper\n",
      "ative\n",
      "Mean of 21 tensors is: tensor([ 0.1604,  0.6692, -0.0946,  0.4496, -0.3485]) (768 features in tensor)\n",
      "Run time for vituperative was 104.25660465005785 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vocal\n",
      "Mean of 50 tensors is: tensor([ 0.2111,  0.1531, -0.1125,  0.1374,  0.6087]) (768 features in tensor)\n",
      "Run time for vocal was 5.036318684928119 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "vocal\n",
      "ized\n",
      "Mean of 50 tensors is: tensor([ 0.2037,  0.2240, -0.0251,  0.1705, -0.0297]) (768 features in tensor)\n",
      "Run time for vocalized was 86.37826194986701 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "voyage\n",
      "Mean of 50 tensors is: tensor([0.2321, 0.4283, 0.0869, 0.2185, 0.2318]) (768 features in tensor)\n",
      "Run time for voyage was 7.290214536711574 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vulgar\n",
      "Mean of 50 tensors is: tensor([ 0.0976, -0.1732, -0.1401, -0.1482, -0.6600]) (768 features in tensor)\n",
      "Run time for vulgar was 8.973958866670728 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vulnerability\n",
      "Mean of 50 tensors is: tensor([ 0.1262,  0.2675,  0.1592,  0.1016, -0.1847]) (768 features in tensor)\n",
      "Run time for vulnerability was 5.688702708110213 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "vulnerable\n",
      "Mean of 50 tensors is: tensor([ 0.1329,  0.0841,  0.1228,  0.3860, -0.7040]) (768 features in tensor)\n",
      "Run time for vulnerable was 4.296861975453794 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "w\n",
      "acky\n",
      "Mean of 50 tensors is: tensor([ 0.1405,  0.7856, -0.2003,  0.7343,  0.6258]) (768 features in tensor)\n",
      "Run time for wacky was 7.5951932249590755 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wagon\n",
      "Mean of 50 tensors is: tensor([-0.0122, -0.0788,  0.1394, -0.3697,  0.4877]) (768 features in tensor)\n",
      "Run time for wagon was 84.51576146949083 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "waist\n",
      "Mean of 50 tensors is: tensor([ 0.2888, -0.0743, -0.0763,  0.8490, -0.3261]) (768 features in tensor)\n",
      "Run time for waist was 5.741712964139879 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "waiting\n",
      "Mean of 50 tensors is: tensor([ 0.0772, -0.3131,  0.2663,  0.3833, -0.7612]) (768 features in tensor)\n",
      "Run time for waiting was 3.840808958746493 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wall\n",
      "Mean of 50 tensors is: tensor([ 0.4990,  0.1364, -0.2337, -0.7723, -0.3213]) (768 features in tensor)\n",
      "Run time for wall was 4.097379929386079 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wander\n",
      "Mean of 50 tensors is: tensor([ 0.1782, -0.4142, -0.2267, -0.0643, -0.4598]) (768 features in tensor)\n",
      "Run time for wander was 5.878817644901574 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "want\n",
      "Mean of 50 tensors is: tensor([-0.0484,  0.3661, -0.1654,  0.4697, -0.2995]) (768 features in tensor)\n",
      "Run time for want was 3.7285522976890206 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wanted\n",
      "Mean of 50 tensors is: tensor([-0.0526,  0.3444, -0.1544,  0.6164,  0.1096]) (768 features in tensor)\n",
      "Run time for wanted was 3.5135752838104963 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wanting\n",
      "Mean of 50 tensors is: tensor([ 0.0773,  0.2632, -0.0707,  0.4535,  0.6749]) (768 features in tensor)\n",
      "Run time for wanting was 4.159856181591749 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "want\n",
      "on\n",
      "Mean of 50 tensors is: tensor([ 0.1892,  0.1845, -0.1985, -0.0609,  0.3985]) (768 features in tensor)\n",
      "Run time for wanton was 14.242998104542494 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "war\n",
      "iness\n",
      "Mean of 50 tensors is: tensor([0.0478, 0.2537, 0.0698, 0.1208, 0.2597]) (768 features in tensor)\n",
      "Run time for wariness was 68.52045404259115 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "warm\n",
      "Mean of 50 tensors is: tensor([ 0.2800, -0.1434,  0.2309, -0.9457,  0.4850]) (768 features in tensor)\n",
      "Run time for warm was 3.5697880582883954 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "warrior\n",
      "Mean of 50 tensors is: tensor([ 0.0008, -0.0634,  0.1315, -0.4765,  0.1280]) (768 features in tensor)\n",
      "Run time for warrior was 9.538647681474686 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wary\n",
      "Mean of 50 tensors is: tensor([0.1409, 0.1340, 0.2187, 0.3368, 0.3408]) (768 features in tensor)\n",
      "Run time for wary was 6.606029476970434 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wasted\n",
      "Mean of 50 tensors is: tensor([ 0.1140,  0.4878,  0.2158, -0.4308,  0.0397]) (768 features in tensor)\n",
      "Run time for wasted was 4.600632170215249 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "watch\n",
      "Mean of 50 tensors is: tensor([ 0.2066, -0.4888,  0.1563, -0.2811, -0.2900]) (768 features in tensor)\n",
      "Run time for watch was 7.03818735294044 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "watch\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.0271,  0.0787,  0.1360,  0.2469, -0.0423]) (768 features in tensor)\n",
      "Run time for watchful was 8.34588404931128 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "watching\n",
      "Mean of 50 tensors is: tensor([ 0.0954, -0.5301,  0.0126,  0.0489,  0.4793]) (768 features in tensor)\n",
      "Run time for watching was 4.052400072105229 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "water\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.3081, -0.3787,  0.2929, -1.0552, -0.2114]) (768 features in tensor)\n",
      "Run time for water was 3.9721388472244143 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "wa\n",
      "ver\n",
      "ing\n",
      "Mean of 50 tensors is: tensor([ 0.0835,  0.3740, -0.4752,  0.0160, -0.1899]) (768 features in tensor)\n",
      "Run time for wavering was 28.041507499292493 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "way\n",
      "Mean of 50 tensors is: tensor([ 0.3791,  0.7630, -0.2124,  0.1575, -0.4916]) (768 features in tensor)\n",
      "Run time for way was 3.9884781166911125 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wealth\n",
      "Mean of 50 tensors is: tensor([ 0.1325,  0.2591,  0.2326, -0.8832,  0.0700]) (768 features in tensor)\n",
      "Run time for wealth was 4.4340178379788995 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "wear\n",
      "iness\n",
      "Mean of 50 tensors is: tensor([ 0.0666,  0.2888,  0.2462, -0.0723,  0.1059]) (768 features in tensor)\n",
      "Run time for weariness was 31.870356270112097 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "weary\n",
      "Mean of 50 tensors is: tensor([-0.0132,  0.1135,  0.1475, -0.5593,  0.5989]) (768 features in tensor)\n",
      "Run time for weary was 6.84427984803915 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "weather\n",
      "Mean of 50 tensors is: tensor([ 0.3472,  0.3038,  0.0256, -0.2657,  0.5314]) (768 features in tensor)\n",
      "Run time for weather was 4.5704950680956244 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "week\n",
      "Mean of 50 tensors is: tensor([ 0.0867,  0.5496,  0.4201, -0.4927, -0.7904]) (768 features in tensor)\n",
      "Run time for week was 3.9617217052727938 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "weekend\n",
      "Mean of 50 tensors is: tensor([ 0.0290,  0.8504,  0.2383, -0.0872, -0.0201]) (768 features in tensor)\n",
      "Run time for weekend was 6.360776759684086 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "weeping\n",
      "Mean of 50 tensors is: tensor([ 0.2218,  0.3767,  0.0730, -0.2574,  1.0940]) (768 features in tensor)\n",
      "Run time for weeping was 7.41564816981554 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "weird\n",
      "Mean of 50 tensors is: tensor([ 0.1150,  0.0702, -0.1818,  0.4117,  0.6952]) (768 features in tensor)\n",
      "Run time for weird was 4.171784615144134 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "welcome\n",
      "Mean of 50 tensors is: tensor([ 0.0349,  0.3462,  0.0311,  0.4552, -0.3744]) (768 features in tensor)\n",
      "Run time for welcome was 3.646166453137994 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "welcoming\n",
      "Mean of 50 tensors is: tensor([0.1571, 0.3119, 0.2428, 0.3839, 0.3223]) (768 features in tensor)\n",
      "Run time for welcoming was 4.530462788417935 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "west\n",
      "Mean of 50 tensors is: tensor([ 0.1261, -0.4316,  0.2688,  0.4335, -0.0692]) (768 features in tensor)\n",
      "Run time for west was 6.524196717888117 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "whatever\n",
      "Mean of 50 tensors is: tensor([ 0.2466, -0.3082, -0.1704,  0.3661,  0.2243]) (768 features in tensor)\n",
      "Run time for whatever was 4.044807530008256 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wheat\n",
      "Mean of 50 tensors is: tensor([ 0.1676,  0.2264, -0.0799, -0.0730,  0.3975]) (768 features in tensor)\n",
      "Run time for wheat was 8.425260403193533 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "whim\n",
      "pering\n",
      "Mean of 50 tensors is: tensor([ 0.1656,  0.4530, -0.4802,  0.4386,  0.4431]) (768 features in tensor)\n",
      "Run time for whimpering was 47.136179958470166 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "whims\n",
      "ical\n",
      "Mean of 50 tensors is: tensor([ 0.3236,  0.9805, -0.0151,  0.4246, -0.1633]) (768 features in tensor)\n",
      "Run time for whimsical was 6.595773806795478 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "whiskey\n",
      "Mean of 50 tensors is: tensor([ 0.0281, -0.0747,  0.0467, -0.2130, -0.0360]) (768 features in tensor)\n",
      "Run time for whiskey was 22.560842027887702 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "whisper\n",
      "Mean of 50 tensors is: tensor([ 0.0828,  0.1433, -0.0872, -0.0969, -0.0713]) (768 features in tensor)\n",
      "Run time for whisper was 9.097458223812282 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "whistle\n",
      "Mean of 50 tensors is: tensor([ 0.0794,  0.2852, -0.1317, -0.0482,  0.2824]) (768 features in tensor)\n",
      "Run time for whistle was 7.1117440070956945 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wicked\n",
      "Mean of 50 tensors is: tensor([ 0.1154,  0.5326, -0.1602, -0.4299,  0.4175]) (768 features in tensor)\n",
      "Run time for wicked was 4.838552189990878 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wide\n",
      "Mean of 50 tensors is: tensor([ 0.0422,  0.5795, -0.3841,  0.5437,  0.2633]) (768 features in tensor)\n",
      "Run time for wide was 4.291366770863533 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wife\n",
      "Mean of 50 tensors is: tensor([ 0.1201,  0.3998,  0.2070,  0.6409, -0.3795]) (768 features in tensor)\n",
      "Run time for wife was 8.171296495944262 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wild\n",
      "Mean of 50 tensors is: tensor([ 0.2847,  0.3166,  0.0737, -0.0555,  0.7620]) (768 features in tensor)\n",
      "Run time for wild was 5.060947929508984 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "willful\n",
      "Mean of 50 tensors is: tensor([ 0.0835,  0.3510,  0.0638, -0.3146, -0.2431]) (768 features in tensor)\n",
      "Run time for willful was 10.094429882243276 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "willing\n",
      "Mean of 50 tensors is: tensor([-0.1058, -0.0724,  0.2304, -0.0276, -0.4288]) (768 features in tensor)\n",
      "Run time for willing was 4.185458336956799 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "w\n",
      "ily\n",
      "Mean of 50 tensors is: tensor([ 0.0480,  0.7933, -0.2242, -0.3867,  0.6412]) (768 features in tensor)\n",
      "Run time for wily was 14.320504194125533 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "win\n",
      "Mean of 50 tensors is: tensor([ 0.1393, -0.2449, -0.1115, -0.2612, -1.2019]) (768 features in tensor)\n",
      "Run time for win was 4.281988955102861 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "window\n",
      "Mean of 50 tensors is: tensor([ 0.2251,  0.3755,  0.0898, -1.1603, -0.3137]) (768 features in tensor)\n",
      "Run time for window was 3.568246697075665 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wine\n",
      "Mean of 50 tensors is: tensor([ 0.0568, -0.0042, -0.1005, -0.4155, -0.4911]) (768 features in tensor)\n",
      "Run time for wine was 4.974408362060785 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wink\n",
      "Mean of 50 tensors is: tensor([ 0.1627, -0.5154, -0.0748, -0.8099, -0.1581]) (768 features in tensor)\n",
      "Run time for wink was 12.680143231526017 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "winner\n",
      "Mean of 50 tensors is: tensor([ 0.1662,  0.0373,  0.1960,  0.0659, -0.7208]) (768 features in tensor)\n",
      "Run time for winner was 4.779368837364018 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "winter\n",
      "Mean of 50 tensors is: tensor([0.3263, 0.4014, 0.0693, 0.1440, 0.8486]) (768 features in tensor)\n",
      "Run time for winter was 4.784844840876758 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wire\n",
      "Mean of 50 tensors is: tensor([ 0.1364,  0.4050, -0.0231, -0.7957,  0.0746]) (768 features in tensor)\n",
      "Run time for wire was 4.466336733661592 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wired\n",
      "Mean of 50 tensors is: tensor([ 0.2300,  0.3859,  0.0150, -0.6608,  0.7790]) (768 features in tensor)\n",
      "Run time for wired was 5.1785445576533675 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wisdom\n",
      "Mean of 50 tensors is: tensor([ 0.3152,  0.1612,  0.1810,  0.1026, -0.4537]) (768 features in tensor)\n",
      "Run time for wisdom was 4.919994077645242 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "wish\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.1874, -0.1598, -0.0118,  0.4787,  0.6070]) (768 features in tensor)\n",
      "Run time for wishful was 7.838338423520327 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "w\n",
      "ist\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.1169,  0.6260, -0.0482,  0.0254,  0.2500]) (768 features in tensor)\n",
      "Run time for wistful was 14.724358434788883 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "w\n",
      "ist\n",
      "fully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.2103,  0.4982, -0.0742, -0.0582,  0.0650]) (768 features in tensor)\n",
      "Run time for wistfully was 31.265798576176167 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "withdraw\n",
      "Mean of 50 tensors is: tensor([ 0.1159,  0.3406,  0.0944,  0.2547, -0.6918]) (768 features in tensor)\n",
      "Run time for withdraw was 4.274333788082004 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "withdrawn\n",
      "Mean of 50 tensors is: tensor([ 0.0320,  0.1449,  0.1124,  0.2429, -0.1145]) (768 features in tensor)\n",
      "Run time for withdrawn was 5.612368171103299 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "withheld\n",
      "Mean of 50 tensors is: tensor([ 0.1312,  0.0270, -0.2623, -0.2566, -0.2409]) (768 features in tensor)\n",
      "Run time for withheld was 7.029346066527069 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "withholding\n",
      "Mean of 50 tensors is: tensor([ 0.1257,  0.0233, -0.2398, -0.2796,  0.0730]) (768 features in tensor)\n",
      "Run time for withholding was 7.087644553743303 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wizard\n",
      "Mean of 50 tensors is: tensor([ 0.0587,  0.2257,  0.1232, -0.0097, -0.3677]) (768 features in tensor)\n",
      "Run time for wizard was 9.531531960703433 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "w\n",
      "oe\n",
      "Mean of 50 tensors is: tensor([ 0.1434,  0.3736,  0.0325, -0.2669,  0.6713]) (768 features in tensor)\n",
      "Run time for woe was 20.11568082217127 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "wo\n",
      "eful\n",
      "Mean of 50 tensors is: tensor([ 0.0287,  0.7424, -0.2275, -0.7764,  0.1932]) (768 features in tensor)\n",
      "Run time for woeful was 30.45940938498825 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "woman\n",
      "Mean of 50 tensors is: tensor([ 0.1005,  0.2567,  0.0950, -0.3247, -0.2370]) (768 features in tensor)\n",
      "Run time for woman was 6.378332301974297 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wonder\n",
      "Mean of 50 tensors is: tensor([ 2.1466e-01,  2.1152e-01, -3.2057e-04,  8.7492e-01,  7.3688e-01]) (768 features in tensor)\n",
      "Run time for wonder was 3.646055738441646 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wonderful\n",
      "Mean of 50 tensors is: tensor([ 0.0900,  1.1529, -0.1470,  0.0029,  0.5666]) (768 features in tensor)\n",
      "Run time for wonderful was 3.506819559261203 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wondering\n",
      "Mean of 50 tensors is: tensor([ 0.1600, -0.0857,  0.2493,  1.2717,  0.7548]) (768 features in tensor)\n",
      "Run time for wondering was 4.375671541318297 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "wonder\n",
      "ment\n",
      "Mean of 50 tensors is: tensor([ 0.1607,  0.5358,  0.1082,  0.5453, -0.2420]) (768 features in tensor)\n",
      "Run time for wonderment was 36.77673429064453 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wood\n",
      "Mean of 50 tensors is: tensor([ 0.2935,  0.0714,  0.3122, -0.6496,  0.2668]) (768 features in tensor)\n",
      "Run time for wood was 4.39545604493469 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "woodland\n",
      "Mean of 50 tensors is: tensor([ 0.2380,  0.0425,  0.0532, -0.3042,  0.3863]) (768 features in tensor)\n",
      "Run time for woodland was 11.807914735749364 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "woo\n",
      "ly\n",
      "Mean of 50 tensors is: tensor([ 0.0683,  0.4186, -0.1613, -0.1881,  0.3733]) (768 features in tensor)\n",
      "Run time for wooly was 14.537936887703836 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "woo\n",
      "zy\n",
      "Mean of 50 tensors is: tensor([ 0.1809,  0.3142, -0.0293, -0.1091,  0.6203]) (768 features in tensor)\n",
      "Run time for woozy was 83.99961954168975 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "word\n",
      "Mean of 50 tensors is: tensor([ 0.2121, -0.0073,  0.0435, -0.4776,  0.1324]) (768 features in tensor)\n",
      "Run time for word was 4.365884510800242 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "worker\n",
      "Mean of 50 tensors is: tensor([-0.0337,  0.2259,  0.0142, -0.8333, -0.3231]) (768 features in tensor)\n",
      "Run time for worker was 6.984149839729071 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "world\n",
      "Mean of 50 tensors is: tensor([-0.0826,  0.3865,  0.0539,  0.0692, -1.2799]) (768 features in tensor)\n",
      "Run time for world was 4.85661490727216 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "worn\n",
      "Mean of 50 tensors is: tensor([ 0.1228,  0.0016, -0.0630,  0.0156, -0.0978]) (768 features in tensor)\n",
      "Run time for worn was 4.2055682353675365 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "worried\n",
      "Mean of 50 tensors is: tensor([ 0.0633,  0.5899,  0.2935,  0.6739, -0.1764]) (768 features in tensor)\n",
      "Run time for worried was 4.173174005933106 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "worrisome\n",
      "Mean of 50 tensors is: tensor([0.0316, 0.5270, 0.0483, 0.4184, 0.7917]) (768 features in tensor)\n",
      "Run time for worrisome was 18.371328021399677 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "worry\n",
      "Mean of 50 tensors is: tensor([-0.0505,  0.5162,  0.0714,  0.5056, -0.7585]) (768 features in tensor)\n",
      "Run time for worry was 4.240548142232001 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "worrying\n",
      "Mean of 50 tensors is: tensor([ 0.0392,  0.4199,  0.1519,  0.5844, -0.0397]) (768 features in tensor)\n",
      "Run time for worrying was 5.255443323403597 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "worry\n",
      "ingly\n",
      "Mean of 50 tensors is: tensor([ 0.1544,  0.6977, -0.1429,  0.1772,  0.2074]) (768 features in tensor)\n",
      "Run time for worryingly was 42.76868692506105 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wounded\n",
      "Mean of 50 tensors is: tensor([ 0.1725,  0.0328,  0.0135, -0.2802,  0.0047]) (768 features in tensor)\n",
      "Run time for wounded was 5.92486353777349 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wow\n",
      "Mean of 50 tensors is: tensor([ 0.1520, -0.1963,  0.2198, -0.1385,  0.7877]) (768 features in tensor)\n",
      "Run time for wow was 4.7094465950503945 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "wrath\n",
      "ful\n",
      "Mean of 50 tensors is: tensor([ 0.3035,  0.3360, -0.0931,  0.2278,  0.1976]) (768 features in tensor)\n",
      "Run time for wrathful was 44.19352541305125 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "wrath\n",
      "fully\n",
      "Mean of 3 tensors is: tensor([ 0.3612,  0.7386, -0.1722, -0.2391,  0.6280]) (768 features in tensor)\n",
      "Run time for wrathfully was 105.3070120587945 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wrecked\n",
      "Mean of 50 tensors is: tensor([0.1659, 0.1835, 0.0104, 0.1957, 0.4291]) (768 features in tensor)\n",
      "Run time for wrecked was 8.510856671258807 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wretched\n",
      "Mean of 50 tensors is: tensor([ 0.2270,  0.3797, -0.2833, -0.4561,  1.2746]) (768 features in tensor)\n",
      "Run time for wretched was 11.730421329848468 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "wrist\n",
      "Mean of 50 tensors is: tensor([ 0.3257,  0.3786, -0.0447,  0.5306,  0.0156]) (768 features in tensor)\n",
      "Run time for wrist was 6.2121867425739765 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "writer\n",
      "Mean of 50 tensors is: tensor([ 0.1855, -0.1250,  0.1851, -0.9493, -0.6113]) (768 features in tensor)\n",
      "Run time for writer was 4.306844766251743 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "wrong\n",
      "ed\n",
      "Mean of 50 tensors is: tensor([ 0.1698,  0.3791,  0.1351, -0.5517, -0.4232]) (768 features in tensor)\n",
      "Run time for wronged was 13.52599791996181 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "w\n",
      "roth\n",
      "Mean of 50 tensors is: tensor([ 0.1180,  0.3513,  0.1838, -0.1158,  0.0068]) (768 features in tensor)\n",
      "Run time for wroth was 61.436968790367246 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "w\n",
      "ry\n",
      "Mean of 50 tensors is: tensor([ 0.2333,  0.4002, -0.2324,  0.0956,  0.5851]) (768 features in tensor)\n",
      "Run time for wry was 13.323693501763046 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "yard\n",
      "Mean of 50 tensors is: tensor([ 0.3871,  0.6864, -0.1226, -0.9918,  0.9385]) (768 features in tensor)\n",
      "Run time for yard was 4.739408787339926 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "yawn\n",
      "Mean of 50 tensors is: tensor([ 0.1870, -0.4101,  0.0616, -1.3317,  0.4077]) (768 features in tensor)\n",
      "Run time for yawn was 21.615207515656948 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "yawn\n",
      "ing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 50 tensors is: tensor([ 0.1381, -0.0495, -0.2294, -0.7276,  0.3529]) (768 features in tensor)\n",
      "Run time for yawning was 15.613852648064494 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "year\n",
      "Mean of 50 tensors is: tensor([-0.0807,  0.4071,  0.0468, -0.2169, -0.4440]) (768 features in tensor)\n",
      "Run time for year was 3.5877525759860873 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "year\n",
      "ning\n",
      "Mean of 50 tensors is: tensor([ 0.1150, -0.1523,  0.0150,  0.2728,  0.1582]) (768 features in tensor)\n",
      "Run time for yearning was 7.917504573240876 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "yell\n",
      "Mean of 50 tensors is: tensor([ 0.2217, -0.1585, -0.2781,  0.0770, -0.4935]) (768 features in tensor)\n",
      "Run time for yell was 7.113133622333407 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "yelling\n",
      "Mean of 50 tensors is: tensor([ 0.0806, -0.3010, -0.4002,  0.4312, -0.0537]) (768 features in tensor)\n",
      "Run time for yelling was 6.7533686356619 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "yielding\n",
      "Mean of 50 tensors is: tensor([ 0.1633,  0.2559,  0.0371,  0.0365, -0.1399]) (768 features in tensor)\n",
      "Run time for yielding was 8.120380541309714 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "young\n",
      "Mean of 50 tensors is: tensor([ 0.0585, -0.2392, -0.1907, -0.4482,  0.4509]) (768 features in tensor)\n",
      "Run time for young was 6.231357118114829 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "y\n",
      "uck\n",
      "Mean of 50 tensors is: tensor([ 0.1817,  0.1180, -0.1073, -0.6321,  0.7490]) (768 features in tensor)\n",
      "Run time for yuck was 40.03733684681356 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "z\n",
      "any\n",
      "Mean of 50 tensors is: tensor([ 0.1252,  0.7297, -0.3363,  0.9335,  0.6003]) (768 features in tensor)\n",
      "Run time for zany was 17.329384959302843 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "z\n",
      "ealous\n",
      "Mean of 50 tensors is: tensor([ 0.0034,  0.3432, -0.0314,  0.2542,  0.1550]) (768 features in tensor)\n",
      "Run time for zealous was 14.01322035305202 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "z\n",
      "en\n",
      "Mean of 50 tensors is: tensor([ 0.1450, -0.1763,  0.1131, -0.0173, -0.3785]) (768 features in tensor)\n",
      "Run time for zen was 8.264354915358126 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "zone\n",
      "Mean of 50 tensors is: tensor([-0.0143,  0.1389,  0.1434,  0.4698,  0.0987]) (768 features in tensor)\n",
      "Run time for zone was 6.246402603574097 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "z\n",
      "oned\n",
      "Mean of 50 tensors is: tensor([ 0.0676,  0.0022, -0.0568, -0.0647,  0.4249]) (768 features in tensor)\n",
      "Run time for zoned was 10.89839977119118 seconds.\n"
     ]
    }
   ],
   "source": [
    "l_output_file = ''\n",
    "l_count_file = ''\n",
    "l_output_file = os.path.join(output_file, 'roberta_base_layer-' + str(LAYER) + '_maxlines-' + str(MAX_LINES) + '_aefg.txt')\n",
    "l_count_file = os.path.join(count_file, 'roberta_base_layer-'+ str(LAYER) + '_maxlines-' + str(MAX_LINES) + '_aefg_counts.txt')\n",
    "print(l_output_file, l_count_file)\n",
    "# Process vocabulary words in the outer loop.\n",
    "for v in vocab:\n",
    "    start = timer()\n",
    "    with open(context_file, 'r') as lines:\n",
    "        v_sum = torch.zeros([1, FEATURE_COUNT])\n",
    "        v_tokens = tokenizer.encode(v)\n",
    "        print(f'\\nThere are {len(v_tokens) - 2} tokens in tokenized vocabulary word:')\n",
    "        for t in v_tokens[1:-1]:\n",
    "            print(tokenizer.decode(t).strip())\n",
    "        count_sentence = 0\n",
    "        count_tensor = 0\n",
    "\n",
    "        # Process all lines in the context file in the inner loop.\n",
    "        for line in lines:\n",
    "            # Check for this vocab word in this line; if found, split the line into individual sentences.\n",
    "            if v in line.lower().split():\n",
    "                for sentence in line.split('.'):\n",
    "                    if v in sentence.lower():\n",
    "                        line = sentence\n",
    "                        count_sentence += 1\n",
    "                        break\n",
    "                # Split the new sentence-based line into tokens.\n",
    "                # Use max_length to avoid overflowing the maximum sequence length for the model.\n",
    "                tokenized_text = tokenizer.encode(line, add_special_tokens=True, max_length=512)\n",
    "                indices = []              \n",
    "\n",
    "                # Check to see whether the vocab word is found in this particular line.\n",
    "                # Initially, some lines may have comprised multiple sentences, which were\n",
    "                # broken out individually above.\n",
    "                for t in v_tokens[1:-1]:\n",
    "                    for i, token_str in enumerate(tokenized_text):\n",
    "                        if tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip():\n",
    "                            indices.append(i)               \n",
    "\n",
    "                ###################################################################################\n",
    "                # If the vocabulary word was found, process the containing line.\n",
    "                if indices:\n",
    "\n",
    "                    # The vocab word was found in this line/sentence, at the locations in indices.\n",
    "                    # Get the feature vectors for all tokens in the line/sentence.\n",
    "                    token_embeddings = create_token_embeddings(tokenized_text)\n",
    "                    token_vecs_layer = get_layer_token_vecs(token_embeddings, LAYER)\n",
    "\n",
    "                    # Get the vocab word's contextual embedding for this line.\n",
    "                    tensor_layer = torch.zeros([1, FEATURE_COUNT])\n",
    "                    for i in range(len(indices)):\n",
    "                        v_index = i % len(v_tokens[1:-1])\n",
    "                        tensor_layer += token_vecs_layer[indices[i]]\n",
    "\n",
    "                    # If our vocab word is broken into more than one token, we need to get the mean of the token embeddings.\n",
    "                    tensor_layer /= len(indices)\n",
    "\n",
    "                    # Add the embedding distilled from this line to the sum of embeddings for all lines.\n",
    "                    v_sum += tensor_layer\n",
    "                    count_tensor += 1\n",
    "                ###################################################################################\n",
    "            # Stop processing lines once we've found 2000 instances of our vocab word.\n",
    "            if count_tensor >= MAX_LINES:\n",
    "                break\n",
    "\n",
    "        # We're done processing all lines of 512 tokens or less containing our vocab word.\n",
    "        # Get the mean embedding for the word.\n",
    "        v_mean = v_sum / count_tensor\n",
    "        print(f'Mean of {count_tensor} tensors is: {v_mean[0][:5]} ({len(v_mean[0])} features in tensor)')\n",
    "        write_embedding(l_output_file, v, v_mean)\n",
    "        try:\n",
    "            with open(l_count_file, 'a') as counts:\n",
    "                counts.write(v + ', ' + str(count_tensor) + '\\n')\n",
    "        except:\n",
    "            print('Wha?! Could not write the sentence count.')\n",
    "    end = timer()\n",
    "    print(f'Run time for {v} was {end - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    vocab = []\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_token_embeddings(tokenized_text):\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)  # Batch size 1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "        encoded_layers = outputs[2]\n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "#         print(f'Size of token embeddings is {token_embeddings.size()}')\n",
    "        return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the last 4 layers' features\n",
    "def sum_last_four_token_vecs(token_embeddings):\n",
    "    token_vecs_sum_last_four = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_last_four.append(sum_vec)\n",
    "\n",
    "#     print ('Shape of summed layers is: %d x %d' % (len(token_vecs_sum_last_four), len(token_vecs_sum_last_four[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_sum_last_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a single layer of the model.\n",
    "def get_layer_token_vecs(token_embeddings, layer_number):\n",
    "    token_vecs_layer = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        layer_vec = token[layer_number]\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_layer.append(layer_vec)\n",
    "\n",
    "#     print ('Shape of summed layers is: %d x %d' % (len(token_vecs_layer), len(token_vecs_layer[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "#         print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal-venv-3.6",
   "language": "python",
   "name": "crystal-venv-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
