{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, RobertaConfig\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/Notebooks/crystal/NLP/transformers/examples'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we're in the transformers directory with fine-tuned model output.\n",
    "os.chdir('/home/jupyter/Notebooks/crystal/NLP/transformers/examples/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# and Transformers documentation: https://huggingface.co/transformers/model_doc/roberta.html#robertaformaskedlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('./output_CC-aefg/')\n",
    "config = RobertaConfig.from_pretrained('./output_CC-aefg/')\n",
    "model = RobertaForMaskedLM.from_pretrained('./output_CC-aefg/', config=config)\n",
    "model.eval()\n",
    "\n",
    "context_file = \"/home/jupyter/Notebooks/crystal/NLP/transformers/examples/CC_WET_test_ab\"\n",
    "output_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/'\n",
    "count_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/'\n",
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/data/vocab_files/FE_vocab_study.txt'\n",
    "vocab = make_vocab(vocab_file)\n",
    "\n",
    "FEATURE_COUNT = 768\n",
    "LAYER = 8\n",
    "MAX_LINES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_layer8_CC-aefg_FEvocab.txt /home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_layer8_CC-aefg_counts.txt\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "st\n",
      "upe\n",
      "f\n",
      "ied\n",
      "Mean of 10 tensors is: tensor([ 0.0427,  0.5158, -0.0076, -0.1682, -0.6346]) (768 features in tensor)\n",
      "Run time for stupefied was 18.170346095925197 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "scorn\n",
      "ful\n",
      "Mean of 18 tensors is: tensor([ 0.2055,  0.2329,  0.0556, -0.0198, -0.1499]) (768 features in tensor)\n",
      "Run time for scornful was 19.136984557146206 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disbel\n",
      "ieving\n",
      "Mean of 26 tensors is: tensor([ 0.1772,  0.5028, -0.0956,  0.5229,  0.2292]) (768 features in tensor)\n",
      "Run time for disbelieving was 19.583309751935303 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disdain\n",
      "ful\n",
      "Mean of 20 tensors is: tensor([ 0.1507,  0.1293,  0.0956, -0.0646,  0.0414]) (768 features in tensor)\n",
      "Run time for disdainful was 19.538508038967848 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "revol\n",
      "ted\n",
      "Mean of 26 tensors is: tensor([ 0.1630,  0.3805,  0.1211,  0.9752, -0.3244]) (768 features in tensor)\n",
      "Run time for revolted was 20.055183418095112 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "m\n",
      "iff\n",
      "ed\n",
      "Mean of 50 tensors is: tensor([0.0168, 0.7489, 0.1398, 0.4718, 0.1107]) (768 features in tensor)\n",
      "Run time for miffed was 22.783655858132988 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "a\n",
      "gh\n",
      "ast\n",
      "Mean of 37 tensors is: tensor([ 0.1117,  0.1480, -0.0424,  0.2947, -0.0697]) (768 features in tensor)\n",
      "Run time for aghast was 20.85243239090778 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "inc\n",
      "ensed\n",
      "Mean of 56 tensors is: tensor([ 0.1752,  0.4669,  0.1911,  0.4523, -0.3358]) (768 features in tensor)\n",
      "Run time for incensed was 22.277820928022265 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "de\n",
      "jected\n",
      "Mean of 35 tensors is: tensor([ 0.2010,  0.4748,  0.0021, -0.0185,  0.0396]) (768 features in tensor)\n",
      "Run time for dejected was 20.318819168023765 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "rep\n",
      "uls\n",
      "ed\n",
      "Mean of 56 tensors is: tensor([ 0.1720,  0.5288,  0.0787,  0.2794, -0.3328]) (768 features in tensor)\n",
      "Run time for repulsed was 23.593957596924156 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "mourn\n",
      "ful\n",
      "Mean of 45 tensors is: tensor([0.2277, 0.8016, 0.0363, 0.1701, 0.8705]) (768 features in tensor)\n",
      "Run time for mournful was 21.06082748202607 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disple\n",
      "ased\n",
      "Mean of 55 tensors is: tensor([ 0.0899,  0.6045,  0.1431,  0.3391, -0.1207]) (768 features in tensor)\n",
      "Run time for displeased was 21.775446766987443 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "infuri\n",
      "ated\n",
      "Mean of 66 tensors is: tensor([ 0.2928,  0.6497,  0.1708,  0.4263, -0.0766]) (768 features in tensor)\n",
      "Run time for infuriated was 22.213207226945087 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aw\n",
      "ed\n",
      "Mean of 72 tensors is: tensor([ 0.1930,  0.6246, -0.0944, -0.4696, -0.2851]) (768 features in tensor)\n",
      "Run time for awed was 22.991184293990955 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "repe\n",
      "lled\n",
      "Mean of 68 tensors is: tensor([ 0.1125,  0.4820,  0.1008,  0.3350, -0.1715]) (768 features in tensor)\n",
      "Run time for repelled was 23.050234009046108 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "resent\n",
      "ful\n",
      "Mean of 74 tensors is: tensor([ 0.0312,  0.1935,  0.1297,  0.4407, -0.0904]) (768 features in tensor)\n",
      "Run time for resentful was 23.805870484095067 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sorrow\n",
      "ful\n",
      "Mean of 76 tensors is: tensor([0.2183, 0.5794, 0.1279, 0.4336, 1.0342]) (768 features in tensor)\n",
      "Run time for sorrowful was 23.63572409003973 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ir\n",
      "ate\n",
      "Mean of 73 tensors is: tensor([ 0.1717,  0.8390,  0.0394,  0.3983, -0.4288]) (768 features in tensor)\n",
      "Run time for irate was 23.257120992988348 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "dismay\n",
      "ed\n",
      "Mean of 95 tensors is: tensor([ 0.0705,  0.5528,  0.1855,  0.4333, -0.1392]) (768 features in tensor)\n",
      "Run time for dismayed was 25.502443260047585 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "el\n",
      "ated\n",
      "Mean of 100 tensors is: tensor([ 0.1805,  0.8596,  0.0687,  0.7083, -0.6579]) (768 features in tensor)\n",
      "Run time for elated was 21.544648116920143 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "enraged\n",
      "Mean of 100 tensors is: tensor([ 0.0643,  0.3276,  0.2042,  0.1692, -0.1543]) (768 features in tensor)\n",
      "Run time for enraged was 24.467868368839845 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "apprehens\n",
      "ive\n",
      "Mean of 100 tensors is: tensor([ 0.1682,  0.2780,  0.1768,  0.8680, -0.5432]) (768 features in tensor)\n",
      "Run time for apprehensive was 19.83481262391433 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bewild\n",
      "ered\n",
      "Mean of 92 tensors is: tensor([-0.0319,  0.4082,  0.0405,  0.3128, -0.0196]) (768 features in tensor)\n",
      "Run time for bewildered was 26.52904421603307 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ast\n",
      "ounded\n",
      "Mean of 100 tensors is: tensor([ 0.1516,  0.5911,  0.0191,  0.5165, -0.4314]) (768 features in tensor)\n",
      "Run time for astounded was 19.147715365048498 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "perplex\n",
      "ed\n",
      "Mean of 100 tensors is: tensor([ 0.1299,  0.5141,  0.1555,  0.2494, -0.2037]) (768 features in tensor)\n",
      "Run time for perplexed was 21.30855063488707 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "appalled\n",
      "Mean of 100 tensors is: tensor([ 0.0507,  0.3330,  0.2171,  0.0262, -0.0469]) (768 features in tensor)\n",
      "Run time for appalled was 19.187359331175685 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "startled\n",
      "Mean of 100 tensors is: tensor([0.1478, 0.4146, 0.2027, 0.5716, 0.4340]) (768 features in tensor)\n",
      "Run time for startled was 20.42087401309982 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "astonished\n",
      "Mean of 100 tensors is: tensor([ 0.1543,  0.5306,  0.0093,  0.2356, -0.3098]) (768 features in tensor)\n",
      "Run time for astonished was 18.148096351185814 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alarmed\n",
      "Mean of 100 tensors is: tensor([ 0.1361,  0.3169,  0.2209,  0.2721, -0.2671]) (768 features in tensor)\n",
      "Run time for alarmed was 18.36010585189797 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "outraged\n",
      "Mean of 100 tensors is: tensor([ 0.0400,  0.1722,  0.2028,  0.0050, -0.0731]) (768 features in tensor)\n",
      "Run time for outraged was 16.36064655194059 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "disgusted\n",
      "Mean of 100 tensors is: tensor([ 0.0792, -0.0772,  0.2872, -0.2075, -0.3011]) (768 features in tensor)\n",
      "Run time for disgusted was 17.56624782201834 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "doubtful\n",
      "Mean of 100 tensors is: tensor([ 0.0699,  0.1080, -0.0978,  0.1772,  0.3014]) (768 features in tensor)\n",
      "Run time for doubtful was 15.675318541936576 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "dissatisfied\n",
      "Mean of 100 tensors is: tensor([-0.1068,  0.4887,  0.2810,  0.2112, -0.2387]) (768 features in tensor)\n",
      "Run time for dissatisfied was 12.493777142139152 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "saddened\n",
      "Mean of 100 tensors is: tensor([0.1636, 0.5957, 0.3492, 0.7464, 1.0365]) (768 features in tensor)\n",
      "Run time for saddened was 12.241575163090602 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "irritated\n",
      "Mean of 100 tensors is: tensor([ 0.0857,  0.4729,  0.2393,  0.3485, -0.0842]) (768 features in tensor)\n",
      "Run time for irritated was 13.769114980939776 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amused\n",
      "Mean of 100 tensors is: tensor([-0.0328,  0.8471,  0.2647,  0.5257, -1.2479]) (768 features in tensor)\n",
      "Run time for amused was 17.7741477410309 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "frightened\n",
      "Mean of 100 tensors is: tensor([0.1783, 0.0813, 0.1911, 0.4598, 0.8012]) (768 features in tensor)\n",
      "Run time for frightened was 14.383920653956011 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "discouraged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 tensors is: tensor([-0.0282,  0.6097,  0.0321,  0.0294, -0.3207]) (768 features in tensor)\n",
      "Run time for discouraged was 12.449121936922893 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "stunned\n",
      "Mean of 100 tensors is: tensor([0.0918, 0.1982, 0.1114, 0.2133, 0.1442]) (768 features in tensor)\n",
      "Run time for stunned was 11.346866101026535 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "fearful\n",
      "Mean of 100 tensors is: tensor([ 0.0600, -0.3105,  0.2105,  0.6244,  0.8857]) (768 features in tensor)\n",
      "Run time for fearful was 13.595802521100268 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "pissed\n",
      "Mean of 100 tensors is: tensor([0.0221, 0.1849, 0.4038, 0.4604, 0.0885]) (768 features in tensor)\n",
      "Run time for pissed was 11.43933018296957 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "terrified\n",
      "Mean of 100 tensors is: tensor([ 0.2471, -0.3437,  0.2816,  0.4974,  0.7751]) (768 features in tensor)\n",
      "Run time for terrified was 12.23605596111156 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "offended\n",
      "Mean of 100 tensors is: tensor([-0.0649,  0.2680,  0.1716,  0.0683, -0.6544]) (768 features in tensor)\n",
      "Run time for offended was 11.92035422497429 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "annoyed\n",
      "Mean of 100 tensors is: tensor([-0.0277,  0.5951,  0.3279,  0.3092, -0.4566]) (768 features in tensor)\n",
      "Run time for annoyed was 10.137360173976049 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "skeptical\n",
      "Mean of 100 tensors is: tensor([ 0.0932,  0.4693,  0.2869, -0.1432,  0.3943]) (768 features in tensor)\n",
      "Run time for skeptical was 10.071664926828817 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "hostile\n",
      "Mean of 100 tensors is: tensor([ 0.0718,  0.2833,  0.0786, -0.2744,  0.8211]) (768 features in tensor)\n",
      "Run time for hostile was 13.66485725203529 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "furious\n",
      "Mean of 100 tensors is: tensor([ 0.0258,  0.5202, -0.1278,  0.6022,  0.0752]) (768 features in tensor)\n",
      "Run time for furious was 12.538103094091639 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bothered\n",
      "Mean of 100 tensors is: tensor([ 0.1442,  0.6313, -0.0278,  0.3084, -0.3434]) (768 features in tensor)\n",
      "Run time for bothered was 10.28029329306446 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "joyful\n",
      "Mean of 100 tensors is: tensor([0.0571, 0.9649, 0.2960, 0.3771, 0.4430]) (768 features in tensor)\n",
      "Run time for joyful was 10.863998379092664 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "uncertain\n",
      "Mean of 100 tensors is: tensor([ 0.0786,  0.1528, -0.0974,  0.1347,  0.2124]) (768 features in tensor)\n",
      "Run time for uncertain was 10.505684163887054 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "cheerful\n",
      "Mean of 100 tensors is: tensor([ 0.1106,  1.0334,  0.3601, -0.2718,  0.6173]) (768 features in tensor)\n",
      "Run time for cheerful was 13.496612626127899 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "depressed\n",
      "Mean of 100 tensors is: tensor([0.0881, 0.6653, 0.4428, 0.0914, 0.4852]) (768 features in tensor)\n",
      "Run time for depressed was 10.159772145096213 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anxious\n",
      "Mean of 100 tensors is: tensor([0.0525, 0.3882, 0.0979, 0.2286, 0.2045]) (768 features in tensor)\n",
      "Run time for anxious was 9.37408501887694 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "frustrated\n",
      "Mean of 100 tensors is: tensor([-0.0512,  0.5500,  0.2906,  0.6785,  0.0820]) (768 features in tensor)\n",
      "Run time for frustrated was 9.388908360153437 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "distressed\n",
      "Mean of 100 tensors is: tensor([0.2273, 0.5174, 0.3208, 0.1637, 0.5587]) (768 features in tensor)\n",
      "Run time for distressed was 9.293214712990448 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bored\n",
      "Mean of 100 tensors is: tensor([-0.2264,  0.0299,  0.4635, -0.0336,  0.5150]) (768 features in tensor)\n",
      "Run time for bored was 9.621086141094565 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "suspicious\n",
      "Mean of 100 tensors is: tensor([-0.0107,  0.4529,  0.2119, -0.0263,  0.2895]) (768 features in tensor)\n",
      "Run time for suspicious was 10.245097037870437 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "shocked\n",
      "Mean of 100 tensors is: tensor([0.0957, 0.3877, 0.1312, 0.6167, 0.1024]) (768 features in tensor)\n",
      "Run time for shocked was 9.10345937195234 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amazed\n",
      "Mean of 100 tensors is: tensor([ 0.1960,  0.6633, -0.1657,  0.3978, -0.3771]) (768 features in tensor)\n",
      "Run time for amazed was 8.60929629392922 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "rejected\n",
      "Mean of 100 tensors is: tensor([-0.0219, -0.0467,  0.0630,  0.2821,  0.5812]) (768 features in tensor)\n",
      "Run time for rejected was 9.004411524161696 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "disappointed\n",
      "Mean of 100 tensors is: tensor([-0.0950,  0.2551,  0.1519,  0.9554,  0.0024]) (768 features in tensor)\n",
      "Run time for disappointed was 9.368248428916559 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "upset\n",
      "Mean of 100 tensors is: tensor([ 0.0070,  0.7346,  0.1495,  0.4211, -0.6577]) (768 features in tensor)\n",
      "Run time for upset was 9.632547436049208 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "delighted\n",
      "Mean of 100 tensors is: tensor([ 0.0222,  0.5193, -0.1295,  0.9253,  0.1915]) (768 features in tensor)\n",
      "Run time for delighted was 8.623198478017002 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "scared\n",
      "Mean of 100 tensors is: tensor([ 0.1834, -0.2903,  0.1685,  0.5033,  0.7534]) (768 features in tensor)\n",
      "Run time for scared was 9.237160164862871 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "worried\n",
      "Mean of 100 tensors is: tensor([-0.0387,  0.5058,  0.3141,  0.6470, -0.0606]) (768 features in tensor)\n",
      "Run time for worried was 9.148563474882394 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "confused\n",
      "Mean of 100 tensors is: tensor([0.0522, 0.3841, 0.1529, 1.0240, 0.2616]) (768 features in tensor)\n",
      "Run time for confused was 9.023905267938972 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "neutral\n",
      "Mean of 100 tensors is: tensor([0.2034, 0.2957, 0.4544, 0.0732, 0.1393]) (768 features in tensor)\n",
      "Run time for neutral was 9.108393697068095 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angry\n",
      "Mean of 100 tensors is: tensor([-0.0935,  0.2143,  0.1785,  0.7616,  0.3859]) (768 features in tensor)\n",
      "Run time for angry was 9.737426033010706 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "satisfied\n",
      "Mean of 100 tensors is: tensor([ 0.1219,  0.4661,  0.0624,  0.3347, -0.6584]) (768 features in tensor)\n",
      "Run time for satisfied was 7.742983402917162 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "surprised\n",
      "Mean of 100 tensors is: tensor([ 0.0425,  0.5705,  0.0578,  1.2133, -0.3284]) (768 features in tensor)\n",
      "Run time for surprised was 8.634889485081658 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "pleased\n",
      "Mean of 100 tensors is: tensor([ 0.1731,  0.7158,  0.0756,  0.7510, -0.2098]) (768 features in tensor)\n",
      "Run time for pleased was 7.095888014882803 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "concerned\n",
      "Mean of 100 tensors is: tensor([-0.0904,  0.6726,  0.4395,  0.3580, -0.2748]) (768 features in tensor)\n",
      "Run time for concerned was 7.964043076150119 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "mad\n",
      "Mean of 100 tensors is: tensor([0.0195, 0.6160, 0.0316, 0.4840, 0.7816]) (768 features in tensor)\n",
      "Run time for mad was 10.536383935948834 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "hurt\n",
      "Mean of 100 tensors is: tensor([ 0.2079,  0.2330,  0.0162, -0.2814, -0.0220]) (768 features in tensor)\n",
      "Run time for hurt was 9.02644597995095 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "excited\n",
      "Mean of 100 tensors is: tensor([ 0.1193,  0.7339,  0.1592,  0.6731, -0.6754]) (768 features in tensor)\n",
      "Run time for excited was 8.815366003895178 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "sad\n",
      "Mean of 100 tensors is: tensor([0.3074, 0.7827, 0.1626, 0.0990, 1.6388]) (768 features in tensor)\n",
      "Run time for sad was 8.304377650143579 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "interested\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 tensors is: tensor([-0.0682,  0.4775,  0.1338,  1.0167, -0.6228]) (768 features in tensor)\n",
      "Run time for interested was 6.914768014103174 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "happy\n",
      "Mean of 100 tensors is: tensor([0.0969, 0.7019, 0.0779, 0.7029, 0.1338]) (768 features in tensor)\n",
      "Run time for happy was 7.6490457970649 seconds.\n"
     ]
    }
   ],
   "source": [
    "l_output_file = ''\n",
    "l_count_file = ''\n",
    "l_output_file = os.path.join(output_file, 'roberta_layer' + str(LAYER) + '_CC-aefg_FEvocab.txt')\n",
    "l_count_file = os.path.join(count_file, 'roberta_layer'+ str(LAYER) + '_CC-aefg_counts.txt')\n",
    "print(l_output_file, l_count_file)\n",
    "# Process vocabulary words in the outer loop.\n",
    "for v in vocab:\n",
    "    start = timer()\n",
    "    with open(context_file, 'r') as lines:\n",
    "        v_sum = torch.zeros([1, FEATURE_COUNT])\n",
    "        v_tokens = tokenizer.encode(v)\n",
    "        print(f'\\nThere are {len(v_tokens) - 2} tokens in tokenized vocabulary word:')\n",
    "        for t in v_tokens[1:-1]:\n",
    "            print(tokenizer.decode(t).strip())\n",
    "        count_sentence = 0\n",
    "        count_tensor = 0\n",
    "\n",
    "        # Process all lines in the context file in the inner loop.\n",
    "        for line in lines:\n",
    "            # Check for this vocab word in this line; if found, split the line into individual sentences.\n",
    "            if v in line.lower().split():\n",
    "                for sentence in line.split('.'):\n",
    "                    if v in sentence.lower():\n",
    "                        line = sentence\n",
    "                        count_sentence += 1\n",
    "                        break\n",
    "                # Split the new sentence-based line into tokens.\n",
    "                # Use max_length to avoid overflowing the maximum sequence length for the model.\n",
    "                tokenized_text = tokenizer.encode(line, add_special_tokens=True, max_length=512)\n",
    "                indices = []              \n",
    "\n",
    "                # Check to see whether the vocab word is found in this particular line.\n",
    "                # Initially, some lines may have comprised multiple sentences, which were\n",
    "                # broken out individually above.\n",
    "                for t in v_tokens[1:-1]:\n",
    "                    for i, token_str in enumerate(tokenized_text):\n",
    "                        if tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip():\n",
    "                            indices.append(i)               \n",
    "\n",
    "                ###################################################################################\n",
    "                # If the vocabulary word was found, process the containing line.\n",
    "                if indices:\n",
    "\n",
    "                    # The vocab word was found in this line/sentence, at the locations in indices.\n",
    "                    # Get the feature vectors for all tokens in the line/sentence.\n",
    "                    token_embeddings = create_token_embeddings(tokenized_text)\n",
    "                    token_vecs_layer = get_layer_token_vecs(token_embeddings, LAYER)\n",
    "\n",
    "                    # Get the vocab word's contextual embedding for this line.\n",
    "                    tensor_layer = torch.zeros([1, FEATURE_COUNT])\n",
    "                    for i in range(len(indices)):\n",
    "                        v_index = i % len(v_tokens[1:-1])\n",
    "                        tensor_layer += token_vecs_layer[indices[i]]\n",
    "\n",
    "                    # If our vocab word is broken into more than one token, we need to get the mean of the token embeddings.\n",
    "                    tensor_layer /= len(indices)\n",
    "\n",
    "                    # Add the embedding distilled from this line to the sum of embeddings for all lines.\n",
    "                    v_sum += tensor_layer\n",
    "                    count_tensor += 1\n",
    "                ###################################################################################\n",
    "            # Stop processing lines once we've found 2000 instances of our vocab word.\n",
    "            if count_tensor >= MAX_LINES:\n",
    "                break\n",
    "\n",
    "        # We're done processing all lines of 512 tokens or less containing our vocab word.\n",
    "        # Get the mean embedding for the word.\n",
    "        v_mean = v_sum / count_tensor\n",
    "        print(f'Mean of {count_tensor} tensors is: {v_mean[0][:5]} ({len(v_mean[0])} features in tensor)')\n",
    "        write_embedding(l_output_file, v, v_mean)\n",
    "        try:\n",
    "            with open(l_count_file, 'a') as counts:\n",
    "                counts.write(v + ', ' + str(count_tensor) + '\\n')\n",
    "        except:\n",
    "            print('Wha?! Could not write the sentence count.')\n",
    "    end = timer()\n",
    "    print(f'Run time for {v} was {end - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    vocab = []\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_token_embeddings(tokenized_text):\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)  # Batch size 1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "        encoded_layers = outputs[2]\n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "#         print(f'Size of token embeddings is {token_embeddings.size()}')\n",
    "        return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the last 4 layers' features\n",
    "def sum_last_four_token_vecs(token_embeddings):\n",
    "    token_vecs_sum_last_four = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_last_four.append(sum_vec)\n",
    "\n",
    "#     print ('Shape of summed layers is: %d x %d' % (len(token_vecs_sum_last_four), len(token_vecs_sum_last_four[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_sum_last_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a single layer of the model.\n",
    "def get_layer_token_vecs(token_embeddings, layer_number):\n",
    "    token_vecs_layer = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        layer_vec = token[layer_number]\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_layer.append(layer_vec)\n",
    "\n",
    "#     print ('Shape of summed layers is: %d x %d' % (len(token_vecs_layer), len(token_vecs_layer[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "#         print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal-venv-3.6",
   "language": "python",
   "name": "crystal-venv-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
