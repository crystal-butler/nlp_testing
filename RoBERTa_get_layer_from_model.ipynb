{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, RobertaConfig\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/Notebooks/crystal/NLP/transformers/examples'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we're in the transformers directory with fine-tuned model output.\n",
    "os.chdir('/home/jupyter/Notebooks/crystal/NLP/transformers/examples/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# and Transformers documentation: https://huggingface.co/transformers/model_doc/roberta.html#robertaformaskedlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('./output_CC-aefg/')\n",
    "config = RobertaConfig.from_pretrained('./output_CC-aefg/')\n",
    "model = RobertaForMaskedLM.from_pretrained('./output_CC-aefg/', config=config)\n",
    "model.eval()\n",
    "\n",
    "context_file = \"/home/jupyter/Notebooks/crystal/NLP/transformers/examples/CC_WET_test_ab\"\n",
    "output_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/'\n",
    "count_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/'\n",
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/data/vocab_files/FE_vocab_study.txt'\n",
    "vocab = make_vocab(vocab_file)\n",
    "\n",
    "FEATURE_COUNT = 768\n",
    "LAYER = 8\n",
    "MAX_LINES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_layer8_CC-aefg_FEvocab.txt /home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_layer8_CC-aefg_counts.txt\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "st\n",
      "upe\n",
      "f\n",
      "ied\n",
      "Mean of 10 tensors is: tensor([ 0.0427,  0.5158, -0.0076, -0.1682, -0.6346]) (768 features in tensor)\n",
      "Run time for stupefied was 18.170346095925197 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "scorn\n",
      "ful\n",
      "Mean of 18 tensors is: tensor([ 0.2055,  0.2329,  0.0556, -0.0198, -0.1499]) (768 features in tensor)\n",
      "Run time for scornful was 19.136984557146206 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disbel\n",
      "ieving\n",
      "Mean of 26 tensors is: tensor([ 0.1772,  0.5028, -0.0956,  0.5229,  0.2292]) (768 features in tensor)\n",
      "Run time for disbelieving was 19.583309751935303 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disdain\n",
      "ful\n",
      "Mean of 20 tensors is: tensor([ 0.1507,  0.1293,  0.0956, -0.0646,  0.0414]) (768 features in tensor)\n",
      "Run time for disdainful was 19.538508038967848 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "revol\n",
      "ted\n",
      "Mean of 26 tensors is: tensor([ 0.1630,  0.3805,  0.1211,  0.9752, -0.3244]) (768 features in tensor)\n",
      "Run time for revolted was 20.055183418095112 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "m\n",
      "iff\n",
      "ed\n",
      "Mean of 50 tensors is: tensor([0.0168, 0.7489, 0.1398, 0.4718, 0.1107]) (768 features in tensor)\n",
      "Run time for miffed was 22.783655858132988 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "a\n",
      "gh\n",
      "ast\n",
      "Mean of 37 tensors is: tensor([ 0.1117,  0.1480, -0.0424,  0.2947, -0.0697]) (768 features in tensor)\n",
      "Run time for aghast was 20.85243239090778 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "inc\n",
      "ensed\n",
      "Mean of 56 tensors is: tensor([ 0.1752,  0.4669,  0.1911,  0.4523, -0.3358]) (768 features in tensor)\n",
      "Run time for incensed was 22.277820928022265 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "de\n",
      "jected\n",
      "Mean of 35 tensors is: tensor([ 0.2010,  0.4748,  0.0021, -0.0185,  0.0396]) (768 features in tensor)\n",
      "Run time for dejected was 20.318819168023765 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "rep\n",
      "uls\n",
      "ed\n",
      "Mean of 56 tensors is: tensor([ 0.1720,  0.5288,  0.0787,  0.2794, -0.3328]) (768 features in tensor)\n",
      "Run time for repulsed was 23.593957596924156 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "mourn\n",
      "ful\n",
      "Mean of 45 tensors is: tensor([0.2277, 0.8016, 0.0363, 0.1701, 0.8705]) (768 features in tensor)\n",
      "Run time for mournful was 21.06082748202607 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "disple\n",
      "ased\n",
      "Mean of 55 tensors is: tensor([ 0.0899,  0.6045,  0.1431,  0.3391, -0.1207]) (768 features in tensor)\n",
      "Run time for displeased was 21.775446766987443 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "infuri\n",
      "ated\n",
      "Mean of 66 tensors is: tensor([ 0.2928,  0.6497,  0.1708,  0.4263, -0.0766]) (768 features in tensor)\n",
      "Run time for infuriated was 22.213207226945087 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aw\n",
      "ed\n",
      "Mean of 72 tensors is: tensor([ 0.1930,  0.6246, -0.0944, -0.4696, -0.2851]) (768 features in tensor)\n",
      "Run time for awed was 22.991184293990955 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "repe\n",
      "lled\n",
      "Mean of 68 tensors is: tensor([ 0.1125,  0.4820,  0.1008,  0.3350, -0.1715]) (768 features in tensor)\n",
      "Run time for repelled was 23.050234009046108 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "resent\n",
      "ful\n",
      "Mean of 74 tensors is: tensor([ 0.0312,  0.1935,  0.1297,  0.4407, -0.0904]) (768 features in tensor)\n",
      "Run time for resentful was 23.805870484095067 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "sorrow\n",
      "ful\n",
      "Mean of 76 tensors is: tensor([0.2183, 0.5794, 0.1279, 0.4336, 1.0342]) (768 features in tensor)\n",
      "Run time for sorrowful was 23.63572409003973 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ir\n",
      "ate\n",
      "Mean of 73 tensors is: tensor([ 0.1717,  0.8390,  0.0394,  0.3983, -0.4288]) (768 features in tensor)\n",
      "Run time for irate was 23.257120992988348 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "dismay\n",
      "ed\n",
      "Mean of 95 tensors is: tensor([ 0.0705,  0.5528,  0.1855,  0.4333, -0.1392]) (768 features in tensor)\n",
      "Run time for dismayed was 25.502443260047585 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "el\n",
      "ated\n",
      "Mean of 100 tensors is: tensor([ 0.1805,  0.8596,  0.0687,  0.7083, -0.6579]) (768 features in tensor)\n",
      "Run time for elated was 21.544648116920143 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "enraged\n",
      "Mean of 100 tensors is: tensor([ 0.0643,  0.3276,  0.2042,  0.1692, -0.1543]) (768 features in tensor)\n",
      "Run time for enraged was 24.467868368839845 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "apprehens\n",
      "ive\n",
      "Mean of 100 tensors is: tensor([ 0.1682,  0.2780,  0.1768,  0.8680, -0.5432]) (768 features in tensor)\n",
      "Run time for apprehensive was 19.83481262391433 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bewild\n",
      "ered\n",
      "Mean of 92 tensors is: tensor([-0.0319,  0.4082,  0.0405,  0.3128, -0.0196]) (768 features in tensor)\n",
      "Run time for bewildered was 26.52904421603307 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ast\n",
      "ounded\n",
      "Mean of 100 tensors is: tensor([ 0.1516,  0.5911,  0.0191,  0.5165, -0.4314]) (768 features in tensor)\n",
      "Run time for astounded was 19.147715365048498 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "perplex\n",
      "ed\n"
     ]
    }
   ],
   "source": [
    "l_output_file = ''\n",
    "l_count_file = ''\n",
    "l_output_file = os.path.join(output_file, 'roberta_layer' + str(LAYER) + '_CC-aefg_FEvocab.txt')\n",
    "l_count_file = os.path.join(count_file, 'roberta_layer'+ str(LAYER) + '_CC-aefg_counts.txt')\n",
    "print(l_output_file, l_count_file)\n",
    "# Process vocabulary words in the outer loop.\n",
    "for v in vocab:\n",
    "    start = timer()\n",
    "    with open(context_file, 'r') as lines:\n",
    "        v_sum = torch.zeros([1, FEATURE_COUNT])\n",
    "        v_tokens = tokenizer.encode(v)\n",
    "        print(f'\\nThere are {len(v_tokens) - 2} tokens in tokenized vocabulary word:')\n",
    "        for t in v_tokens[1:-1]:\n",
    "            print(tokenizer.decode(t).strip())\n",
    "        count_sentence = 0\n",
    "        count_tensor = 0\n",
    "\n",
    "        # Process all lines in the context file in the inner loop.\n",
    "        for line in lines:\n",
    "            # Check for this vocab word in this line; if found, split the line into individual sentences.\n",
    "            if v in line.lower().split():\n",
    "                for sentence in line.split('.'):\n",
    "                    if v in sentence.lower():\n",
    "                        line = sentence\n",
    "                        count_sentence += 1\n",
    "                        break\n",
    "                # Split the new sentence-based line into tokens.\n",
    "                # Use max_length to avoid overflowing the maximum sequence length for the model.\n",
    "                tokenized_text = tokenizer.encode(line, add_special_tokens=True, max_length=512)\n",
    "                indices = []              \n",
    "\n",
    "                # Check to see whether the vocab word is found in this particular line.\n",
    "                # Initially, some lines may have comprised multiple sentences, which were\n",
    "                # broken out individually above.\n",
    "                for t in v_tokens[1:-1]:\n",
    "                    for i, token_str in enumerate(tokenized_text):\n",
    "                        if tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip():\n",
    "                            indices.append(i)               \n",
    "\n",
    "                ###################################################################################\n",
    "                # If the vocabulary word was found, process the containing line.\n",
    "                if indices:\n",
    "\n",
    "                    # The vocab word was found in this line/sentence, at the locations in indices.\n",
    "                    # Get the feature vectors for all tokens in the line/sentence.\n",
    "                    token_embeddings = create_token_embeddings(tokenized_text)\n",
    "                    token_vecs_layer = get_layer_token_vecs(token_embeddings, LAYER)\n",
    "\n",
    "                    # Get the vocab word's contextual embedding for this line.\n",
    "                    tensor_layer = torch.zeros([1, FEATURE_COUNT])\n",
    "                    for i in range(len(indices)):\n",
    "                        v_index = i % len(v_tokens[1:-1])\n",
    "                        tensor_layer += token_vecs_layer[indices[i]]\n",
    "\n",
    "                    # If our vocab word is broken into more than one token, we need to get the mean of the token embeddings.\n",
    "                    tensor_layer /= len(indices)\n",
    "\n",
    "                    # Add the embedding distilled from this line to the sum of embeddings for all lines.\n",
    "                    v_sum += tensor_layer\n",
    "                    count_tensor += 1\n",
    "                ###################################################################################\n",
    "            # Stop processing lines once we've found 2000 instances of our vocab word.\n",
    "            if count_tensor >= MAX_LINES:\n",
    "                break\n",
    "\n",
    "        # We're done processing all lines of 512 tokens or less containing our vocab word.\n",
    "        # Get the mean embedding for the word.\n",
    "        v_mean = v_sum / count_tensor\n",
    "        print(f'Mean of {count_tensor} tensors is: {v_mean[0][:5]} ({len(v_mean[0])} features in tensor)')\n",
    "        write_embedding(l_output_file, v, v_mean)\n",
    "        try:\n",
    "            with open(l_count_file, 'a') as counts:\n",
    "                counts.write(v + ', ' + str(count_tensor) + '\\n')\n",
    "        except:\n",
    "            print('Wha?! Could not write the sentence count.')\n",
    "    end = timer()\n",
    "    print(f'Run time for {v} was {end - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    vocab = []\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_token_embeddings(tokenized_text):\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)  # Batch size 1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "        encoded_layers = outputs[2]\n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "#         print(f'Size of token embeddings is {token_embeddings.size()}')\n",
    "        return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the last 4 layers' features\n",
    "def sum_last_four_token_vecs(token_embeddings):\n",
    "    token_vecs_sum_last_four = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_last_four.append(sum_vec)\n",
    "\n",
    "#     print ('Shape of summed layers is: %d x %d' % (len(token_vecs_sum_last_four), len(token_vecs_sum_last_four[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_sum_last_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a single layer of the model.\n",
    "def get_layer_token_vecs(token_embeddings, layer_number):\n",
    "    token_vecs_layer = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        layer_vec = token[layer_number]\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_layer.append(layer_vec)\n",
    "\n",
    "#     print ('Shape of summed layers is: %d x %d' % (len(token_vecs_layer), len(token_vecs_layer[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "#         print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal-venv-3.6",
   "language": "python",
   "name": "crystal-venv-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
