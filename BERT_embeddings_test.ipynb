{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/Notebooks/crystal/NLP/nlp_testing'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disgusted'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(tokenizer.vocab.keys())[5000:5020]\n",
    "list(tokenizer.vocab.keys())[17733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'she', 'made', 'a', 'disgusted', 'po', '##ut', '[SEP]', 'her', 'disgusted', 'expression', 'was', 'con', '##tag', '##ious', '[SEP]']\n",
      "[CLS]           101\n",
      "she           2,016\n",
      "made          2,081\n",
      "a             1,037\n",
      "disgusted    17,733\n",
      "po           13,433\n",
      "##ut          4,904\n",
      "[SEP]           102\n",
      "her           2,014\n",
      "disgusted    17,733\n",
      "expression    3,670\n",
      "was           2,001\n",
      "con           9,530\n",
      "##tag        15,900\n",
      "##ious        6,313\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"disgusted\"\n",
    "text = \"[CLS] She made a disgusted pout [SEP] Her disgusted expression was contagious [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[26575], [19557, 14740], [11113, 16368], [11113, 16368, 5596], [11113, 16368, 24413], [11113, 16368, 22787], [11113, 20936, 22966], [11113, 28819], [9962], [9063], [9920], [3970], [10564], [16222, 5358, 5302, 16616], [8885], [15802, 4630], [16222, 28393, 2094], [16222, 10383, 7062], [5496], [16723], [9078, 15185, 2594], [24171], [3161], [11325], [29502], [5587, 3709], [17005], [6449], [4748, 21223], [4748, 28741, 2290], [4748, 16338], [4748, 14028, 10980, 2140], [21358, 7011, 8553], [5360], [12242, 3686], [21358, 29301, 2098], [21358, 12792, 2098], [10028, 26878], [4452], [12943, 24065], [25817], [12943, 17643, 21596], [14974], [9376], [12943, 16523, 2666, 3726], [12943, 16523, 2666, 7178], [12943, 14949, 2102], [21568], [3283, 2290], [3283, 3490, 5422], [5993, 3085], [12943, 8303, 3512], [2250, 4974], [8598], [19260], [8598, 2075], [9499], [22333], [7344, 4383], [27395], [24251, 2094], [2035, 12228], [2632, 21511], [25933, 7062], [15261], [21606], [6429], [16290], [12479], [2572, 5638, 17479, 5897], [2572, 5638, 24879], [2572, 8189, 3468], [26445, 3085], [26445, 21170], [11770], [9778], [17826], [20253], [4963], [18748], [14136], [4854], [17076, 3367], [21782], [21782, 2098], [6579], [2019, 16339, 17759], [17466], [11654], [15703], [17379, 2594], [14405, 23692, 3490, 5422], [11436], [26481], [11162], [3424, 6895, 24952, 3726], [3424, 6895, 4502, 7062], [3424, 20166], [16111, 2100], [10089], [11480], [23403], [9706, 8988, 16530], [9706, 17308], [29352], [29279], [10439, 8095, 15787], [10439, 25063], [10439, 5243, 7741], [10439, 2890, 7405, 6024], [25809], [10439, 2890, 10222, 12742], [14300], [4844], [10439, 22046], [7475], [6685, 8082], [18391], [24416], [15818], [15818, 2135], [7976], [14984], [22344], [20865, 3512], [20865, 14547], [20077], [8916], [22741], [23819], [2004, 24826, 25848], [7161], [2012, 6528, 6024], [2012, 6528, 6024, 2791], [6296], [13642, 22373], [13642, 22573], [13642, 2869, 3258], [13642, 2869, 3512], [18568], [9992], [15497], [20256], [5204], [7073], [15180], [15180, 2094], [15180, 3367, 29314], [9643], [9596], [9596, 2791], [12946, 2094], [2067, 11774, 2098], [6649], [8670, 18142], [29088], [8670, 4246, 2989], [17776], [7221, 2389], [19372], [24234, 3993], [27910], [4562, 4509], [3786], [7854], [2793, 6777, 18450], [2022, 11263, 28090], [12858], [11693, 28121, 3351], [11693, 28121, 4726], [11693, 28121, 28392], [11693, 19231, 2098], [20252, 3064], [19337, 12474, 2989], [4330, 17071, 10127], [4330, 17071, 4765], [7495], [2022, 7606, 2098], [2022, 7606, 13665], [3841, 6777, 9890, 5897], [25786], [3841, 25438, 2098], [2022, 11657], [2022, 15172], [2022, 16416, 7178], [2022, 2890, 6199], [2022, 19763, 8450], [2190, 2098], [14583], [12056], [24683], [2022, 29602, 4063, 3672], [12170], [12170, 12798, 2271], [2978], [12344], [8618], [8618, 26760, 15558], [24114], [20857], [8744], [1038, 25002], [25590], [21657], [1038, 19738, 2854], [10190], [8682], [20641], [28279, 14097], [13670], [13670, 3993], [13670, 7699], [1038, 15909, 5369], [10676], [2630], [5132], [14441, 2075], [14969], [25771], [14154, 6238, 2075], [8945, 14083, 3993], [22132, 11533], [16018], [19651, 3334, 3560], [7782], [11471], [29556], [11771], [11250], [5391, 2121], [11655, 4095, 2791], [28557, 3723], [9191], [4408], [7987, 2923, 2989], [3714], [3714, 27693], [3714, 27693, 2135], [28902], [25565, 2100], [18618], [7987, 2271, 4226], [11829], [27569], [20716], [18917], [26352, 7583], [20934, 18232, 3372], [10859, 2098], [6402], [21305], [5747, 2098], [7980, 2100], [6187, 6292], [20177], [2655, 3560], [2655, 13901], [5475], [23674], [5475, 2791], [2064, 4890], [2064, 5794, 5484, 3560], [5214], [6178, 7277, 6313], [14408, 21967], [12481], [2729, 23301], [6176], [23358], [11922], [4937, 3723], [6187, 19966, 2594], [14046, 5649], [17145], [28778], [12964], [7401], [8292, 3619, 5397], [8857], [3056], [15775, 25031], [15775, 24860], [15775, 24860, 2098], [15775, 24860, 7228], [4119], [8315], [10368], [19633], [5338], [11084, 2098], [11951], [25869, 2100], [22673], [5048, 2100], [25471], [18350], [24867], [15138, 3238], [15138, 2100], [18178, 2229, 2100], [3108, 2100], [9610, 3207], [9610, 4667], [24282], [24282, 2135], [2775, 10359], [10720], [23362], [27017], [9090, 4842], [9610, 14536, 2100], [16480, 3917, 2594], [16480, 5339, 2989], [15375], [8057, 2989], [14684, 12190, 4509], [25022, 11890, 18163, 5051, 6593], [18856, 22591, 13288], [13249], [3154], [8555], [12266], [2485], [2701], [2485, 14359, 2098], [18856, 6977], [9789, 3238], [13514], [18856, 23128], [10338, 17683, 2094], [10338, 9961], [10338, 28632], [24995], [2522, 29076, 13471, 2102], [3147], [5067], [8902, 7393, 3512], [16844, 3550], [4337, 3512], [21699], [7216], [6625], [7216, 2098], [29257], [7991], [4012, 28732, 15172], [4012, 28732, 18514], [4012, 23041, 25184], [15398], [29353], [17824], [6975], [4012, 24759, 10732, 5897], [4012, 24759, 10732, 9407], [4012, 24759, 10732, 3372], [4012, 24759, 10732, 20630], [17612], [17949], [3605], [22346, 2075], [4012, 14289, 4877, 3512], [14091], [9530, 11788, 2075], [9530, 3401, 17572], [8279], [16966], [6693], [5142], [4986], [9530, 6895, 6632, 7062], [9530, 23633], [28525], [24707, 11020, 18537], [25805, 2989], [7023], [9657], [28415], [4736, 2098], [9530, 14876, 8630], [9530, 21001], [13111, 2389], [5457], [6724], [26478, 19825, 2140], [26478, 8609, 20350, 2100], [9530, 3490, 6455], [9715], [4603], [5136, 3686], [6195], [9530, 19454, 2075], [9530, 13102, 7895, 29469, 2389], [9530, 13102, 24771], [9530, 6238, 9323], [9530, 16643, 17585, 2094], [27570], [10202], [15077], [4838], [9530, 18532, 15725], [25247], [9530, 18532, 24759, 3370], [9530, 18532, 24759, 8082], [17152], [17152, 8918], [4180], [4180, 2098], [29308], [4180, 2135], [4180, 3672], [27894], [10043], [9530, 18886, 2618], [4758], [9756], [6801], [9530, 11667, 20113], [6427], [4658], [10791], [11601, 4818], [26103], [19813], [16592, 2135], [2522, 2100], [18081, 3762], [7477, 2100], [27987, 2100], [28343], [4689], [13675, 2098, 16203], [17109], [11146, 13976, 2368], [13675, 23180], [4187], [2892], [28629, 27405], [13587], [10311], [10560], [5390], [6933], [26483], [12731, 14277, 3085], [23626], [12731, 9488, 2015], [10628], [8025], [6276], [22330, 8713], [26881], [22330, 8713, 2964], [17488, 15204, 3401], [4907, 5149], [4795], [27148], [4830, 16671, 2098], [2154, 16200, 3286], [2154, 16200, 10631, 3070], [19720], [4830, 17269], [9252], [2757, 9739], [5981], [20767], [2139, 27773, 7690], [11703, 20175, 3993], [11703, 7416, 7178], [11703, 7416, 6455], [11703, 7416, 6455, 2135], [17575], [11703, 22048], [10561], [13079], [4056], [4154], [3249], [3639, 3238], [5600], [19674], [27836], [13366, 13776], [2139, 3654, 3351], [2139, 16307, 2075], [2139, 24455], [2139, 20614, 3258], [15063], [3972, 20755, 15172], [12208], [15936], [26380], [3972, 15735, 3560], [3972, 15735, 2819], [3972, 12672], [3972, 14499, 2389], [9694], [17183, 11219, 2075], [17183, 14088], [13614, 2094], [9703, 7941, 3550], [17183, 5397], [6380], [7939, 23709, 6129], [22595], [2139, 24759, 6525, 3468], [2139, 28139, 18252], [14777], [6245], [17676], [4315, 22043, 2094], [4315, 19969], [4315, 17417, 3726], [4315, 18170, 7062], [4792], [4078, 24771], [4078, 9711, 2271], [4078, 19425], [13905], [13905, 2098], [13905, 2075], [7143], [15561], [4078, 18136, 2063], [4078, 26029, 16454], [4078, 3775, 24518], [3908], [12230], [9128], [4340], [12515], [28283, 5596], [20010, 4355], [20010, 4355, 3085], [20010, 4355, 2075], [20010, 20026, 4765], [13879], [14386, 4630], [6548, 4509], [14386, 3560], [14386, 7741], [4487, 26989, 16454], [29454, 14049], [29454, 29206, 2102], [11737, 9148, 16190], [18704], [21090], [21090, 3085], [18185], [9364], [15640], [10520], [21406], [4487, 3736, 9397, 22046], [12537], [4487, 19022, 20806, 18697], [4487, 19022, 20806, 23559], [5860, 11795, 2075], [12532, 13344, 28507, 3064], [12532, 2213, 8873, 3064], [17964], [17964, 2098], [12532, 5897, 17724], [23657], [12532, 3619, 19425], [27648], [27648, 2098], [19575, 2098], [22585], [5456], [5860, 20026, 19185], [6936], [25134], [25134, 2098], [25134, 3993], [25134, 7699], [4487, 5054, 14856, 3064], [4487, 5054, 3654, 5999], [29591, 2094], [4487, 28745, 15532, 14782], [4487, 28745, 15532, 24007], [12721], [17733], [17733, 2135], [19424], [9841, 14644, 6528, 2098], [9841, 21821, 2102], [4487, 27572, 24117, 2098], [4487, 11493, 20464, 21280], [4487, 7741, 2368, 8918], [4487, 11493, 3334, 4355], [4487, 11493, 3334, 17944], [4487, 2015, 5558, 18447, 2098], [18959], [18966], [4487, 14540, 17471, 3070], [4487, 26212, 2140], [4487, 11512], [20006], [20006, 2098], [19776, 3512], [4487, 6499, 8270, 11638], [8761, 2135], [4487, 21748, 25099, 2094], [4487, 13102, 11215], [4487, 27694, 16594], [4487, 13102, 12054, 3258, 3686], [4487, 13102, 15735, 3064], [4487, 13102, 15735, 3064, 2791], [4487, 13102, 19738, 6924], [28606], [4487, 2015, 15549, 3388], [4487, 2015, 15549, 12870, 2094], [27770], [4487, 21338, 2229, 5051, 6593, 3993], [20275], [23217, 3512], [28237], [25956], [4487, 11488, 7315, 12031], [4487, 11393, 11873], [4487, 24137, 7405, 3064], [4487, 7092, 4630], [4487, 9153, 2378], [6802], [4487, 9153, 13473], [4487, 9153, 13473, 3993], [11116], [25348], [12893], [24305], [12893, 2075], [29245], [29245, 3993], [29245, 2075], [12491], [18356], [26489, 6292], [2079, 2571, 3993], [2079, 7096, 4509], [7444], [21949], [14383, 3170, 7999], [2589], [20076], [2079, 5051, 2100], [11089, 2075], [4797], [4797, 2121], [21888], [21888, 2135], [21888, 2791], [4797, 2075], [2079, 3126], [2091], [2091, 10526], [2091, 27693], [2091, 27693, 2791], [2091, 13181, 17101], [2079, 6774], [11055], [6918], [4567], [14436], [21794], [14436, 2075], [12802], [3959, 2100], [2852, 14644, 2100], [5533], [2852, 15568, 2100], [25483], [7144], [15967, 2791], [12931, 2666, 3723], [22917], [22917, 2135], [10634], [12873], [12873, 14876, 8630], [12873, 21001], [12873, 3367, 29314], [4241, 2213, 21001], [4241, 5051], [4241, 24759, 28775, 24826, 2015], [1040, 7274, 8458, 29180], [9461], [9461, 2791], [17300], [3733], [1041, 8569, 23697, 3372], [19069], [14925, 16677], [14925, 16677, 3973], [3968, 6292], [18823], [1041, 4246, 5313, 11461], [13059, 6553], [13059, 16774, 7476], [1041, 17603, 11411, 2271], [3449, 4383], [3449, 3370], [18042], [26475], [10339], [14325], [7861, 16313, 14050], [7861, 23684], [6832], [7603, 3238], [7861, 15069, 16530], [7861, 25940], [26452], [23397], [4064], [4372, 22591, 5596], [22454], [6628], [15846], [11434], [2203, 14644, 2098], [2203, 27242], [16762], [18114], [4372, 2121, 5856, 5422], [5117], [25540, 25725, 2098], [25540, 25725, 3672], [26757, 4588], [5959], [9107], [20195], [4372, 7138, 6675], [4372, 16383], [4372, 11231, 2072], [18835], [4372, 29181, 2075], [4372, 2527, 13876, 12165], [21474], [4372, 2705, 7941, 3709], [4372, 19877, 2098], [12024], [14727], [4372, 4588, 2098], [4211, 2094], [4372, 24918], [21103], [14253, 3973], [24211], [20286], [7327, 8458, 29180], [23208], [9345, 12742], [4763], [23408, 11045], [4654, 10732, 28483, 3064], [4654, 2389, 3064], [12843], [4654, 3022, 4842, 3686], [24379], [4654, 3022, 29487], [7568], [23885], [8277], [4654, 20464, 8067, 3508], [4654, 20464, 8067, 7062], [9069], [15575], [15095, 3512], [4654, 26415, 9250], [4654, 26415, 8156], [15284], [5987, 4630], [17626], [8074], [4863], [9990], [18077, 3512], [11355], [7524], [22570], [4654, 21436, 4630], [4654, 11314, 4630], [4654, 11314, 2098], [3239], [7168], [4320], [2227, 20771], [4945], [8143], [4189], [8275], [6904, 6834], [6904, 21928], [6904, 15630, 9072], [5470, 12070], [5470, 26336], [2521, 2102], [15677], [3435, 28173, 3560], [16342], [16342, 2094], [6346, 16294, 4667], [11119], [6904, 7962, 2075], [6904, 5422], [3571], [8615], [19725], [14892], [22518], [10069, 8462], [10768, 19250, 4757], [7349], [7408, 3468], [24664, 16206], [10768, 10415, 9956, 2271], [27863], [10768, 21735], [17037, 3512], [10882, 24291, 2100], [10882, 10497, 4509], [9205], [15443], [3554], [2986], [2736], [3813], [3869, 2100], [8081, 4383], [4964], [13109, 7875, 4059, 14083, 2098], [19091], [4257], [13109, 4887, 24360], [3462, 2100], [11238, 9739, 2102], [9357], [27978, 3370], [27978, 10450, 3560], [27978, 2100], [2723, 2098], [19857, 7382, 11636, 2098], [19857, 24167], [3579], [4208], [7995], [17910, 2098], [13219], [2005, 4783, 22397], [27206, 4667], [3140], [28552], [2005, 21156, 2098], [2005, 10626, 2078], [19590], [2830], [12487], [25312, 6593, 6313], [13072], [15762], [27105], [25312, 18533], [25312, 17269], [22783], [10424, 2368, 14272, 2094], [10424, 3388, 3993], [2767, 20942], [5379], [25966], [10363], [17115], [10424, 8004, 3593], [10424, 20573, 2100], [10424, 23518, 5484], [11330], [14587], [7708], [10424, 24237, 2100], [10206], [9135], [16829], [11865, 7583], [11865, 6562], [4569], [6057], [9943], [20322], [9943, 2791], [20555], [6519, 6024], [8111], [28554, 2100], [26033, 2098], [26033, 2075], [12008], [8535], [17054], [5637], [16448], [8991, 4818], [7132], [10218], [1043, 14949, 14626], [21025, 14968], [17565], [21783], [5580], [5580, 24589], [5580, 20282, 2050], [5580, 2791], [5580, 14045], [10982], [16124], [19724], [18874], [18874, 3993], [18874, 7699], [1043, 29521], [1043, 4135, 5844], [24067], [24067, 2100], [8652, 7999], [10156], [1043, 12942], [1043, 11802, 2140], [2175, 5910, 26945, 2098], [2204], [27243], [13761, 2100], [2882, 10735, 2063], [8794], [24665, 10450, 10451], [6542], [2307], [20505], [14806], [9940], [24665, 2666, 7178], [24665, 25587], [11844], [25898], [11844, 26217], [5861], [11478], [6218, 2075], [7977], [17500], [24665, 7140, 11714], [13349], [22413], [24665, 15979], [24665, 6784, 4726], [27038], [24665, 25438, 2989], [24665, 24237, 2100], [20696], [20696, 2075], [13802], [5905], [26546], [27912], [2431, 27693], [12705], [5292, 21112, 2015], [8404], [3407], [28186], [2524], [15015], [17631], [5292, 22155, 2094], [8401], [5223], [5223, 3993], [22650], [11150], [5292, 18533, 2100], [11171], [26710], [4641, 20459, 2063], [2540, 15395], [2540, 29162], [18627], [8072, 6799], [9685], [3082, 27693], [17752, 2571], [18235, 20952, 5313], [2002, 5740, 2271], [14044], [13346], [20221], [24626], [2002, 28032, 5844], [13431], [2152], [7570, 10820, 2075], [7570, 7712, 16975], [7481], [13556], [3246], [17772], [17772, 2791], [20625], [5327], [7109, 2100], [9202], [14603], [7570, 18752, 12031], [7570, 18752, 14116], [5469], [10420], [18258], [2980], [2980, 19040], [21301, 9961], [21301, 2100], [15716], [15716, 2094], [14910, 21884], [26608], [14910, 15148], [20364], [8562], [8562, 2098], [14742], [9012], [7501], [14682], [3480], [3480, 3993], [11878], [20261], [24033], [23760], [23760, 19620], [1044, 22571, 17048, 3550], [1044, 22571, 10085, 14778, 7476], [29004], [25614], [10041, 2594], [21591], [9217], [5665], [28575], [26838], [26275], [19209], [17727, 8445, 4818], [17727, 12054, 19798], [17727, 12054, 3512], [28011], [17380], [17727, 11124, 3560], [17727, 18617, 2389], [17727, 8743, 21820, 2102], [17727, 4509], [20467], [17727, 10626, 2075], [2590], [7622], [17727, 23004], [16389], [14710], [27118, 28228, 19879, 2618], [27118, 25970, 6024], [27118, 21041, 3468], [27118, 14317, 4765, 2594], [19907], [28647, 2094], [4297, 8743, 8113], [4297, 8743, 18679], [4297, 17572], [4297, 25377, 2890, 10222, 19307], [4297, 5644, 24330, 8918], [4297, 5596, 15859, 3723], [4297, 5596, 16203], [4297, 5596, 21227], [4297, 5313, 17585], [4297, 9496, 3560], [27427, 8586, 11514, 5886, 3085], [27427, 8586, 19969], [27427, 8586, 17417, 3726], [24436], [24436, 2135], [27427, 25593], [11424, 16136], [1999, 15878, 25475], [1999, 8743], [1999, 27753, 24133], [14092], [14092, 3012], [1999, 10258, 14074, 2094], [11900], [21672], [1999, 27942, 15070], [26402, 2098], [26402, 2075], [1999, 27605, 9289], [5229], [7036], [1999, 24343], [1999, 15549, 4892], [1999, 15549, 28032, 3512], [9577], [16021, 26775, 23056], [16021, 29150], [16021, 8586, 25137], [16021, 6132, 13043], [16021, 28173, 3560], [16021, 2378, 24133], [20616], [29204], [22604], [16021, 9890, 3372], [16021, 7140, 14483, 3401], [16021, 7140, 14483, 2102], [4427], [18988], [16021, 3775, 16961], [16021, 26310], [16021, 12083, 8551, 14776], [16021, 7934], [23637], [23979], [4454], [6387], [20531], [8015], [11806], [7848], [21249], [21935], [3037], [4699], [6970, 20614, 2075], [4722, 6026], [6970, 3217, 16961], [22602], [28028], [24439], [2046, 3917, 4630], [2046, 9048, 12921], [20014, 27611], [18896], [23824], [17174, 13102, 22471, 3512], [11241], [8556], [15025], [14064, 2100], [1999, 5737, 20255, 4383], [2920], [11209, 11020, 7028], [11209, 2618], [20868, 2063], [20868, 12879, 5313], [20868, 8126], [19313], [19728], [20868, 6072, 4747, 10421], [20868, 17728, 3468], [20868, 17728, 6321], [15560], [17373], [7275], [14855, 15499], [12323, 2094], [15723, 5596], [15723, 4892], [14855, 16671, 2100], [5730, 2098], [9981], [15333, 7999], [15333, 16643, 3070], [10147, 7096, 2098], [10147, 12079, 2100], [8183, 15431], [16644], [22193], [28801], [8183, 18660], [6569], [6569, 3993], [6569, 20938], [6569, 3238], [6569, 3560], [18414, 14454, 4630], [18414, 14454, 3370], [16646, 2389], [13325], [8689, 2389], [18414, 14808, 6313], [5376, 2100], [15123], [10326], [2785], [2785, 27693], [3610], [4209], [2113, 3709, 3654, 3468], [3716, 3085], [12849, 19603], [3768, 8447, 17417, 9289], [3768, 7393, 3334], [18749, 12356], [12559, 14083, 2063], [20342, 12380, 3468], [20342, 24360], [5869, 6895, 24918], [4756], [5870], [7239], [13971], [2975], [3393, 7474, 3560], [3393, 19159], [3389, 4892], [3389, 2854], [2292, 7698], [2292, 8167, 12863], [2504, 4974, 2098], [24992, 2094], [5622, 17062, 5740, 2271], [22185], [2422, 27693], [5423, 5669], [5962], [2862, 3238], [17133], [22135, 3593], [8209], [8840, 8988], [8840, 8988, 2063], [8840, 22314], [8840, 8988, 14045], [5299], [20334], [9479], [15752], [2559], [8840, 16585], [3279], [2439], [5189], [10223, 6508], [2293], [8295], [2659, 20942], [11320, 14615], [11516, 3993], [11516, 2075], [11516, 2100], [4688], [5506], [24890, 2098], [12013], [15451, 8663, 6528, 2102], [3287, 8873, 13013], [3287, 6767, 16136], [28238], [24391], [16007, 27881], [29310, 9289], [23624, 14289, 26255], [8348, 2098], [3040], [2812], [15902], [19960, 29293], [28997], [11463, 2319, 9905, 10415], [22247], [11463, 8261], [19854], [24060], [5177], [21442, 11272], [12831], [2033, 6491, 11124, 5422], [2771, 15388], [10256], [8117, 6129], [2568, 3993], [2568, 3238], [22243], [14719, 2705], [14719, 2705, 3993], [28616, 4630, 8093, 7361, 2594], [25166], [25723], [25723, 2791], [13736], [14624], [28616, 23795], [28616, 19738, 2094], [11094, 7946, 2102], [11094, 7946, 24475, 5313], [11094, 7946, 3436], [28947], [12934, 7301], [19545], [19545, 2135], [10754], [18847, 5524], [6071], [14434], [9587, 5051, 2100], [22822, 9232], [22294, 7810], [12774], [9587, 14287, 3993], [9587, 14287, 20938], [16236], [20521], [2333], [8494, 20043], [12954], [25303], [3315], [14163, 7741], [20327], [20101], [22124], [22581], [8075], [17529], [2026, 16643, 10451], [15743], [18996, 4691], [4867], [11808], [3019], [3267, 2094], [20355], [19029], [19029, 3064], [6583, 8557, 3560], [23927], [11265, 14971, 6313], [11265, 16961], [4997], [11265, 20697, 7730], [15486], [11265, 17460], [9113, 2094], [10627], [6091], [12531], [6091, 2791], [24524, 23402, 3372], [5658, 14782], [8699], [21083], [3835], [20810], [2512, 8671, 2666, 2546], [2512, 18598, 6651], [2512, 18598, 4630], [2512, 9006, 22930, 9080], [2512, 9006, 24759, 2937, 2102], [2512, 24759, 17854, 2098], [2512, 5054, 19570, 2389], [3671], [4451, 2100], [16839, 9080, 12863], [16839, 2100], [15903], [15578, 10265, 3372], [4874, 2075], [22224], [7863], [14723], [27885, 3669, 4726], [18333], [27885, 8043, 18941], [14158], [15896], [27885, 16643, 12556], [4548], [5976], [21045, 3560], [2125], [15807], [5805], [13958, 2989], [3100], [2006], [2330], [2330, 2791], [4941], [4559, 2389], [6728, 19811], [27451], [21931], [13063], [13892, 2594], [2030, 27415], [15068, 2818], [2041], [27719], [2041, 26775, 2100], [2041, 2098], [2041, 3122, 4509], [19006], [23558], [22430], [2058, 4783, 22397], [2058, 10288, 17847, 2094], [2058, 24793, 2098], [28604], [15849, 16344, 5575], [13394], [2058, 6198, 2098], [2058, 13088, 10593, 2102], [3255], [22295], [9145], [16267], [6634], [16035], [6634, 4801], [22348], [19810], [13459], [13135], [11752], [5776], [9161, 6026], [8724], [20490], [9379], [14099], [16740], [21392, 7178], [21392, 24968], [25636, 3512], [27233, 7685], [2566, 28687], [2566, 8873, 20617, 2271], [2566, 4801], [2566, 19386, 2098], [2566, 19386, 2075], [14516], [16115, 3468], [2566, 20689, 8270], [2566, 16070], [21877, 5874], [21877, 18719, 26725], [21877, 18719, 23738, 2594], [20739, 6850], [9964, 2075], [9004, 22618], [11612], [9004, 7068, 3372], [3856], [14628], [18521], [25020], [14255, 4226, 2094], [9421], [6770, 19210], [6770, 18424], [12063], [12063, 2075], [20228, 19629, 3064], [20228, 19629, 3508], [20228, 6305, 3593], [5810], [5810, 6024], [4041], [18378], [22608], [16418], [8242], [7537], [24820], [22512, 23086], [5165], [5165, 2094], [20228, 2937, 2102], [20699], [13433, 25593], [4197], [22303], [13205], [13433, 8737, 3560], [29211], [29211, 2075], [13433, 17686], [3769], [20540], [3893], [13433, 28032, 7730], [4298], [13433, 4904], [13433, 20807], [13433, 4904, 2100], [3928], [25192], [26418, 2075], [3653, 10010, 6313], [21659], [18024, 2094], [23962], [4810], [8225], [12097], [3653, 6528, 20771], [6620, 3993], [26927, 13871, 4509], [3539, 2094], [2797], [6364], [14848, 2075], [7098], [26422], [27895], [19157], [4013, 22776], [29198, 2075], [18224], [27756], [22900, 4509], [16405, 11124, 2571], [16405, 16989, 18436], [14248], [16385, 2075], [26136, 13043], [7196], [17022, 4509], [3800, 3993], [21954], [2404], [5128], [14909], [11989, 3672], [24209, 2389, 5244], [26260, 14045], [10861, 3022, 2100], [10861, 19282], [21068], [11242], [11242, 2135], [4251], [4251, 2791], [27565], [21864, 15952], [19461, 27586, 2389], [10958, 17062], [14513, 2098], [23751], [7385], [28374], [14202], [17559], [2743, 27108, 3560], [9744], [9680, 2102], [19252], [16806, 3070], [22643], [3201], [12393], [26350], [22614], [2128, 8569, 3489], [21195], [28667, 22048], [18555], [27429], [27429, 2075], [10842], [9185], [21346], [25416, 5313, 11461], [11193], [9038], [9038, 3993], [5837], [21936], [13893], [2128, 5558, 23553], [23370], [8363], [21660], [4335], [7653], [2128, 3669, 7178], [11542], [11206], [23124], [23124, 3993], [16360, 21148], [16360, 16119], [16360, 3217, 6776], [16360, 3217, 6776, 3993], [16360, 15916, 7229, 3401], [16360, 15916, 16885], [24571], [16360, 23316], [24501, 4765], [24501, 4765, 3993], [24501, 26951], [20234], [9235], [8172], [5295], [24501, 18622, 10127], [5012], [13070], [9507, 4765], [22363], [24501, 4747, 10421], [10395], [26651], [2717, 3993], [8345], [15035], [15035, 2791], [19868], [19355], [2128, 9080, 15370], [2128, 9080, 2401, 7062], [2128, 15222, 8950, 2075], [2128, 4588, 10127], [2128, 4588, 4765], [7195, 3993], [7065, 7869, 3372], [10073, 2098], [7065, 23316], [19556], [11841], [15544, 3709], [11421, 3560], [15544, 19510, 2098], [11950], [20996, 25698, 4095], [25223, 3709], [5931], [27384, 2094], [12726], [13413, 3993], [21766, 28579], [19379, 19185], [18399, 3709], [18101], [6517], [6517, 4181], [6517, 24589], [6517, 6553], [12039], [16183, 20113], [26308, 3436], [2624, 6593, 16339, 27678, 2271], [22856], [6369, 20023, 2063], [20066, 7685], [20954], [22473], [18906, 5280, 2594], [21871, 6508], [2938, 2098], [2938, 15070], [17251], [9967], [8510], [13225], [14784, 3170], [7842, 14194, 2100], [9576], [9446, 3550], [12665], [6015], [12459], [7932], [8040, 16102, 2368, 19699, 13765, 3207], [8040, 29122, 2075], [8040, 7245, 2121], [8040, 7245, 2075], [8040, 9691], [8040, 23846, 2094], [8040, 9691, 3993], [19981], [19981, 2075], [6978], [7491], [8040, 22134, 5498, 6774], [10203], [6575], [28607], [28607, 2135], [5851], [7367, 13701], [26962], [23182], [2156, 20744], [2969], [18753], [23069], [25388], [3809], [27994], [14262, 14762, 2063], [2275], [5729], [21146, 14075], [22824], [16697], [15311], [9467], [9467, 2094], [9467, 12172, 2094], [9467, 3993], [9467, 3238], [4629], [8351, 4509], [8351, 4509, 2791], [5806, 2098], [5670, 2100], [5213], [7135], [16880], [16880, 2135], [3184], [11245], [11273], [14021, 15603, 2094], [11004], [11004, 2791], [5305], [5305, 2368], [5305, 6675], [6682], [25030], [4333], [9033, 21202, 7971], [10021], [21934, 27851], [21934, 4842], [21934, 4842, 2075], [3722], [17839], [18006], [8254, 3993], [4823], [16491], [16491, 2135], [9033, 6774], [15315, 23606, 2594], [18386], [18386, 2135], [27936], [11080, 2100], [8301, 6916, 4095], [19840], [22889, 5243, 9096], [17056], [13554], [10453, 2232, 3993], [4030], [23667, 17701, 2232], [18230], [15488, 27292, 2100], [6047], [14368], [2868], [27420], [5629], [15081], [15081, 2075], [15488, 11614, 7999], [15488, 9541, 8450], [5744], [20673], [20673, 2791], [7488], [10245, 7685], [1055, 11802, 4801], [24845], [15022], [24845, 2075], [24845, 2100], [13583, 2100], [1055, 19755], [1055, 19755, 2075], [1055, 24045, 4371], [1055, 24045, 6774], [1055, 13542, 2121], [1055, 13542, 7999], [1055, 3490, 3207], [1055, 25518, 4590, 2075], [1055, 3490, 15985, 2075], [1055, 25083, 18477, 2232], [1055, 25083, 3762], [1055, 3630, 4140, 2100], [1055, 17048, 3723], [27084, 19210], [3730], [19487], [14017, 28775, 24826, 2015], [14348], [22560], [28587], [28587, 2135], [2061, 2213, 3630, 16136], [28384, 2094], [14699], [14038], [14038, 3993], [3374], [14768], [19835], [12403, 6129], [12403, 10074], [4092], [28699, 6313], [23250], [25146], [2985], [24462], [4382, 3238], [8741], [8741, 3993], [19582], [11867, 14659, 2098], [5490, 5657, 10631, 4095], [14648], [23883], [6237], [4582], [3340, 16344, 12722], [2318], [9696], [2110, 2135], [26261, 4215, 24333], [6706], [22150, 2100], [21734], [19986], [3886, 2075], [3886, 2100], [8665], [10551], [27146], [2358, 10128, 2989], [2145], [29435], [25194], [27136, 2100], [13551], [2358, 19419], [2358, 19419, 2389], [2358, 10893, 2094], [2962, 2094], [4040, 2075], [24166], [18890], [3442], [12250], [4326], [13233], [16654], [9384], [2844], [4930], [14205], [14205, 2791], [2996, 2271], [5702], [22475, 2098], [19280], [9860], [24646, 5051, 7011, 7542], [24646, 5051, 10451], [24646, 5051, 12031], [5236], [24646, 17822, 3560], [10514, 10696], [20442], [28341], [4942, 27876], [6114], [6592, 3512], [21396, 6834], [21396, 4801], [21396, 7770], [21396, 7770, 2791], [11559], [6020], [19113], [13712], [16081, 2075], [16341], [2469], [7505, 2135], [4474], [4527], [11341], [10889], [7505, 2890, 13876, 25090, 3560], [8343], [8343, 2075], [23873], [10928], [10027], [21501], [10027, 2791], [25430, 27609, 2075], [25082], [13026], [25353, 8737, 25457, 6774], [11883], [11937, 26243, 14287], [2831, 8082], [3331], [9092, 9080, 3550], [16985, 2102], [5510, 3993], [11937, 4779, 2989], [19982, 3372], [29442], [21642], [7697, 3993], [7697, 2100], [18381], [12216], [22148], [22553], [22553, 8918], [16312], [2702, 20113], [8616], [24605], [9049], [15225], [6980], [19943], [10215], [7404], [7404, 3550], [7404, 6026], [28774, 3366], [3231, 2100], [8915, 10649, 2100], [18836], [3241], [2245], [16465], [16465, 2791], [5081], [5561], [8701], [16082], [6908], [8505, 3367, 29314], [28409], [26019], [16356, 3709], [5079], [7563, 2098], [4389], [4389, 15000, 5669], [5199, 3593], [5199, 3593, 2135], [5199, 3593, 2791], [5458], [5458, 2135], [5458, 2791], [14841, 28345, 4383], [23691], [4416], [29026], [5028], [7823], [9121, 2075], [13800], [13800, 2389], [25283, 26147], [25283, 26147, 3012], [9099, 23901], [12603, 23355], [10226], [29461, 23267], [29461, 23267, 3370], [12225, 3334], [24026], [25251], [11587], [13460, 8462], [19817, 7140, 9709], [19836], [3404, 13966], [10722, 12274, 7096, 8918], [22609], [5519, 2243, 2135], [8529, 10024, 3351], [8529, 10024, 3351, 3560], [24720], [14477, 23806, 4383], [14477, 7606, 2098], [14477, 9397, 2890, 7405, 6024], [14477, 9397, 3217, 6776, 3085], [14477, 18116, 6024], [14477, 4757, 24270], [11499], [4895, 8671, 2666, 2546], [23653], [4895, 8671, 25587], [4895, 18384, 27190], [4895, 10010, 2075], [9662], [9662, 2135], [12503], [4895, 6895, 14762], [8796], [4895, 9006, 22930, 3064], [4895, 9006, 23041, 25184], [4895, 9006, 28139, 22342, 2075], [4895, 9006, 25013], [4895, 8663, 17119, 7228], [4895, 8663, 20740, 3372], [4895, 8663, 6371, 11788], [4895, 3597, 25918, 8082], [4895, 10841, 9488, 2271], [6151, 8586, 14097], [2104, 11774, 2098], [4824], [6151, 2229, 7895, 3468], [27880], [16655, 21369, 2135], [16655, 21369, 2791], [15491], [16655, 18938, 19301], [16655, 3372, 9825, 7951, 4588], [16655, 2595, 17847, 2094], [9223], [16261], [4895, 27753, 23393, 3085], [4895, 7011, 5422], [4895, 7959, 18809, 2290], [4895, 14876, 7874, 2098], [4895, 29278, 6810, 2368], [4895, 29278, 23795], [4895, 15628, 18935], [15140], [4895, 19699, 9013, 18718], [12511], [4895, 12053, 2098], [4895, 5714, 19811], [4895, 2378, 29021], [4895, 7076, 21649], [4895, 18447, 18702, 3064], [4895, 2378, 6767, 26832], [4310], [4406, 3085], [4895, 5302, 7178], [4895, 3678, 7178], [16010], [4895, 28139, 19362, 2098], [4895, 15549, 3388], [4895, 16416, 15277], [4895, 6072, 16116], [16591, 27361], [4895, 26919, 3709], [4895, 16846, 2483, 10451], [4895, 21678, 3709], [4895, 6499, 7405, 3468], [4895, 13102, 25508, 2075], [25982], [4895, 3367, 15532, 2290], [7736], [12422], [4895, 26210, 18098, 5084], [4895, 13203, 5051, 11873], [4895, 26760, 4710, 2098], [4895, 6508, 8737, 8988, 16530], [22154], [4895, 13181, 12083, 3709], [4895, 24669, 2075], [18162], [4895, 16535, 4892], [4895, 8545, 22499, 6562], [4895, 4381], [15175], [4895, 10139, 14273, 2075], [2039], [27999], [2039, 26644], [2039, 23270, 2100], [6314], [2039, 26143], [11809], [10030], [12436, 10841, 3560], [3158, 15549, 14740], [2310, 29122, 4765], [2310, 15465, 3993], [15779, 3560], [2310, 2595], [2310, 18684, 3508], [2310, 19068], [13925], [13846], [6819, 20142, 4630], [25047], [12700, 3560], [19354, 29201, 3512], [4808], [6355], [17947, 3560], [6819, 8525, 4842, 8082], [5554], [5554, 3550], [29364], [18130], [8211], [11333, 17413], [3403], [2359], [5782], [2215, 2239], [2162, 9961], [4010], [15705], [13842], [3422], [3422, 3993], [3666], [4400, 4892], [4929, 9961], [16040], [19750], [6881], [6160], [18066], [3649], [28544, 2075], [1059, 14341, 19570, 2389], [7204], [13300], [2317], [10433], [3748], [2097, 3993], [5627], [19863, 2100], [16837], [17502], [4299, 3993], [15536, 3367, 3993], [15536, 3367, 7699], [10632], [9633], [2007, 24850], [2007, 23410], [24185, 2063], [24185, 12879, 5313], [4687], [6603], [4687, 3672], [12121, 2100], [15854, 9096], [6247], [5191], [24185, 18752, 14045], [4737], [15366], [15366, 2135], [5303], [10166], [14532, 3993], [14532, 7699], [18480], [23277, 29574], [3308, 2098], [23277, 14573], [24639], [8038, 7962], [8038, 7962, 2075], [29479], [14315], [13175], [21336], [9805, 3600], [23564, 4890], [27838, 23067, 2271], [16729], [4224, 2094]]\n",
      "['aba', '##shed']\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the tokens as belonging to sentence \"0\" or \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "# segments_ids = [0,0,0]\n",
    "print (segments_ids)\n",
    "print(indexed_tokens)\n",
    "print(tokenized_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 1 at dim 2 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-cadda5ea5c45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert inputs to PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexed_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msegments_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegments_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load pre-trained model (weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 1 at dim 2 (got 2)"
     ]
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n",
      "Number of batches: 1\n",
      "Number of tokens: 2\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 1\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For our token, select its feature values from layer 5.\n",
    "token_i = 1\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "# print(vec)\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Type of encoded_layers:  <class 'list'>\n",
      "Tensor shape for each layer:  torch.Size([1, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# `encoded_layers` is a Python list.\n",
    "print('     Type of encoded_layers: ', type(encoded_layers))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', encoded_layers[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 3, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 3, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 3 x 3072\n",
      "tensor([-0.0637,  0.3612, -0.0899,  ..., -0.4850, -0.3801,  0.8791])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 3 x 768\n"
     ]
    }
   ],
   "source": [
    "# Sum the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 3 x 3072\n",
      "tensor([-0.0637,  0.3612, -0.0899,  ..., -0.4850, -0.3801,  0.8791])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_first = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[0], token[1], token[2], token[3]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_first.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_first), len(token_vecs_cat_first[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 3 x 768\n"
     ]
    }
   ],
   "source": [
    "# Sum the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_first = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[:4], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_first.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_first), len(token_vecs_sum_first[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 3 x 3072\n",
      "tensor([-0.0637,  0.3612, -0.0899,  ..., -0.4850, -0.3801,  0.8791])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_middle1 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[4], token[5], token[6], token[7]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_middle1.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_middle1), len(token_vecs_cat_middle1[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 3 x 768\n"
     ]
    }
   ],
   "source": [
    "# Sum the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_middle1 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[4:8], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_middle1.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_middle1), len(token_vecs_sum_middle1[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 3 x 3072\n",
      "tensor([-0.0637,  0.3612, -0.0899,  ..., -0.4850, -0.3801,  0.8791])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_middle2 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[8], token[9], token[10], token[11]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_middle2.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_middle2), len(token_vecs_cat_middle2[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 3 x 768\n"
     ]
    }
   ],
   "source": [
    "# Sum the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_middle2 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[8:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_middle2.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_middle2), len(token_vecs_sum_middle2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 3 x 9216\n",
      "tensor([-0.0637,  0.3612, -0.0899,  ..., -0.4850, -0.3801,  0.8791])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_all = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[0], token[1], token[2], token[3], token[4], token[5], token[6], token[7], token[8], token[9], token[10], token[11]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_all.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_all), len(token_vecs_cat_all[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 3 x 768\n"
     ]
    }
   ],
   "source": [
    "# Sum the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_all = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_all.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_all), len(token_vecs_sum_all[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: torch.Size([768])\n",
      "tensor(-0.1680)\n",
      "tensor(-0.1680)\n",
      "Shape of sentences vector is: 768\n",
      "tensor(-0.1680)\n"
     ]
    }
   ],
   "source": [
    "# Make a single vector to represent the pair of sentences by averaging across tokens.\n",
    "# `encoded_layers` has shape [12 x 1 x 22 x 768]\n",
    "sentences_vec = []\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = encoded_layers[11][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "for s in sentence_embedding:\n",
    "    sentences_vec.append(s)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n",
    "print(sentence_embedding[767])\n",
    "print(sentence_embedding[-1])\n",
    "print(f'Shape of sentences vector is: {len(sentences_vec)}')\n",
    "print(sentences_vec[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------------\n",
    "BEGIN TESTING STATIC CONTEXTUAL EMBEDDING CREATION\n",
    "--------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2189 words in the vocabulary.\n",
      "\n",
      "It took 0.001582167111337185 seconds to read the vocabulary file into memory.\n",
      "Test word is abhor.\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "vocab = []\n",
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/vocab_files/vocab_checked.txt'\n",
    "with open(vocab_file, 'r') as v:\n",
    "    vocab = v.read().splitlines()\n",
    "end = timer()\n",
    "run_time = end - start\n",
    "print(f'There are {len(vocab)} words in the vocabulary.\\n')\n",
    "print(f'It took {run_time} seconds to read the vocabulary file into memory.')\n",
    "print(f'Test word is {vocab[2]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added ['aback'] to the tokenized_text array.\n",
      "aback        26,575\n",
      "Added ['aba', '##shed'] to the tokenized_text array.\n",
      "aba          19,557\n",
      "##shed       14,740\n",
      "Added ['ab', '##hor'] to the tokenized_text array.\n",
      "ab           11,113\n",
      "##hor        16,368\n",
      "Added ['ab', '##hor', '##red'] to the tokenized_text array.\n",
      "ab           11,113\n",
      "##hor        16,368\n",
      "##red         5,596\n",
      "Added ['ab', '##hor', '##rence'] to the tokenized_text array.\n",
      "ab           11,113\n",
      "##hor        16,368\n",
      "##rence      24,413\n",
      "Added ['ab', '##hor', '##rent'] to the tokenized_text array.\n",
      "ab           11,113\n",
      "##hor        16,368\n",
      "##rent       22,787\n",
      "Added ['ab', '##omi', '##nable'] to the tokenized_text array.\n",
      "ab           11,113\n",
      "##omi        20,936\n",
      "##nable      22,966\n",
      "Added ['ab', '##ound'] to the tokenized_text array.\n",
      "ab           11,113\n",
      "##ound       28,819\n",
      "Added ['absent'] to the tokenized_text array.\n",
      "absent        9,962\n",
      "Added ['absorbed'] to the tokenized_text array.\n",
      "absorbed      9,063\n",
      "Added ['acceptance'] to the tokenized_text array.\n",
      "acceptance    9,920\n",
      "Added ['accepted'] to the tokenized_text array.\n",
      "accepted      3,970\n",
      "Added ['accepting'] to the tokenized_text array.\n",
      "accepting    10,564\n",
      "Added ['acc', '##om', '##mo', '##dating'] to the tokenized_text array.\n",
      "acc          16,222\n",
      "##om          5,358\n",
      "##mo          5,302\n",
      "##dating     16,616\n",
      "Added ['accomplished'] to the tokenized_text array.\n",
      "accomplished  8,885\n",
      "Added ['accord', '##ant'] to the tokenized_text array.\n",
      "accord       15,802\n",
      "##ant         4,630\n",
      "Added ['acc', '##urse', '##d'] to the tokenized_text array.\n",
      "acc          16,222\n",
      "##urse       28,393\n",
      "##d           2,094\n",
      "Added ['acc', '##usa', '##tory'] to the tokenized_text array.\n",
      "acc          16,222\n",
      "##usa        10,383\n",
      "##tory        7,062\n",
      "Added ['accused'] to the tokenized_text array.\n",
      "accused       5,496\n",
      "Added ['accusing'] to the tokenized_text array.\n",
      "accusing     16,723\n",
      "Added ['ace', '##rb', '##ic'] to the tokenized_text array.\n",
      "ace           9,078\n",
      "##rb         15,185\n",
      "##ic          2,594\n",
      "Added ['acidic'] to the tokenized_text array.\n",
      "acidic       24,171\n",
      "Added ['active'] to the tokenized_text array.\n",
      "active        3,161\n",
      "Added ['acute'] to the tokenized_text array.\n",
      "acute        11,325\n",
      "Added ['adamant'] to the tokenized_text array.\n",
      "adamant      29,502\n",
      "Added ['add', '##led'] to the tokenized_text array.\n",
      "add           5,587\n",
      "##led         3,709\n",
      "Added ['admiration'] to the tokenized_text array.\n",
      "admiration   17,005\n",
      "Added ['admit'] to the tokenized_text array.\n",
      "admit         6,449\n",
      "Added ['ad', '##oration'] to the tokenized_text array.\n",
      "ad            4,748\n",
      "##oration    21,223\n",
      "Added ['ad', '##orin', '##g'] to the tokenized_text array.\n",
      "ad            4,748\n",
      "##orin       28,741\n",
      "##g           2,290\n",
      "Added ['ad', '##rift'] to the tokenized_text array.\n",
      "ad            4,748\n",
      "##rift       16,338\n",
      "Added ['ad', '##vers', '##aria', '##l'] to the tokenized_text array.\n",
      "ad            4,748\n",
      "##vers       14,028\n",
      "##aria       10,980\n",
      "##l           2,140\n",
      "Added ['af', '##fa', '##bility'] to the tokenized_text array.\n",
      "af           21,358\n",
      "##fa          7,011\n",
      "##bility      8,553\n",
      "Added ['affected'] to the tokenized_text array.\n",
      "affected      5,360\n",
      "Added ['affection', '##ate'] to the tokenized_text array.\n",
      "affection    12,242\n",
      "##ate         3,686\n",
      "Added ['af', '##flict', '##ed'] to the tokenized_text array.\n",
      "af           21,358\n",
      "##flict      29,301\n",
      "##ed          2,098\n",
      "Added ['af', '##front', '##ed'] to the tokenized_text array.\n",
      "af           21,358\n",
      "##front      12,792\n",
      "##ed          2,098\n",
      "Added ['afl', '##utter'] to the tokenized_text array.\n",
      "afl          10,028\n",
      "##utter      26,878\n",
      "Added ['afraid'] to the tokenized_text array.\n",
      "afraid        4,452\n",
      "Added ['ag', '##ape'] to the tokenized_text array.\n",
      "ag           12,943\n",
      "##ape        24,065\n",
      "Added ['aggravated'] to the tokenized_text array.\n",
      "aggravated   25,817\n",
      "Added ['ag', '##gra', '##vation'] to the tokenized_text array.\n",
      "ag           12,943\n",
      "##gra        17,643\n",
      "##vation     21,596\n",
      "Added ['aggression'] to the tokenized_text array.\n",
      "aggression   14,974\n",
      "Added ['aggressive'] to the tokenized_text array.\n",
      "aggressive    9,376\n",
      "Added ['ag', '##gr', '##ie', '##ve'] to the tokenized_text array.\n",
      "ag           12,943\n",
      "##gr         16,523\n",
      "##ie          2,666\n",
      "##ve          3,726\n",
      "Added ['ag', '##gr', '##ie', '##ved'] to the tokenized_text array.\n",
      "ag           12,943\n",
      "##gr         16,523\n",
      "##ie          2,666\n",
      "##ved         7,178\n",
      "Added ['ag', '##has', '##t'] to the tokenized_text array.\n",
      "ag           12,943\n",
      "##has        14,949\n",
      "##t           2,102\n",
      "Added ['agitated'] to the tokenized_text array.\n",
      "agitated     21,568\n",
      "Added ['ago', '##g'] to the tokenized_text array.\n",
      "ago           3,283\n",
      "##g           2,290\n",
      "Added ['ago', '##ni', '##zed'] to the tokenized_text array.\n",
      "ago           3,283\n",
      "##ni          3,490\n",
      "##zed         5,422\n",
      "Added ['agree', '##able'] to the tokenized_text array.\n",
      "agree         5,993\n",
      "##able        3,085\n",
      "Added ['ag', '##ress', '##ive'] to the tokenized_text array.\n",
      "ag           12,943\n",
      "##ress        8,303\n",
      "##ive         3,512\n",
      "Added ['air', '##head'] to the tokenized_text array.\n",
      "air           2,250\n",
      "##head        4,974\n",
      "Added ['alarm'] to the tokenized_text array.\n",
      "alarm         8,598\n",
      "Added ['alarmed'] to the tokenized_text array.\n",
      "alarmed      19,260\n",
      "Added ['alarm', '##ing'] to the tokenized_text array.\n",
      "alarm         8,598\n",
      "##ing         2,075\n",
      "Added ['alert'] to the tokenized_text array.\n",
      "alert         9,499\n",
      "Added ['alerted'] to the tokenized_text array.\n",
      "alerted      22,333\n",
      "Added ['alien', '##ated'] to the tokenized_text array.\n",
      "alien         7,344\n",
      "##ated        4,383\n",
      "Added ['allergic'] to the tokenized_text array.\n",
      "allergic     27,395\n",
      "Added ['alleviate', '##d'] to the tokenized_text array.\n",
      "alleviate    24,251\n",
      "##d           2,094\n",
      "Added ['all', '##uring'] to the tokenized_text array.\n",
      "all           2,035\n",
      "##uring      12,228\n",
      "Added ['al', '##oof'] to the tokenized_text array.\n",
      "al            2,632\n",
      "##oof        21,511\n",
      "Added ['ama', '##tory'] to the tokenized_text array.\n",
      "ama          25,933\n",
      "##tory        7,062\n",
      "Added ['amazed'] to the tokenized_text array.\n",
      "amazed       15,261\n",
      "Added ['amazement'] to the tokenized_text array.\n",
      "amazement    21,606\n",
      "Added ['amazing'] to the tokenized_text array.\n",
      "amazing       6,429\n",
      "Added ['ambition'] to the tokenized_text array.\n",
      "ambition     16,290\n",
      "Added ['ambitious'] to the tokenized_text array.\n",
      "ambitious    12,479\n",
      "Added ['am', '##bi', '##vale', '##nce'] to the tokenized_text array.\n",
      "am            2,572\n",
      "##bi          5,638\n",
      "##vale       17,479\n",
      "##nce         5,897\n",
      "Added ['am', '##bi', '##valent'] to the tokenized_text array.\n",
      "am            2,572\n",
      "##bi          5,638\n",
      "##valent     24,879\n",
      "Added ['am', '##ena', '##ble'] to the tokenized_text array.\n",
      "am            2,572\n",
      "##ena         8,189\n",
      "##ble         3,468\n",
      "Added ['ami', '##able'] to the tokenized_text array.\n",
      "ami          26,445\n",
      "##able        3,085\n",
      "Added ['ami', '##cable'] to the tokenized_text array.\n",
      "ami          26,445\n",
      "##cable      21,170\n",
      "Added ['amused'] to the tokenized_text array.\n",
      "amused       11,770\n",
      "Added ['amusement'] to the tokenized_text array.\n",
      "amusement     9,778\n",
      "Added ['analytical'] to the tokenized_text array.\n",
      "analytical   17,826\n",
      "Added ['analyzing'] to the tokenized_text array.\n",
      "analyzing    20,253\n",
      "Added ['anger'] to the tokenized_text array.\n",
      "anger         4,963\n",
      "Added ['angered'] to the tokenized_text array.\n",
      "angered      18,748\n",
      "Added ['angrily'] to the tokenized_text array.\n",
      "angrily      14,136\n",
      "Added ['angry'] to the tokenized_text array.\n",
      "angry         4,854\n",
      "Added ['ang', '##st'] to the tokenized_text array.\n",
      "ang          17,076\n",
      "##st          3,367\n",
      "Added ['anguish'] to the tokenized_text array.\n",
      "anguish      21,782\n",
      "Added ['anguish', '##ed'] to the tokenized_text array.\n",
      "anguish      21,782\n",
      "##ed          2,098\n",
      "Added ['animated'] to the tokenized_text array.\n",
      "animated      6,579\n",
      "Added ['an', '##imo', '##sity'] to the tokenized_text array.\n",
      "an            2,019\n",
      "##imo        16,339\n",
      "##sity       17,759\n",
      "Added ['annoyance'] to the tokenized_text array.\n",
      "annoyance    17,466\n",
      "Added ['annoyed'] to the tokenized_text array.\n",
      "annoyed      11,654\n",
      "Added ['annoying'] to the tokenized_text array.\n",
      "annoying     15,703\n",
      "Added ['antagonist', '##ic'] to the tokenized_text array.\n",
      "antagonist   17,379\n",
      "##ic          2,594\n",
      "Added ['ant', '##ago', '##ni', '##zed'] to the tokenized_text array.\n",
      "ant          14,405\n",
      "##ago        23,692\n",
      "##ni          3,490\n",
      "##zed         5,422\n",
      "Added ['anticipated'] to the tokenized_text array.\n",
      "anticipated  11,436\n",
      "Added ['anticipating'] to the tokenized_text array.\n",
      "anticipating 26,481\n",
      "Added ['anticipation'] to the tokenized_text array.\n",
      "anticipation 11,162\n",
      "Added ['anti', '##ci', '##pati', '##ve'] to the tokenized_text array.\n",
      "anti          3,424\n",
      "##ci          6,895\n",
      "##pati       24,952\n",
      "##ve          3,726\n",
      "Added ['anti', '##ci', '##pa', '##tory'] to the tokenized_text array.\n",
      "anti          3,424\n",
      "##ci          6,895\n",
      "##pa          4,502\n",
      "##tory        7,062\n",
      "Added ['anti', '##pathy'] to the tokenized_text array.\n",
      "anti          3,424\n",
      "##pathy      20,166\n",
      "Added ['ants', '##y'] to the tokenized_text array.\n",
      "ants         16,111\n",
      "##y           2,100\n",
      "Added ['anxiety'] to the tokenized_text array.\n",
      "anxiety      10,089\n",
      "Added ['anxious'] to the tokenized_text array.\n",
      "anxious      11,480\n",
      "Added ['anxiously'] to the tokenized_text array.\n",
      "anxiously    23,403\n",
      "Added ['ap', '##ath', '##etic'] to the tokenized_text array.\n",
      "ap            9,706\n",
      "##ath         8,988\n",
      "##etic       16,530\n",
      "Added ['ap', '##athy'] to the tokenized_text array.\n",
      "ap            9,706\n",
      "##athy       17,308\n",
      "Added ['apologetic'] to the tokenized_text array.\n",
      "apologetic   29,352\n",
      "Added ['appalled'] to the tokenized_text array.\n",
      "appalled     29,279\n",
      "Added ['app', '##all', '##ingly'] to the tokenized_text array.\n",
      "app          10,439\n",
      "##all         8,095\n",
      "##ingly      15,787\n",
      "Added ['app', '##eased'] to the tokenized_text array.\n",
      "app          10,439\n",
      "##eased      25,063\n",
      "Added ['app', '##ea', '##sing'] to the tokenized_text array.\n",
      "app          10,439\n",
      "##ea          5,243\n",
      "##sing        7,741\n",
      "Added ['app', '##re', '##cia', '##tive'] to the tokenized_text array.\n",
      "app          10,439\n",
      "##re          2,890\n",
      "##cia         7,405\n",
      "##tive        6,024\n",
      "Added ['apprehension'] to the tokenized_text array.\n",
      "apprehension 25,809\n",
      "Added ['app', '##re', '##hen', '##sive'] to the tokenized_text array.\n",
      "app          10,439\n",
      "##re          2,890\n",
      "##hen        10,222\n",
      "##sive       12,742\n",
      "Added ['approve'] to the tokenized_text array.\n",
      "approve      14,300\n",
      "Added ['approved'] to the tokenized_text array.\n",
      "approved      4,844\n",
      "Added ['app', '##roving'] to the tokenized_text array.\n",
      "app          10,439\n",
      "##roving     22,046\n",
      "Added ['argue'] to the tokenized_text array.\n",
      "argue         7,475\n",
      "Added ['argument', '##ative'] to the tokenized_text array.\n",
      "argument      6,685\n",
      "##ative       8,082\n",
      "Added ['aroused'] to the tokenized_text array.\n",
      "aroused      18,391\n",
      "Added ['arrogance'] to the tokenized_text array.\n",
      "arrogance    24,416\n",
      "Added ['arrogant'] to the tokenized_text array.\n",
      "arrogant     15,818\n",
      "Added ['arrogant', '##ly'] to the tokenized_text array.\n",
      "arrogant     15,818\n",
      "##ly          2,135\n",
      "Added ['artificial'] to the tokenized_text array.\n",
      "artificial    7,976\n",
      "Added ['ashamed'] to the tokenized_text array.\n",
      "ashamed      14,984\n",
      "Added ['aspiring'] to the tokenized_text array.\n",
      "aspiring     22,344\n",
      "Added ['assert', '##ive'] to the tokenized_text array.\n",
      "assert       20,865\n",
      "##ive         3,512\n",
      "Added ['assert', '##ively'] to the tokenized_text array.\n",
      "assert       20,865\n",
      "##ively      14,547\n",
      "Added ['assessing'] to the tokenized_text array.\n",
      "assessing    20,077\n",
      "Added ['assured'] to the tokenized_text array.\n",
      "assured       8,916\n",
      "Added ['astonished'] to the tokenized_text array.\n",
      "astonished   22,741\n",
      "Added ['astonishment'] to the tokenized_text array.\n",
      "astonishment 23,819\n",
      "Added ['as', '##tou', '##nded'] to the tokenized_text array.\n",
      "as            2,004\n",
      "##tou        24,826\n",
      "##nded       25,848\n",
      "Added ['attempting'] to the tokenized_text array.\n",
      "attempting    7,161\n",
      "Added ['at', '##ten', '##tive'] to the tokenized_text array.\n",
      "at            2,012\n",
      "##ten         6,528\n",
      "##tive        6,024\n",
      "Added ['at', '##ten', '##tive', '##ness'] to the tokenized_text array.\n",
      "at            2,012\n",
      "##ten         6,528\n",
      "##tive        6,024\n",
      "##ness        2,791\n",
      "Added ['attracted'] to the tokenized_text array.\n",
      "attracted     6,296\n",
      "Added ['ave', '##nging'] to the tokenized_text array.\n",
      "ave          13,642\n",
      "##nging      22,373\n",
      "Added ['ave', '##rse'] to the tokenized_text array.\n",
      "ave          13,642\n",
      "##rse        22,573\n",
      "Added ['ave', '##rs', '##ion'] to the tokenized_text array.\n",
      "ave          13,642\n",
      "##rs          2,869\n",
      "##ion         3,258\n",
      "Added ['ave', '##rs', '##ive'] to the tokenized_text array.\n",
      "ave          13,642\n",
      "##rs          2,869\n",
      "##ive         3,512\n",
      "Added ['avid'] to the tokenized_text array.\n",
      "avid         18,568\n",
      "Added ['avoiding'] to the tokenized_text array.\n",
      "avoiding      9,992\n",
      "Added ['awaiting'] to the tokenized_text array.\n",
      "awaiting     15,497\n",
      "Added ['awakened'] to the tokenized_text array.\n",
      "awakened     20,256\n",
      "Added ['aware'] to the tokenized_text array.\n",
      "aware         5,204\n",
      "Added ['awareness'] to the tokenized_text array.\n",
      "awareness     7,073\n",
      "Added ['awe'] to the tokenized_text array.\n",
      "awe          15,180\n",
      "Added ['awe', '##d'] to the tokenized_text array.\n",
      "awe          15,180\n",
      "##d           2,094\n",
      "Added ['awe', '##st', '##ruck'] to the tokenized_text array.\n",
      "awe          15,180\n",
      "##st          3,367\n",
      "##ruck       29,314\n",
      "Added ['awful'] to the tokenized_text array.\n",
      "awful         9,643\n",
      "Added ['awkward'] to the tokenized_text array.\n",
      "awkward       9,596\n",
      "Added ['awkward', '##ness'] to the tokenized_text array.\n",
      "awkward       9,596\n",
      "##ness        2,791\n",
      "Added ['axe', '##d'] to the tokenized_text array.\n",
      "axe          12,946\n",
      "##d           2,094\n",
      "Added ['back', '##hand', '##ed'] to the tokenized_text array.\n",
      "back          2,067\n",
      "##hand       11,774\n",
      "##ed          2,098\n",
      "Added ['badly'] to the tokenized_text array.\n",
      "badly         6,649\n",
      "Added ['ba', '##ffle'] to the tokenized_text array.\n",
      "ba            8,670\n",
      "##ffle       18,142\n",
      "Added ['baffled'] to the tokenized_text array.\n",
      "baffled      29,088\n",
      "Added ['ba', '##ff', '##ling'] to the tokenized_text array.\n",
      "ba            8,670\n",
      "##ff          4,246\n",
      "##ling        2,989\n",
      "Added ['baked'] to the tokenized_text array.\n",
      "baked        17,776\n",
      "Added ['ban', '##al'] to the tokenized_text array.\n",
      "ban           7,221\n",
      "##al          2,389\n",
      "Added ['barking'] to the tokenized_text array.\n",
      "barking      19,372\n",
      "Added ['bash', '##ful'] to the tokenized_text array.\n",
      "bash         24,234\n",
      "##ful         3,993\n",
      "Added ['beaming'] to the tokenized_text array.\n",
      "beaming      27,910\n",
      "Added ['bear', '##ish'] to the tokenized_text array.\n",
      "bear          4,562\n",
      "##ish         4,509\n",
      "Added ['beat'] to the tokenized_text array.\n",
      "beat          3,786\n",
      "Added ['beaten'] to the tokenized_text array.\n",
      "beaten        7,854\n",
      "Added ['bed', '##ev', '##iled'] to the tokenized_text array.\n",
      "bed           2,793\n",
      "##ev          6,777\n",
      "##iled       18,450\n",
      "Added ['be', '##fu', '##ddled'] to the tokenized_text array.\n",
      "be            2,022\n",
      "##fu         11,263\n",
      "##ddled      28,090\n",
      "Added ['begging'] to the tokenized_text array.\n",
      "begging      12,858\n",
      "Added ['beg', '##rud', '##ge'] to the tokenized_text array.\n",
      "beg          11,693\n",
      "##rud        28,121\n",
      "##ge          3,351\n",
      "Added ['beg', '##rud', '##ging'] to the tokenized_text array.\n",
      "beg          11,693\n",
      "##rud        28,121\n",
      "##ging        4,726\n",
      "Added ['beg', '##rud', '##gingly'] to the tokenized_text array.\n",
      "beg          11,693\n",
      "##rud        28,121\n",
      "##gingly     28,392\n",
      "Added ['beg', '##uil', '##ed'] to the tokenized_text array.\n",
      "beg          11,693\n",
      "##uil        19,231\n",
      "##ed          2,098\n",
      "Added ['bela', '##ted'] to the tokenized_text array.\n",
      "bela         20,252\n",
      "##ted         3,064\n",
      "Added ['bel', '##itt', '##ling'] to the tokenized_text array.\n",
      "bel          19,337\n",
      "##itt        12,474\n",
      "##ling        2,989\n",
      "Added ['bell', '##iger', '##ence'] to the tokenized_text array.\n",
      "bell          4,330\n",
      "##iger       17,071\n",
      "##ence       10,127\n",
      "Added ['bell', '##iger', '##ent'] to the tokenized_text array.\n",
      "bell          4,330\n",
      "##iger       17,071\n",
      "##ent         4,765\n",
      "Added ['belonging'] to the tokenized_text array.\n",
      "belonging     7,495\n",
      "Added ['be', '##mus', '##ed'] to the tokenized_text array.\n",
      "be            2,022\n",
      "##mus         7,606\n",
      "##ed          2,098\n",
      "Added ['be', '##mus', '##ement'] to the tokenized_text array.\n",
      "be            2,022\n",
      "##mus         7,606\n",
      "##ement      13,665\n",
      "Added ['ben', '##ev', '##ole', '##nce'] to the tokenized_text array.\n",
      "ben           3,841\n",
      "##ev          6,777\n",
      "##ole         9,890\n",
      "##nce         5,897\n",
      "Added ['benevolent'] to the tokenized_text array.\n",
      "benevolent   25,786\n",
      "Added ['ben', '##umb', '##ed'] to the tokenized_text array.\n",
      "ben           3,841\n",
      "##umb        25,438\n",
      "##ed          2,098\n",
      "Added ['be', '##rate'] to the tokenized_text array.\n",
      "be            2,022\n",
      "##rate       11,657\n",
      "Added ['be', '##rating'] to the tokenized_text array.\n",
      "be            2,022\n",
      "##rating     15,172\n",
      "Added ['be', '##rea', '##ved'] to the tokenized_text array.\n",
      "be            2,022\n",
      "##rea        16,416\n",
      "##ved         7,178\n",
      "Added ['be', '##re', '##ft'] to the tokenized_text array.\n",
      "be            2,022\n",
      "##re          2,890\n",
      "##ft          6,199\n",
      "Added ['be', '##see', '##ching'] to the tokenized_text array.\n",
      "be            2,022\n",
      "##see        19,763\n",
      "##ching       8,450\n",
      "Added ['best', '##ed'] to the tokenized_text array.\n",
      "best          2,190\n",
      "##ed          2,098\n",
      "Added ['betrayal'] to the tokenized_text array.\n",
      "betrayal     14,583\n",
      "Added ['betrayed'] to the tokenized_text array.\n",
      "betrayed     12,056\n",
      "Added ['bewildered'] to the tokenized_text array.\n",
      "bewildered   24,683\n",
      "Added ['be', '##wil', '##der', '##ment'] to the tokenized_text array.\n",
      "be            2,022\n",
      "##wil        29,602\n",
      "##der         4,063\n",
      "##ment        3,672\n",
      "Added ['bi'] to the tokenized_text array.\n",
      "bi           12,170\n",
      "Added ['bi', '##lio', '##us'] to the tokenized_text array.\n",
      "bi           12,170\n",
      "##lio        12,798\n",
      "##us          2,271\n",
      "Added ['bit'] to the tokenized_text array.\n",
      "bit           2,978\n",
      "Added ['biting'] to the tokenized_text array.\n",
      "biting       12,344\n",
      "Added ['bitter'] to the tokenized_text array.\n",
      "bitter        8,618\n",
      "Added ['bitter', '##sw', '##eet'] to the tokenized_text array.\n",
      "bitter        8,618\n",
      "##sw         26,760\n",
      "##eet        15,558\n",
      "Added ['blaming'] to the tokenized_text array.\n",
      "blaming      24,114\n",
      "Added ['bland'] to the tokenized_text array.\n",
      "bland        20,857\n",
      "Added ['blank'] to the tokenized_text array.\n",
      "blank         8,744\n",
      "Added ['b', '##lase'] to the tokenized_text array.\n",
      "b             1,038\n",
      "##lase       25,002\n",
      "Added ['blazed'] to the tokenized_text array.\n",
      "blazed       25,590\n",
      "Added ['bleak'] to the tokenized_text array.\n",
      "bleak        21,657\n",
      "Added ['b', '##lea', '##ry'] to the tokenized_text array.\n",
      "b             1,038\n",
      "##lea        19,738\n",
      "##ry          2,854\n",
      "Added ['blessed'] to the tokenized_text array.\n",
      "blessed      10,190\n",
      "Added ['blew'] to the tokenized_text array.\n",
      "blew          8,682\n",
      "Added ['blinded'] to the tokenized_text array.\n",
      "blinded      20,641\n",
      "Added ['blinds', '##ided'] to the tokenized_text array.\n",
      "blinds       28,279\n",
      "##ided       14,097\n",
      "Added ['bliss'] to the tokenized_text array.\n",
      "bliss        13,670\n",
      "Added ['bliss', '##ful'] to the tokenized_text array.\n",
      "bliss        13,670\n",
      "##ful         3,993\n",
      "Added ['bliss', '##fully'] to the tokenized_text array.\n",
      "bliss        13,670\n",
      "##fully       7,699\n",
      "Added ['b', '##lit', '##he'] to the tokenized_text array.\n",
      "b             1,038\n",
      "##lit        15,909\n",
      "##he          5,369\n",
      "Added ['blown'] to the tokenized_text array.\n",
      "blown        10,676\n",
      "Added ['blue'] to the tokenized_text array.\n",
      "blue          2,630\n",
      "Added ['blues'] to the tokenized_text array.\n",
      "blues         5,132\n",
      "Added ['bluff', '##ing'] to the tokenized_text array.\n",
      "bluff        14,441\n",
      "##ing         2,075\n",
      "Added ['blunt'] to the tokenized_text array.\n",
      "blunt        14,969\n",
      "Added ['blushing'] to the tokenized_text array.\n",
      "blushing     25,771\n",
      "Added ['blu', '##ster', '##ing'] to the tokenized_text array.\n",
      "blu          14,154\n",
      "##ster        6,238\n",
      "##ing         2,075\n",
      "Added ['bo', '##ast', '##ful'] to the tokenized_text array.\n",
      "bo            8,945\n",
      "##ast        14,083\n",
      "##ful         3,993\n",
      "Added ['bog', '##gled'] to the tokenized_text array.\n",
      "bog          22,132\n",
      "##gled       11,533\n",
      "Added ['boiling'] to the tokenized_text array.\n",
      "boiling      16,018\n",
      "Added ['bois', '##ter', '##ous'] to the tokenized_text array.\n",
      "bois         19,651\n",
      "##ter         3,334\n",
      "##ous         3,560\n",
      "Added ['bold'] to the tokenized_text array.\n",
      "bold          7,782\n",
      "Added ['bored'] to the tokenized_text array.\n",
      "bored        11,471\n",
      "Added ['boredom'] to the tokenized_text array.\n",
      "boredom      29,556\n",
      "Added ['boring'] to the tokenized_text array.\n",
      "boring       11,771\n",
      "Added ['bothered'] to the tokenized_text array.\n",
      "bothered     11,250\n",
      "Added ['bound', '##er'] to the tokenized_text array.\n",
      "bound         5,391\n",
      "##er          2,121\n",
      "Added ['bra', '##sh', '##ness'] to the tokenized_text array.\n",
      "bra          11,655\n",
      "##sh          4,095\n",
      "##ness        2,791\n",
      "Added ['brat', '##ty'] to the tokenized_text array.\n",
      "brat         28,557\n",
      "##ty          3,723\n",
      "Added ['brave'] to the tokenized_text array.\n",
      "brave         9,191\n",
      "Added ['bright'] to the tokenized_text array.\n",
      "bright        4,408\n",
      "Added ['br', '##ist', '##ling'] to the tokenized_text array.\n",
      "br            7,987\n",
      "##ist         2,923\n",
      "##ling        2,989\n",
      "Added ['broken'] to the tokenized_text array.\n",
      "broken        3,714\n",
      "Added ['broken', '##hearted'] to the tokenized_text array.\n",
      "broken        3,714\n",
      "##hearted    27,693\n",
      "Added ['broken', '##hearted', '##ly'] to the tokenized_text array.\n",
      "broken        3,714\n",
      "##hearted    27,693\n",
      "##ly          2,135\n",
      "Added ['brooding'] to the tokenized_text array.\n",
      "brooding     28,902\n",
      "Added ['brood', '##y'] to the tokenized_text array.\n",
      "brood        25,565\n",
      "##y           2,100\n",
      "Added ['bruised'] to the tokenized_text array.\n",
      "bruised      18,618\n",
      "Added ['br', '##us', '##que'] to the tokenized_text array.\n",
      "br            7,987\n",
      "##us          2,271\n",
      "##que         4,226\n",
      "Added ['bug'] to the tokenized_text array.\n",
      "bug          11,829\n",
      "Added ['bulging'] to the tokenized_text array.\n",
      "bulging      27,569\n",
      "Added ['bully'] to the tokenized_text array.\n",
      "bully        20,716\n",
      "Added ['bullying'] to the tokenized_text array.\n",
      "bullying     18,917\n",
      "Added ['bum', '##med'] to the tokenized_text array.\n",
      "bum          26,352\n",
      "##med         7,583\n",
      "Added ['bu', '##oya', '##nt'] to the tokenized_text array.\n",
      "bu           20,934\n",
      "##oya        18,232\n",
      "##nt          3,372\n",
      "Added ['burden', '##ed'] to the tokenized_text array.\n",
      "burden       10,859\n",
      "##ed          2,098\n",
      "Added ['burn'] to the tokenized_text array.\n",
      "burn          6,402\n",
      "Added ['bursting'] to the tokenized_text array.\n",
      "bursting     21,305\n",
      "Added ['bush', '##ed'] to the tokenized_text array.\n",
      "bush          5,747\n",
      "##ed          2,098\n",
      "Added ['cage', '##y'] to the tokenized_text array.\n",
      "cage          7,980\n",
      "##y           2,100\n",
      "Added ['ca', '##gy'] to the tokenized_text array.\n",
      "ca            6,187\n",
      "##gy          6,292\n",
      "Added ['calculating'] to the tokenized_text array.\n",
      "calculating  20,177\n",
      "Added ['call', '##ous'] to the tokenized_text array.\n",
      "call          2,655\n",
      "##ous         3,560\n",
      "Added ['call', '##used'] to the tokenized_text array.\n",
      "call          2,655\n",
      "##used       13,901\n",
      "Added ['calm'] to the tokenized_text array.\n",
      "calm          5,475\n",
      "Added ['calming'] to the tokenized_text array.\n",
      "calming      23,674\n",
      "Added ['calm', '##ness'] to the tokenized_text array.\n",
      "calm          5,475\n",
      "##ness        2,791\n",
      "Added ['can', '##ny'] to the tokenized_text array.\n",
      "can           2,064\n",
      "##ny          4,890\n",
      "Added ['can', '##tan', '##ker', '##ous'] to the tokenized_text array.\n",
      "can           2,064\n",
      "##tan         5,794\n",
      "##ker         5,484\n",
      "##ous         3,560\n",
      "Added ['capable'] to the tokenized_text array.\n",
      "capable       5,214\n",
      "Added ['cap', '##ric', '##ious'] to the tokenized_text array.\n",
      "cap           6,178\n",
      "##ric         7,277\n",
      "##ious        6,313\n",
      "Added ['capt', '##ivated'] to the tokenized_text array.\n",
      "capt         14,408\n",
      "##ivated     21,967\n",
      "Added ['captive'] to the tokenized_text array.\n",
      "captive      12,481\n",
      "Added ['care', '##free'] to the tokenized_text array.\n",
      "care          2,729\n",
      "##free       23,301\n",
      "Added ['careful'] to the tokenized_text array.\n",
      "careful       6,176\n",
      "Added ['careless'] to the tokenized_text array.\n",
      "careless     23,358\n",
      "Added ['caring'] to the tokenized_text array.\n",
      "caring       11,922\n",
      "Added ['cat', '##ty'] to the tokenized_text array.\n",
      "cat           4,937\n",
      "##ty          3,723\n",
      "Added ['ca', '##ust', '##ic'] to the tokenized_text array.\n",
      "ca            6,187\n",
      "##ust        19,966\n",
      "##ic          2,594\n",
      "Added ['caution', '##ary'] to the tokenized_text array.\n",
      "caution      14,046\n",
      "##ary         5,649\n",
      "Added ['cautious'] to the tokenized_text array.\n",
      "cautious     17,145\n",
      "Added ['cavalier'] to the tokenized_text array.\n",
      "cavalier     28,778\n",
      "Added ['celebrating'] to the tokenized_text array.\n",
      "celebrating  12,964\n",
      "Added ['celebration'] to the tokenized_text array.\n",
      "celebration   7,401\n",
      "Added ['ce', '##ns', '##ure'] to the tokenized_text array.\n",
      "ce            8,292\n",
      "##ns          3,619\n",
      "##ure         5,397\n",
      "Added ['centered'] to the tokenized_text array.\n",
      "centered      8,857\n",
      "Added ['certain'] to the tokenized_text array.\n",
      "certain       3,056\n",
      "Added ['cha', '##fed'] to the tokenized_text array.\n",
      "cha          15,775\n",
      "##fed        25,031\n",
      "Added ['cha', '##grin'] to the tokenized_text array.\n",
      "cha          15,775\n",
      "##grin       24,860\n",
      "Added ['cha', '##grin', '##ed'] to the tokenized_text array.\n",
      "cha          15,775\n",
      "##grin       24,860\n",
      "##ed          2,098\n",
      "Added ['cha', '##grin', '##ned'] to the tokenized_text array.\n",
      "cha          15,775\n",
      "##grin       24,860\n",
      "##ned         7,228\n",
      "Added ['challenge'] to the tokenized_text array.\n",
      "challenge     4,119\n",
      "Added ['challenged'] to the tokenized_text array.\n",
      "challenged    8,315\n",
      "Added ['challenging'] to the tokenized_text array.\n",
      "challenging  10,368\n",
      "Added ['chaotic'] to the tokenized_text array.\n",
      "chaotic      19,633\n",
      "Added ['charged'] to the tokenized_text array.\n",
      "charged       5,338\n",
      "Added ['charm', '##ed'] to the tokenized_text array.\n",
      "charm        11,084\n",
      "##ed          2,098\n",
      "Added ['charming'] to the tokenized_text array.\n",
      "charming     11,951\n",
      "Added ['char', '##y'] to the tokenized_text array.\n",
      "char         25,869\n",
      "##y           2,100\n",
      "Added ['cheated'] to the tokenized_text array.\n",
      "cheated      22,673\n",
      "Added ['cheek', '##y'] to the tokenized_text array.\n",
      "cheek         5,048\n",
      "##y           2,100\n",
      "Added ['cheered'] to the tokenized_text array.\n",
      "cheered      25,471\n",
      "Added ['cheerful'] to the tokenized_text array.\n",
      "cheerful     18,350\n",
      "Added ['cheering'] to the tokenized_text array.\n",
      "cheering     24,867\n",
      "Added ['cheer', '##less'] to the tokenized_text array.\n",
      "cheer        15,138\n",
      "##less        3,238\n",
      "Added ['cheer', '##y'] to the tokenized_text array.\n",
      "cheer        15,138\n",
      "##y           2,100\n",
      "Added ['che', '##es', '##y'] to the tokenized_text array.\n",
      "che          18,178\n",
      "##es          2,229\n",
      "##y           2,100\n",
      "Added ['chest', '##y'] to the tokenized_text array.\n",
      "chest         3,108\n",
      "##y           2,100\n",
      "Added ['chi', '##de'] to the tokenized_text array.\n",
      "chi           9,610\n",
      "##de          3,207\n",
      "Added ['chi', '##ding'] to the tokenized_text array.\n",
      "chi           9,610\n",
      "##ding        4,667\n",
      "Added ['childish'] to the tokenized_text array.\n",
      "childish     24,282\n",
      "Added ['childish', '##ly'] to the tokenized_text array.\n",
      "childish     24,282\n",
      "##ly          2,135\n",
      "Added ['child', '##like'] to the tokenized_text array.\n",
      "child         2,775\n",
      "##like       10,359\n",
      "Added ['chill'] to the tokenized_text array.\n",
      "chill        10,720\n",
      "Added ['chilled'] to the tokenized_text array.\n",
      "chilled      23,362\n",
      "Added ['chilling'] to the tokenized_text array.\n",
      "chilling     27,017\n",
      "Added ['chip', '##per'] to the tokenized_text array.\n",
      "chip          9,090\n",
      "##per         4,842\n",
      "Added ['chi', '##rp', '##y'] to the tokenized_text array.\n",
      "chi           9,610\n",
      "##rp         14,536\n",
      "##y           2,100\n",
      "Added ['cho', '##ler', '##ic'] to the tokenized_text array.\n",
      "cho          16,480\n",
      "##ler         3,917\n",
      "##ic          2,594\n",
      "Added ['cho', '##rt', '##ling'] to the tokenized_text array.\n",
      "cho          16,480\n",
      "##rt          5,339\n",
      "##ling        2,989\n",
      "Added ['chuckle'] to the tokenized_text array.\n",
      "chuckle      15,375\n",
      "Added ['chuck', '##ling'] to the tokenized_text array.\n",
      "chuck         8,057\n",
      "##ling        2,989\n",
      "Added ['chu', '##rl', '##ish'] to the tokenized_text array.\n",
      "chu          14,684\n",
      "##rl         12,190\n",
      "##ish         4,509\n",
      "Added ['ci', '##rc', '##ums', '##pe', '##ct'] to the tokenized_text array.\n",
      "ci           25,022\n",
      "##rc         11,890\n",
      "##ums        18,163\n",
      "##pe          5,051\n",
      "##ct          6,593\n",
      "Added ['cl', '##amo', '##rous'] to the tokenized_text array.\n",
      "cl           18,856\n",
      "##amo        22,591\n",
      "##rous       13,288\n",
      "Added ['clash'] to the tokenized_text array.\n",
      "clash        13,249\n",
      "Added ['clear'] to the tokenized_text array.\n",
      "clear         3,154\n",
      "Added ['clenched'] to the tokenized_text array.\n",
      "clenched      8,555\n",
      "Added ['clever'] to the tokenized_text array.\n",
      "clever       12,266\n",
      "Added ['close'] to the tokenized_text array.\n",
      "close         2,485\n",
      "Added ['closed'] to the tokenized_text array.\n",
      "closed        2,701\n",
      "Added ['close', '##mouth', '##ed'] to the tokenized_text array.\n",
      "close         2,485\n",
      "##mouth      14,359\n",
      "##ed          2,098\n",
      "Added ['cl', '##oy'] to the tokenized_text array.\n",
      "cl           18,856\n",
      "##oy          6,977\n",
      "Added ['clue', '##less'] to the tokenized_text array.\n",
      "clue          9,789\n",
      "##less        3,238\n",
      "Added ['clutched'] to the tokenized_text array.\n",
      "clutched     13,514\n",
      "Added ['cl', '##uttered'] to the tokenized_text array.\n",
      "cl           18,856\n",
      "##uttered    23,128\n",
      "Added ['cock', '##eye', '##d'] to the tokenized_text array.\n",
      "cock         10,338\n",
      "##eye        17,683\n",
      "##d           2,094\n",
      "Added ['cock', '##iness'] to the tokenized_text array.\n",
      "cock         10,338\n",
      "##iness       9,961\n",
      "Added ['cock', '##sure'] to the tokenized_text array.\n",
      "cock         10,338\n",
      "##sure       28,632\n",
      "Added ['cocky'] to the tokenized_text array.\n",
      "cocky        24,995\n",
      "Added ['co', '##gni', '##zan', '##t'] to the tokenized_text array.\n",
      "co            2,522\n",
      "##gni        29,076\n",
      "##zan        13,471\n",
      "##t           2,102\n",
      "Added ['cold'] to the tokenized_text array.\n",
      "cold          3,147\n",
      "Added ['collected'] to the tokenized_text array.\n",
      "collected     5,067\n",
      "Added ['col', '##lus', '##ive'] to the tokenized_text array.\n",
      "col           8,902\n",
      "##lus         7,393\n",
      "##ive         3,512\n",
      "Added ['colon', '##ized'] to the tokenized_text array.\n",
      "colon        16,844\n",
      "##ized        3,550\n",
      "Added ['combat', '##ive'] to the tokenized_text array.\n",
      "combat        4,337\n",
      "##ive         3,512\n",
      "Added ['comedic'] to the tokenized_text array.\n",
      "comedic      21,699\n",
      "Added ['comfort'] to the tokenized_text array.\n",
      "comfort       7,216\n",
      "Added ['comfortable'] to the tokenized_text array.\n",
      "comfortable   6,625\n",
      "Added ['comfort', '##ed'] to the tokenized_text array.\n",
      "comfort       7,216\n",
      "##ed          2,098\n",
      "Added ['comical'] to the tokenized_text array.\n",
      "comical      29,257\n",
      "Added ['commanding'] to the tokenized_text array.\n",
      "commanding    7,991\n",
      "Added ['com', '##mise', '##rating'] to the tokenized_text array.\n",
      "com           4,012\n",
      "##mise       28,732\n",
      "##rating     15,172\n",
      "Added ['com', '##mise', '##rative'] to the tokenized_text array.\n",
      "com           4,012\n",
      "##mise       28,732\n",
      "##rative     18,514\n",
      "Added ['com', '##mun', '##icative'] to the tokenized_text array.\n",
      "com           4,012\n",
      "##mun        23,041\n",
      "##icative    25,184\n",
      "Added ['compassion'] to the tokenized_text array.\n",
      "compassion   15,398\n",
      "Added ['compassionate'] to the tokenized_text array.\n",
      "compassionate 29,353\n",
      "Added ['competent'] to the tokenized_text array.\n",
      "competent    17,824\n",
      "Added ['competitive'] to the tokenized_text array.\n",
      "competitive   6,975\n",
      "Added ['com', '##pl', '##ace', '##nce'] to the tokenized_text array.\n",
      "com           4,012\n",
      "##pl         24,759\n",
      "##ace        10,732\n",
      "##nce         5,897\n",
      "Added ['com', '##pl', '##ace', '##ncy'] to the tokenized_text array.\n",
      "com           4,012\n",
      "##pl         24,759\n",
      "##ace        10,732\n",
      "##ncy         9,407\n",
      "Added ['com', '##pl', '##ace', '##nt'] to the tokenized_text array.\n",
      "com           4,012\n",
      "##pl         24,759\n",
      "##ace        10,732\n",
      "##nt          3,372\n",
      "Added ['com', '##pl', '##ace', '##ntly'] to the tokenized_text array.\n",
      "com           4,012\n",
      "##pl         24,759\n",
      "##ace        10,732\n",
      "##ntly       20,630\n",
      "Added ['complain'] to the tokenized_text array.\n",
      "complain     17,612\n",
      "Added ['complaining'] to the tokenized_text array.\n",
      "complaining  17,949\n",
      "Added ['composed'] to the tokenized_text array.\n",
      "composed      3,605\n",
      "Added ['comprehend', '##ing'] to the tokenized_text array.\n",
      "comprehend   22,346\n",
      "##ing         2,075\n",
      "Added ['com', '##pu', '##ls', '##ive'] to the tokenized_text array.\n",
      "com           4,012\n",
      "##pu         14,289\n",
      "##ls          4,877\n",
      "##ive         3,512\n",
      "Added ['concealed'] to the tokenized_text array.\n",
      "concealed    14,091\n",
      "Added ['con', '##ced', '##ing'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##ced        11,788\n",
      "##ing         2,075\n",
      "Added ['con', '##ce', '##ited'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##ce          3,401\n",
      "##ited       17,572\n",
      "Added ['concentrated'] to the tokenized_text array.\n",
      "concentrated  8,279\n",
      "Added ['concentrating'] to the tokenized_text array.\n",
      "concentrating 16,966\n",
      "Added ['concentration'] to the tokenized_text array.\n",
      "concentration  6,693\n",
      "Added ['concern'] to the tokenized_text array.\n",
      "concern       5,142\n",
      "Added ['concerned'] to the tokenized_text array.\n",
      "concerned     4,986\n",
      "Added ['con', '##ci', '##lia', '##tory'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##ci          6,895\n",
      "##lia         6,632\n",
      "##tory        7,062\n",
      "Added ['con', '##clusive'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##clusive    23,633\n",
      "Added ['condemning'] to the tokenized_text array.\n",
      "condemning   28,525\n",
      "Added ['conde', '##sc', '##ending'] to the tokenized_text array.\n",
      "conde        24,707\n",
      "##sc         11,020\n",
      "##ending     18,537\n",
      "Added ['condo', '##ling'] to the tokenized_text array.\n",
      "condo        25,805\n",
      "##ling        2,989\n",
      "Added ['confidence'] to the tokenized_text array.\n",
      "confidence    7,023\n",
      "Added ['confident'] to the tokenized_text array.\n",
      "confident     9,657\n",
      "Added ['confidently'] to the tokenized_text array.\n",
      "confidently  28,415\n",
      "Added ['conflict', '##ed'] to the tokenized_text array.\n",
      "conflict      4,736\n",
      "##ed          2,098\n",
      "Added ['con', '##fo', '##und'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##fo         14,876\n",
      "##und         8,630\n",
      "Added ['con', '##founded'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##founded    21,001\n",
      "Added ['confrontation', '##al'] to the tokenized_text array.\n",
      "confrontation 13,111\n",
      "##al          2,389\n",
      "Added ['confused'] to the tokenized_text array.\n",
      "confused      5,457\n",
      "Added ['confusion'] to the tokenized_text array.\n",
      "confusion     6,724\n",
      "Added ['cong', '##enia', '##l'] to the tokenized_text array.\n",
      "cong         26,478\n",
      "##enia       19,825\n",
      "##l           2,140\n",
      "Added ['cong', '##rat', '##ulator', '##y'] to the tokenized_text array.\n",
      "cong         26,478\n",
      "##rat         8,609\n",
      "##ulator     20,350\n",
      "##y           2,100\n",
      "Added ['con', '##ni', '##ving'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##ni          3,490\n",
      "##ving        6,455\n",
      "Added ['conscious'] to the tokenized_text array.\n",
      "conscious     9,715\n",
      "Added ['conservative'] to the tokenized_text array.\n",
      "conservative  4,603\n",
      "Added ['consider', '##ate'] to the tokenized_text array.\n",
      "consider      5,136\n",
      "##ate         3,686\n",
      "Added ['considering'] to the tokenized_text array.\n",
      "considering   6,195\n",
      "Added ['con', '##sol', '##ing'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##sol        19,454\n",
      "##ing         2,075\n",
      "Added ['con', '##sp', '##ira', '##tori', '##al'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##sp         13,102\n",
      "##ira         7,895\n",
      "##tori       29,469\n",
      "##al          2,389\n",
      "Added ['con', '##sp', '##iring'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##sp         13,102\n",
      "##iring      24,771\n",
      "Added ['con', '##ster', '##nation'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##ster        6,238\n",
      "##nation      9,323\n",
      "Added ['con', '##sti', '##pate', '##d'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##sti        16,643\n",
      "##pate       17,585\n",
      "##d           2,094\n",
      "Added ['constrained'] to the tokenized_text array.\n",
      "constrained  27,570\n",
      "Added ['consumed'] to the tokenized_text array.\n",
      "consumed     10,202\n",
      "Added ['consuming'] to the tokenized_text array.\n",
      "consuming    15,077\n",
      "Added ['contained'] to the tokenized_text array.\n",
      "contained     4,838\n",
      "Added ['con', '##tem', '##plate'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##tem        18,532\n",
      "##plate      15,725\n",
      "Added ['contemplating'] to the tokenized_text array.\n",
      "contemplating 25,247\n",
      "Added ['con', '##tem', '##pl', '##ation'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##tem        18,532\n",
      "##pl         24,759\n",
      "##ation       3,370\n",
      "Added ['con', '##tem', '##pl', '##ative'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##tem        18,532\n",
      "##pl         24,759\n",
      "##ative       8,082\n",
      "Added ['contempt'] to the tokenized_text array.\n",
      "contempt     17,152\n",
      "Added ['contempt', '##uous'] to the tokenized_text array.\n",
      "contempt     17,152\n",
      "##uous        8,918\n",
      "Added ['content'] to the tokenized_text array.\n",
      "content       4,180\n",
      "Added ['content', '##ed'] to the tokenized_text array.\n",
      "content       4,180\n",
      "##ed          2,098\n",
      "Added ['contentious'] to the tokenized_text array.\n",
      "contentious  29,308\n",
      "Added ['content', '##ly'] to the tokenized_text array.\n",
      "content       4,180\n",
      "##ly          2,135\n",
      "Added ['content', '##ment'] to the tokenized_text array.\n",
      "content       4,180\n",
      "##ment        3,672\n",
      "Added ['contradictory'] to the tokenized_text array.\n",
      "contradictory 27,894\n",
      "Added ['contrary'] to the tokenized_text array.\n",
      "contrary     10,043\n",
      "Added ['con', '##tri', '##te'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##tri        18,886\n",
      "##te          2,618\n",
      "Added ['controlled'] to the tokenized_text array.\n",
      "controlled    4,758\n",
      "Added ['controlling'] to the tokenized_text array.\n",
      "controlling   9,756\n",
      "Added ['controversial'] to the tokenized_text array.\n",
      "controversial  6,801\n",
      "Added ['con', '##tum', '##acious'] to the tokenized_text array.\n",
      "con           9,530\n",
      "##tum        11,667\n",
      "##acious     20,113\n",
      "Added ['convinced'] to the tokenized_text array.\n",
      "convinced     6,427\n",
      "Added ['cool'] to the tokenized_text array.\n",
      "cool          4,658\n",
      "Added ['cooperative'] to the tokenized_text array.\n",
      "cooperative  10,791\n",
      "Added ['cord', '##ial'] to the tokenized_text array.\n",
      "cord         11,601\n",
      "##ial         4,818\n",
      "Added ['courageous'] to the tokenized_text array.\n",
      "courageous   26,103\n",
      "Added ['covert'] to the tokenized_text array.\n",
      "covert       19,813\n",
      "Added ['coward', '##ly'] to the tokenized_text array.\n",
      "coward       16,592\n",
      "##ly          2,135\n",
      "Added ['co', '##y'] to the tokenized_text array.\n",
      "co            2,522\n",
      "##y           2,100\n",
      "Added ['crab', '##by'] to the tokenized_text array.\n",
      "crab         18,081\n",
      "##by          3,762\n",
      "Added ['craft', '##y'] to the tokenized_text array.\n",
      "craft         7,477\n",
      "##y           2,100\n",
      "Added ['crank', '##y'] to the tokenized_text array.\n",
      "crank        27,987\n",
      "##y           2,100\n",
      "Added ['crazed'] to the tokenized_text array.\n",
      "crazed       28,343\n",
      "Added ['crazy'] to the tokenized_text array.\n",
      "crazy         4,689\n",
      "Added ['cr', '##ed', '##ulous'] to the tokenized_text array.\n",
      "cr           13,675\n",
      "##ed          2,098\n",
      "##ulous      16,203\n",
      "Added ['creepy'] to the tokenized_text array.\n",
      "creepy       17,109\n",
      "Added ['crest', '##fall', '##en'] to the tokenized_text array.\n",
      "crest        11,146\n",
      "##fall       13,976\n",
      "##en          2,368\n",
      "Added ['cr', '##inging'] to the tokenized_text array.\n",
      "cr           13,675\n",
      "##inging     23,180\n",
      "Added ['critical'] to the tokenized_text array.\n",
      "critical      4,187\n",
      "Added ['cross'] to the tokenized_text array.\n",
      "cross         2,892\n",
      "Added ['crotch', '##ety'] to the tokenized_text array.\n",
      "crotch       28,629\n",
      "##ety        27,405\n",
      "Added ['crude'] to the tokenized_text array.\n",
      "crude        13,587\n",
      "Added ['cruel'] to the tokenized_text array.\n",
      "cruel        10,311\n",
      "Added ['crushed'] to the tokenized_text array.\n",
      "crushed      10,560\n",
      "Added ['cry'] to the tokenized_text array.\n",
      "cry           5,390\n",
      "Added ['crying'] to the tokenized_text array.\n",
      "crying        6,933\n",
      "Added ['cryptic'] to the tokenized_text array.\n",
      "cryptic      26,483\n",
      "Added ['cu', '##lp', '##able'] to the tokenized_text array.\n",
      "cu           12,731\n",
      "##lp         14,277\n",
      "##able        3,085\n",
      "Added ['cunning'] to the tokenized_text array.\n",
      "cunning      23,626\n",
      "Added ['cu', '##rio', '##s'] to the tokenized_text array.\n",
      "cu           12,731\n",
      "##rio         9,488\n",
      "##s           2,015\n",
      "Added ['curiosity'] to the tokenized_text array.\n",
      "curiosity    10,628\n",
      "Added ['curious'] to the tokenized_text array.\n",
      "curious       8,025\n",
      "Added ['cutting'] to the tokenized_text array.\n",
      "cutting       6,276\n",
      "Added ['cy', '##nic'] to the tokenized_text array.\n",
      "cy           22,330\n",
      "##nic         8,713\n",
      "Added ['cynical'] to the tokenized_text array.\n",
      "cynical      26,881\n",
      "Added ['cy', '##nic', '##ism'] to the tokenized_text array.\n",
      "cy           22,330\n",
      "##nic         8,713\n",
      "##ism         2,964\n",
      "Added ['dal', '##lian', '##ce'] to the tokenized_text array.\n",
      "dal          17,488\n",
      "##lian       15,204\n",
      "##ce          3,401\n",
      "Added ['dan', '##dy'] to the tokenized_text array.\n",
      "dan           4,907\n",
      "##dy          5,149\n",
      "Added ['dangerous'] to the tokenized_text array.\n",
      "dangerous     4,795\n",
      "Added ['darkly'] to the tokenized_text array.\n",
      "darkly       27,148\n",
      "Added ['da', '##unt', '##ed'] to the tokenized_text array.\n",
      "da            4,830\n",
      "##unt        16,671\n",
      "##ed          2,098\n",
      "Added ['day', '##dre', '##am'] to the tokenized_text array.\n",
      "day           2,154\n",
      "##dre        16,200\n",
      "##am          3,286\n",
      "Added ['day', '##dre', '##ami', '##ng'] to the tokenized_text array.\n",
      "day           2,154\n",
      "##dre        16,200\n",
      "##ami        10,631\n",
      "##ng          3,070\n",
      "Added ['dazed'] to the tokenized_text array.\n",
      "dazed        19,720\n",
      "Added ['da', '##zzled'] to the tokenized_text array.\n",
      "da            4,830\n",
      "##zzled      17,269\n",
      "Added ['deadly'] to the tokenized_text array.\n",
      "deadly        9,252\n",
      "Added ['dead', '##pan'] to the tokenized_text array.\n",
      "dead          2,757\n",
      "##pan         9,739\n",
      "Added ['debate'] to the tokenized_text array.\n",
      "debate        5,981\n",
      "Added ['debating'] to the tokenized_text array.\n",
      "debating     20,767\n",
      "Added ['de', '##bau', '##ched'] to the tokenized_text array.\n",
      "de            2,139\n",
      "##bau        27,773\n",
      "##ched        7,690\n",
      "Added ['dec', '##eit', '##ful'] to the tokenized_text array.\n",
      "dec          11,703\n",
      "##eit        20,175\n",
      "##ful         3,993\n",
      "Added ['dec', '##ei', '##ved'] to the tokenized_text array.\n",
      "dec          11,703\n",
      "##ei          7,416\n",
      "##ved         7,178\n",
      "Added ['dec', '##ei', '##ving'] to the tokenized_text array.\n",
      "dec          11,703\n",
      "##ei          7,416\n",
      "##ving        6,455\n",
      "Added ['dec', '##ei', '##ving', '##ly'] to the tokenized_text array.\n",
      "dec          11,703\n",
      "##ei          7,416\n",
      "##ving        6,455\n",
      "##ly          2,135\n",
      "Added ['deception'] to the tokenized_text array.\n",
      "deception    17,575\n",
      "Added ['dec', '##eptive'] to the tokenized_text array.\n",
      "dec          11,703\n",
      "##eptive     22,048\n",
      "Added ['deciding'] to the tokenized_text array.\n",
      "deciding     10,561\n",
      "Added ['decisive'] to the tokenized_text array.\n",
      "decisive     13,079\n",
      "Added ['dedicated'] to the tokenized_text array.\n",
      "dedicated     4,056\n",
      "Added ['defeat'] to the tokenized_text array.\n",
      "defeat        4,154\n",
      "Added ['defeated'] to the tokenized_text array.\n",
      "defeated      3,249\n",
      "Added ['defense', '##less'] to the tokenized_text array.\n",
      "defense       3,639\n",
      "##less        3,238\n",
      "Added ['defensive'] to the tokenized_text array.\n",
      "defensive     5,600\n",
      "Added ['defiance'] to the tokenized_text array.\n",
      "defiance     19,674\n",
      "Added ['defiant'] to the tokenized_text array.\n",
      "defiant      27,836\n",
      "Added ['def', '##lated'] to the tokenized_text array.\n",
      "def          13,366\n",
      "##lated      13,776\n",
      "Added ['de', '##ga', '##ge'] to the tokenized_text array.\n",
      "de            2,139\n",
      "##ga          3,654\n",
      "##ge          3,351\n",
      "Added ['de', '##grad', '##ing'] to the tokenized_text array.\n",
      "de            2,139\n",
      "##grad       16,307\n",
      "##ing         2,075\n",
      "Added ['de', '##jected'] to the tokenized_text array.\n",
      "de            2,139\n",
      "##jected     24,455\n",
      "Added ['de', '##ject', '##ion'] to the tokenized_text array.\n",
      "de            2,139\n",
      "##ject       20,614\n",
      "##ion         3,258\n",
      "Added ['deliberate'] to the tokenized_text array.\n",
      "deliberate   15,063\n",
      "Added ['del', '##ibe', '##rating'] to the tokenized_text array.\n",
      "del           3,972\n",
      "##ibe        20,755\n",
      "##rating     15,172\n",
      "Added ['delight'] to the tokenized_text array.\n",
      "delight      12,208\n",
      "Added ['delighted'] to the tokenized_text array.\n",
      "delighted    15,936\n",
      "Added ['delightful'] to the tokenized_text array.\n",
      "delightful   26,380\n",
      "Added ['del', '##iri', '##ous'] to the tokenized_text array.\n",
      "del           3,972\n",
      "##iri        15,735\n",
      "##ous         3,560\n",
      "Added ['del', '##iri', '##um'] to the tokenized_text array.\n",
      "del           3,972\n",
      "##iri        15,735\n",
      "##um          2,819\n",
      "Added ['del', '##ude'] to the tokenized_text array.\n",
      "del           3,972\n",
      "##ude        12,672\n",
      "Added ['del', '##usion', '##al'] to the tokenized_text array.\n",
      "del           3,972\n",
      "##usion      14,499\n",
      "##al          2,389\n",
      "Added ['demanding'] to the tokenized_text array.\n",
      "demanding     9,694\n",
      "Added ['dem', '##ean', '##ing'] to the tokenized_text array.\n",
      "dem          17,183\n",
      "##ean        11,219\n",
      "##ing         2,075\n",
      "Added ['dem', '##ented'] to the tokenized_text array.\n",
      "dem          17,183\n",
      "##ented      14,088\n",
      "Added ['demise', '##d'] to the tokenized_text array.\n",
      "demise       13,614\n",
      "##d           2,094\n",
      "Added ['demo', '##ral', '##ized'] to the tokenized_text array.\n",
      "demo          9,703\n",
      "##ral         7,941\n",
      "##ized        3,550\n",
      "Added ['dem', '##ure'] to the tokenized_text array.\n",
      "dem          17,183\n",
      "##ure         5,397\n",
      "Added ['denied'] to the tokenized_text array.\n",
      "denied        6,380\n",
      "Added ['den', '##oun', '##cing'] to the tokenized_text array.\n",
      "den           7,939\n",
      "##oun        23,709\n",
      "##cing        6,129\n",
      "Added ['depleted'] to the tokenized_text array.\n",
      "depleted     22,595\n",
      "Added ['de', '##pl', '##ora', '##ble'] to the tokenized_text array.\n",
      "de            2,139\n",
      "##pl         24,759\n",
      "##ora         6,525\n",
      "##ble         3,468\n",
      "Added ['de', '##pre', '##cating'] to the tokenized_text array.\n",
      "de            2,139\n",
      "##pre        28,139\n",
      "##cating     18,252\n",
      "Added ['depressed'] to the tokenized_text array.\n",
      "depressed    14,777\n",
      "Added ['depression'] to the tokenized_text array.\n",
      "depression    6,245\n",
      "Added ['deprived'] to the tokenized_text array.\n",
      "deprived     17,676\n",
      "Added ['der', '##ange', '##d'] to the tokenized_text array.\n",
      "der           4,315\n",
      "##ange       22,043\n",
      "##d           2,094\n",
      "Added ['der', '##ision'] to the tokenized_text array.\n",
      "der           4,315\n",
      "##ision      19,969\n",
      "Added ['der', '##isi', '##ve'] to the tokenized_text array.\n",
      "der           4,315\n",
      "##isi        17,417\n",
      "##ve          3,726\n",
      "Added ['der', '##oga', '##tory'] to the tokenized_text array.\n",
      "der           4,315\n",
      "##oga        18,170\n",
      "##tory        7,062\n",
      "Added ['desire'] to the tokenized_text array.\n",
      "desire        4,792\n",
      "Added ['des', '##iring'] to the tokenized_text array.\n",
      "des           4,078\n",
      "##iring      24,771\n",
      "Added ['des', '##iro', '##us'] to the tokenized_text array.\n",
      "des           4,078\n",
      "##iro         9,711\n",
      "##us          2,271\n",
      "Added ['des', '##olate'] to the tokenized_text array.\n",
      "des           4,078\n",
      "##olate      19,425\n",
      "Added ['despair'] to the tokenized_text array.\n",
      "despair      13,905\n",
      "Added ['despair', '##ed'] to the tokenized_text array.\n",
      "despair      13,905\n",
      "##ed          2,098\n",
      "Added ['despair', '##ing'] to the tokenized_text array.\n",
      "despair      13,905\n",
      "##ing         2,075\n",
      "Added ['desperate'] to the tokenized_text array.\n",
      "desperate     7,143\n",
      "Added ['desperation'] to the tokenized_text array.\n",
      "desperation  15,561\n",
      "Added ['des', '##pis', '##e'] to the tokenized_text array.\n",
      "des           4,078\n",
      "##pis        18,136\n",
      "##e           2,063\n",
      "Added ['des', '##pon', '##dent'] to the tokenized_text array.\n",
      "des           4,078\n",
      "##pon        26,029\n",
      "##dent       16,454\n",
      "Added ['des', '##ti', '##tute'] to the tokenized_text array.\n",
      "des           4,078\n",
      "##ti          3,775\n",
      "##tute       24,518\n",
      "Added ['destroyed'] to the tokenized_text array.\n",
      "destroyed     3,908\n",
      "Added ['detached'] to the tokenized_text array.\n",
      "detached     12,230\n",
      "Added ['determination'] to the tokenized_text array.\n",
      "determination  9,128\n",
      "Added ['determined'] to the tokenized_text array.\n",
      "determined    4,340\n",
      "Added ['determining'] to the tokenized_text array.\n",
      "determining  12,515\n",
      "Added ['deter', '##red'] to the tokenized_text array.\n",
      "deter        28,283\n",
      "##red         5,596\n",
      "Added ['det', '##est'] to the tokenized_text array.\n",
      "det          20,010\n",
      "##est         4,355\n",
      "Added ['det', '##est', '##able'] to the tokenized_text array.\n",
      "det          20,010\n",
      "##est         4,355\n",
      "##able        3,085\n",
      "Added ['det', '##est', '##ing'] to the tokenized_text array.\n",
      "det          20,010\n",
      "##est         4,355\n",
      "##ing         2,075\n",
      "Added ['det', '##rim', '##ent'] to the tokenized_text array.\n",
      "det          20,010\n",
      "##rim        20,026\n",
      "##ent         4,765\n",
      "Added ['devastated'] to the tokenized_text array.\n",
      "devastated   13,879\n",
      "Added ['devi', '##ant'] to the tokenized_text array.\n",
      "devi         14,386\n",
      "##ant         4,630\n",
      "Added ['devil', '##ish'] to the tokenized_text array.\n",
      "devil         6,548\n",
      "##ish         4,509\n",
      "Added ['devi', '##ous'] to the tokenized_text array.\n",
      "devi         14,386\n",
      "##ous         3,560\n",
      "Added ['devi', '##sing'] to the tokenized_text array.\n",
      "devi         14,386\n",
      "##sing        7,741\n",
      "Added ['di', '##ffi', '##dent'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##ffi        26,989\n",
      "##dent       16,454\n",
      "Added ['dil', '##atory'] to the tokenized_text array.\n",
      "dil          29,454\n",
      "##atory      14,049\n",
      "Added ['dil', '##igen', '##t'] to the tokenized_text array.\n",
      "dil          29,454\n",
      "##igen       29,206\n",
      "##t           2,102\n",
      "Added ['dim', '##wi', '##tted'] to the tokenized_text array.\n",
      "dim          11,737\n",
      "##wi          9,148\n",
      "##tted       16,190\n",
      "Added ['dire'] to the tokenized_text array.\n",
      "dire         18,704\n",
      "Added ['disagree'] to the tokenized_text array.\n",
      "disagree     21,090\n",
      "Added ['disagree', '##able'] to the tokenized_text array.\n",
      "disagree     21,090\n",
      "##able        3,085\n",
      "Added ['disagreement'] to the tokenized_text array.\n",
      "disagreement 18,185\n",
      "Added ['disappointed'] to the tokenized_text array.\n",
      "disappointed  9,364\n",
      "Added ['disappointing'] to the tokenized_text array.\n",
      "disappointing 15,640\n",
      "Added ['disappointment'] to the tokenized_text array.\n",
      "disappointment 10,520\n",
      "Added ['disapproval'] to the tokenized_text array.\n",
      "disapproval  21,406\n",
      "Added ['di', '##sa', '##pp', '##roving'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sa          3,736\n",
      "##pp          9,397\n",
      "##roving     22,046\n",
      "Added ['disbelief'] to the tokenized_text array.\n",
      "disbelief    12,537\n",
      "Added ['di', '##sb', '##eli', '##eve'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sb         19,022\n",
      "##eli        20,806\n",
      "##eve        18,697\n",
      "Added ['di', '##sb', '##eli', '##eving'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sb         19,022\n",
      "##eli        20,806\n",
      "##eving      23,559\n",
      "Added ['disc', '##ern', '##ing'] to the tokenized_text array.\n",
      "disc          5,860\n",
      "##ern        11,795\n",
      "##ing         2,075\n",
      "Added ['disco', '##mbo', '##bula', '##ted'] to the tokenized_text array.\n",
      "disco        12,532\n",
      "##mbo        13,344\n",
      "##bula       28,507\n",
      "##ted         3,064\n",
      "Added ['disco', '##m', '##fi', '##ted'] to the tokenized_text array.\n",
      "disco        12,532\n",
      "##m           2,213\n",
      "##fi          8,873\n",
      "##ted         3,064\n",
      "Added ['discomfort'] to the tokenized_text array.\n",
      "discomfort   17,964\n",
      "Added ['discomfort', '##ed'] to the tokenized_text array.\n",
      "discomfort   17,964\n",
      "##ed          2,098\n",
      "Added ['disco', '##nce', '##rted'] to the tokenized_text array.\n",
      "disco        12,532\n",
      "##nce         5,897\n",
      "##rted       17,724\n",
      "Added ['disconnected'] to the tokenized_text array.\n",
      "disconnected 23,657\n",
      "Added ['disco', '##ns', '##olate'] to the tokenized_text array.\n",
      "disco        12,532\n",
      "##ns          3,619\n",
      "##olate      19,425\n",
      "Added ['discontent'] to the tokenized_text array.\n",
      "discontent   27,648\n",
      "Added ['discontent', '##ed'] to the tokenized_text array.\n",
      "discontent   27,648\n",
      "##ed          2,098\n",
      "Added ['discount', '##ed'] to the tokenized_text array.\n",
      "discount     19,575\n",
      "##ed          2,098\n",
      "Added ['discouraged'] to the tokenized_text array.\n",
      "discouraged  22,585\n",
      "Added ['discovery'] to the tokenized_text array.\n",
      "discovery     5,456\n",
      "Added ['disc', '##rim', '##inating'] to the tokenized_text array.\n",
      "disc          5,860\n",
      "##rim        20,026\n",
      "##inating    19,185\n",
      "Added ['discussed'] to the tokenized_text array.\n",
      "discussed     6,936\n",
      "Added ['disdain'] to the tokenized_text array.\n",
      "disdain      25,134\n",
      "Added ['disdain', '##ed'] to the tokenized_text array.\n",
      "disdain      25,134\n",
      "##ed          2,098\n",
      "Added ['disdain', '##ful'] to the tokenized_text array.\n",
      "disdain      25,134\n",
      "##ful         3,993\n",
      "Added ['disdain', '##fully'] to the tokenized_text array.\n",
      "disdain      25,134\n",
      "##fully       7,699\n",
      "Added ['di', '##sen', '##chan', '##ted'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sen         5,054\n",
      "##chan       14,856\n",
      "##ted         3,064\n",
      "Added ['di', '##sen', '##ga', '##ged'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sen         5,054\n",
      "##ga          3,654\n",
      "##ged         5,999\n",
      "Added ['disgrace', '##d'] to the tokenized_text array.\n",
      "disgrace     29,591\n",
      "##d           2,094\n",
      "Added ['di', '##sg', '##run', '##tled'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sg         28,745\n",
      "##run        15,532\n",
      "##tled       14,782\n",
      "Added ['di', '##sg', '##run', '##tlement'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sg         28,745\n",
      "##run        15,532\n",
      "##tlement    24,007\n",
      "Added ['disgust'] to the tokenized_text array.\n",
      "disgust      12,721\n",
      "Added ['disgusted'] to the tokenized_text array.\n",
      "disgusted    17,733\n",
      "Added ['disgusted', '##ly'] to the tokenized_text array.\n",
      "disgusted    17,733\n",
      "##ly          2,135\n",
      "Added ['disgusting'] to the tokenized_text array.\n",
      "disgusting   19,424\n",
      "Added ['dish', '##ear', '##ten', '##ed'] to the tokenized_text array.\n",
      "dish          9,841\n",
      "##ear        14,644\n",
      "##ten         6,528\n",
      "##ed          2,098\n",
      "Added ['dish', '##ones', '##t'] to the tokenized_text array.\n",
      "dish          9,841\n",
      "##ones       21,821\n",
      "##t           2,102\n",
      "Added ['di', '##sil', '##lusion', '##ed'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sil        27,572\n",
      "##lusion     24,117\n",
      "##ed          2,098\n",
      "Added ['di', '##sin', '##cl', '##ined'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sin        11,493\n",
      "##cl         20,464\n",
      "##ined       21,280\n",
      "Added ['di', '##sing', '##en', '##uous'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sing        7,741\n",
      "##en          2,368\n",
      "##uous        8,918\n",
      "Added ['di', '##sin', '##ter', '##est'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sin        11,493\n",
      "##ter         3,334\n",
      "##est         4,355\n",
      "Added ['di', '##sin', '##ter', '##ested'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sin        11,493\n",
      "##ter         3,334\n",
      "##ested      17,944\n",
      "Added ['di', '##s', '##jo', '##int', '##ed'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##s           2,015\n",
      "##jo          5,558\n",
      "##int        18,447\n",
      "##ed          2,098\n",
      "Added ['dislike'] to the tokenized_text array.\n",
      "dislike      18,959\n",
      "Added ['disliked'] to the tokenized_text array.\n",
      "disliked     18,966\n",
      "Added ['di', '##sl', '##iki', '##ng'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sl         14,540\n",
      "##iki        17,471\n",
      "##ng          3,070\n",
      "Added ['di', '##sma', '##l'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sma        26,212\n",
      "##l           2,140\n",
      "Added ['di', '##sman'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sman       11,512\n",
      "Added ['dismay'] to the tokenized_text array.\n",
      "dismay       20,006\n",
      "Added ['dismay', '##ed'] to the tokenized_text array.\n",
      "dismay       20,006\n",
      "##ed          2,098\n",
      "Added ['dismiss', '##ive'] to the tokenized_text array.\n",
      "dismiss      19,776\n",
      "##ive         3,512\n",
      "Added ['di', '##so', '##bed', '##ient'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##so          6,499\n",
      "##bed         8,270\n",
      "##ient       11,638\n",
      "Added ['disorder', '##ly'] to the tokenized_text array.\n",
      "disorder      8,761\n",
      "##ly          2,135\n",
      "Added ['di', '##sor', '##iente', '##d'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sor        21,748\n",
      "##iente      25,099\n",
      "##d           2,094\n",
      "Added ['di', '##sp', '##air'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sp         13,102\n",
      "##air        11,215\n",
      "Added ['di', '##spar', '##aging'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##spar       27,694\n",
      "##aging      16,594\n",
      "Added ['di', '##sp', '##ass', '##ion', '##ate'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sp         13,102\n",
      "##ass        12,054\n",
      "##ion         3,258\n",
      "##ate         3,686\n",
      "Added ['di', '##sp', '##iri', '##ted'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sp         13,102\n",
      "##iri        15,735\n",
      "##ted         3,064\n",
      "Added ['di', '##sp', '##iri', '##ted', '##ness'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sp         13,102\n",
      "##iri        15,735\n",
      "##ted         3,064\n",
      "##ness        2,791\n",
      "Added ['di', '##sp', '##lea', '##sed'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sp         13,102\n",
      "##lea        19,738\n",
      "##sed         6,924\n",
      "Added ['displeasure'] to the tokenized_text array.\n",
      "displeasure  28,606\n",
      "Added ['di', '##s', '##qui', '##et'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##s           2,015\n",
      "##qui        15,549\n",
      "##et          3,388\n",
      "Added ['di', '##s', '##qui', '##ete', '##d'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##s           2,015\n",
      "##qui        15,549\n",
      "##ete        12,870\n",
      "##d           2,094\n",
      "Added ['disregard'] to the tokenized_text array.\n",
      "disregard    27,770\n",
      "Added ['di', '##sr', '##es', '##pe', '##ct', '##ful'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sr         21,338\n",
      "##es          2,229\n",
      "##pe          5,051\n",
      "##ct          6,593\n",
      "##ful         3,993\n",
      "Added ['disrupted'] to the tokenized_text array.\n",
      "disrupted    20,275\n",
      "Added ['disrupt', '##ive'] to the tokenized_text array.\n",
      "disrupt      23,217\n",
      "##ive         3,512\n",
      "Added ['dissatisfaction'] to the tokenized_text array.\n",
      "dissatisfaction 28,237\n",
      "Added ['dissatisfied'] to the tokenized_text array.\n",
      "dissatisfied 25,956\n",
      "Added ['di', '##ssa', '##tis', '##fy'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##ssa        11,488\n",
      "##tis         7,315\n",
      "##fy         12,031\n",
      "Added ['di', '##sse', '##cting'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sse        11,393\n",
      "##cting      11,873\n",
      "Added ['di', '##sso', '##cia', '##ted'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sso        24,137\n",
      "##cia         7,405\n",
      "##ted         3,064\n",
      "Added ['di', '##sson', '##ant'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sson        7,092\n",
      "##ant         4,630\n",
      "Added ['di', '##sta', '##in'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sta         9,153\n",
      "##in          2,378\n",
      "Added ['distant'] to the tokenized_text array.\n",
      "distant       6,802\n",
      "Added ['di', '##sta', '##ste'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sta         9,153\n",
      "##ste        13,473\n",
      "Added ['di', '##sta', '##ste', '##ful'] to the tokenized_text array.\n",
      "di            4,487\n",
      "##sta         9,153\n",
      "##ste        13,473\n",
      "##ful         3,993\n",
      "Added ['distracted'] to the tokenized_text array.\n",
      "distracted   11,116\n",
      "Added ['distraught'] to the tokenized_text array.\n",
      "distraught   25,348\n",
      "Added ['distress'] to the tokenized_text array.\n",
      "distress     12,893\n",
      "Added ['distressed'] to the tokenized_text array.\n",
      "distressed   24,305\n",
      "Added ['distress', '##ing'] to the tokenized_text array.\n",
      "distress     12,893\n",
      "##ing         2,075\n",
      "Added ['distrust'] to the tokenized_text array.\n",
      "distrust     29,245\n",
      "Added ['distrust', '##ful'] to the tokenized_text array.\n",
      "distrust     29,245\n",
      "##ful         3,993\n",
      "Added ['distrust', '##ing'] to the tokenized_text array.\n",
      "distrust     29,245\n",
      "##ing         2,075\n",
      "Added ['disturbed'] to the tokenized_text array.\n",
      "disturbed    12,491\n",
      "Added ['diverted'] to the tokenized_text array.\n",
      "diverted     18,356\n",
      "Added ['dod', '##gy'] to the tokenized_text array.\n",
      "dod          26,489\n",
      "##gy          6,292\n",
      "Added ['do', '##le', '##ful'] to the tokenized_text array.\n",
      "do            2,079\n",
      "##le          2,571\n",
      "##ful         3,993\n",
      "Added ['do', '##lt', '##ish'] to the tokenized_text array.\n",
      "do            2,079\n",
      "##lt          7,096\n",
      "##ish         4,509\n",
      "Added ['dominant'] to the tokenized_text array.\n",
      "dominant      7,444\n",
      "Added ['dominating'] to the tokenized_text array.\n",
      "dominating   21,949\n",
      "Added ['dom', '##ine', '##ering'] to the tokenized_text array.\n",
      "dom          14,383\n",
      "##ine         3,170\n",
      "##ering       7,999\n",
      "Added ['done'] to the tokenized_text array.\n",
      "done          2,589\n",
      "Added ['doomed'] to the tokenized_text array.\n",
      "doomed       20,076\n",
      "Added ['do', '##pe', '##y'] to the tokenized_text array.\n",
      "do            2,079\n",
      "##pe          5,051\n",
      "##y           2,100\n",
      "Added ['dot', '##ing'] to the tokenized_text array.\n",
      "dot          11,089\n",
      "##ing         2,075\n",
      "Added ['doubt'] to the tokenized_text array.\n",
      "doubt         4,797\n",
      "Added ['doubt', '##er'] to the tokenized_text array.\n",
      "doubt         4,797\n",
      "##er          2,121\n",
      "Added ['doubtful'] to the tokenized_text array.\n",
      "doubtful     21,888\n",
      "Added ['doubtful', '##ly'] to the tokenized_text array.\n",
      "doubtful     21,888\n",
      "##ly          2,135\n",
      "Added ['doubtful', '##ness'] to the tokenized_text array.\n",
      "doubtful     21,888\n",
      "##ness        2,791\n",
      "Added ['doubt', '##ing'] to the tokenized_text array.\n",
      "doubt         4,797\n",
      "##ing         2,075\n",
      "Added ['do', '##ur'] to the tokenized_text array.\n",
      "do            2,079\n",
      "##ur          3,126\n",
      "Added ['down'] to the tokenized_text array.\n",
      "down          2,091\n",
      "Added ['down', '##cast'] to the tokenized_text array.\n",
      "down          2,091\n",
      "##cast       10,526\n",
      "Added ['down', '##hearted'] to the tokenized_text array.\n",
      "down          2,091\n",
      "##hearted    27,693\n",
      "Added ['down', '##hearted', '##ness'] to the tokenized_text array.\n",
      "down          2,091\n",
      "##hearted    27,693\n",
      "##ness        2,791\n",
      "Added ['down', '##tro', '##dden'] to the tokenized_text array.\n",
      "down          2,091\n",
      "##tro        13,181\n",
      "##dden       17,101\n",
      "Added ['do', '##zing'] to the tokenized_text array.\n",
      "do            2,079\n",
      "##zing        6,774\n",
      "Added ['drained'] to the tokenized_text array.\n",
      "drained      11,055\n",
      "Added ['dramatic'] to the tokenized_text array.\n",
      "dramatic      6,918\n",
      "Added ['drawn'] to the tokenized_text array.\n",
      "drawn         4,567\n",
      "Added ['dread'] to the tokenized_text array.\n",
      "dread        14,436\n",
      "Added ['dreadful'] to the tokenized_text array.\n",
      "dreadful     21,794\n",
      "Added ['dread', '##ing'] to the tokenized_text array.\n",
      "dread        14,436\n",
      "##ing         2,075\n",
      "Added ['dreaming'] to the tokenized_text array.\n",
      "dreaming     12,802\n",
      "Added ['dream', '##y'] to the tokenized_text array.\n",
      "dream         3,959\n",
      "##y           2,100\n",
      "Added ['dr', '##ear', '##y'] to the tokenized_text array.\n",
      "dr            2,852\n",
      "##ear        14,644\n",
      "##y           2,100\n",
      "Added ['driven'] to the tokenized_text array.\n",
      "driven        5,533\n",
      "Added ['dr', '##ows', '##y'] to the tokenized_text array.\n",
      "dr            2,852\n",
      "##ows        15,568\n",
      "##y           2,100\n",
      "Added ['drugged'] to the tokenized_text array.\n",
      "drugged      25,483\n",
      "Added ['drunk'] to the tokenized_text array.\n",
      "drunk         7,144\n",
      "Added ['drunken', '##ness'] to the tokenized_text array.\n",
      "drunken      15,967\n",
      "##ness        2,791\n",
      "Added ['dub', '##ie', '##ty'] to the tokenized_text array.\n",
      "dub          12,931\n",
      "##ie          2,666\n",
      "##ty          3,723\n",
      "Added ['dubious'] to the tokenized_text array.\n",
      "dubious      22,917\n",
      "Added ['dubious', '##ly'] to the tokenized_text array.\n",
      "dubious      22,917\n",
      "##ly          2,135\n",
      "Added ['dull'] to the tokenized_text array.\n",
      "dull         10,634\n",
      "Added ['dumb'] to the tokenized_text array.\n",
      "dumb         12,873\n",
      "Added ['dumb', '##fo', '##und'] to the tokenized_text array.\n",
      "dumb         12,873\n",
      "##fo         14,876\n",
      "##und         8,630\n",
      "Added ['dumb', '##founded'] to the tokenized_text array.\n",
      "dumb         12,873\n",
      "##founded    21,001\n",
      "Added ['dumb', '##st', '##ruck'] to the tokenized_text array.\n",
      "dumb         12,873\n",
      "##st          3,367\n",
      "##ruck       29,314\n",
      "Added ['du', '##m', '##founded'] to the tokenized_text array.\n",
      "du            4,241\n",
      "##m           2,213\n",
      "##founded    21,001\n",
      "Added ['du', '##pe'] to the tokenized_text array.\n",
      "du            4,241\n",
      "##pe          5,051\n",
      "Added ['du', '##pl', '##ici', '##tou', '##s'] to the tokenized_text array.\n",
      "du            4,241\n",
      "##pl         24,759\n",
      "##ici        28,775\n",
      "##tou        24,826\n",
      "##s           2,015\n",
      "Added ['d', '##ys', '##ph', '##oric'] to the tokenized_text array.\n",
      "d             1,040\n",
      "##ys          7,274\n",
      "##ph          8,458\n",
      "##oric       29,180\n",
      "Added ['eager'] to the tokenized_text array.\n",
      "eager         9,461\n",
      "Added ['eager', '##ness'] to the tokenized_text array.\n",
      "eager         9,461\n",
      "##ness        2,791\n",
      "Added ['earnest'] to the tokenized_text array.\n",
      "earnest      17,300\n",
      "Added ['easy'] to the tokenized_text array.\n",
      "easy          3,733\n",
      "Added ['e', '##bu', '##llie', '##nt'] to the tokenized_text array.\n",
      "e             1,041\n",
      "##bu          8,569\n",
      "##llie       23,697\n",
      "##nt          3,372\n",
      "Added ['ecstasy'] to the tokenized_text array.\n",
      "ecstasy      19,069\n",
      "Added ['ec', '##static'] to the tokenized_text array.\n",
      "ec           14,925\n",
      "##static     16,677\n",
      "Added ['ec', '##static', '##ally'] to the tokenized_text array.\n",
      "ec           14,925\n",
      "##static     16,677\n",
      "##ally        3,973\n",
      "Added ['ed', '##gy'] to the tokenized_text array.\n",
      "ed            3,968\n",
      "##gy          6,292\n",
      "Added ['eerie'] to the tokenized_text array.\n",
      "eerie        18,823\n",
      "Added ['e', '##ff', '##ul', '##gent'] to the tokenized_text array.\n",
      "e             1,041\n",
      "##ff          4,246\n",
      "##ul          5,313\n",
      "##gent       11,461\n",
      "Added ['ego', '##istic'] to the tokenized_text array.\n",
      "ego          13,059\n",
      "##istic       6,553\n",
      "Added ['ego', '##tist', '##ical'] to the tokenized_text array.\n",
      "ego          13,059\n",
      "##tist       16,774\n",
      "##ical        7,476\n",
      "Added ['e', '##gre', '##gio', '##us'] to the tokenized_text array.\n",
      "e             1,041\n",
      "##gre        17,603\n",
      "##gio        11,411\n",
      "##us          2,271\n",
      "Added ['el', '##ated'] to the tokenized_text array.\n",
      "el            3,449\n",
      "##ated        4,383\n",
      "Added ['el', '##ation'] to the tokenized_text array.\n",
      "el            3,449\n",
      "##ation       3,370\n",
      "Added ['electrified'] to the tokenized_text array.\n",
      "electrified  18,042\n",
      "Added ['elusive'] to the tokenized_text array.\n",
      "elusive      26,475\n",
      "Added ['embarrassed'] to the tokenized_text array.\n",
      "embarrassed  10,339\n",
      "Added ['embarrassment'] to the tokenized_text array.\n",
      "embarrassment 14,325\n",
      "Added ['em', '##bit', '##tered'] to the tokenized_text array.\n",
      "em            7,861\n",
      "##bit        16,313\n",
      "##tered      14,050\n",
      "Added ['em', '##body'] to the tokenized_text array.\n",
      "em            7,861\n",
      "##body       23,684\n",
      "Added ['emotional'] to the tokenized_text array.\n",
      "emotional     6,832\n",
      "Added ['emotion', '##less'] to the tokenized_text array.\n",
      "emotion       7,603\n",
      "##less        3,238\n",
      "Added ['em', '##path', '##etic'] to the tokenized_text array.\n",
      "em            7,861\n",
      "##path       15,069\n",
      "##etic       16,530\n",
      "Added ['em', '##pathic'] to the tokenized_text array.\n",
      "em            7,861\n",
      "##pathic     25,940\n",
      "Added ['empathy'] to the tokenized_text array.\n",
      "empathy      26,452\n",
      "Added ['emptiness'] to the tokenized_text array.\n",
      "emptiness    23,397\n",
      "Added ['empty'] to the tokenized_text array.\n",
      "empty         4,064\n",
      "Added ['en', '##amo', '##red'] to the tokenized_text array.\n",
      "en            4,372\n",
      "##amo        22,591\n",
      "##red         5,596\n",
      "Added ['enchanted'] to the tokenized_text array.\n",
      "enchanted    22,454\n",
      "Added ['encouraged'] to the tokenized_text array.\n",
      "encouraged    6,628\n",
      "Added ['encouragement'] to the tokenized_text array.\n",
      "encouragement 15,846\n",
      "Added ['encouraging'] to the tokenized_text array.\n",
      "encouraging  11,434\n",
      "Added ['end', '##ear', '##ed'] to the tokenized_text array.\n",
      "end           2,203\n",
      "##ear        14,644\n",
      "##ed          2,098\n",
      "Added ['end', '##earing'] to the tokenized_text array.\n",
      "end           2,203\n",
      "##earing     27,242\n",
      "Added ['enduring'] to the tokenized_text array.\n",
      "enduring     16,762\n",
      "Added ['energetic'] to the tokenized_text array.\n",
      "energetic    18,114\n",
      "Added ['en', '##er', '##gi', '##zed'] to the tokenized_text array.\n",
      "en            4,372\n",
      "##er          2,121\n",
      "##gi          5,856\n",
      "##zed         5,422\n",
      "Added ['engaged'] to the tokenized_text array.\n",
      "engaged       5,117\n",
      "Added ['eng', '##ross', '##ed'] to the tokenized_text array.\n",
      "eng          25,540\n",
      "##ross       25,725\n",
      "##ed          2,098\n",
      "Added ['eng', '##ross', '##ment'] to the tokenized_text array.\n",
      "eng          25,540\n",
      "##ross       25,725\n",
      "##ment        3,672\n",
      "Added ['enigma', '##tic'] to the tokenized_text array.\n",
      "enigma       26,757\n",
      "##tic         4,588\n",
      "Added ['enjoy'] to the tokenized_text array.\n",
      "enjoy         5,959\n",
      "Added ['enjoying'] to the tokenized_text array.\n",
      "enjoying      9,107\n",
      "Added ['enjoyment'] to the tokenized_text array.\n",
      "enjoyment    20,195\n",
      "Added ['en', '##light', '##ened'] to the tokenized_text array.\n",
      "en            4,372\n",
      "##light       7,138\n",
      "##ened        6,675\n",
      "Added ['en', '##mity'] to the tokenized_text array.\n",
      "en            4,372\n",
      "##mity       16,383\n",
      "Added ['en', '##nu', '##i'] to the tokenized_text array.\n",
      "en            4,372\n",
      "##nu         11,231\n",
      "##i           2,072\n",
      "Added ['enraged'] to the tokenized_text array.\n",
      "enraged      18,835\n",
      "Added ['en', '##rag', '##ing'] to the tokenized_text array.\n",
      "en            4,372\n",
      "##rag        29,181\n",
      "##ing         2,075\n",
      "Added ['en', '##ra', '##pt', '##ured'] to the tokenized_text array.\n",
      "en            4,372\n",
      "##ra          2,527\n",
      "##pt         13,876\n",
      "##ured       12,165\n",
      "Added ['entertained'] to the tokenized_text array.\n",
      "entertained  21,474\n",
      "Added ['en', '##th', '##ral', '##led'] to the tokenized_text array.\n",
      "en            4,372\n",
      "##th          2,705\n",
      "##ral         7,941\n",
      "##led         3,709\n",
      "Added ['en', '##thus', '##ed'] to the tokenized_text array.\n",
      "en            4,372\n",
      "##thus       19,877\n",
      "##ed          2,098\n",
      "Added ['enthusiasm'] to the tokenized_text array.\n",
      "enthusiasm   12,024\n",
      "Added ['enthusiastic'] to the tokenized_text array.\n",
      "enthusiastic 14,727\n",
      "Added ['en', '##tic', '##ed'] to the tokenized_text array.\n",
      "en            4,372\n",
      "##tic         4,588\n",
      "##ed          2,098\n",
      "Added ['entrance', '##d'] to the tokenized_text array.\n",
      "entrance      4,211\n",
      "##d           2,094\n",
      "Added ['en', '##vious'] to the tokenized_text array.\n",
      "en            4,372\n",
      "##vious      24,918\n",
      "Added ['envy'] to the tokenized_text array.\n",
      "envy         21,103\n",
      "Added ['erotic', '##ally'] to the tokenized_text array.\n",
      "erotic       14,253\n",
      "##ally        3,973\n",
      "Added ['estranged'] to the tokenized_text array.\n",
      "estranged    24,211\n",
      "Added ['etched'] to the tokenized_text array.\n",
      "etched       20,286\n",
      "Added ['eu', '##ph', '##oric'] to the tokenized_text array.\n",
      "eu            7,327\n",
      "##ph          8,458\n",
      "##oric       29,180\n",
      "Added ['evaluating'] to the tokenized_text array.\n",
      "evaluating   23,208\n",
      "Added ['eva', '##sive'] to the tokenized_text array.\n",
      "eva           9,345\n",
      "##sive       12,742\n",
      "Added ['evil'] to the tokenized_text array.\n",
      "evil          4,763\n",
      "Added ['ev', '##oke'] to the tokenized_text array.\n",
      "ev           23,408\n",
      "##oke        11,045\n",
      "Added ['ex', '##ace', '##rba', '##ted'] to the tokenized_text array.\n",
      "ex            4,654\n",
      "##ace        10,732\n",
      "##rba        28,483\n",
      "##ted         3,064\n",
      "Added ['ex', '##al', '##ted'] to the tokenized_text array.\n",
      "ex            4,654\n",
      "##al          2,389\n",
      "##ted         3,064\n",
      "Added ['examining'] to the tokenized_text array.\n",
      "examining    12,843\n",
      "Added ['ex', '##as', '##per', '##ate'] to the tokenized_text array.\n",
      "ex            4,654\n",
      "##as          3,022\n",
      "##per         4,842\n",
      "##ate         3,686\n",
      "Added ['exasperated'] to the tokenized_text array.\n",
      "exasperated  24,379\n",
      "Added ['ex', '##as', '##peration'] to the tokenized_text array.\n",
      "ex            4,654\n",
      "##as          3,022\n",
      "##peration   29,487\n",
      "Added ['excited'] to the tokenized_text array.\n",
      "excited       7,568\n",
      "Added ['excitedly'] to the tokenized_text array.\n",
      "excitedly    23,885\n",
      "Added ['excitement'] to the tokenized_text array.\n",
      "excitement    8,277\n",
      "Added ['ex', '##cl', '##ama', '##tion'] to the tokenized_text array.\n",
      "ex            4,654\n",
      "##cl         20,464\n",
      "##ama         8,067\n",
      "##tion        3,508\n",
      "Added ['ex', '##cl', '##ama', '##tory'] to the tokenized_text array.\n",
      "ex            4,654\n",
      "##cl         20,464\n",
      "##ama         8,067\n",
      "##tory        7,062\n",
      "Added ['exhausted'] to the tokenized_text array.\n",
      "exhausted     9,069\n",
      "Added ['exhaustion'] to the tokenized_text array.\n",
      "exhaustion   15,575\n",
      "Added ['exhaust', '##ive'] to the tokenized_text array.\n",
      "exhaust      15,095\n",
      "##ive         3,512\n",
      "Added ['ex', '##hila', '##rated'] to the tokenized_text array.\n",
      "ex            4,654\n",
      "##hila       26,415\n",
      "##rated       9,250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added ['ex', '##hila', '##ration'] to the tokenized_text array.\n",
      "ex            4,654\n",
      "##hila       26,415\n",
      "##ration      8,156\n",
      "Added ['exited'] to the tokenized_text array.\n",
      "exited       15,284\n",
      "Added ['expect', '##ant'] to the tokenized_text array.\n",
      "expect        5,987\n",
      "##ant         4,630\n",
      "Added ['expectation'] to the tokenized_text array.\n",
      "expectation  17,626\n",
      "Added ['expecting'] to the tokenized_text array.\n",
      "expecting     8,074\n",
      "Added ['explain'] to the tokenized_text array.\n",
      "explain       4,863\n",
      "Added ['explaining'] to the tokenized_text array.\n",
      "explaining    9,990\n",
      "Added ['exploit', '##ive'] to the tokenized_text array.\n",
      "exploit      18,077\n",
      "##ive         3,512\n",
      "Added ['explosive'] to the tokenized_text array.\n",
      "explosive    11,355\n",
      "Added ['exposure'] to the tokenized_text array.\n",
      "exposure      7,524\n",
      "Added ['expressive'] to the tokenized_text array.\n",
      "expressive   22,570\n",
      "Added ['ex', '##uber', '##ant'] to the tokenized_text array.\n",
      "ex            4,654\n",
      "##uber       21,436\n",
      "##ant         4,630\n",
      "Added ['ex', '##ult', '##ant'] to the tokenized_text array.\n",
      "ex            4,654\n",
      "##ult        11,314\n",
      "##ant         4,630\n",
      "Added ['ex', '##ult', '##ed'] to the tokenized_text array.\n",
      "ex            4,654\n",
      "##ult        11,314\n",
      "##ed          2,098\n",
      "Added ['eye'] to the tokenized_text array.\n",
      "eye           3,239\n",
      "Added ['eyed'] to the tokenized_text array.\n",
      "eyed          7,168\n",
      "Added ['faced'] to the tokenized_text array.\n",
      "faced         4,320\n",
      "Added ['face', '##tious'] to the tokenized_text array.\n",
      "face          2,227\n",
      "##tious      20,771\n",
      "Added ['failure'] to the tokenized_text array.\n",
      "failure       4,945\n",
      "Added ['faint'] to the tokenized_text array.\n",
      "faint         8,143\n",
      "Added ['fair'] to the tokenized_text array.\n",
      "fair          4,189\n",
      "Added ['fake'] to the tokenized_text array.\n",
      "fake          8,275\n",
      "Added ['fa', '##king'] to the tokenized_text array.\n",
      "fa            6,904\n",
      "##king        6,834\n",
      "Added ['fa', '##lter'] to the tokenized_text array.\n",
      "fa            6,904\n",
      "##lter       21,928\n",
      "Added ['fa', '##mis', '##hed'] to the tokenized_text array.\n",
      "fa            6,904\n",
      "##mis        15,630\n",
      "##hed         9,072\n",
      "Added ['fan', '##atic'] to the tokenized_text array.\n",
      "fan           5,470\n",
      "##atic       12,070\n",
      "Added ['fan', '##ciful'] to the tokenized_text array.\n",
      "fan           5,470\n",
      "##ciful      26,336\n",
      "Added ['far', '##t'] to the tokenized_text array.\n",
      "far           2,521\n",
      "##t           2,102\n",
      "Added ['fascinated'] to the tokenized_text array.\n",
      "fascinated   15,677\n",
      "Added ['fast', '##idi', '##ous'] to the tokenized_text array.\n",
      "fast          3,435\n",
      "##idi        28,173\n",
      "##ous         3,560\n",
      "Added ['fatigue'] to the tokenized_text array.\n",
      "fatigue      16,342\n",
      "Added ['fatigue', '##d'] to the tokenized_text array.\n",
      "fatigue      16,342\n",
      "##d           2,094\n",
      "Added ['fault', '##fin', '##ding'] to the tokenized_text array.\n",
      "fault         6,346\n",
      "##fin        16,294\n",
      "##ding        4,667\n",
      "Added ['favorable'] to the tokenized_text array.\n",
      "favorable    11,119\n",
      "Added ['fa', '##wn', '##ing'] to the tokenized_text array.\n",
      "fa            6,904\n",
      "##wn          7,962\n",
      "##ing         2,075\n",
      "Added ['fa', '##zed'] to the tokenized_text array.\n",
      "fa            6,904\n",
      "##zed         5,422\n",
      "Added ['fear'] to the tokenized_text array.\n",
      "fear          3,571\n",
      "Added ['feared'] to the tokenized_text array.\n",
      "feared        8,615\n",
      "Added ['fearful'] to the tokenized_text array.\n",
      "fearful      19,725\n",
      "Added ['fearing'] to the tokenized_text array.\n",
      "fearing      14,892\n",
      "Added ['fearless'] to the tokenized_text array.\n",
      "fearless     22,518\n",
      "Added ['fears', '##ome'] to the tokenized_text array.\n",
      "fears        10,069\n",
      "##ome         8,462\n",
      "Added ['fe', '##ckle', '##ss'] to the tokenized_text array.\n",
      "fe           10,768\n",
      "##ckle       19,250\n",
      "##ss          4,757\n",
      "Added ['fed'] to the tokenized_text array.\n",
      "fed           7,349\n",
      "Added ['fee', '##ble'] to the tokenized_text array.\n",
      "fee           7,408\n",
      "##ble         3,468\n",
      "Added ['fei', '##gn'] to the tokenized_text array.\n",
      "fei          24,664\n",
      "##gn         16,206\n",
      "Added ['fe', '##lic', '##ito', '##us'] to the tokenized_text array.\n",
      "fe           10,768\n",
      "##lic        10,415\n",
      "##ito         9,956\n",
      "##us          2,271\n",
      "Added ['ferocious'] to the tokenized_text array.\n",
      "ferocious    27,863\n",
      "Added ['fe', '##rocity'] to the tokenized_text array.\n",
      "fe           10,768\n",
      "##rocity     21,735\n",
      "Added ['fest', '##ive'] to the tokenized_text array.\n",
      "fest         17,037\n",
      "##ive         3,512\n",
      "Added ['fi', '##dget', '##y'] to the tokenized_text array.\n",
      "fi           10,882\n",
      "##dget       24,291\n",
      "##y           2,100\n",
      "Added ['fi', '##end', '##ish'] to the tokenized_text array.\n",
      "fi           10,882\n",
      "##end        10,497\n",
      "##ish         4,509\n",
      "Added ['fierce'] to the tokenized_text array.\n",
      "fierce        9,205\n",
      "Added ['fiery'] to the tokenized_text array.\n",
      "fiery        15,443\n",
      "Added ['fighting'] to the tokenized_text array.\n",
      "fighting      3,554\n",
      "Added ['fine'] to the tokenized_text array.\n",
      "fine          2,986\n",
      "Added ['finished'] to the tokenized_text array.\n",
      "finished      2,736\n",
      "Added ['firm'] to the tokenized_text array.\n",
      "firm          3,813\n",
      "Added ['fish', '##y'] to the tokenized_text array.\n",
      "fish          3,869\n",
      "##y           2,100\n",
      "Added ['fix', '##ated'] to the tokenized_text array.\n",
      "fix           8,081\n",
      "##ated        4,383\n",
      "Added ['fixed'] to the tokenized_text array.\n",
      "fixed         4,964\n",
      "Added ['fl', '##ab', '##berg', '##ast', '##ed'] to the tokenized_text array.\n",
      "fl           13,109\n",
      "##ab          7,875\n",
      "##berg        4,059\n",
      "##ast        14,083\n",
      "##ed          2,098\n",
      "Added ['flaming'] to the tokenized_text array.\n",
      "flaming      19,091\n",
      "Added ['flat'] to the tokenized_text array.\n",
      "flat          4,257\n",
      "Added ['fl', '##au', '##nting'] to the tokenized_text array.\n",
      "fl           13,109\n",
      "##au          4,887\n",
      "##nting      24,360\n",
      "Added ['flight', '##y'] to the tokenized_text array.\n",
      "flight        3,462\n",
      "##y           2,100\n",
      "Added ['flip', '##pan', '##t'] to the tokenized_text array.\n",
      "flip         11,238\n",
      "##pan         9,739\n",
      "##t           2,102\n",
      "Added ['flipped'] to the tokenized_text array.\n",
      "flipped       9,357\n",
      "Added ['flirt', '##ation'] to the tokenized_text array.\n",
      "flirt        27,978\n",
      "##ation       3,370\n",
      "Added ['flirt', '##ati', '##ous'] to the tokenized_text array.\n",
      "flirt        27,978\n",
      "##ati        10,450\n",
      "##ous         3,560\n",
      "Added ['flirt', '##y'] to the tokenized_text array.\n",
      "flirt        27,978\n",
      "##y           2,100\n",
      "Added ['floor', '##ed'] to the tokenized_text array.\n",
      "floor         2,723\n",
      "##ed          2,098\n",
      "Added ['flu', '##mm', '##ox', '##ed'] to the tokenized_text array.\n",
      "flu          19,857\n",
      "##mm          7,382\n",
      "##ox         11,636\n",
      "##ed          2,098\n",
      "Added ['flu', '##stered'] to the tokenized_text array.\n",
      "flu          19,857\n",
      "##stered     24,167\n",
      "Added ['focus'] to the tokenized_text array.\n",
      "focus         3,579\n",
      "Added ['focused'] to the tokenized_text array.\n",
      "focused       4,208\n",
      "Added ['focusing'] to the tokenized_text array.\n",
      "focusing      7,995\n",
      "Added ['foil', '##ed'] to the tokenized_text array.\n",
      "foil         17,910\n",
      "##ed          2,098\n",
      "Added ['foolish'] to the tokenized_text array.\n",
      "foolish      13,219\n",
      "Added ['for', '##be', '##aring'] to the tokenized_text array.\n",
      "for           2,005\n",
      "##be          4,783\n",
      "##aring      22,397\n",
      "Added ['forbid', '##ding'] to the tokenized_text array.\n",
      "forbid       27,206\n",
      "##ding        4,667\n",
      "Added ['forced'] to the tokenized_text array.\n",
      "forced        3,140\n",
      "Added ['forceful'] to the tokenized_text array.\n",
      "forceful     28,552\n",
      "Added ['for', '##feit', '##ed'] to the tokenized_text array.\n",
      "for           2,005\n",
      "##feit       21,156\n",
      "##ed          2,098\n",
      "Added ['for', '##lor', '##n'] to the tokenized_text array.\n",
      "for           2,005\n",
      "##lor        10,626\n",
      "##n           2,078\n",
      "Added ['fortunate'] to the tokenized_text array.\n",
      "fortunate    19,590\n",
      "Added ['forward'] to the tokenized_text array.\n",
      "forward       2,830\n",
      "Added ['foul'] to the tokenized_text array.\n",
      "foul         12,487\n",
      "Added ['fra', '##ct', '##ious'] to the tokenized_text array.\n",
      "fra          25,312\n",
      "##ct          6,593\n",
      "##ious        6,313\n",
      "Added ['fragile'] to the tokenized_text array.\n",
      "fragile      13,072\n",
      "Added ['frantic'] to the tokenized_text array.\n",
      "frantic      15,762\n",
      "Added ['fraudulent'] to the tokenized_text array.\n",
      "fraudulent   27,105\n",
      "Added ['fra', '##ught'] to the tokenized_text array.\n",
      "fra          25,312\n",
      "##ught       18,533\n",
      "Added ['fra', '##zzled'] to the tokenized_text array.\n",
      "fra          25,312\n",
      "##zzled      17,269\n",
      "Added ['freaked'] to the tokenized_text array.\n",
      "freaked      22,783\n",
      "Added ['fr', '##en', '##zie', '##d'] to the tokenized_text array.\n",
      "fr           10,424\n",
      "##en          2,368\n",
      "##zie        14,272\n",
      "##d           2,094\n",
      "Added ['fr', '##et', '##ful'] to the tokenized_text array.\n",
      "fr           10,424\n",
      "##et          3,388\n",
      "##ful         3,993\n",
      "Added ['friend', '##liness'] to the tokenized_text array.\n",
      "friend        2,767\n",
      "##liness     20,942\n",
      "Added ['friendly'] to the tokenized_text array.\n",
      "friendly      5,379\n",
      "Added ['fright'] to the tokenized_text array.\n",
      "fright       25,966\n",
      "Added ['frightened'] to the tokenized_text array.\n",
      "frightened   10,363\n",
      "Added ['frightening'] to the tokenized_text array.\n",
      "frightening  17,115\n",
      "Added ['fr', '##ig', '##id'] to the tokenized_text array.\n",
      "fr           10,424\n",
      "##ig          8,004\n",
      "##id          3,593\n",
      "Added ['fr', '##isk', '##y'] to the tokenized_text array.\n",
      "fr           10,424\n",
      "##isk        20,573\n",
      "##y           2,100\n",
      "Added ['fr', '##olic', '##ker'] to the tokenized_text array.\n",
      "fr           10,424\n",
      "##olic       23,518\n",
      "##ker         5,484\n",
      "Added ['frown'] to the tokenized_text array.\n",
      "frown        11,330\n",
      "Added ['frowning'] to the tokenized_text array.\n",
      "frowning     14,587\n",
      "Added ['frozen'] to the tokenized_text array.\n",
      "frozen        7,708\n",
      "Added ['fr', '##ump', '##y'] to the tokenized_text array.\n",
      "fr           10,424\n",
      "##ump        24,237\n",
      "##y           2,100\n",
      "Added ['frustrated'] to the tokenized_text array.\n",
      "frustrated   10,206\n",
      "Added ['frustration'] to the tokenized_text array.\n",
      "frustration   9,135\n",
      "Added ['fulfilled'] to the tokenized_text array.\n",
      "fulfilled    16,829\n",
      "Added ['fu', '##med'] to the tokenized_text array.\n",
      "fu           11,865\n",
      "##med         7,583\n",
      "Added ['fu', '##ming'] to the tokenized_text array.\n",
      "fu           11,865\n",
      "##ming        6,562\n",
      "Added ['fun'] to the tokenized_text array.\n",
      "fun           4,569\n",
      "Added ['funny'] to the tokenized_text array.\n",
      "funny         6,057\n",
      "Added ['furious'] to the tokenized_text array.\n",
      "furious       9,943\n",
      "Added ['furiously'] to the tokenized_text array.\n",
      "furiously    20,322\n",
      "Added ['furious', '##ness'] to the tokenized_text array.\n",
      "furious       9,943\n",
      "##ness        2,791\n",
      "Added ['furrowed'] to the tokenized_text array.\n",
      "furrowed     20,555\n",
      "Added ['fur', '##tive'] to the tokenized_text array.\n",
      "fur           6,519\n",
      "##tive        6,024\n",
      "Added ['fury'] to the tokenized_text array.\n",
      "fury          8,111\n",
      "Added ['fuss', '##y'] to the tokenized_text array.\n",
      "fuss         28,554\n",
      "##y           2,100\n",
      "Added ['gall', '##ed'] to the tokenized_text array.\n",
      "gall         26,033\n",
      "##ed          2,098\n",
      "Added ['gall', '##ing'] to the tokenized_text array.\n",
      "gall         26,033\n",
      "##ing         2,075\n",
      "Added ['gasp'] to the tokenized_text array.\n",
      "gasp         12,008\n",
      "Added ['gasped'] to the tokenized_text array.\n",
      "gasped        8,535\n",
      "Added ['gasping'] to the tokenized_text array.\n",
      "gasping      17,054\n",
      "Added ['gay'] to the tokenized_text array.\n",
      "gay           5,637\n",
      "Added ['gazing'] to the tokenized_text array.\n",
      "gazing       16,448\n",
      "Added ['gen', '##ial'] to the tokenized_text array.\n",
      "gen           8,991\n",
      "##ial         4,818\n",
      "Added ['gentle'] to the tokenized_text array.\n",
      "gentle        7,132\n",
      "Added ['genuine'] to the tokenized_text array.\n",
      "genuine      10,218\n",
      "Added ['g', '##has', '##tly'] to the tokenized_text array.\n",
      "g             1,043\n",
      "##has        14,949\n",
      "##tly        14,626\n",
      "Added ['gi', '##ddy'] to the tokenized_text array.\n",
      "gi           21,025\n",
      "##ddy        14,968\n",
      "Added ['giggle'] to the tokenized_text array.\n",
      "giggle       17,565\n",
      "Added ['giggling'] to the tokenized_text array.\n",
      "giggling     21,783\n",
      "Added ['glad'] to the tokenized_text array.\n",
      "glad          5,580\n",
      "Added ['glad', '##dened'] to the tokenized_text array.\n",
      "glad          5,580\n",
      "##dened      24,589\n",
      "Added ['glad', '##iol', '##a'] to the tokenized_text array.\n",
      "glad          5,580\n",
      "##iol        20,282\n",
      "##a           2,050\n",
      "Added ['glad', '##ness'] to the tokenized_text array.\n",
      "glad          5,580\n",
      "##ness        2,791\n",
      "Added ['glad', '##some'] to the tokenized_text array.\n",
      "glad          5,580\n",
      "##some       14,045\n",
      "Added ['glare'] to the tokenized_text array.\n",
      "glare        10,982\n",
      "Added ['glaring'] to the tokenized_text array.\n",
      "glaring      16,124\n",
      "Added ['glazed'] to the tokenized_text array.\n",
      "glazed       19,724\n",
      "Added ['glee'] to the tokenized_text array.\n",
      "glee         18,874\n",
      "Added ['glee', '##ful'] to the tokenized_text array.\n",
      "glee         18,874\n",
      "##ful         3,993\n",
      "Added ['glee', '##fully'] to the tokenized_text array.\n",
      "glee         18,874\n",
      "##fully       7,699\n",
      "Added ['g', '##lib'] to the tokenized_text array.\n",
      "g             1,043\n",
      "##lib        29,521\n",
      "Added ['g', '##lo', '##ating'] to the tokenized_text array.\n",
      "g             1,043\n",
      "##lo          4,135\n",
      "##ating       5,844\n",
      "Added ['gloom'] to the tokenized_text array.\n",
      "gloom        24,067\n",
      "Added ['gloom', '##y'] to the tokenized_text array.\n",
      "gloom        24,067\n",
      "##y           2,100\n",
      "Added ['glow', '##ering'] to the tokenized_text array.\n",
      "glow          8,652\n",
      "##ering       7,999\n",
      "Added ['glowing'] to the tokenized_text array.\n",
      "glowing      10,156\n",
      "Added ['g', '##lum'] to the tokenized_text array.\n",
      "g             1,043\n",
      "##lum        12,942\n",
      "Added ['g', '##nar', '##l'] to the tokenized_text array.\n",
      "g             1,043\n",
      "##nar        11,802\n",
      "##l           2,140\n",
      "Added ['go', '##bs', '##mack', '##ed'] to the tokenized_text array.\n",
      "go            2,175\n",
      "##bs          5,910\n",
      "##mack       26,945\n",
      "##ed          2,098\n",
      "Added ['good'] to the tokenized_text array.\n",
      "good          2,204\n",
      "Added ['goofy'] to the tokenized_text array.\n",
      "goofy        27,243\n",
      "Added ['gossip', '##y'] to the tokenized_text array.\n",
      "gossip       13,761\n",
      "##y           2,100\n",
      "Added ['grand', '##ios', '##e'] to the tokenized_text array.\n",
      "grand         2,882\n",
      "##ios        10,735\n",
      "##e           2,063\n",
      "Added ['grateful'] to the tokenized_text array.\n",
      "grateful      8,794\n",
      "Added ['gr', '##ati', '##fied'] to the tokenized_text array.\n",
      "gr           24,665\n",
      "##ati        10,450\n",
      "##fied       10,451\n",
      "Added ['grave'] to the tokenized_text array.\n",
      "grave         6,542\n",
      "Added ['great'] to the tokenized_text array.\n",
      "great         2,307\n",
      "Added ['greedy'] to the tokenized_text array.\n",
      "greedy       20,505\n",
      "Added ['greeting'] to the tokenized_text array.\n",
      "greeting     14,806\n",
      "Added ['grief'] to the tokenized_text array.\n",
      "grief         9,940\n",
      "Added ['gr', '##ie', '##ved'] to the tokenized_text array.\n",
      "gr           24,665\n",
      "##ie          2,666\n",
      "##ved         7,178\n",
      "Added ['gr', '##ieving'] to the tokenized_text array.\n",
      "gr           24,665\n",
      "##ieving     25,587\n",
      "Added ['grim'] to the tokenized_text array.\n",
      "grim         11,844\n",
      "Added ['grimace'] to the tokenized_text array.\n",
      "grimace      25,898\n",
      "Added ['grim', '##acing'] to the tokenized_text array.\n",
      "grim         11,844\n",
      "##acing      26,217\n",
      "Added ['grin'] to the tokenized_text array.\n",
      "grin          5,861\n",
      "Added ['grinning'] to the tokenized_text array.\n",
      "grinning     11,478\n",
      "Added ['grip', '##ing'] to the tokenized_text array.\n",
      "grip          6,218\n",
      "##ing         2,075\n",
      "Added ['gross'] to the tokenized_text array.\n",
      "gross         7,977\n",
      "Added ['grossed'] to the tokenized_text array.\n",
      "grossed      17,500\n",
      "Added ['gr', '##ou', '##chy'] to the tokenized_text array.\n",
      "gr           24,665\n",
      "##ou          7,140\n",
      "##chy        11,714\n",
      "Added ['growl'] to the tokenized_text array.\n",
      "growl        13,349\n",
      "Added ['growling'] to the tokenized_text array.\n",
      "growling     22,413\n",
      "Added ['gr', '##udge'] to the tokenized_text array.\n",
      "gr           24,665\n",
      "##udge       15,979\n",
      "Added ['gr', '##ud', '##ging'] to the tokenized_text array.\n",
      "gr           24,665\n",
      "##ud          6,784\n",
      "##ging        4,726\n",
      "Added ['gruff'] to the tokenized_text array.\n",
      "gruff        27,038\n",
      "Added ['gr', '##umb', '##ling'] to the tokenized_text array.\n",
      "gr           24,665\n",
      "##umb        25,438\n",
      "##ling        2,989\n",
      "Added ['gr', '##ump', '##y'] to the tokenized_text array.\n",
      "gr           24,665\n",
      "##ump        24,237\n",
      "##y           2,100\n",
      "Added ['grunt'] to the tokenized_text array.\n",
      "grunt        20,696\n",
      "Added ['grunt', '##ing'] to the tokenized_text array.\n",
      "grunt        20,696\n",
      "##ing         2,075\n",
      "Added ['guarded'] to the tokenized_text array.\n",
      "guarded      13,802\n",
      "Added ['guilty'] to the tokenized_text array.\n",
      "guilty        5,905\n",
      "Added ['gulp'] to the tokenized_text array.\n",
      "gulp         26,546\n",
      "Added ['haggard'] to the tokenized_text array.\n",
      "haggard      27,912\n",
      "Added ['half', '##hearted'] to the tokenized_text array.\n",
      "half          2,431\n",
      "##hearted    27,693\n",
      "Added ['halted'] to the tokenized_text array.\n",
      "halted       12,705\n",
      "Added ['ha', '##ples', '##s'] to the tokenized_text array.\n",
      "ha            5,292\n",
      "##ples       21,112\n",
      "##s           2,015\n",
      "Added ['happiness'] to the tokenized_text array.\n",
      "happiness     8,404\n",
      "Added ['happy'] to the tokenized_text array.\n",
      "happy         3,407\n",
      "Added ['harassed'] to the tokenized_text array.\n",
      "harassed     28,186\n",
      "Added ['hard'] to the tokenized_text array.\n",
      "hard          2,524\n",
      "Added ['hardened'] to the tokenized_text array.\n",
      "hardened     15,015\n",
      "Added ['harmful'] to the tokenized_text array.\n",
      "harmful      17,631\n",
      "Added ['ha', '##rrie', '##d'] to the tokenized_text array.\n",
      "ha            5,292\n",
      "##rrie       22,155\n",
      "##d           2,094\n",
      "Added ['harsh'] to the tokenized_text array.\n",
      "harsh         8,401\n",
      "Added ['hate'] to the tokenized_text array.\n",
      "hate          5,223\n",
      "Added ['hate', '##ful'] to the tokenized_text array.\n",
      "hate          5,223\n",
      "##ful         3,993\n",
      "Added ['hating'] to the tokenized_text array.\n",
      "hating       22,650\n",
      "Added ['hatred'] to the tokenized_text array.\n",
      "hatred       11,150\n",
      "Added ['ha', '##ught', '##y'] to the tokenized_text array.\n",
      "ha            5,292\n",
      "##ught       18,533\n",
      "##y           2,100\n",
      "Added ['haunted'] to the tokenized_text array.\n",
      "haunted      11,171\n",
      "Added ['hazy'] to the tokenized_text array.\n",
      "hazy         26,710\n",
      "Added ['heads', '##hak', '##e'] to the tokenized_text array.\n",
      "heads         4,641\n",
      "##hak        20,459\n",
      "##e           2,063\n",
      "Added ['heart', '##ache'] to the tokenized_text array.\n",
      "heart         2,540\n",
      "##ache       15,395\n",
      "Added ['heart', '##broken'] to the tokenized_text array.\n",
      "heart         2,540\n",
      "##broken     29,162\n",
      "Added ['hearted'] to the tokenized_text array.\n",
      "hearted      18,627\n",
      "Added ['hearts', '##ick'] to the tokenized_text array.\n",
      "hearts        8,072\n",
      "##ick         6,799\n",
      "Added ['heated'] to the tokenized_text array.\n",
      "heated        9,685\n",
      "Added ['heavy', '##hearted'] to the tokenized_text array.\n",
      "heavy         3,082\n",
      "##hearted    27,693\n",
      "Added ['heck', '##le'] to the tokenized_text array.\n",
      "heck         17,752\n",
      "##le          2,571\n",
      "Added ['hee', '##df', '##ul'] to the tokenized_text array.\n",
      "hee          18,235\n",
      "##df         20,952\n",
      "##ul          5,313\n",
      "Added ['he', '##ino', '##us'] to the tokenized_text array.\n",
      "he            2,002\n",
      "##ino         5,740\n",
      "##us          2,271\n",
      "Added ['helpful'] to the tokenized_text array.\n",
      "helpful      14,044\n",
      "Added ['helpless'] to the tokenized_text array.\n",
      "helpless     13,346\n",
      "Added ['hesitant'] to the tokenized_text array.\n",
      "hesitant     20,221\n",
      "Added ['hesitantly'] to the tokenized_text array.\n",
      "hesitantly   24,626\n",
      "Added ['he', '##sit', '##ating'] to the tokenized_text array.\n",
      "he            2,002\n",
      "##sit        28,032\n",
      "##ating       5,844\n",
      "Added ['hesitation'] to the tokenized_text array.\n",
      "hesitation   13,431\n",
      "Added ['high'] to the tokenized_text array.\n",
      "high          2,152\n",
      "Added ['ho', '##ller', '##ing'] to the tokenized_text array.\n",
      "ho            7,570\n",
      "##ller       10,820\n",
      "##ing         2,075\n",
      "Added ['ho', '##mic', '##idal'] to the tokenized_text array.\n",
      "ho            7,570\n",
      "##mic         7,712\n",
      "##idal       16,975\n",
      "Added ['honest'] to the tokenized_text array.\n",
      "honest        7,481\n",
      "Added ['honorable'] to the tokenized_text array.\n",
      "honorable    13,556\n",
      "Added ['hope'] to the tokenized_text array.\n",
      "hope          3,246\n",
      "Added ['hopeful'] to the tokenized_text array.\n",
      "hopeful      17,772\n",
      "Added ['hopeful', '##ness'] to the tokenized_text array.\n",
      "hopeful      17,772\n",
      "##ness        2,791\n",
      "Added ['hopeless'] to the tokenized_text array.\n",
      "hopeless     20,625\n",
      "Added ['hoping'] to the tokenized_text array.\n",
      "hoping        5,327\n",
      "Added ['horn', '##y'] to the tokenized_text array.\n",
      "horn          7,109\n",
      "##y           2,100\n",
      "Added ['horrible'] to the tokenized_text array.\n",
      "horrible      9,202\n",
      "Added ['horrified'] to the tokenized_text array.\n",
      "horrified    14,603\n",
      "Added ['ho', '##rri', '##fy'] to the tokenized_text array.\n",
      "ho            7,570\n",
      "##rri        18,752\n",
      "##fy         12,031\n",
      "Added ['ho', '##rri', '##fying'] to the tokenized_text array.\n",
      "ho            7,570\n",
      "##rri        18,752\n",
      "##fying      14,116\n",
      "Added ['horror'] to the tokenized_text array.\n",
      "horror        5,469\n",
      "Added ['hostile'] to the tokenized_text array.\n",
      "hostile      10,420\n",
      "Added ['hostility'] to the tokenized_text array.\n",
      "hostility    18,258\n",
      "Added ['hot'] to the tokenized_text array.\n",
      "hot           2,980\n",
      "Added ['hot', '##shot'] to the tokenized_text array.\n",
      "hot           2,980\n",
      "##shot       19,040\n",
      "Added ['huff', '##iness'] to the tokenized_text array.\n",
      "huff         21,301\n",
      "##iness       9,961\n",
      "Added ['huff', '##y'] to the tokenized_text array.\n",
      "huff         21,301\n",
      "##y           2,100\n",
      "Added ['humble'] to the tokenized_text array.\n",
      "humble       15,716\n",
      "Added ['humble', '##d'] to the tokenized_text array.\n",
      "humble       15,716\n",
      "##d           2,094\n",
      "Added ['hum', '##drum'] to the tokenized_text array.\n",
      "hum          14,910\n",
      "##drum       21,884\n",
      "Added ['humiliated'] to the tokenized_text array.\n",
      "humiliated   26,608\n",
      "Added ['hum', '##ility'] to the tokenized_text array.\n",
      "hum          14,910\n",
      "##ility      15,148\n",
      "Added ['humming'] to the tokenized_text array.\n",
      "humming      20,364\n",
      "Added ['humor'] to the tokenized_text array.\n",
      "humor         8,562\n",
      "Added ['humor', '##ed'] to the tokenized_text array.\n",
      "humor         8,562\n",
      "##ed          2,098\n",
      "Added ['humorous'] to the tokenized_text array.\n",
      "humorous     14,742\n",
      "Added ['hunger'] to the tokenized_text array.\n",
      "hunger        9,012\n",
      "Added ['hungry'] to the tokenized_text array.\n",
      "hungry        7,501\n",
      "Added ['hunted'] to the tokenized_text array.\n",
      "hunted       14,682\n",
      "Added ['hurt'] to the tokenized_text array.\n",
      "hurt          3,480\n",
      "Added ['hurt', '##ful'] to the tokenized_text array.\n",
      "hurt          3,480\n",
      "##ful         3,993\n",
      "Added ['hurting'] to the tokenized_text array.\n",
      "hurting      11,878\n",
      "Added ['hush'] to the tokenized_text array.\n",
      "hush         20,261\n",
      "Added ['hushed'] to the tokenized_text array.\n",
      "hushed       24,033\n",
      "Added ['hyper'] to the tokenized_text array.\n",
      "hyper        23,760\n",
      "Added ['hyper', '##active'] to the tokenized_text array.\n",
      "hyper        23,760\n",
      "##active     19,620\n",
      "Added ['h', '##yp', '##not', '##ized'] to the tokenized_text array.\n",
      "h             1,044\n",
      "##yp         22,571\n",
      "##not        17,048\n",
      "##ized        3,550\n",
      "Added ['h', '##yp', '##oc', '##rit', '##ical'] to the tokenized_text array.\n",
      "h             1,044\n",
      "##yp         22,571\n",
      "##oc         10,085\n",
      "##rit        14,778\n",
      "##ical        7,476\n",
      "Added ['hysteria'] to the tokenized_text array.\n",
      "hysteria     29,004\n",
      "Added ['hysterical'] to the tokenized_text array.\n",
      "hysterical   25,614\n",
      "Added ['idiot', '##ic'] to the tokenized_text array.\n",
      "idiot        10,041\n",
      "##ic          2,594\n",
      "Added ['ignorant'] to the tokenized_text array.\n",
      "ignorant     21,591\n",
      "Added ['ignoring'] to the tokenized_text array.\n",
      "ignoring      9,217\n",
      "Added ['ill'] to the tokenized_text array.\n",
      "ill           5,665\n",
      "Added ['imaginative'] to the tokenized_text array.\n",
      "imaginative  28,575\n",
      "Added ['immature'] to the tokenized_text array.\n",
      "immature     26,838\n",
      "Added ['immersed'] to the tokenized_text array.\n",
      "immersed     26,275\n",
      "Added ['impacted'] to the tokenized_text array.\n",
      "impacted     19,209\n",
      "Added ['imp', '##art', '##ial'] to the tokenized_text array.\n",
      "imp          17,727\n",
      "##art         8,445\n",
      "##ial         4,818\n",
      "Added ['imp', '##ass', '##ioned'] to the tokenized_text array.\n",
      "imp          17,727\n",
      "##ass        12,054\n",
      "##ioned      19,798\n",
      "Added ['imp', '##ass', '##ive'] to the tokenized_text array.\n",
      "imp          17,727\n",
      "##ass        12,054\n",
      "##ive         3,512\n",
      "Added ['impatience'] to the tokenized_text array.\n",
      "impatience   28,011\n",
      "Added ['impatient'] to the tokenized_text array.\n",
      "impatient    17,380\n",
      "Added ['imp', '##eri', '##ous'] to the tokenized_text array.\n",
      "imp          17,727\n",
      "##eri        11,124\n",
      "##ous         3,560\n",
      "Added ['imp', '##erson', '##al'] to the tokenized_text array.\n",
      "imp          17,727\n",
      "##erson      18,617\n",
      "##al          2,389\n",
      "Added ['imp', '##ert', '##inen', '##t'] to the tokenized_text array.\n",
      "imp          17,727\n",
      "##ert         8,743\n",
      "##inen       21,820\n",
      "##t           2,102\n",
      "Added ['imp', '##ish'] to the tokenized_text array.\n",
      "imp          17,727\n",
      "##ish         4,509\n",
      "Added ['implicated'] to the tokenized_text array.\n",
      "implicated   20,467\n",
      "Added ['imp', '##lor', '##ing'] to the tokenized_text array.\n",
      "imp          17,727\n",
      "##lor        10,626\n",
      "##ing         2,075\n",
      "Added ['important'] to the tokenized_text array.\n",
      "important     2,590\n",
      "Added ['impressed'] to the tokenized_text array.\n",
      "impressed     7,622\n",
      "Added ['imp', '##ulsive'] to the tokenized_text array.\n",
      "imp          17,727\n",
      "##ulsive     23,004\n",
      "Added ['inactive'] to the tokenized_text array.\n",
      "inactive     16,389\n",
      "Added ['inadequate'] to the tokenized_text array.\n",
      "inadequate   14,710\n",
      "Added ['ina', '##rti', '##cula', '##te'] to the tokenized_text array.\n",
      "ina          27,118\n",
      "##rti        28,228\n",
      "##cula       19,879\n",
      "##te          2,618\n",
      "Added ['ina', '##tten', '##tive'] to the tokenized_text array.\n",
      "ina          27,118\n",
      "##tten       25,970\n",
      "##tive        6,024\n",
      "Added ['ina', '##udi', '##ble'] to the tokenized_text array.\n",
      "ina          27,118\n",
      "##udi        21,041\n",
      "##ble         3,468\n",
      "Added ['ina', '##uth', '##ent', '##ic'] to the tokenized_text array.\n",
      "ina          27,118\n",
      "##uth        14,317\n",
      "##ent         4,765\n",
      "##ic          2,594\n",
      "Added ['incapable'] to the tokenized_text array.\n",
      "incapable    19,907\n",
      "Added ['incense', '##d'] to the tokenized_text array.\n",
      "incense      28,647\n",
      "##d           2,094\n",
      "Added ['inc', '##ert', '##ain'] to the tokenized_text array.\n",
      "inc           4,297\n",
      "##ert         8,743\n",
      "##ain         8,113\n",
      "Added ['inc', '##ert', '##itude'] to the tokenized_text array.\n",
      "inc           4,297\n",
      "##ert         8,743\n",
      "##itude      18,679\n",
      "Added ['inc', '##ited'] to the tokenized_text array.\n",
      "inc           4,297\n",
      "##ited       17,572\n",
      "Added ['inc', '##omp', '##re', '##hen', '##sible'] to the tokenized_text array.\n",
      "inc           4,297\n",
      "##omp        25,377\n",
      "##re          2,890\n",
      "##hen        10,222\n",
      "##sible      19,307\n",
      "Added ['inc', '##ons', '##pic', '##uous'] to the tokenized_text array.\n",
      "inc           4,297\n",
      "##ons         5,644\n",
      "##pic        24,330\n",
      "##uous        8,918\n",
      "Added ['inc', '##red', '##uli', '##ty'] to the tokenized_text array.\n",
      "inc           4,297\n",
      "##red         5,596\n",
      "##uli        15,859\n",
      "##ty          3,723\n",
      "Added ['inc', '##red', '##ulous'] to the tokenized_text array.\n",
      "inc           4,297\n",
      "##red         5,596\n",
      "##ulous      16,203\n",
      "Added ['inc', '##red', '##ulously'] to the tokenized_text array.\n",
      "inc           4,297\n",
      "##red         5,596\n",
      "##ulously    21,227\n",
      "Added ['inc', '##ul', '##pate'] to the tokenized_text array.\n",
      "inc           4,297\n",
      "##ul          5,313\n",
      "##pate       17,585\n",
      "Added ['inc', '##uri', '##ous'] to the tokenized_text array.\n",
      "inc           4,297\n",
      "##uri         9,496\n",
      "##ous         3,560\n",
      "Added ['ind', '##ec', '##ip', '##her', '##able'] to the tokenized_text array.\n",
      "ind          27,427\n",
      "##ec          8,586\n",
      "##ip         11,514\n",
      "##her         5,886\n",
      "##able        3,085\n",
      "Added ['ind', '##ec', '##ision'] to the tokenized_text array.\n",
      "ind          27,427\n",
      "##ec          8,586\n",
      "##ision      19,969\n",
      "Added ['ind', '##ec', '##isi', '##ve'] to the tokenized_text array.\n",
      "ind          27,427\n",
      "##ec          8,586\n",
      "##isi        17,417\n",
      "##ve          3,726\n",
      "Added ['indifferent'] to the tokenized_text array.\n",
      "indifferent  24,436\n",
      "Added ['indifferent', '##ly'] to the tokenized_text array.\n",
      "indifferent  24,436\n",
      "##ly          2,135\n",
      "Added ['ind', '##ignant'] to the tokenized_text array.\n",
      "ind          27,427\n",
      "##ignant     25,593\n",
      "Added ['indo', '##lent'] to the tokenized_text array.\n",
      "indo         11,424\n",
      "##lent       16,136\n",
      "Added ['in', '##eb', '##riated'] to the tokenized_text array.\n",
      "in            1,999\n",
      "##eb         15,878\n",
      "##riated     25,475\n",
      "Added ['in', '##ert'] to the tokenized_text array.\n",
      "in            1,999\n",
      "##ert         8,743\n",
      "Added ['in', '##fat', '##uating'] to the tokenized_text array.\n",
      "in            1,999\n",
      "##fat        27,753\n",
      "##uating     24,133\n",
      "Added ['inferior'] to the tokenized_text array.\n",
      "inferior     14,092\n",
      "Added ['inferior', '##ity'] to the tokenized_text array.\n",
      "inferior     14,092\n",
      "##ity         3,012\n",
      "Added ['in', '##fl', '##ame', '##d'] to the tokenized_text array.\n",
      "in            1,999\n",
      "##fl         10,258\n",
      "##ame        14,074\n",
      "##d           2,094\n",
      "Added ['informal'] to the tokenized_text array.\n",
      "informal     11,900\n",
      "Added ['informing'] to the tokenized_text array.\n",
      "informing    21,672\n",
      "Added ['in', '##fur', '##iated'] to the tokenized_text array.\n",
      "in            1,999\n",
      "##fur        27,942\n",
      "##iated      15,070\n",
      "Added ['inhibit', '##ed'] to the tokenized_text array.\n",
      "inhibit      26,402\n",
      "##ed          2,098\n",
      "Added ['inhibit', '##ing'] to the tokenized_text array.\n",
      "inhibit      26,402\n",
      "##ing         2,075\n",
      "Added ['in', '##imi', '##cal'] to the tokenized_text array.\n",
      "in            1,999\n",
      "##imi        27,605\n",
      "##cal         9,289\n",
      "Added ['injured'] to the tokenized_text array.\n",
      "injured       5,229\n",
      "Added ['innocent'] to the tokenized_text array.\n",
      "innocent      7,036\n",
      "Added ['in', '##patient'] to the tokenized_text array.\n",
      "in            1,999\n",
      "##patient    24,343\n",
      "Added ['in', '##qui', '##ring'] to the tokenized_text array.\n",
      "in            1,999\n",
      "##qui        15,549\n",
      "##ring        4,892\n",
      "Added ['in', '##qui', '##sit', '##ive'] to the tokenized_text array.\n",
      "in            1,999\n",
      "##qui        15,549\n",
      "##sit        28,032\n",
      "##ive         3,512\n",
      "Added ['insane'] to the tokenized_text array.\n",
      "insane        9,577\n",
      "Added ['ins', '##cr', '##utable'] to the tokenized_text array.\n",
      "ins          16,021\n",
      "##cr         26,775\n",
      "##utable     23,056\n",
      "Added ['ins', '##ecure'] to the tokenized_text array.\n",
      "ins          16,021\n",
      "##ecure      29,150\n",
      "Added ['ins', '##ec', '##urity'] to the tokenized_text array.\n",
      "ins          16,021\n",
      "##ec          8,586\n",
      "##urity      25,137\n",
      "Added ['ins', '##ens', '##itive'] to the tokenized_text array.\n",
      "ins          16,021\n",
      "##ens         6,132\n",
      "##itive      13,043\n",
      "Added ['ins', '##idi', '##ous'] to the tokenized_text array.\n",
      "ins          16,021\n",
      "##idi        28,173\n",
      "##ous         3,560\n",
      "Added ['ins', '##in', '##uating'] to the tokenized_text array.\n",
      "ins          16,021\n",
      "##in          2,378\n",
      "##uating     24,133\n",
      "Added ['insistence'] to the tokenized_text array.\n",
      "insistence   20,616\n",
      "Added ['insistent'] to the tokenized_text array.\n",
      "insistent    29,204\n",
      "Added ['insisting'] to the tokenized_text array.\n",
      "insisting    22,604\n",
      "Added ['ins', '##ole', '##nt'] to the tokenized_text array.\n",
      "ins          16,021\n",
      "##ole         9,890\n",
      "##nt          3,372\n",
      "Added ['ins', '##ou', '##cian', '##ce'] to the tokenized_text array.\n",
      "ins          16,021\n",
      "##ou          7,140\n",
      "##cian       14,483\n",
      "##ce          3,401\n",
      "Added ['ins', '##ou', '##cian', '##t'] to the tokenized_text array.\n",
      "ins          16,021\n",
      "##ou          7,140\n",
      "##cian       14,483\n",
      "##t           2,102\n",
      "Added ['inspired'] to the tokenized_text array.\n",
      "inspired      4,427\n",
      "Added ['inspiring'] to the tokenized_text array.\n",
      "inspiring    18,988\n",
      "Added ['ins', '##ti', '##gating'] to the tokenized_text array.\n",
      "ins          16,021\n",
      "##ti          3,775\n",
      "##gating     16,961\n",
      "Added ['ins', '##tructing'] to the tokenized_text array.\n",
      "ins          16,021\n",
      "##tructing   26,310\n",
      "Added ['ins', '##ub', '##ord', '##inate'] to the tokenized_text array.\n",
      "ins          16,021\n",
      "##ub         12,083\n",
      "##ord         8,551\n",
      "##inate      14,776\n",
      "Added ['ins', '##ular'] to the tokenized_text array.\n",
      "ins          16,021\n",
      "##ular        7,934\n",
      "Added ['insulted'] to the tokenized_text array.\n",
      "insulted     23,637\n",
      "Added ['insulting'] to the tokenized_text array.\n",
      "insulting    23,979\n",
      "Added ['intelligence'] to the tokenized_text array.\n",
      "intelligence  4,454\n",
      "Added ['intense'] to the tokenized_text array.\n",
      "intense       6,387\n",
      "Added ['intensely'] to the tokenized_text array.\n",
      "intensely    20,531\n",
      "Added ['intensity'] to the tokenized_text array.\n",
      "intensity     8,015\n",
      "Added ['intensive'] to the tokenized_text array.\n",
      "intensive    11,806\n",
      "Added ['intent'] to the tokenized_text array.\n",
      "intent        7,848\n",
      "Added ['intentional'] to the tokenized_text array.\n",
      "intentional  21,249\n",
      "Added ['interacting'] to the tokenized_text array.\n",
      "interacting  21,935\n",
      "Added ['interest'] to the tokenized_text array.\n",
      "interest      3,037\n",
      "Added ['interested'] to the tokenized_text array.\n",
      "interested    4,699\n",
      "Added ['inter', '##ject', '##ing'] to the tokenized_text array.\n",
      "inter         6,970\n",
      "##ject       20,614\n",
      "##ing         2,075\n",
      "Added ['internal', '##izing'] to the tokenized_text array.\n",
      "internal      4,722\n",
      "##izing       6,026\n",
      "Added ['inter', '##ro', '##gating'] to the tokenized_text array.\n",
      "inter         6,970\n",
      "##ro          3,217\n",
      "##gating     16,961\n",
      "Added ['interrupting'] to the tokenized_text array.\n",
      "interrupting 22,602\n",
      "Added ['intimidated'] to the tokenized_text array.\n",
      "intimidated  28,028\n",
      "Added ['intimidating'] to the tokenized_text array.\n",
      "intimidating 24,439\n",
      "Added ['into', '##ler', '##ant'] to the tokenized_text array.\n",
      "into          2,046\n",
      "##ler         3,917\n",
      "##ant         4,630\n",
      "Added ['into', '##xi', '##cated'] to the tokenized_text array.\n",
      "into          2,046\n",
      "##xi          9,048\n",
      "##cated      12,921\n",
      "Added ['int', '##rigue'] to the tokenized_text array.\n",
      "int          20,014\n",
      "##rigue      27,611\n",
      "Added ['intrigued'] to the tokenized_text array.\n",
      "intrigued    18,896\n",
      "Added ['intriguing'] to the tokenized_text array.\n",
      "intriguing   23,824\n",
      "Added ['intro', '##sp', '##ect', '##ive'] to the tokenized_text array.\n",
      "intro        17,174\n",
      "##sp         13,102\n",
      "##ect        22,471\n",
      "##ive         3,512\n",
      "Added ['invested'] to the tokenized_text array.\n",
      "invested     11,241\n",
      "Added ['investigate'] to the tokenized_text array.\n",
      "investigate   8,556\n",
      "Added ['investigative'] to the tokenized_text array.\n",
      "investigative 15,025\n",
      "Added ['investigator', '##y'] to the tokenized_text array.\n",
      "investigator 14,064\n",
      "##y           2,100\n",
      "Added ['in', '##vi', '##gor', '##ated'] to the tokenized_text array.\n",
      "in            1,999\n",
      "##vi          5,737\n",
      "##gor        20,255\n",
      "##ated        4,383\n",
      "Added ['involved'] to the tokenized_text array.\n",
      "involved      2,920\n",
      "Added ['ira', '##sc', '##ible'] to the tokenized_text array.\n",
      "ira          11,209\n",
      "##sc         11,020\n",
      "##ible        7,028\n",
      "Added ['ira', '##te'] to the tokenized_text array.\n",
      "ira          11,209\n",
      "##te          2,618\n",
      "Added ['ir', '##e'] to the tokenized_text array.\n",
      "ir           20,868\n",
      "##e           2,063\n",
      "Added ['ir', '##ef', '##ul'] to the tokenized_text array.\n",
      "ir           20,868\n",
      "##ef         12,879\n",
      "##ul          5,313\n",
      "Added ['ir', '##ked'] to the tokenized_text array.\n",
      "ir           20,868\n",
      "##ked         8,126\n",
      "Added ['ironic'] to the tokenized_text array.\n",
      "ironic       19,313\n",
      "Added ['irony'] to the tokenized_text array.\n",
      "irony        19,728\n",
      "Added ['ir', '##res', '##ol', '##ute'] to the tokenized_text array.\n",
      "ir           20,868\n",
      "##res         6,072\n",
      "##ol          4,747\n",
      "##ute        10,421\n",
      "Added ['ir', '##rita', '##ble'] to the tokenized_text array.\n",
      "ir           20,868\n",
      "##rita       17,728\n",
      "##ble         3,468\n",
      "Added ['ir', '##rita', '##bly'] to the tokenized_text array.\n",
      "ir           20,868\n",
      "##rita       17,728\n",
      "##bly         6,321\n",
      "Added ['irritated'] to the tokenized_text array.\n",
      "irritated    15,560\n",
      "Added ['irritation'] to the tokenized_text array.\n",
      "irritation   17,373\n",
      "Added ['isolated'] to the tokenized_text array.\n",
      "isolated      7,275\n",
      "Added ['ja', '##bbed'] to the tokenized_text array.\n",
      "ja           14,855\n",
      "##bbed       15,499\n",
      "Added ['jade', '##d'] to the tokenized_text array.\n",
      "jade         12,323\n",
      "##d           2,094\n",
      "Added ['jar', '##red'] to the tokenized_text array.\n",
      "jar          15,723\n",
      "##red         5,596\n",
      "Added ['jar', '##ring'] to the tokenized_text array.\n",
      "jar          15,723\n",
      "##ring        4,892\n",
      "Added ['ja', '##unt', '##y'] to the tokenized_text array.\n",
      "ja           14,855\n",
      "##unt        16,671\n",
      "##y           2,100\n",
      "Added ['jaw', '##ed'] to the tokenized_text array.\n",
      "jaw           5,730\n",
      "##ed          2,098\n",
      "Added ['jealous'] to the tokenized_text array.\n",
      "jealous       9,981\n",
      "Added ['je', '##ering'] to the tokenized_text array.\n",
      "je           15,333\n",
      "##ering       7,999\n",
      "Added ['je', '##sti', '##ng'] to the tokenized_text array.\n",
      "je           15,333\n",
      "##sti        16,643\n",
      "##ng          3,070\n",
      "Added ['ji', '##lt', '##ed'] to the tokenized_text array.\n",
      "ji           10,147\n",
      "##lt          7,096\n",
      "##ed          2,098\n",
      "Added ['ji', '##tter', '##y'] to the tokenized_text array.\n",
      "ji           10,147\n",
      "##tter       12,079\n",
      "##y           2,100\n",
      "Added ['jo', '##cular'] to the tokenized_text array.\n",
      "jo            8,183\n",
      "##cular      15,431\n",
      "Added ['joking'] to the tokenized_text array.\n",
      "joking       16,644\n",
      "Added ['jolly'] to the tokenized_text array.\n",
      "jolly        22,193\n",
      "Added ['jolted'] to the tokenized_text array.\n",
      "jolted       28,801\n",
      "Added ['jo', '##vial'] to the tokenized_text array.\n",
      "jo            8,183\n",
      "##vial       18,660\n",
      "Added ['joy'] to the tokenized_text array.\n",
      "joy           6,569\n",
      "Added ['joy', '##ful'] to the tokenized_text array.\n",
      "joy           6,569\n",
      "##ful         3,993\n",
      "Added ['joy', '##fulness'] to the tokenized_text array.\n",
      "joy           6,569\n",
      "##fulness    20,938\n",
      "Added ['joy', '##less'] to the tokenized_text array.\n",
      "joy           6,569\n",
      "##less        3,238\n",
      "Added ['joy', '##ous'] to the tokenized_text array.\n",
      "joy           6,569\n",
      "##ous         3,560\n",
      "Added ['ju', '##bil', '##ant'] to the tokenized_text array.\n",
      "ju           18,414\n",
      "##bil        14,454\n",
      "##ant         4,630\n",
      "Added ['ju', '##bil', '##ation'] to the tokenized_text array.\n",
      "ju           18,414\n",
      "##bil        14,454\n",
      "##ation       3,370\n",
      "Added ['judgement', '##al'] to the tokenized_text array.\n",
      "judgement    16,646\n",
      "##al          2,389\n",
      "Added ['judging'] to the tokenized_text array.\n",
      "judging      13,325\n",
      "Added ['judgment', '##al'] to the tokenized_text array.\n",
      "judgment      8,689\n",
      "##al          2,389\n",
      "Added ['ju', '##dic', '##ious'] to the tokenized_text array.\n",
      "ju           18,414\n",
      "##dic        14,808\n",
      "##ious        6,313\n",
      "Added ['jump', '##y'] to the tokenized_text array.\n",
      "jump          5,376\n",
      "##y           2,100\n",
      "Added ['justified'] to the tokenized_text array.\n",
      "justified    15,123\n",
      "Added ['keen'] to the tokenized_text array.\n",
      "keen         10,326\n",
      "Added ['kind'] to the tokenized_text array.\n",
      "kind          2,785\n",
      "Added ['kind', '##hearted'] to the tokenized_text array.\n",
      "kind          2,785\n",
      "##hearted    27,693\n",
      "Added ['kiss'] to the tokenized_text array.\n",
      "kiss          3,610\n",
      "Added ['knowing'] to the tokenized_text array.\n",
      "knowing       4,209\n",
      "Added ['know', '##led', '##ga', '##ble'] to the tokenized_text array.\n",
      "know          2,113\n",
      "##led         3,709\n",
      "##ga          3,654\n",
      "##ble         3,468\n",
      "Added ['knowledge', '##able'] to the tokenized_text array.\n",
      "knowledge     3,716\n",
      "##able        3,085\n",
      "Added ['ko', '##sher'] to the tokenized_text array.\n",
      "ko           12,849\n",
      "##sher       19,603\n",
      "Added ['lack', '##ada', '##isi', '##cal'] to the tokenized_text array.\n",
      "lack          3,768\n",
      "##ada         8,447\n",
      "##isi        17,417\n",
      "##cal         9,289\n",
      "Added ['lack', '##lus', '##ter'] to the tokenized_text array.\n",
      "lack          3,768\n",
      "##lus         7,393\n",
      "##ter         3,334\n",
      "Added ['lac', '##onic'] to the tokenized_text array.\n",
      "lac          18,749\n",
      "##onic       12,356\n",
      "Added ['lamb', '##ast', '##e'] to the tokenized_text array.\n",
      "lamb         12,559\n",
      "##ast        14,083\n",
      "##e           2,063\n",
      "Added ['lame', '##nta', '##ble'] to the tokenized_text array.\n",
      "lame         20,342\n",
      "##nta        12,380\n",
      "##ble         3,468\n",
      "Added ['lame', '##nting'] to the tokenized_text array.\n",
      "lame         20,342\n",
      "##nting      24,360\n",
      "Added ['las', '##ci', '##vious'] to the tokenized_text array.\n",
      "las           5,869\n",
      "##ci          6,895\n",
      "##vious      24,918\n",
      "Added ['laugh'] to the tokenized_text array.\n",
      "laugh         4,756\n",
      "Added ['laughing'] to the tokenized_text array.\n",
      "laughing      5,870\n",
      "Added ['laughter'] to the tokenized_text array.\n",
      "laughter      7,239\n",
      "Added ['lazy'] to the tokenized_text array.\n",
      "lazy         13,971\n",
      "Added ['leaving'] to the tokenized_text array.\n",
      "leaving       2,975\n",
      "Added ['le', '##cher', '##ous'] to the tokenized_text array.\n",
      "le            3,393\n",
      "##cher        7,474\n",
      "##ous         3,560\n",
      "Added ['le', '##cturing'] to the tokenized_text array.\n",
      "le            3,393\n",
      "##cturing    19,159\n",
      "Added ['lee', '##ring'] to the tokenized_text array.\n",
      "lee           3,389\n",
      "##ring        4,892\n",
      "Added ['lee', '##ry'] to the tokenized_text array.\n",
      "lee           3,389\n",
      "##ry          2,854\n",
      "Added ['let', '##down'] to the tokenized_text array.\n",
      "let           2,292\n",
      "##down        7,698\n",
      "Added ['let', '##har', '##gic'] to the tokenized_text array.\n",
      "let           2,292\n",
      "##har         8,167\n",
      "##gic        12,863\n",
      "Added ['level', '##head', '##ed'] to the tokenized_text array.\n",
      "level         2,504\n",
      "##head        4,974\n",
      "##ed          2,098\n",
      "Added ['lew', '##d'] to the tokenized_text array.\n",
      "lew          24,992\n",
      "##d           2,094\n",
      "Added ['li', '##bid', '##ino', '##us'] to the tokenized_text array.\n",
      "li            5,622\n",
      "##bid        17,062\n",
      "##ino         5,740\n",
      "##us          2,271\n",
      "Added ['lifeless'] to the tokenized_text array.\n",
      "lifeless     22,185\n",
      "Added ['light', '##hearted'] to the tokenized_text array.\n",
      "light         2,422\n",
      "##hearted    27,693\n",
      "Added ['lip', '##ped'] to the tokenized_text array.\n",
      "lip           5,423\n",
      "##ped         5,669\n",
      "Added ['listening'] to the tokenized_text array.\n",
      "listening     5,962\n",
      "Added ['list', '##less'] to the tokenized_text array.\n",
      "list          2,862\n",
      "##less        3,238\n",
      "Added ['lively'] to the tokenized_text array.\n",
      "lively       17,133\n",
      "Added ['liv', '##id'] to the tokenized_text array.\n",
      "liv          22,135\n",
      "##id          3,593\n",
      "Added ['loaded'] to the tokenized_text array.\n",
      "loaded        8,209\n",
      "Added ['lo', '##ath'] to the tokenized_text array.\n",
      "lo            8,840\n",
      "##ath         8,988\n",
      "Added ['lo', '##ath', '##e'] to the tokenized_text array.\n",
      "lo            8,840\n",
      "##ath         8,988\n",
      "##e           2,063\n",
      "Added ['lo', '##athing'] to the tokenized_text array.\n",
      "lo            8,840\n",
      "##athing     22,314\n",
      "Added ['lo', '##ath', '##some'] to the tokenized_text array.\n",
      "lo            8,840\n",
      "##ath         8,988\n",
      "##some       14,045\n",
      "Added ['locked'] to the tokenized_text array.\n",
      "locked        5,299\n",
      "Added ['loneliness'] to the tokenized_text array.\n",
      "loneliness   20,334\n",
      "Added ['lonely'] to the tokenized_text array.\n",
      "lonely        9,479\n",
      "Added ['longing'] to the tokenized_text array.\n",
      "longing      15,752\n",
      "Added ['looking'] to the tokenized_text array.\n",
      "looking       2,559\n",
      "Added ['lo', '##ony'] to the tokenized_text array.\n",
      "lo            8,840\n",
      "##ony        16,585\n",
      "Added ['loss'] to the tokenized_text array.\n",
      "loss          3,279\n",
      "Added ['lost'] to the tokenized_text array.\n",
      "lost          2,439\n",
      "Added ['loud'] to the tokenized_text array.\n",
      "loud          5,189\n",
      "Added ['lou', '##sy'] to the tokenized_text array.\n",
      "lou          10,223\n",
      "##sy          6,508\n",
      "Added ['love'] to the tokenized_text array.\n",
      "love          2,293\n",
      "Added ['loving'] to the tokenized_text array.\n",
      "loving        8,295\n",
      "Added ['low', '##liness'] to the tokenized_text array.\n",
      "low           2,659\n",
      "##liness     20,942\n",
      "Added ['lu', '##rid'] to the tokenized_text array.\n",
      "lu           11,320\n",
      "##rid        14,615\n",
      "Added ['lust', '##ful'] to the tokenized_text array.\n",
      "lust         11,516\n",
      "##ful         3,993\n",
      "Added ['lust', '##ing'] to the tokenized_text array.\n",
      "lust         11,516\n",
      "##ing         2,075\n",
      "Added ['lust', '##y'] to the tokenized_text array.\n",
      "lust         11,516\n",
      "##y           2,100\n",
      "Added ['lying'] to the tokenized_text array.\n",
      "lying         4,688\n",
      "Added ['mad'] to the tokenized_text array.\n",
      "mad           5,506\n",
      "Added ['madden', '##ed'] to the tokenized_text array.\n",
      "madden       24,890\n",
      "##ed          2,098\n",
      "Added ['madness'] to the tokenized_text array.\n",
      "madness      12,013\n",
      "Added ['mal', '##con', '##ten', '##t'] to the tokenized_text array.\n",
      "mal          15,451\n",
      "##con         8,663\n",
      "##ten         6,528\n",
      "##t           2,102\n",
      "Added ['male', '##fi', '##cent'] to the tokenized_text array.\n",
      "male          3,287\n",
      "##fi          8,873\n",
      "##cent       13,013\n",
      "Added ['male', '##vo', '##lent'] to the tokenized_text array.\n",
      "male          3,287\n",
      "##vo          6,767\n",
      "##lent       16,136\n",
      "Added ['malice'] to the tokenized_text array.\n",
      "malice       28,238\n",
      "Added ['malicious'] to the tokenized_text array.\n",
      "malicious    24,391\n",
      "Added ['mali', '##gnant'] to the tokenized_text array.\n",
      "mali         16,007\n",
      "##gnant      27,881\n",
      "Added ['mania', '##cal'] to the tokenized_text array.\n",
      "mania        29,310\n",
      "##cal         9,289\n",
      "Added ['mani', '##pu', '##lative'] to the tokenized_text array.\n",
      "mani         23,624\n",
      "##pu         14,289\n",
      "##lative     26,255\n",
      "Added ['marvel', '##ed'] to the tokenized_text array.\n",
      "marvel        8,348\n",
      "##ed          2,098\n",
      "Added ['master'] to the tokenized_text array.\n",
      "master        3,040\n",
      "Added ['mean'] to the tokenized_text array.\n",
      "mean          2,812\n",
      "Added ['meaningful'] to the tokenized_text array.\n",
      "meaningful   15,902\n",
      "Added ['med', '##itative'] to the tokenized_text array.\n",
      "med          19,960\n",
      "##itative    29,293\n",
      "Added ['meek'] to the tokenized_text array.\n",
      "meek         28,997\n",
      "Added ['mel', '##an', '##cho', '##lic'] to the tokenized_text array.\n",
      "mel          11,463\n",
      "##an          2,319\n",
      "##cho         9,905\n",
      "##lic        10,415\n",
      "Added ['melancholy'] to the tokenized_text array.\n",
      "melancholy   22,247\n",
      "Added ['mel', '##low'] to the tokenized_text array.\n",
      "mel          11,463\n",
      "##low         8,261\n",
      "Added ['menace'] to the tokenized_text array.\n",
      "menace       19,854\n",
      "Added ['menacing'] to the tokenized_text array.\n",
      "menacing     24,060\n",
      "Added ['mental'] to the tokenized_text array.\n",
      "mental        5,177\n",
      "Added ['mer', '##rily'] to the tokenized_text array.\n",
      "mer          21,442\n",
      "##rily       11,272\n",
      "Added ['merry'] to the tokenized_text array.\n",
      "merry        12,831\n",
      "Added ['me', '##sm', '##eri', '##zed'] to the tokenized_text array.\n",
      "me            2,033\n",
      "##sm          6,491\n",
      "##eri        11,124\n",
      "##zed         5,422\n",
      "Added ['mi', '##ffed'] to the tokenized_text array.\n",
      "mi            2,771\n",
      "##ffed       15,388\n",
      "Added ['mild'] to the tokenized_text array.\n",
      "mild         10,256\n",
      "Added ['min', '##cing'] to the tokenized_text array.\n",
      "min           8,117\n",
      "##cing        6,129\n",
      "Added ['mind', '##ful'] to the tokenized_text array.\n",
      "mind          2,568\n",
      "##ful         3,993\n",
      "Added ['mind', '##less'] to the tokenized_text array.\n",
      "mind          2,568\n",
      "##less        3,238\n",
      "Added ['mirrored'] to the tokenized_text array.\n",
      "mirrored     22,243\n",
      "Added ['mir', '##th'] to the tokenized_text array.\n",
      "mir          14,719\n",
      "##th          2,705\n",
      "Added ['mir', '##th', '##ful'] to the tokenized_text array.\n",
      "mir          14,719\n",
      "##th          2,705\n",
      "##ful         3,993\n",
      "Added ['mis', '##ant', '##hr', '##op', '##ic'] to the tokenized_text array.\n",
      "mis          28,616\n",
      "##ant         4,630\n",
      "##hr          8,093\n",
      "##op          7,361\n",
      "##ic          2,594\n",
      "Added ['mischief'] to the tokenized_text array.\n",
      "mischief     25,166\n",
      "Added ['mischievous'] to the tokenized_text array.\n",
      "mischievous  25,723\n",
      "Added ['mischievous', '##ness'] to the tokenized_text array.\n",
      "mischievous  25,723\n",
      "##ness        2,791\n",
      "Added ['miserable'] to the tokenized_text array.\n",
      "miserable    13,736\n",
      "Added ['misery'] to the tokenized_text array.\n",
      "misery       14,624\n",
      "Added ['mis', '##giving'] to the tokenized_text array.\n",
      "mis          28,616\n",
      "##giving     23,795\n",
      "Added ['mis', '##lea', '##d'] to the tokenized_text array.\n",
      "mis          28,616\n",
      "##lea        19,738\n",
      "##d           2,094\n",
      "Added ['mist', '##rus', '##t'] to the tokenized_text array.\n",
      "mist         11,094\n",
      "##rus         7,946\n",
      "##t           2,102\n",
      "Added ['mist', '##rus', '##tf', '##ul'] to the tokenized_text array.\n",
      "mist         11,094\n",
      "##rus         7,946\n",
      "##tf         24,475\n",
      "##ul          5,313\n",
      "Added ['mist', '##rus', '##ting'] to the tokenized_text array.\n",
      "mist         11,094\n",
      "##rus         7,946\n",
      "##ting        3,436\n",
      "Added ['misunderstood'] to the tokenized_text array.\n",
      "misunderstood 28,947\n",
      "Added ['mock', '##ery'] to the tokenized_text array.\n",
      "mock         12,934\n",
      "##ery         7,301\n",
      "Added ['mocking'] to the tokenized_text array.\n",
      "mocking      19,545\n",
      "Added ['mocking', '##ly'] to the tokenized_text array.\n",
      "mocking      19,545\n",
      "##ly          2,135\n",
      "Added ['modest'] to the tokenized_text array.\n",
      "modest       10,754\n",
      "Added ['mono', '##tone'] to the tokenized_text array.\n",
      "mono         18,847\n",
      "##tone        5,524\n",
      "Added ['monster'] to the tokenized_text array.\n",
      "monster       6,071\n",
      "Added ['moody'] to the tokenized_text array.\n",
      "moody        14,434\n",
      "Added ['mo', '##pe', '##y'] to the tokenized_text array.\n",
      "mo            9,587\n",
      "##pe          5,051\n",
      "##y           2,100\n",
      "Added ['mor', '##ose'] to the tokenized_text array.\n",
      "mor          22,822\n",
      "##ose         9,232\n",
      "Added ['mort', '##ified'] to the tokenized_text array.\n",
      "mort         22,294\n",
      "##ified       7,810\n",
      "Added ['motivated'] to the tokenized_text array.\n",
      "motivated    12,774\n",
      "Added ['mo', '##urn', '##ful'] to the tokenized_text array.\n",
      "mo            9,587\n",
      "##urn        14,287\n",
      "##ful         3,993\n",
      "Added ['mo', '##urn', '##fulness'] to the tokenized_text array.\n",
      "mo            9,587\n",
      "##urn        14,287\n",
      "##fulness    20,938\n",
      "Added ['mourning'] to the tokenized_text array.\n",
      "mourning     16,236\n",
      "Added ['mouthed'] to the tokenized_text array.\n",
      "mouthed      20,521\n",
      "Added ['moved'] to the tokenized_text array.\n",
      "moved         2,333\n",
      "Added ['mud', '##dled'] to the tokenized_text array.\n",
      "mud           8,494\n",
      "##dled       20,043\n",
      "Added ['mum'] to the tokenized_text array.\n",
      "mum          12,954\n",
      "Added ['murderous'] to the tokenized_text array.\n",
      "murderous    25,303\n",
      "Added ['musical'] to the tokenized_text array.\n",
      "musical       3,315\n",
      "Added ['mu', '##sing'] to the tokenized_text array.\n",
      "mu           14,163\n",
      "##sing        7,741\n",
      "Added ['muster'] to the tokenized_text array.\n",
      "muster       20,327\n",
      "Added ['mute'] to the tokenized_text array.\n",
      "mute         20,101\n",
      "Added ['muted'] to the tokenized_text array.\n",
      "muted        22,124\n",
      "Added ['muttering'] to the tokenized_text array.\n",
      "muttering    22,581\n",
      "Added ['mysterious'] to the tokenized_text array.\n",
      "mysterious    8,075\n",
      "Added ['mystical'] to the tokenized_text array.\n",
      "mystical     17,529\n",
      "Added ['my', '##sti', '##fied'] to the tokenized_text array.\n",
      "my            2,026\n",
      "##sti        16,643\n",
      "##fied       10,451\n",
      "Added ['naive'] to the tokenized_text array.\n",
      "naive        15,743\n",
      "Added ['nap', '##ping'] to the tokenized_text array.\n",
      "nap          18,996\n",
      "##ping        4,691\n",
      "Added ['narrow'] to the tokenized_text array.\n",
      "narrow        4,867\n",
      "Added ['nasty'] to the tokenized_text array.\n",
      "nasty        11,808\n",
      "Added ['natural'] to the tokenized_text array.\n",
      "natural       3,019\n",
      "Added ['nature', '##d'] to the tokenized_text array.\n",
      "nature        3,267\n",
      "##d           2,094\n",
      "Added ['naughty'] to the tokenized_text array.\n",
      "naughty      20,355\n",
      "Added ['nausea'] to the tokenized_text array.\n",
      "nausea       19,029\n",
      "Added ['nausea', '##ted'] to the tokenized_text array.\n",
      "nausea       19,029\n",
      "##ted         3,064\n",
      "Added ['na', '##use', '##ous'] to the tokenized_text array.\n",
      "na            6,583\n",
      "##use         8,557\n",
      "##ous         3,560\n",
      "Added ['needy'] to the tokenized_text array.\n",
      "needy        23,927\n",
      "Added ['ne', '##far', '##ious'] to the tokenized_text array.\n",
      "ne           11,265\n",
      "##far        14,971\n",
      "##ious        6,313\n",
      "Added ['ne', '##gating'] to the tokenized_text array.\n",
      "ne           11,265\n",
      "##gating     16,961\n",
      "Added ['negative'] to the tokenized_text array.\n",
      "negative      4,997\n",
      "Added ['ne', '##gat', '##ivity'] to the tokenized_text array.\n",
      "ne           11,265\n",
      "##gat        20,697\n",
      "##ivity       7,730\n",
      "Added ['neglected'] to the tokenized_text array.\n",
      "neglected    15,486\n",
      "Added ['ne', '##rdy'] to the tokenized_text array.\n",
      "ne           11,265\n",
      "##rdy        17,460\n",
      "Added ['nerve', '##d'] to the tokenized_text array.\n",
      "nerve         9,113\n",
      "##d           2,094\n",
      "Added ['nerves'] to the tokenized_text array.\n",
      "nerves       10,627\n",
      "Added ['nervous'] to the tokenized_text array.\n",
      "nervous       6,091\n",
      "Added ['nervously'] to the tokenized_text array.\n",
      "nervously    12,531\n",
      "Added ['nervous', '##ness'] to the tokenized_text array.\n",
      "nervous       6,091\n",
      "##ness        2,791\n",
      "Added ['nes', '##cie', '##nt'] to the tokenized_text array.\n",
      "nes          24,524\n",
      "##cie        23,402\n",
      "##nt          3,372\n",
      "Added ['net', '##tled'] to the tokenized_text array.\n",
      "net           5,658\n",
      "##tled       14,782\n",
      "Added ['neutral'] to the tokenized_text array.\n",
      "neutral       8,699\n",
      "Added ['neutrality'] to the tokenized_text array.\n",
      "neutrality   21,083\n",
      "Added ['nice'] to the tokenized_text array.\n",
      "nice          3,835\n",
      "Added ['noisy'] to the tokenized_text array.\n",
      "noisy        20,810\n",
      "Added ['non', '##bel', '##ie', '##f'] to the tokenized_text array.\n",
      "non           2,512\n",
      "##bel         8,671\n",
      "##ie          2,666\n",
      "##f           2,546\n",
      "Added ['non', '##chal', '##ance'] to the tokenized_text array.\n",
      "non           2,512\n",
      "##chal       18,598\n",
      "##ance        6,651\n",
      "Added ['non', '##chal', '##ant'] to the tokenized_text array.\n",
      "non           2,512\n",
      "##chal       18,598\n",
      "##ant         4,630\n",
      "Added ['non', '##com', '##mit', '##tal'] to the tokenized_text array.\n",
      "non           2,512\n",
      "##com         9,006\n",
      "##mit        22,930\n",
      "##tal         9,080\n",
      "Added ['non', '##com', '##pl', '##ian', '##t'] to the tokenized_text array.\n",
      "non           2,512\n",
      "##com         9,006\n",
      "##pl         24,759\n",
      "##ian         2,937\n",
      "##t           2,102\n",
      "Added ['non', '##pl', '##uss', '##ed'] to the tokenized_text array.\n",
      "non           2,512\n",
      "##pl         24,759\n",
      "##uss        17,854\n",
      "##ed          2,098\n",
      "Added ['non', '##sen', '##sic', '##al'] to the tokenized_text array.\n",
      "non           2,512\n",
      "##sen         5,054\n",
      "##sic        19,570\n",
      "##al          2,389\n",
      "Added ['normal'] to the tokenized_text array.\n",
      "normal        3,671\n",
      "Added ['nose', '##y'] to the tokenized_text array.\n",
      "nose          4,451\n",
      "##y           2,100\n",
      "Added ['nos', '##tal', '##gic'] to the tokenized_text array.\n",
      "nos          16,839\n",
      "##tal         9,080\n",
      "##gic        12,863\n",
      "Added ['nos', '##y'] to the tokenized_text array.\n",
      "nos          16,839\n",
      "##y           2,100\n",
      "Added ['numb'] to the tokenized_text array.\n",
      "numb         15,903\n",
      "Added ['obe', '##die', '##nt'] to the tokenized_text array.\n",
      "obe          15,578\n",
      "##die        10,265\n",
      "##nt          3,372\n",
      "Added ['object', '##ing'] to the tokenized_text array.\n",
      "object        4,874\n",
      "##ing         2,075\n",
      "Added ['objection'] to the tokenized_text array.\n",
      "objection    22,224\n",
      "Added ['objective'] to the tokenized_text array.\n",
      "objective     7,863\n",
      "Added ['obliged'] to the tokenized_text array.\n",
      "obliged      14,723\n",
      "Added ['ob', '##li', '##ging'] to the tokenized_text array.\n",
      "ob           27,885\n",
      "##li          3,669\n",
      "##ging        4,726\n",
      "Added ['oblivious'] to the tokenized_text array.\n",
      "oblivious    18,333\n",
      "Added ['ob', '##ser', '##vant'] to the tokenized_text array.\n",
      "ob           27,885\n",
      "##ser         8,043\n",
      "##vant       18,941\n",
      "Added ['observing'] to the tokenized_text array.\n",
      "observing    14,158\n",
      "Added ['obsessed'] to the tokenized_text array.\n",
      "obsessed     15,896\n",
      "Added ['ob', '##sti', '##nate'] to the tokenized_text array.\n",
      "ob           27,885\n",
      "##sti        16,643\n",
      "##nate       12,556\n",
      "Added ['occupied'] to the tokenized_text array.\n",
      "occupied      4,548\n",
      "Added ['odd'] to the tokenized_text array.\n",
      "odd           5,976\n",
      "Added ['odi', '##ous'] to the tokenized_text array.\n",
      "odi          21,045\n",
      "##ous         3,560\n",
      "Added ['off'] to the tokenized_text array.\n",
      "off           2,125\n",
      "Added ['offended'] to the tokenized_text array.\n",
      "offended     15,807\n",
      "Added ['offensive'] to the tokenized_text array.\n",
      "offensive     5,805\n",
      "Added ['og', '##ling'] to the tokenized_text array.\n",
      "og           13,958\n",
      "##ling        2,989\n",
      "Added ['okay'] to the tokenized_text array.\n",
      "okay          3,100\n",
      "Added ['on'] to the tokenized_text array.\n",
      "on            2,006\n",
      "Added ['open'] to the tokenized_text array.\n",
      "open          2,330\n",
      "Added ['open', '##ness'] to the tokenized_text array.\n",
      "open          2,330\n",
      "##ness        2,791\n",
      "Added ['opposed'] to the tokenized_text array.\n",
      "opposed       4,941\n",
      "Added ['opposition', '##al'] to the tokenized_text array.\n",
      "opposition    4,559\n",
      "##al          2,389\n",
      "Added ['op', '##pressed'] to the tokenized_text array.\n",
      "op            6,728\n",
      "##pressed    19,811\n",
      "Added ['optimism'] to the tokenized_text array.\n",
      "optimism     27,451\n",
      "Added ['optimistic'] to the tokenized_text array.\n",
      "optimistic   21,931\n",
      "Added ['ordering'] to the tokenized_text array.\n",
      "ordering     13,063\n",
      "Added ['orgasm', '##ic'] to the tokenized_text array.\n",
      "orgasm       13,892\n",
      "##ic          2,594\n",
      "Added ['or', '##nery'] to the tokenized_text array.\n",
      "or            2,030\n",
      "##nery       27,415\n",
      "Added ['ou', '##ch'] to the tokenized_text array.\n",
      "ou           15,068\n",
      "##ch          2,818\n",
      "Added ['out'] to the tokenized_text array.\n",
      "out           2,041\n",
      "Added ['outburst'] to the tokenized_text array.\n",
      "outburst     27,719\n",
      "Added ['out', '##cr', '##y'] to the tokenized_text array.\n",
      "out           2,041\n",
      "##cr         26,775\n",
      "##y           2,100\n",
      "Added ['out', '##ed'] to the tokenized_text array.\n",
      "out           2,041\n",
      "##ed          2,098\n",
      "Added ['out', '##land', '##ish'] to the tokenized_text array.\n",
      "out           2,041\n",
      "##land        3,122\n",
      "##ish         4,509\n",
      "Added ['outrage'] to the tokenized_text array.\n",
      "outrage      19,006\n",
      "Added ['outraged'] to the tokenized_text array.\n",
      "outraged     23,558\n",
      "Added ['outspoken'] to the tokenized_text array.\n",
      "outspoken    22,430\n",
      "Added ['over', '##be', '##aring'] to the tokenized_text array.\n",
      "over          2,058\n",
      "##be          4,783\n",
      "##aring      22,397\n",
      "Added ['over', '##ex', '##cite', '##d'] to the tokenized_text array.\n",
      "over          2,058\n",
      "##ex         10,288\n",
      "##cite       17,847\n",
      "##d           2,094\n",
      "Added ['over', '##joy', '##ed'] to the tokenized_text array.\n",
      "over          2,058\n",
      "##joy        24,793\n",
      "##ed          2,098\n",
      "Added ['overshadowed'] to the tokenized_text array.\n",
      "overshadowed 28,604\n",
      "Added ['overs', '##tr', '##ung'] to the tokenized_text array.\n",
      "overs        15,849\n",
      "##tr         16,344\n",
      "##ung         5,575\n",
      "Added ['overwhelmed'] to the tokenized_text array.\n",
      "overwhelmed  13,394\n",
      "Added ['over', '##work', '##ed'] to the tokenized_text array.\n",
      "over          2,058\n",
      "##work        6,198\n",
      "##ed          2,098\n",
      "Added ['over', '##wr', '##ough', '##t'] to the tokenized_text array.\n",
      "over          2,058\n",
      "##wr         13,088\n",
      "##ough       10,593\n",
      "##t           2,102\n",
      "Added ['pain'] to the tokenized_text array.\n",
      "pain          3,255\n",
      "Added ['pained'] to the tokenized_text array.\n",
      "pained       22,295\n",
      "Added ['painful'] to the tokenized_text array.\n",
      "painful       9,145\n",
      "Added ['painfully'] to the tokenized_text array.\n",
      "painfully    16,267\n",
      "Added ['panic'] to the tokenized_text array.\n",
      "panic         6,634\n",
      "Added ['panicked'] to the tokenized_text array.\n",
      "panicked     16,035\n",
      "Added ['panic', '##ky'] to the tokenized_text array.\n",
      "panic         6,634\n",
      "##ky          4,801\n",
      "Added ['paralyzed'] to the tokenized_text array.\n",
      "paralyzed    22,348\n",
      "Added ['paranoid'] to the tokenized_text array.\n",
      "paranoid     19,810\n",
      "Added ['passionate'] to the tokenized_text array.\n",
      "passionate   13,459\n",
      "Added ['passive'] to the tokenized_text array.\n",
      "passive      13,135\n",
      "Added ['patience'] to the tokenized_text array.\n",
      "patience     11,752\n",
      "Added ['patient'] to the tokenized_text array.\n",
      "patient       5,776\n",
      "Added ['patron', '##izing'] to the tokenized_text array.\n",
      "patron        9,161\n",
      "##izing       6,026\n",
      "Added ['pause'] to the tokenized_text array.\n",
      "pause         8,724\n",
      "Added ['pausing'] to the tokenized_text array.\n",
      "pausing      20,490\n",
      "Added ['peaceful'] to the tokenized_text array.\n",
      "peaceful      9,379\n",
      "Added ['peculiar'] to the tokenized_text array.\n",
      "peculiar     14,099\n",
      "Added ['peering'] to the tokenized_text array.\n",
      "peering      16,740\n",
      "Added ['pee', '##ved'] to the tokenized_text array.\n",
      "pee          21,392\n",
      "##ved         7,178\n",
      "Added ['pee', '##vish'] to the tokenized_text array.\n",
      "pee          21,392\n",
      "##vish       24,968\n",
      "Added ['pens', '##ive'] to the tokenized_text array.\n",
      "pens         25,636\n",
      "##ive         3,512\n",
      "Added ['pep', '##py'] to the tokenized_text array.\n",
      "pep          27,233\n",
      "##py          7,685\n",
      "Added ['per', '##ceptive'] to the tokenized_text array.\n",
      "per           2,566\n",
      "##ceptive    28,687\n",
      "Added ['per', '##fi', '##dio', '##us'] to the tokenized_text array.\n",
      "per           2,566\n",
      "##fi          8,873\n",
      "##dio        20,617\n",
      "##us          2,271\n",
      "Added ['per', '##ky'] to the tokenized_text array.\n",
      "per           2,566\n",
      "##ky          4,801\n",
      "Added ['per', '##plex', '##ed'] to the tokenized_text array.\n",
      "per           2,566\n",
      "##plex       19,386\n",
      "##ed          2,098\n",
      "Added ['per', '##plex', '##ing'] to the tokenized_text array.\n",
      "per           2,566\n",
      "##plex       19,386\n",
      "##ing         2,075\n",
      "Added ['persistent'] to the tokenized_text array.\n",
      "persistent   14,516\n",
      "Added ['persona', '##ble'] to the tokenized_text array.\n",
      "persona      16,115\n",
      "##ble         3,468\n",
      "Added ['per', '##tur', '##bed'] to the tokenized_text array.\n",
      "per           2,566\n",
      "##tur        20,689\n",
      "##bed         8,270\n",
      "Added ['per', '##verse'] to the tokenized_text array.\n",
      "per           2,566\n",
      "##verse      16,070\n",
      "Added ['pe', '##sky'] to the tokenized_text array.\n",
      "pe           21,877\n",
      "##sky         5,874\n",
      "Added ['pe', '##ssi', '##mism'] to the tokenized_text array.\n",
      "pe           21,877\n",
      "##ssi        18,719\n",
      "##mism       26,725\n",
      "Added ['pe', '##ssi', '##mist', '##ic'] to the tokenized_text array.\n",
      "pe           21,877\n",
      "##ssi        18,719\n",
      "##mist       23,738\n",
      "##ic          2,594\n",
      "Added ['pest', '##ered'] to the tokenized_text array.\n",
      "pest         20,739\n",
      "##ered        6,850\n",
      "Added ['petition', '##ing'] to the tokenized_text array.\n",
      "petition      9,964\n",
      "##ing         2,075\n",
      "Added ['pet', '##rified'] to the tokenized_text array.\n",
      "pet           9,004\n",
      "##rified     22,618\n",
      "Added ['petty'] to the tokenized_text array.\n",
      "petty        11,612\n",
      "Added ['pet', '##ula', '##nt'] to the tokenized_text array.\n",
      "pet           9,004\n",
      "##ula         7,068\n",
      "##nt          3,372\n",
      "Added ['picked'] to the tokenized_text array.\n",
      "picked        3,856\n",
      "Added ['piercing'] to the tokenized_text array.\n",
      "piercing     14,628\n",
      "Added ['pinched'] to the tokenized_text array.\n",
      "pinched      18,521\n",
      "Added ['pious'] to the tokenized_text array.\n",
      "pious        25,020\n",
      "Added ['pi', '##que', '##d'] to the tokenized_text array.\n",
      "pi           14,255\n",
      "##que         4,226\n",
      "##d           2,094\n",
      "Added ['pissed'] to the tokenized_text array.\n",
      "pissed        9,421\n",
      "Added ['pit', '##iable'] to the tokenized_text array.\n",
      "pit           6,770\n",
      "##iable      19,210\n",
      "Added ['pit', '##iful'] to the tokenized_text array.\n",
      "pit           6,770\n",
      "##iful       18,424\n",
      "Added ['pity'] to the tokenized_text array.\n",
      "pity         12,063\n",
      "Added ['pity', '##ing'] to the tokenized_text array.\n",
      "pity         12,063\n",
      "##ing         2,075\n",
      "Added ['pl', '##aca', '##ted'] to the tokenized_text array.\n",
      "pl           20,228\n",
      "##aca        19,629\n",
      "##ted         3,064\n",
      "Added ['pl', '##aca', '##tion'] to the tokenized_text array.\n",
      "pl           20,228\n",
      "##aca        19,629\n",
      "##tion        3,508\n",
      "Added ['pl', '##ac', '##id'] to the tokenized_text array.\n",
      "pl           20,228\n",
      "##ac          6,305\n",
      "##id          3,593\n",
      "Added ['plain'] to the tokenized_text array.\n",
      "plain         5,810\n",
      "Added ['plain', '##tive'] to the tokenized_text array.\n",
      "plain         5,810\n",
      "##tive        6,024\n",
      "Added ['planning'] to the tokenized_text array.\n",
      "planning      4,041\n",
      "Added ['playful'] to the tokenized_text array.\n",
      "playful      18,378\n",
      "Added ['playfully'] to the tokenized_text array.\n",
      "playfully    22,608\n",
      "Added ['pleading'] to the tokenized_text array.\n",
      "pleading     16,418\n",
      "Added ['pleasant'] to the tokenized_text array.\n",
      "pleasant      8,242\n",
      "Added ['pleased'] to the tokenized_text array.\n",
      "pleased       7,537\n",
      "Added ['pleasing'] to the tokenized_text array.\n",
      "pleasing     24,820\n",
      "Added ['pleas', '##urable'] to the tokenized_text array.\n",
      "pleas        22,512\n",
      "##urable     23,086\n",
      "Added ['pleasure'] to the tokenized_text array.\n",
      "pleasure      5,165\n",
      "Added ['pleasure', '##d'] to the tokenized_text array.\n",
      "pleasure      5,165\n",
      "##d           2,094\n",
      "Added ['pl', '##ian', '##t'] to the tokenized_text array.\n",
      "pl           20,228\n",
      "##ian         2,937\n",
      "##t           2,102\n",
      "Added ['plotting'] to the tokenized_text array.\n",
      "plotting     20,699\n",
      "Added ['po', '##ignant'] to the tokenized_text array.\n",
      "po           13,433\n",
      "##ignant     25,593\n",
      "Added ['pointed'] to the tokenized_text array.\n",
      "pointed       4,197\n",
      "Added ['poised'] to the tokenized_text array.\n",
      "poised       22,303\n",
      "Added ['polite'] to the tokenized_text array.\n",
      "polite       13,205\n",
      "Added ['po', '##mp', '##ous'] to the tokenized_text array.\n",
      "po           13,433\n",
      "##mp          8,737\n",
      "##ous         3,560\n",
      "Added ['ponder'] to the tokenized_text array.\n",
      "ponder       29,211\n",
      "Added ['ponder', '##ing'] to the tokenized_text array.\n",
      "ponder       29,211\n",
      "##ing         2,075\n",
      "Added ['po', '##oping'] to the tokenized_text array.\n",
      "po           13,433\n",
      "##oping      17,686\n",
      "Added ['pop'] to the tokenized_text array.\n",
      "pop           3,769\n",
      "Added ['posing'] to the tokenized_text array.\n",
      "posing       20,540\n",
      "Added ['positive'] to the tokenized_text array.\n",
      "positive      3,893\n",
      "Added ['po', '##sit', '##ivity'] to the tokenized_text array.\n",
      "po           13,433\n",
      "##sit        28,032\n",
      "##ivity       7,730\n",
      "Added ['possibly'] to the tokenized_text array.\n",
      "possibly      4,298\n",
      "Added ['po', '##ut'] to the tokenized_text array.\n",
      "po           13,433\n",
      "##ut          4,904\n",
      "Added ['po', '##uting'] to the tokenized_text array.\n",
      "po           13,433\n",
      "##uting      20,807\n",
      "Added ['po', '##ut', '##y'] to the tokenized_text array.\n",
      "po           13,433\n",
      "##ut          4,904\n",
      "##y           2,100\n",
      "Added ['powerful'] to the tokenized_text array.\n",
      "powerful      3,928\n",
      "Added ['powerless'] to the tokenized_text array.\n",
      "powerless    25,192\n",
      "Added ['prank', '##ing'] to the tokenized_text array.\n",
      "prank        26,418\n",
      "##ing         2,075\n",
      "Added ['pre', '##car', '##ious'] to the tokenized_text array.\n",
      "pre           3,653\n",
      "##car        10,010\n",
      "##ious        6,313\n",
      "Added ['predatory'] to the tokenized_text array.\n",
      "predatory    21,659\n",
      "Added ['prejudice', '##d'] to the tokenized_text array.\n",
      "prejudice    18,024\n",
      "##d           2,094\n",
      "Added ['preoccupied'] to the tokenized_text array.\n",
      "preoccupied  23,962\n",
      "Added ['prepared'] to the tokenized_text array.\n",
      "prepared      4,810\n",
      "Added ['preparing'] to the tokenized_text array.\n",
      "preparing     8,225\n",
      "Added ['pretending'] to the tokenized_text array.\n",
      "pretending   12,097\n",
      "Added ['pre', '##ten', '##tious'] to the tokenized_text array.\n",
      "pre           3,653\n",
      "##ten         6,528\n",
      "##tious      20,771\n",
      "Added ['pride', '##ful'] to the tokenized_text array.\n",
      "pride         6,620\n",
      "##ful         3,993\n",
      "Added ['pri', '##gg', '##ish'] to the tokenized_text array.\n",
      "pri          26,927\n",
      "##gg         13,871\n",
      "##ish         4,509\n",
      "Added ['prime', '##d'] to the tokenized_text array.\n",
      "prime         3,539\n",
      "##d           2,094\n",
      "Added ['private'] to the tokenized_text array.\n",
      "private       2,797\n",
      "Added ['processing'] to the tokenized_text array.\n",
      "processing    6,364\n",
      "Added ['proposition', '##ing'] to the tokenized_text array.\n",
      "proposition  14,848\n",
      "##ing         2,075\n",
      "Added ['proud'] to the tokenized_text array.\n",
      "proud         7,098\n",
      "Added ['provocative'] to the tokenized_text array.\n",
      "provocative  26,422\n",
      "Added ['provoke'] to the tokenized_text array.\n",
      "provoke      27,895\n",
      "Added ['provoked'] to the tokenized_text array.\n",
      "provoked     19,157\n",
      "Added ['pro', '##voking'] to the tokenized_text array.\n",
      "pro           4,013\n",
      "##voking     22,776\n",
      "Added ['pry', '##ing'] to the tokenized_text array.\n",
      "pry          29,198\n",
      "##ing         2,075\n",
      "Added ['psycho'] to the tokenized_text array.\n",
      "psycho       18,224\n",
      "Added ['psychotic'] to the tokenized_text array.\n",
      "psychotic    27,756\n",
      "Added ['puck', '##ish'] to the tokenized_text array.\n",
      "puck         22,900\n",
      "##ish         4,509\n",
      "Added ['pu', '##eri', '##le'] to the tokenized_text array.\n",
      "pu           16,405\n",
      "##eri        11,124\n",
      "##le          2,571\n",
      "Added ['pu', '##gna', '##cious'] to the tokenized_text array.\n",
      "pu           16,405\n",
      "##gna        16,989\n",
      "##cious      18,436\n",
      "Added ['punished'] to the tokenized_text array.\n",
      "punished     14,248\n",
      "Added ['punish', '##ing'] to the tokenized_text array.\n",
      "punish       16,385\n",
      "##ing         2,075\n",
      "Added ['pun', '##itive'] to the tokenized_text array.\n",
      "pun          26,136\n",
      "##itive      13,043\n",
      "Added ['punk'] to the tokenized_text array.\n",
      "punk          7,196\n",
      "Added ['puppy', '##ish'] to the tokenized_text array.\n",
      "puppy        17,022\n",
      "##ish         4,509\n",
      "Added ['purpose', '##ful'] to the tokenized_text array.\n",
      "purpose       3,800\n",
      "##ful         3,993\n",
      "Added ['pursed'] to the tokenized_text array.\n",
      "pursed       21,954\n",
      "Added ['put'] to the tokenized_text array.\n",
      "put           2,404\n",
      "Added ['putting'] to the tokenized_text array.\n",
      "putting       5,128\n",
      "Added ['puzzled'] to the tokenized_text array.\n",
      "puzzled      14,909\n",
      "Added ['puzzle', '##ment'] to the tokenized_text array.\n",
      "puzzle       11,989\n",
      "##ment        3,672\n",
      "Added ['qu', '##al', '##ms'] to the tokenized_text array.\n",
      "qu           24,209\n",
      "##al          2,389\n",
      "##ms          5,244\n",
      "Added ['quarrel', '##some'] to the tokenized_text array.\n",
      "quarrel      26,260\n",
      "##some       14,045\n",
      "Added ['que', '##as', '##y'] to the tokenized_text array.\n",
      "que          10,861\n",
      "##as          3,022\n",
      "##y           2,100\n",
      "Added ['que', '##nched'] to the tokenized_text array.\n",
      "que          10,861\n",
      "##nched      19,282\n",
      "Added ['questionable'] to the tokenized_text array.\n",
      "questionable 21,068\n",
      "Added ['questioning'] to the tokenized_text array.\n",
      "questioning  11,242\n",
      "Added ['questioning', '##ly'] to the tokenized_text array.\n",
      "questioning  11,242\n",
      "##ly          2,135\n",
      "Added ['quiet'] to the tokenized_text array.\n",
      "quiet         4,251\n",
      "Added ['quiet', '##ness'] to the tokenized_text array.\n",
      "quiet         4,251\n",
      "##ness        2,791\n",
      "Added ['quilt'] to the tokenized_text array.\n",
      "quilt        27,565\n",
      "Added ['qui', '##rky'] to the tokenized_text array.\n",
      "qui          21,864\n",
      "##rky        15,952\n",
      "Added ['quiz', '##zic', '##al'] to the tokenized_text array.\n",
      "quiz         19,461\n",
      "##zic        27,586\n",
      "##al          2,389\n",
      "Added ['ra', '##bid'] to the tokenized_text array.\n",
      "ra           10,958\n",
      "##bid        17,062\n",
      "Added ['rack', '##ed'] to the tokenized_text array.\n",
      "rack         14,513\n",
      "##ed          2,098\n",
      "Added ['radiant'] to the tokenized_text array.\n",
      "radiant      23,751\n",
      "Added ['rage'] to the tokenized_text array.\n",
      "rage          7,385\n",
      "Added ['raged'] to the tokenized_text array.\n",
      "raged        28,374\n",
      "Added ['ragged'] to the tokenized_text array.\n",
      "ragged       14,202\n",
      "Added ['raging'] to the tokenized_text array.\n",
      "raging       17,559\n",
      "Added ['ran', '##cor', '##ous'] to the tokenized_text array.\n",
      "ran           2,743\n",
      "##cor        27,108\n",
      "##ous         3,560\n",
      "Added ['randy'] to the tokenized_text array.\n",
      "randy         9,744\n",
      "Added ['rap', '##t'] to the tokenized_text array.\n",
      "rap           9,680\n",
      "##t           2,102\n",
      "Added ['rattled'] to the tokenized_text array.\n",
      "rattled      19,252\n",
      "Added ['ravi', '##ng'] to the tokenized_text array.\n",
      "ravi         16,806\n",
      "##ng          3,070\n",
      "Added ['reactive'] to the tokenized_text array.\n",
      "reactive     22,643\n",
      "Added ['ready'] to the tokenized_text array.\n",
      "ready         3,201\n",
      "Added ['realization'] to the tokenized_text array.\n",
      "realization  12,393\n",
      "Added ['reassured'] to the tokenized_text array.\n",
      "reassured    26,350\n",
      "Added ['rebellious'] to the tokenized_text array.\n",
      "rebellious   22,614\n",
      "Added ['re', '##bu', '##ke'] to the tokenized_text array.\n",
      "re            2,128\n",
      "##bu          8,569\n",
      "##ke          3,489\n",
      "Added ['recalling'] to the tokenized_text array.\n",
      "recalling    21,195\n",
      "Added ['rec', '##eptive'] to the tokenized_text array.\n",
      "rec          28,667\n",
      "##eptive     22,048\n",
      "Added ['reckless'] to the tokenized_text array.\n",
      "reckless     18,555\n",
      "Added ['recoil'] to the tokenized_text array.\n",
      "recoil       27,429\n",
      "Added ['recoil', '##ing'] to the tokenized_text array.\n",
      "recoil       27,429\n",
      "##ing         2,075\n",
      "Added ['reflecting'] to the tokenized_text array.\n",
      "reflecting   10,842\n",
      "Added ['reflection'] to the tokenized_text array.\n",
      "reflection    9,185\n",
      "Added ['reflective'] to the tokenized_text array.\n",
      "reflective   21,346\n",
      "Added ['ref', '##ul', '##gent'] to the tokenized_text array.\n",
      "ref          25,416\n",
      "##ul          5,313\n",
      "##gent       11,461\n",
      "Added ['refusing'] to the tokenized_text array.\n",
      "refusing     11,193\n",
      "Added ['regret'] to the tokenized_text array.\n",
      "regret        9,038\n",
      "Added ['regret', '##ful'] to the tokenized_text array.\n",
      "regret        9,038\n",
      "##ful         3,993\n",
      "Added ['rejected'] to the tokenized_text array.\n",
      "rejected      5,837\n",
      "Added ['rejecting'] to the tokenized_text array.\n",
      "rejecting    21,936\n",
      "Added ['rejection'] to the tokenized_text array.\n",
      "rejection    13,893\n",
      "Added ['re', '##jo', '##icing'] to the tokenized_text array.\n",
      "re            2,128\n",
      "##jo          5,558\n",
      "##icing      23,553\n",
      "Added ['relaxation'] to the tokenized_text array.\n",
      "relaxation   23,370\n",
      "Added ['relaxed'] to the tokenized_text array.\n",
      "relaxed       8,363\n",
      "Added ['relentless'] to the tokenized_text array.\n",
      "relentless   21,660\n",
      "Added ['relief'] to the tokenized_text array.\n",
      "relief        4,335\n",
      "Added ['relieved'] to the tokenized_text array.\n",
      "relieved      7,653\n",
      "Added ['re', '##li', '##ved'] to the tokenized_text array.\n",
      "re            2,128\n",
      "##li          3,669\n",
      "##ved         7,178\n",
      "Added ['reluctant'] to the tokenized_text array.\n",
      "reluctant    11,542\n",
      "Added ['reluctantly'] to the tokenized_text array.\n",
      "reluctantly  11,206\n",
      "Added ['remorse'] to the tokenized_text array.\n",
      "remorse      23,124\n",
      "Added ['remorse', '##ful'] to the tokenized_text array.\n",
      "remorse      23,124\n",
      "##ful         3,993\n",
      "Added ['rep', '##elled'] to the tokenized_text array.\n",
      "rep          16,360\n",
      "##elled      21,148\n",
      "Added ['rep', '##ressed'] to the tokenized_text array.\n",
      "rep          16,360\n",
      "##ressed     16,119\n",
      "Added ['rep', '##ro', '##ach'] to the tokenized_text array.\n",
      "rep          16,360\n",
      "##ro          3,217\n",
      "##ach         6,776\n",
      "Added ['rep', '##ro', '##ach', '##ful'] to the tokenized_text array.\n",
      "rep          16,360\n",
      "##ro          3,217\n",
      "##ach         6,776\n",
      "##ful         3,993\n",
      "Added ['rep', '##ug', '##nan', '##ce'] to the tokenized_text array.\n",
      "rep          16,360\n",
      "##ug         15,916\n",
      "##nan         7,229\n",
      "##ce          3,401\n",
      "Added ['rep', '##ug', '##nant'] to the tokenized_text array.\n",
      "rep          16,360\n",
      "##ug         15,916\n",
      "##nant       16,885\n",
      "Added ['repulsed'] to the tokenized_text array.\n",
      "repulsed     24,571\n",
      "Added ['rep', '##ulsion'] to the tokenized_text array.\n",
      "rep          16,360\n",
      "##ulsion     23,316\n",
      "Added ['res', '##ent'] to the tokenized_text array.\n",
      "res          24,501\n",
      "##ent         4,765\n",
      "Added ['res', '##ent', '##ful'] to the tokenized_text array.\n",
      "res          24,501\n",
      "##ent         4,765\n",
      "##ful         3,993\n",
      "Added ['res', '##enting'] to the tokenized_text array.\n",
      "res          24,501\n",
      "##enting     26,951\n",
      "Added ['resentment'] to the tokenized_text array.\n",
      "resentment   20,234\n",
      "Added ['reserved'] to the tokenized_text array.\n",
      "reserved      9,235\n",
      "Added ['resignation'] to the tokenized_text array.\n",
      "resignation   8,172\n",
      "Added ['resigned'] to the tokenized_text array.\n",
      "resigned      5,295\n",
      "Added ['res', '##ili', '##ence'] to the tokenized_text array.\n",
      "res          24,501\n",
      "##ili        18,622\n",
      "##ence       10,127\n",
      "Added ['resistance'] to the tokenized_text array.\n",
      "resistance    5,012\n",
      "Added ['resistant'] to the tokenized_text array.\n",
      "resistant    13,070\n",
      "Added ['resist', '##ent'] to the tokenized_text array.\n",
      "resist        9,507\n",
      "##ent         4,765\n",
      "Added ['resisting'] to the tokenized_text array.\n",
      "resisting    22,363\n",
      "Added ['res', '##ol', '##ute'] to the tokenized_text array.\n",
      "res          24,501\n",
      "##ol          4,747\n",
      "##ute        10,421\n",
      "Added ['resolved'] to the tokenized_text array.\n",
      "resolved     10,395\n",
      "Added ['responsive'] to the tokenized_text array.\n",
      "responsive   26,651\n",
      "Added ['rest', '##ful'] to the tokenized_text array.\n",
      "rest          2,717\n",
      "##ful         3,993\n",
      "Added ['resting'] to the tokenized_text array.\n",
      "resting       8,345\n",
      "Added ['restless'] to the tokenized_text array.\n",
      "restless     15,035\n",
      "Added ['restless', '##ness'] to the tokenized_text array.\n",
      "restless     15,035\n",
      "##ness        2,791\n",
      "Added ['restrained'] to the tokenized_text array.\n",
      "restrained   19,868\n",
      "Added ['restraint'] to the tokenized_text array.\n",
      "restraint    19,355\n",
      "Added ['re', '##tal', '##iating'] to the tokenized_text array.\n",
      "re            2,128\n",
      "##tal         9,080\n",
      "##iating     15,370\n",
      "Added ['re', '##tal', '##ia', '##tory'] to the tokenized_text array.\n",
      "re            2,128\n",
      "##tal         9,080\n",
      "##ia          2,401\n",
      "##tory        7,062\n",
      "Added ['re', '##thi', '##nk', '##ing'] to the tokenized_text array.\n",
      "re            2,128\n",
      "##thi        15,222\n",
      "##nk          8,950\n",
      "##ing         2,075\n",
      "Added ['re', '##tic', '##ence'] to the tokenized_text array.\n",
      "re            2,128\n",
      "##tic         4,588\n",
      "##ence       10,127\n",
      "Added ['re', '##tic', '##ent'] to the tokenized_text array.\n",
      "re            2,128\n",
      "##tic         4,588\n",
      "##ent         4,765\n",
      "Added ['revenge', '##ful'] to the tokenized_text array.\n",
      "revenge       7,195\n",
      "##ful         3,993\n",
      "Added ['rev', '##ere', '##nt'] to the tokenized_text array.\n",
      "rev           7,065\n",
      "##ere         7,869\n",
      "##nt          3,372\n",
      "Added ['revolt', '##ed'] to the tokenized_text array.\n",
      "revolt       10,073\n",
      "##ed          2,098\n",
      "Added ['rev', '##ulsion'] to the tokenized_text array.\n",
      "rev           7,065\n",
      "##ulsion     23,316\n",
      "Added ['righteous'] to the tokenized_text array.\n",
      "righteous    19,556\n",
      "Added ['rigid'] to the tokenized_text array.\n",
      "rigid        11,841\n",
      "Added ['ri', '##led'] to the tokenized_text array.\n",
      "ri           15,544\n",
      "##led         3,709\n",
      "Added ['riot', '##ous'] to the tokenized_text array.\n",
      "riot         11,421\n",
      "##ous         3,560\n",
      "Added ['ri', '##vet', '##ed'] to the tokenized_text array.\n",
      "ri           15,544\n",
      "##vet        19,510\n",
      "##ed          2,098\n",
      "Added ['roar'] to the tokenized_text array.\n",
      "roar         11,950\n",
      "Added ['ro', '##gui', '##sh'] to the tokenized_text array.\n",
      "ro           20,996\n",
      "##gui        25,698\n",
      "##sh          4,095\n",
      "Added ['roi', '##led'] to the tokenized_text array.\n",
      "roi          25,223\n",
      "##led         3,709\n",
      "Added ['rough'] to the tokenized_text array.\n",
      "rough         5,931\n",
      "Added ['rouse', '##d'] to the tokenized_text array.\n",
      "rouse        27,384\n",
      "##d           2,094\n",
      "Added ['rude'] to the tokenized_text array.\n",
      "rude         12,726\n",
      "Added ['rue', '##ful'] to the tokenized_text array.\n",
      "rue          13,413\n",
      "##ful         3,993\n",
      "Added ['ru', '##ffled'] to the tokenized_text array.\n",
      "ru           21,766\n",
      "##ffled      28,579\n",
      "Added ['rum', '##inating'] to the tokenized_text array.\n",
      "rum          19,379\n",
      "##inating    19,185\n",
      "Added ['rust', '##led'] to the tokenized_text array.\n",
      "rust         18,399\n",
      "##led         3,709\n",
      "Added ['ruthless'] to the tokenized_text array.\n",
      "ruthless     18,101\n",
      "Added ['sad'] to the tokenized_text array.\n",
      "sad           6,517\n",
      "Added ['sad', '##den'] to the tokenized_text array.\n",
      "sad           6,517\n",
      "##den         4,181\n",
      "Added ['sad', '##dened'] to the tokenized_text array.\n",
      "sad           6,517\n",
      "##dened      24,589\n",
      "Added ['sad', '##istic'] to the tokenized_text array.\n",
      "sad           6,517\n",
      "##istic       6,553\n",
      "Added ['sadness'] to the tokenized_text array.\n",
      "sadness      12,039\n",
      "Added ['sal', '##acious'] to the tokenized_text array.\n",
      "sal          16,183\n",
      "##acious     20,113\n",
      "Added ['saliva', '##ting'] to the tokenized_text array.\n",
      "saliva       26,308\n",
      "##ting        3,436\n",
      "Added ['san', '##ct', '##imo', '##nio', '##us'] to the tokenized_text array.\n",
      "san           2,624\n",
      "##ct          6,593\n",
      "##imo        16,339\n",
      "##nio        27,678\n",
      "##us          2,271\n",
      "Added ['sane'] to the tokenized_text array.\n",
      "sane         22,856\n",
      "Added ['sang', '##uin', '##e'] to the tokenized_text array.\n",
      "sang          6,369\n",
      "##uin        20,023\n",
      "##e           2,063\n",
      "Added ['sap', '##py'] to the tokenized_text array.\n",
      "sap          20,066\n",
      "##py          7,685\n",
      "Added ['sarcasm'] to the tokenized_text array.\n",
      "sarcasm      20,954\n",
      "Added ['sarcastic'] to the tokenized_text array.\n",
      "sarcastic    22,473\n",
      "Added ['sar', '##don', '##ic'] to the tokenized_text array.\n",
      "sar          18,906\n",
      "##don         5,280\n",
      "##ic          2,594\n",
      "Added ['sas', '##sy'] to the tokenized_text array.\n",
      "sas          21,871\n",
      "##sy          6,508\n",
      "Added ['sat', '##ed'] to the tokenized_text array.\n",
      "sat           2,938\n",
      "##ed          2,098\n",
      "Added ['sat', '##iated'] to the tokenized_text array.\n",
      "sat           2,938\n",
      "##iated      15,070\n",
      "Added ['satirical'] to the tokenized_text array.\n",
      "satirical    17,251\n",
      "Added ['satisfaction'] to the tokenized_text array.\n",
      "satisfaction  9,967\n",
      "Added ['satisfied'] to the tokenized_text array.\n",
      "satisfied     8,510\n",
      "Added ['satisfy'] to the tokenized_text array.\n",
      "satisfy      13,225\n",
      "Added ['saturn', '##ine'] to the tokenized_text array.\n",
      "saturn       14,784\n",
      "##ine         3,170\n",
      "Added ['sa', '##uc', '##y'] to the tokenized_text array.\n",
      "sa            7,842\n",
      "##uc         14,194\n",
      "##y           2,100\n",
      "Added ['savage'] to the tokenized_text array.\n",
      "savage        9,576\n",
      "Added ['scandal', '##ized'] to the tokenized_text array.\n",
      "scandal       9,446\n",
      "##ized        3,550\n",
      "Added ['scare'] to the tokenized_text array.\n",
      "scare        12,665\n",
      "Added ['scared'] to the tokenized_text array.\n",
      "scared        6,015\n",
      "Added ['scary'] to the tokenized_text array.\n",
      "scary        12,459\n",
      "Added ['scattered'] to the tokenized_text array.\n",
      "scattered     7,932\n",
      "Added ['sc', '##had', '##en', '##fr', '##eu', '##de'] to the tokenized_text array.\n",
      "sc            8,040\n",
      "##had        16,102\n",
      "##en          2,368\n",
      "##fr         19,699\n",
      "##eu         13,765\n",
      "##de          3,207\n",
      "Added ['sc', '##hem', '##ing'] to the tokenized_text array.\n",
      "sc            8,040\n",
      "##hem        29,122\n",
      "##ing         2,075\n",
      "Added ['sc', '##off', '##er'] to the tokenized_text array.\n",
      "sc            8,040\n",
      "##off         7,245\n",
      "##er          2,121\n",
      "Added ['sc', '##off', '##ing'] to the tokenized_text array.\n",
      "sc            8,040\n",
      "##off         7,245\n",
      "##ing         2,075\n",
      "Added ['sc', '##orn'] to the tokenized_text array.\n",
      "sc            8,040\n",
      "##orn         9,691\n",
      "Added ['sc', '##orne', '##d'] to the tokenized_text array.\n",
      "sc            8,040\n",
      "##orne       23,846\n",
      "##d           2,094\n",
      "Added ['sc', '##orn', '##ful'] to the tokenized_text array.\n",
      "sc            8,040\n",
      "##orn         9,691\n",
      "##ful         3,993\n",
      "Added ['scowl'] to the tokenized_text array.\n",
      "scowl        19,981\n",
      "Added ['scowl', '##ing'] to the tokenized_text array.\n",
      "scowl        19,981\n",
      "##ing         2,075\n",
      "Added ['scream'] to the tokenized_text array.\n",
      "scream        6,978\n",
      "Added ['screaming'] to the tokenized_text array.\n",
      "screaming     7,491\n",
      "Added ['sc', '##rut', '##ini', '##zing'] to the tokenized_text array.\n",
      "sc            8,040\n",
      "##rut        22,134\n",
      "##ini         5,498\n",
      "##zing        6,774\n",
      "Added ['sealed'] to the tokenized_text array.\n",
      "sealed       10,203\n",
      "Added ['searching'] to the tokenized_text array.\n",
      "searching     6,575\n",
      "Added ['secretive'] to the tokenized_text array.\n",
      "secretive    28,607\n",
      "Added ['secretive', '##ly'] to the tokenized_text array.\n",
      "secretive    28,607\n",
      "##ly          2,135\n",
      "Added ['secure'] to the tokenized_text array.\n",
      "secure        5,851\n",
      "Added ['se', '##date'] to the tokenized_text array.\n",
      "se            7,367\n",
      "##date       13,701\n",
      "Added ['seduction'] to the tokenized_text array.\n",
      "seduction    26,962\n",
      "Added ['seductive'] to the tokenized_text array.\n",
      "seductive    23,182\n",
      "Added ['see', '##thing'] to the tokenized_text array.\n",
      "see           2,156\n",
      "##thing      20,744\n",
      "Added ['self'] to the tokenized_text array.\n",
      "self          2,969\n",
      "Added ['sensual'] to the tokenized_text array.\n",
      "sensual      18,753\n",
      "Added ['sentimental'] to the tokenized_text array.\n",
      "sentimental  23,069\n",
      "Added ['serene'] to the tokenized_text array.\n",
      "serene       25,388\n",
      "Added ['serious'] to the tokenized_text array.\n",
      "serious       3,809\n",
      "Added ['seriousness'] to the tokenized_text array.\n",
      "seriousness  27,994\n",
      "Added ['ser', '##vil', '##e'] to the tokenized_text array.\n",
      "ser          14,262\n",
      "##vil        14,762\n",
      "##e           2,063\n",
      "Added ['set'] to the tokenized_text array.\n",
      "set           2,275\n",
      "Added ['severe'] to the tokenized_text array.\n",
      "severe        5,729\n",
      "Added ['sha', '##bby'] to the tokenized_text array.\n",
      "sha          21,146\n",
      "##bby        14,075\n",
      "Added ['shady'] to the tokenized_text array.\n",
      "shady        22,824\n",
      "Added ['shaken'] to the tokenized_text array.\n",
      "shaken       16,697\n",
      "Added ['shaky'] to the tokenized_text array.\n",
      "shaky        15,311\n",
      "Added ['shame'] to the tokenized_text array.\n",
      "shame         9,467\n",
      "Added ['shame', '##d'] to the tokenized_text array.\n",
      "shame         9,467\n",
      "##d           2,094\n",
      "Added ['shame', '##face', '##d'] to the tokenized_text array.\n",
      "shame         9,467\n",
      "##face       12,172\n",
      "##d           2,094\n",
      "Added ['shame', '##ful'] to the tokenized_text array.\n",
      "shame         9,467\n",
      "##ful         3,993\n",
      "Added ['shame', '##less'] to the tokenized_text array.\n",
      "shame         9,467\n",
      "##less        3,238\n",
      "Added ['sharp'] to the tokenized_text array.\n",
      "sharp         4,629\n",
      "Added ['sheep', '##ish'] to the tokenized_text array.\n",
      "sheep         8,351\n",
      "##ish         4,509\n",
      "Added ['sheep', '##ish', '##ness'] to the tokenized_text array.\n",
      "sheep         8,351\n",
      "##ish         4,509\n",
      "##ness        2,791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added ['shell', '##ed'] to the tokenized_text array.\n",
      "shell         5,806\n",
      "##ed          2,098\n",
      "Added ['shift', '##y'] to the tokenized_text array.\n",
      "shift         5,670\n",
      "##y           2,100\n",
      "Added ['shock'] to the tokenized_text array.\n",
      "shock         5,213\n",
      "Added ['shocked'] to the tokenized_text array.\n",
      "shocked       7,135\n",
      "Added ['shocking'] to the tokenized_text array.\n",
      "shocking     16,880\n",
      "Added ['shocking', '##ly'] to the tokenized_text array.\n",
      "shocking     16,880\n",
      "##ly          2,135\n",
      "Added ['shook'] to the tokenized_text array.\n",
      "shook         3,184\n",
      "Added ['shout'] to the tokenized_text array.\n",
      "shout        11,245\n",
      "Added ['shouting'] to the tokenized_text array.\n",
      "shouting     11,273\n",
      "Added ['sh', '##rew', '##d'] to the tokenized_text array.\n",
      "sh           14,021\n",
      "##rew        15,603\n",
      "##d           2,094\n",
      "Added ['shy'] to the tokenized_text array.\n",
      "shy          11,004\n",
      "Added ['shy', '##ness'] to the tokenized_text array.\n",
      "shy          11,004\n",
      "##ness        2,791\n",
      "Added ['sick'] to the tokenized_text array.\n",
      "sick          5,305\n",
      "Added ['sick', '##en'] to the tokenized_text array.\n",
      "sick          5,305\n",
      "##en          2,368\n",
      "Added ['sick', '##ened'] to the tokenized_text array.\n",
      "sick          5,305\n",
      "##ened        6,675\n",
      "Added ['sigh'] to the tokenized_text array.\n",
      "sigh          6,682\n",
      "Added ['silenced'] to the tokenized_text array.\n",
      "silenced     25,030\n",
      "Added ['silent'] to the tokenized_text array.\n",
      "silent        4,333\n",
      "Added ['si', '##llin', '##ess'] to the tokenized_text array.\n",
      "si            9,033\n",
      "##llin       21,202\n",
      "##ess         7,971\n",
      "Added ['silly'] to the tokenized_text array.\n",
      "silly        10,021\n",
      "Added ['sim', '##mering'] to the tokenized_text array.\n",
      "sim          21,934\n",
      "##mering     27,851\n",
      "Added ['sim', '##per'] to the tokenized_text array.\n",
      "sim          21,934\n",
      "##per         4,842\n",
      "Added ['sim', '##per', '##ing'] to the tokenized_text array.\n",
      "sim          21,934\n",
      "##per         4,842\n",
      "##ing         2,075\n",
      "Added ['simple'] to the tokenized_text array.\n",
      "simple        3,722\n",
      "Added ['simplicity'] to the tokenized_text array.\n",
      "simplicity   17,839\n",
      "Added ['sincere'] to the tokenized_text array.\n",
      "sincere      18,006\n",
      "Added ['sin', '##ful'] to the tokenized_text array.\n",
      "sin           8,254\n",
      "##ful         3,993\n",
      "Added ['singing'] to the tokenized_text array.\n",
      "singing       4,823\n",
      "Added ['sinister'] to the tokenized_text array.\n",
      "sinister     16,491\n",
      "Added ['sinister', '##ly'] to the tokenized_text array.\n",
      "sinister     16,491\n",
      "##ly          2,135\n",
      "Added ['si', '##zing'] to the tokenized_text array.\n",
      "si            9,033\n",
      "##zing        6,774\n",
      "Added ['sk', '##ept', '##ic'] to the tokenized_text array.\n",
      "sk           15,315\n",
      "##ept        23,606\n",
      "##ic          2,594\n",
      "Added ['skeptical'] to the tokenized_text array.\n",
      "skeptical    18,386\n",
      "Added ['skeptical', '##ly'] to the tokenized_text array.\n",
      "skeptical    18,386\n",
      "##ly          2,135\n",
      "Added ['skepticism'] to the tokenized_text array.\n",
      "skepticism   27,936\n",
      "Added ['sketch', '##y'] to the tokenized_text array.\n",
      "sketch       11,080\n",
      "##y           2,100\n",
      "Added ['ski', '##tti', '##sh'] to the tokenized_text array.\n",
      "ski           8,301\n",
      "##tti         6,916\n",
      "##sh          4,095\n",
      "Added ['slack'] to the tokenized_text array.\n",
      "slack        19,840\n",
      "Added ['sl', '##ea', '##zy'] to the tokenized_text array.\n",
      "sl           22,889\n",
      "##ea          5,243\n",
      "##zy          9,096\n",
      "Added ['sleepy'] to the tokenized_text array.\n",
      "sleepy       17,056\n",
      "Added ['slick'] to the tokenized_text array.\n",
      "slick        13,554\n",
      "Added ['slot', '##h', '##ful'] to the tokenized_text array.\n",
      "slot         10,453\n",
      "##h           2,232\n",
      "##ful         3,993\n",
      "Added ['slow'] to the tokenized_text array.\n",
      "slow          4,030\n",
      "Added ['slug', '##gis', '##h'] to the tokenized_text array.\n",
      "slug         23,667\n",
      "##gis        17,701\n",
      "##h           2,232\n",
      "Added ['sly'] to the tokenized_text array.\n",
      "sly          18,230\n",
      "Added ['sm', '##arm', '##y'] to the tokenized_text array.\n",
      "sm           15,488\n",
      "##arm        27,292\n",
      "##y           2,100\n",
      "Added ['smart'] to the tokenized_text array.\n",
      "smart         6,047\n",
      "Added ['smashed'] to the tokenized_text array.\n",
      "smashed      14,368\n",
      "Added ['smile'] to the tokenized_text array.\n",
      "smile         2,868\n",
      "Added ['smiley'] to the tokenized_text array.\n",
      "smiley       27,420\n",
      "Added ['smiling'] to the tokenized_text array.\n",
      "smiling       5,629\n",
      "Added ['smirk'] to the tokenized_text array.\n",
      "smirk        15,081\n",
      "Added ['smirk', '##ing'] to the tokenized_text array.\n",
      "smirk        15,081\n",
      "##ing         2,075\n",
      "Added ['sm', '##old', '##ering'] to the tokenized_text array.\n",
      "sm           15,488\n",
      "##old        11,614\n",
      "##ering       7,999\n",
      "Added ['sm', '##oo', '##ching'] to the tokenized_text array.\n",
      "sm           15,488\n",
      "##oo          9,541\n",
      "##ching       8,450\n",
      "Added ['smooth'] to the tokenized_text array.\n",
      "smooth        5,744\n",
      "Added ['smug'] to the tokenized_text array.\n",
      "smug         20,673\n",
      "Added ['smug', '##ness'] to the tokenized_text array.\n",
      "smug         20,673\n",
      "##ness        2,791\n",
      "Added ['snake'] to the tokenized_text array.\n",
      "snake         7,488\n",
      "Added ['snap', '##py'] to the tokenized_text array.\n",
      "snap         10,245\n",
      "##py          7,685\n",
      "Added ['s', '##nar', '##ky'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##nar        11,802\n",
      "##ky          4,801\n",
      "Added ['snarl'] to the tokenized_text array.\n",
      "snarl        24,845\n",
      "Added ['snarled'] to the tokenized_text array.\n",
      "snarled      15,022\n",
      "Added ['snarl', '##ing'] to the tokenized_text array.\n",
      "snarl        24,845\n",
      "##ing         2,075\n",
      "Added ['snarl', '##y'] to the tokenized_text array.\n",
      "snarl        24,845\n",
      "##y           2,100\n",
      "Added ['sneak', '##y'] to the tokenized_text array.\n",
      "sneak        13,583\n",
      "##y           2,100\n",
      "Added ['s', '##neer'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##neer       19,755\n",
      "Added ['s', '##neer', '##ing'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##neer       19,755\n",
      "##ing         2,075\n",
      "Added ['s', '##nee', '##ze'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##nee        24,045\n",
      "##ze          4,371\n",
      "Added ['s', '##nee', '##zing'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##nee        24,045\n",
      "##zing        6,774\n",
      "Added ['s', '##nick', '##er'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##nick       13,542\n",
      "##er          2,121\n",
      "Added ['s', '##nick', '##ering'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##nick       13,542\n",
      "##ering       7,999\n",
      "Added ['s', '##ni', '##de'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##ni          3,490\n",
      "##de          3,207\n",
      "Added ['s', '##nig', '##ger', '##ing'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##nig        25,518\n",
      "##ger         4,590\n",
      "##ing         2,075\n",
      "Added ['s', '##ni', '##vel', '##ing'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##ni          3,490\n",
      "##vel        15,985\n",
      "##ing         2,075\n",
      "Added ['s', '##nob', '##bis', '##h'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##nob        25,083\n",
      "##bis        18,477\n",
      "##h           2,232\n",
      "Added ['s', '##nob', '##by'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##nob        25,083\n",
      "##by          3,762\n",
      "Added ['s', '##no', '##ot', '##y'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##no          3,630\n",
      "##ot          4,140\n",
      "##y           2,100\n",
      "Added ['s', '##not', '##ty'] to the tokenized_text array.\n",
      "s             1,055\n",
      "##not        17,048\n",
      "##ty          3,723\n",
      "Added ['soc', '##iable'] to the tokenized_text array.\n",
      "soc          27,084\n",
      "##iable      19,210\n",
      "Added ['soft'] to the tokenized_text array.\n",
      "soft          3,730\n",
      "Added ['solemn'] to the tokenized_text array.\n",
      "solemn       19,487\n",
      "Added ['sol', '##ici', '##tou', '##s'] to the tokenized_text array.\n",
      "sol          14,017\n",
      "##ici        28,775\n",
      "##tou        24,826\n",
      "##s           2,015\n",
      "Added ['solitary'] to the tokenized_text array.\n",
      "solitary     14,348\n",
      "Added ['solitude'] to the tokenized_text array.\n",
      "solitude     22,560\n",
      "Added ['somber'] to the tokenized_text array.\n",
      "somber       28,587\n",
      "Added ['somber', '##ly'] to the tokenized_text array.\n",
      "somber       28,587\n",
      "##ly          2,135\n",
      "Added ['so', '##m', '##no', '##lent'] to the tokenized_text array.\n",
      "so            2,061\n",
      "##m           2,213\n",
      "##no          3,630\n",
      "##lent       16,136\n",
      "Added ['soothe', '##d'] to the tokenized_text array.\n",
      "soothe       28,384\n",
      "##d           2,094\n",
      "Added ['sore'] to the tokenized_text array.\n",
      "sore         14,699\n",
      "Added ['sorrow'] to the tokenized_text array.\n",
      "sorrow       14,038\n",
      "Added ['sorrow', '##ful'] to the tokenized_text array.\n",
      "sorrow       14,038\n",
      "##ful         3,993\n",
      "Added ['sorry'] to the tokenized_text array.\n",
      "sorry         3,374\n",
      "Added ['sour'] to the tokenized_text array.\n",
      "sour         14,768\n",
      "Added ['spaced'] to the tokenized_text array.\n",
      "spaced       19,835\n",
      "Added ['spa', '##cing'] to the tokenized_text array.\n",
      "spa          12,403\n",
      "##cing        6,129\n",
      "Added ['spa', '##stic'] to the tokenized_text array.\n",
      "spa          12,403\n",
      "##stic       10,074\n",
      "Added ['speaking'] to the tokenized_text array.\n",
      "speaking      4,092\n",
      "Added ['spec', '##ious'] to the tokenized_text array.\n",
      "spec         28,699\n",
      "##ious        6,313\n",
      "Added ['speculative'] to the tokenized_text array.\n",
      "speculative  23,250\n",
      "Added ['speechless'] to the tokenized_text array.\n",
      "speechless   25,146\n",
      "Added ['spent'] to the tokenized_text array.\n",
      "spent         2,985\n",
      "Added ['spirited'] to the tokenized_text array.\n",
      "spirited     24,462\n",
      "Added ['spirit', '##less'] to the tokenized_text array.\n",
      "spirit        4,382\n",
      "##less        3,238\n",
      "Added ['spite'] to the tokenized_text array.\n",
      "spite         8,741\n",
      "Added ['spite', '##ful'] to the tokenized_text array.\n",
      "spite         8,741\n",
      "##ful         3,993\n",
      "Added ['spoiled'] to the tokenized_text array.\n",
      "spoiled      19,582\n",
      "Added ['sp', '##ook', '##ed'] to the tokenized_text array.\n",
      "sp           11,867\n",
      "##ook        14,659\n",
      "##ed          2,098\n",
      "Added ['sq', '##ue', '##ami', '##sh'] to the tokenized_text array.\n",
      "sq            5,490\n",
      "##ue          5,657\n",
      "##ami        10,631\n",
      "##sh          4,095\n",
      "Added ['staggered'] to the tokenized_text array.\n",
      "staggered    14,648\n",
      "Added ['stalker'] to the tokenized_text array.\n",
      "stalker      23,883\n",
      "Added ['stare'] to the tokenized_text array.\n",
      "stare         6,237\n",
      "Added ['staring'] to the tokenized_text array.\n",
      "staring       4,582\n",
      "Added ['stars', '##tr', '##uck'] to the tokenized_text array.\n",
      "stars         3,340\n",
      "##tr         16,344\n",
      "##uck        12,722\n",
      "Added ['started'] to the tokenized_text array.\n",
      "started       2,318\n",
      "Added ['startled'] to the tokenized_text array.\n",
      "startled      9,696\n",
      "Added ['state', '##ly'] to the tokenized_text array.\n",
      "state         2,110\n",
      "##ly          2,135\n",
      "Added ['ste', '##ad', '##fast'] to the tokenized_text array.\n",
      "ste          26,261\n",
      "##ad          4,215\n",
      "##fast       24,333\n",
      "Added ['steady'] to the tokenized_text array.\n",
      "steady        6,706\n",
      "Added ['stealth', '##y'] to the tokenized_text array.\n",
      "stealth      22,150\n",
      "##y           2,100\n",
      "Added ['steamed'] to the tokenized_text array.\n",
      "steamed      21,734\n",
      "Added ['steaming'] to the tokenized_text array.\n",
      "steaming     19,986\n",
      "Added ['steel', '##ing'] to the tokenized_text array.\n",
      "steel         3,886\n",
      "##ing         2,075\n",
      "Added ['steel', '##y'] to the tokenized_text array.\n",
      "steel         3,886\n",
      "##y           2,100\n",
      "Added ['stern'] to the tokenized_text array.\n",
      "stern         8,665\n",
      "Added ['stiff'] to the tokenized_text array.\n",
      "stiff        10,551\n",
      "Added ['stifled'] to the tokenized_text array.\n",
      "stifled      27,146\n",
      "Added ['st', '##if', '##ling'] to the tokenized_text array.\n",
      "st            2,358\n",
      "##if         10,128\n",
      "##ling        2,989\n",
      "Added ['still'] to the tokenized_text array.\n",
      "still         2,145\n",
      "Added ['stillness'] to the tokenized_text array.\n",
      "stillness    29,435\n",
      "Added ['stimulated'] to the tokenized_text array.\n",
      "stimulated   25,194\n",
      "Added ['stink', '##y'] to the tokenized_text array.\n",
      "stink        27,136\n",
      "##y           2,100\n",
      "Added ['stirred'] to the tokenized_text array.\n",
      "stirred      13,551\n",
      "Added ['st', '##oic'] to the tokenized_text array.\n",
      "st            2,358\n",
      "##oic        19,419\n",
      "Added ['st', '##oic', '##al'] to the tokenized_text array.\n",
      "st            2,358\n",
      "##oic        19,419\n",
      "##al          2,389\n",
      "Added ['st', '##oli', '##d'] to the tokenized_text array.\n",
      "st            2,358\n",
      "##oli        10,893\n",
      "##d           2,094\n",
      "Added ['stone', '##d'] to the tokenized_text array.\n",
      "stone         2,962\n",
      "##d           2,094\n",
      "Added ['storm', '##ing'] to the tokenized_text array.\n",
      "storm         4,040\n",
      "##ing         2,075\n",
      "Added ['stormy'] to the tokenized_text array.\n",
      "stormy       24,166\n",
      "Added ['stout'] to the tokenized_text array.\n",
      "stout        18,890\n",
      "Added ['straight'] to the tokenized_text array.\n",
      "straight      3,442\n",
      "Added ['strained'] to the tokenized_text array.\n",
      "strained     12,250\n",
      "Added ['strange'] to the tokenized_text array.\n",
      "strange       4,326\n",
      "Added ['stressed'] to the tokenized_text array.\n",
      "stressed     13,233\n",
      "Added ['stricken'] to the tokenized_text array.\n",
      "stricken     16,654\n",
      "Added ['strict'] to the tokenized_text array.\n",
      "strict        9,384\n",
      "Added ['strong'] to the tokenized_text array.\n",
      "strong        2,844\n",
      "Added ['struck'] to the tokenized_text array.\n",
      "struck        4,930\n",
      "Added ['stubborn'] to the tokenized_text array.\n",
      "stubborn     14,205\n",
      "Added ['stubborn', '##ness'] to the tokenized_text array.\n",
      "stubborn     14,205\n",
      "##ness        2,791\n",
      "Added ['studio', '##us'] to the tokenized_text array.\n",
      "studio        2,996\n",
      "##us          2,271\n",
      "Added ['studying'] to the tokenized_text array.\n",
      "studying      5,702\n",
      "Added ['stump', '##ed'] to the tokenized_text array.\n",
      "stump        22,475\n",
      "##ed          2,098\n",
      "Added ['stung'] to the tokenized_text array.\n",
      "stung        19,280\n",
      "Added ['stunned'] to the tokenized_text array.\n",
      "stunned       9,860\n",
      "Added ['stu', '##pe', '##fa', '##ction'] to the tokenized_text array.\n",
      "stu          24,646\n",
      "##pe          5,051\n",
      "##fa          7,011\n",
      "##ction       7,542\n",
      "Added ['stu', '##pe', '##fied'] to the tokenized_text array.\n",
      "stu          24,646\n",
      "##pe          5,051\n",
      "##fied       10,451\n",
      "Added ['stu', '##pe', '##fy'] to the tokenized_text array.\n",
      "stu          24,646\n",
      "##pe          5,051\n",
      "##fy         12,031\n",
      "Added ['stupid'] to the tokenized_text array.\n",
      "stupid        5,236\n",
      "Added ['stu', '##por', '##ous'] to the tokenized_text array.\n",
      "stu          24,646\n",
      "##por        17,822\n",
      "##ous         3,560\n",
      "Added ['su', '##ave'] to the tokenized_text array.\n",
      "su           10,514\n",
      "##ave        10,696\n",
      "Added ['subdued'] to the tokenized_text array.\n",
      "subdued      20,442\n",
      "Added ['sublime'] to the tokenized_text array.\n",
      "sublime      28,341\n",
      "Added ['sub', '##missive'] to the tokenized_text array.\n",
      "sub           4,942\n",
      "##missive    27,876\n",
      "Added ['suffering'] to the tokenized_text array.\n",
      "suffering     6,114\n",
      "Added ['suggest', '##ive'] to the tokenized_text array.\n",
      "suggest       6,592\n",
      "##ive         3,512\n",
      "Added ['sul', '##king'] to the tokenized_text array.\n",
      "sul          21,396\n",
      "##king        6,834\n",
      "Added ['sul', '##ky'] to the tokenized_text array.\n",
      "sul          21,396\n",
      "##ky          4,801\n",
      "Added ['sul', '##len'] to the tokenized_text array.\n",
      "sul          21,396\n",
      "##len         7,770\n",
      "Added ['sul', '##len', '##ness'] to the tokenized_text array.\n",
      "sul          21,396\n",
      "##len         7,770\n",
      "##ness        2,791\n",
      "Added ['sunny'] to the tokenized_text array.\n",
      "sunny        11,559\n",
      "Added ['superior'] to the tokenized_text array.\n",
      "superior      6,020\n",
      "Added ['superiority'] to the tokenized_text array.\n",
      "superiority  19,113\n",
      "Added ['suppressed'] to the tokenized_text array.\n",
      "suppressed   13,712\n",
      "Added ['suppress', '##ing'] to the tokenized_text array.\n",
      "suppress     16,081\n",
      "##ing         2,075\n",
      "Added ['suppression'] to the tokenized_text array.\n",
      "suppression  16,341\n",
      "Added ['sure'] to the tokenized_text array.\n",
      "sure          2,469\n",
      "Added ['sur', '##ly'] to the tokenized_text array.\n",
      "sur           7,505\n",
      "##ly          2,135\n",
      "Added ['surprise'] to the tokenized_text array.\n",
      "surprise      4,474\n",
      "Added ['surprised'] to the tokenized_text array.\n",
      "surprised     4,527\n",
      "Added ['surprising'] to the tokenized_text array.\n",
      "surprising   11,341\n",
      "Added ['surprisingly'] to the tokenized_text array.\n",
      "surprisingly 10,889\n",
      "Added ['sur', '##re', '##pt', '##iti', '##ous'] to the tokenized_text array.\n",
      "sur           7,505\n",
      "##re          2,890\n",
      "##pt         13,876\n",
      "##iti        25,090\n",
      "##ous         3,560\n",
      "Added ['suspect'] to the tokenized_text array.\n",
      "suspect       8,343\n",
      "Added ['suspect', '##ing'] to the tokenized_text array.\n",
      "suspect       8,343\n",
      "##ing         2,075\n",
      "Added ['suspense'] to the tokenized_text array.\n",
      "suspense     23,873\n",
      "Added ['suspicion'] to the tokenized_text array.\n",
      "suspicion    10,928\n",
      "Added ['suspicious'] to the tokenized_text array.\n",
      "suspicious   10,027\n",
      "Added ['suspiciously'] to the tokenized_text array.\n",
      "suspiciously 21,501\n",
      "Added ['suspicious', '##ness'] to the tokenized_text array.\n",
      "suspicious   10,027\n",
      "##ness        2,791\n",
      "Added ['sw', '##agger', '##ing'] to the tokenized_text array.\n",
      "sw           25,430\n",
      "##agger      27,609\n",
      "##ing         2,075\n",
      "Added ['swearing'] to the tokenized_text array.\n",
      "swearing     25,082\n",
      "Added ['sympathetic'] to the tokenized_text array.\n",
      "sympathetic  13,026\n",
      "Added ['sy', '##mp', '##athi', '##zing'] to the tokenized_text array.\n",
      "sy           25,353\n",
      "##mp          8,737\n",
      "##athi       25,457\n",
      "##zing        6,774\n",
      "Added ['sympathy'] to the tokenized_text array.\n",
      "sympathy     11,883\n",
      "Added ['ta', '##cit', '##urn'] to the tokenized_text array.\n",
      "ta           11,937\n",
      "##cit        26,243\n",
      "##urn        14,287\n",
      "Added ['talk', '##ative'] to the tokenized_text array.\n",
      "talk          2,831\n",
      "##ative       8,082\n",
      "Added ['talking'] to the tokenized_text array.\n",
      "talking       3,331\n",
      "Added ['tan', '##tal', '##ized'] to the tokenized_text array.\n",
      "tan           9,092\n",
      "##tal         9,080\n",
      "##ized        3,550\n",
      "Added ['tar', '##t'] to the tokenized_text array.\n",
      "tar          16,985\n",
      "##t           2,102\n",
      "Added ['taste', '##ful'] to the tokenized_text array.\n",
      "taste         5,510\n",
      "##ful         3,993\n",
      "Added ['ta', '##tt', '##ling'] to the tokenized_text array.\n",
      "ta           11,937\n",
      "##tt          4,779\n",
      "##ling        2,989\n",
      "Added ['tau', '##nt'] to the tokenized_text array.\n",
      "tau          19,982\n",
      "##nt          3,372\n",
      "Added ['taunting'] to the tokenized_text array.\n",
      "taunting     29,442\n",
      "Added ['taut'] to the tokenized_text array.\n",
      "taut         21,642\n",
      "Added ['tear', '##ful'] to the tokenized_text array.\n",
      "tear          7,697\n",
      "##ful         3,993\n",
      "Added ['tear', '##y'] to the tokenized_text array.\n",
      "tear          7,697\n",
      "##y           2,100\n",
      "Added ['tease'] to the tokenized_text array.\n",
      "tease        18,381\n",
      "Added ['teasing'] to the tokenized_text array.\n",
      "teasing      12,216\n",
      "Added ['tempered'] to the tokenized_text array.\n",
      "tempered     22,148\n",
      "Added ['tempest'] to the tokenized_text array.\n",
      "tempest      22,553\n",
      "Added ['tempest', '##uous'] to the tokenized_text array.\n",
      "tempest      22,553\n",
      "##uous        8,918\n",
      "Added ['tempted'] to the tokenized_text array.\n",
      "tempted      16,312\n",
      "Added ['ten', '##acious'] to the tokenized_text array.\n",
      "ten           2,702\n",
      "##acious     20,113\n",
      "Added ['tender'] to the tokenized_text array.\n",
      "tender        8,616\n",
      "Added ['tenderness'] to the tokenized_text array.\n",
      "tenderness   24,605\n",
      "Added ['tense'] to the tokenized_text array.\n",
      "tense         9,049\n",
      "Added ['tensed'] to the tokenized_text array.\n",
      "tensed       15,225\n",
      "Added ['tension'] to the tokenized_text array.\n",
      "tension       6,980\n",
      "Added ['tentative'] to the tokenized_text array.\n",
      "tentative    19,943\n",
      "Added ['terrified'] to the tokenized_text array.\n",
      "terrified    10,215\n",
      "Added ['terror'] to the tokenized_text array.\n",
      "terror        7,404\n",
      "Added ['terror', '##ized'] to the tokenized_text array.\n",
      "terror        7,404\n",
      "##ized        3,550\n",
      "Added ['terror', '##izing'] to the tokenized_text array.\n",
      "terror        7,404\n",
      "##izing       6,026\n",
      "Added ['ter', '##se'] to the tokenized_text array.\n",
      "ter          28,774\n",
      "##se          3,366\n",
      "Added ['test', '##y'] to the tokenized_text array.\n",
      "test          3,231\n",
      "##y           2,100\n",
      "Added ['te', '##tch', '##y'] to the tokenized_text array.\n",
      "te            8,915\n",
      "##tch        10,649\n",
      "##y           2,100\n",
      "Added ['thankful'] to the tokenized_text array.\n",
      "thankful     18,836\n",
      "Added ['thinking'] to the tokenized_text array.\n",
      "thinking      3,241\n",
      "Added ['thought'] to the tokenized_text array.\n",
      "thought       2,245\n",
      "Added ['thoughtful'] to the tokenized_text array.\n",
      "thoughtful   16,465\n",
      "Added ['thoughtful', '##ness'] to the tokenized_text array.\n",
      "thoughtful   16,465\n",
      "##ness        2,791\n",
      "Added ['threat'] to the tokenized_text array.\n",
      "threat        5,081\n",
      "Added ['threatened'] to the tokenized_text array.\n",
      "threatened    5,561\n",
      "Added ['threatening'] to the tokenized_text array.\n",
      "threatening   8,701\n",
      "Added ['thrilled'] to the tokenized_text array.\n",
      "thrilled     16,082\n",
      "Added ['thrown'] to the tokenized_text array.\n",
      "thrown        6,908\n",
      "Added ['thunder', '##st', '##ruck'] to the tokenized_text array.\n",
      "thunder       8,505\n",
      "##st          3,367\n",
      "##ruck       29,314\n",
      "Added ['thwarted'] to the tokenized_text array.\n",
      "thwarted     28,409\n",
      "Added ['ticked'] to the tokenized_text array.\n",
      "ticked       26,019\n",
      "Added ['tick', '##led'] to the tokenized_text array.\n",
      "tick         16,356\n",
      "##led         3,709\n",
      "Added ['tied'] to the tokenized_text array.\n",
      "tied          5,079\n",
      "Added ['tier', '##ed'] to the tokenized_text array.\n",
      "tier          7,563\n",
      "##ed          2,098\n",
      "Added ['tight'] to the tokenized_text array.\n",
      "tight         4,389\n",
      "Added ['tight', '##lip', '##ped'] to the tokenized_text array.\n",
      "tight         4,389\n",
      "##lip        15,000\n",
      "##ped         5,669\n",
      "Added ['tim', '##id'] to the tokenized_text array.\n",
      "tim           5,199\n",
      "##id          3,593\n",
      "Added ['tim', '##id', '##ly'] to the tokenized_text array.\n",
      "tim           5,199\n",
      "##id          3,593\n",
      "##ly          2,135\n",
      "Added ['tim', '##id', '##ness'] to the tokenized_text array.\n",
      "tim           5,199\n",
      "##id          3,593\n",
      "##ness        2,791\n",
      "Added ['tired'] to the tokenized_text array.\n",
      "tired         5,458\n",
      "Added ['tired', '##ly'] to the tokenized_text array.\n",
      "tired         5,458\n",
      "##ly          2,135\n",
      "Added ['tired', '##ness'] to the tokenized_text array.\n",
      "tired         5,458\n",
      "##ness        2,791\n",
      "Added ['ti', '##till', '##ated'] to the tokenized_text array.\n",
      "ti           14,841\n",
      "##till       28,345\n",
      "##ated        4,383\n",
      "Added ['tolerant'] to the tokenized_text array.\n",
      "tolerant     23,691\n",
      "Added ['tongue'] to the tokenized_text array.\n",
      "tongue        4,416\n",
      "Added ['tormented'] to the tokenized_text array.\n",
      "tormented    29,026\n",
      "Added ['touched'] to the tokenized_text array.\n",
      "touched       5,028\n",
      "Added ['tough'] to the tokenized_text array.\n",
      "tough         7,823\n",
      "Added ['toy', '##ing'] to the tokenized_text array.\n",
      "toy           9,121\n",
      "##ing         2,075\n",
      "Added ['tragic'] to the tokenized_text array.\n",
      "tragic       13,800\n",
      "Added ['tragic', '##al'] to the tokenized_text array.\n",
      "tragic       13,800\n",
      "##al          2,389\n",
      "Added ['tran', '##quil'] to the tokenized_text array.\n",
      "tran         25,283\n",
      "##quil       26,147\n",
      "Added ['tran', '##quil', '##ity'] to the tokenized_text array.\n",
      "tran         25,283\n",
      "##quil       26,147\n",
      "##ity         3,012\n",
      "Added ['trans', '##fixed'] to the tokenized_text array.\n",
      "trans         9,099\n",
      "##fixed      23,901\n",
      "Added ['trauma', '##tized'] to the tokenized_text array.\n",
      "trauma       12,603\n",
      "##tized      23,355\n",
      "Added ['trembling'] to the tokenized_text array.\n",
      "trembling    10,226\n",
      "Added ['tre', '##pid'] to the tokenized_text array.\n",
      "tre          29,461\n",
      "##pid        23,267\n",
      "Added ['tre', '##pid', '##ation'] to the tokenized_text array.\n",
      "tre          29,461\n",
      "##pid        23,267\n",
      "##ation       3,370\n",
      "Added ['tricks', '##ter'] to the tokenized_text array.\n",
      "tricks       12,225\n",
      "##ter         3,334\n",
      "Added ['tricky'] to the tokenized_text array.\n",
      "tricky       24,026\n",
      "Added ['triumphant'] to the tokenized_text array.\n",
      "triumphant   25,251\n",
      "Added ['troubled'] to the tokenized_text array.\n",
      "troubled     11,587\n",
      "Added ['troubles', '##ome'] to the tokenized_text array.\n",
      "troubles     13,460\n",
      "##ome         8,462\n",
      "Added ['tr', '##ou', '##bling'] to the tokenized_text array.\n",
      "tr           19,817\n",
      "##ou          7,140\n",
      "##bling       9,709\n",
      "Added ['trusting'] to the tokenized_text array.\n",
      "trusting     19,836\n",
      "Added ['trust', '##worthy'] to the tokenized_text array.\n",
      "trust         3,404\n",
      "##worthy     13,966\n",
      "Added ['tu', '##mu', '##lt', '##uous'] to the tokenized_text array.\n",
      "tu           10,722\n",
      "##mu         12,274\n",
      "##lt          7,096\n",
      "##uous        8,918\n",
      "Added ['turbulent'] to the tokenized_text array.\n",
      "turbulent    22,609\n",
      "Added ['twin', '##k', '##ly'] to the tokenized_text array.\n",
      "twin          5,519\n",
      "##k           2,243\n",
      "##ly          2,135\n",
      "Added ['um', '##bra', '##ge'] to the tokenized_text array.\n",
      "um            8,529\n",
      "##bra        10,024\n",
      "##ge          3,351\n",
      "Added ['um', '##bra', '##ge', '##ous'] to the tokenized_text array.\n",
      "um            8,529\n",
      "##bra        10,024\n",
      "##ge          3,351\n",
      "##ous         3,560\n",
      "Added ['unaffected'] to the tokenized_text array.\n",
      "unaffected   24,720\n",
      "Added ['una', '##git', '##ated'] to the tokenized_text array.\n",
      "una          14,477\n",
      "##git        23,806\n",
      "##ated        4,383\n",
      "Added ['una', '##mus', '##ed'] to the tokenized_text array.\n",
      "una          14,477\n",
      "##mus         7,606\n",
      "##ed          2,098\n",
      "Added ['una', '##pp', '##re', '##cia', '##tive'] to the tokenized_text array.\n",
      "una          14,477\n",
      "##pp          9,397\n",
      "##re          2,890\n",
      "##cia         7,405\n",
      "##tive        6,024\n",
      "Added ['una', '##pp', '##ro', '##ach', '##able'] to the tokenized_text array.\n",
      "una          14,477\n",
      "##pp          9,397\n",
      "##ro          3,217\n",
      "##ach         6,776\n",
      "##able        3,085\n",
      "Added ['una', '##sser', '##tive'] to the tokenized_text array.\n",
      "una          14,477\n",
      "##sser       18,116\n",
      "##tive        6,024\n",
      "Added ['una', '##ss', '##uming'] to the tokenized_text array.\n",
      "una          14,477\n",
      "##ss          4,757\n",
      "##uming      24,270\n",
      "Added ['unaware'] to the tokenized_text array.\n",
      "unaware      11,499\n",
      "Added ['un', '##bel', '##ie', '##f'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##bel         8,671\n",
      "##ie          2,666\n",
      "##f           2,546\n",
      "Added ['unbelievable'] to the tokenized_text array.\n",
      "unbelievable 23,653\n",
      "Added ['un', '##bel', '##ieving'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##bel         8,671\n",
      "##ieving     25,587\n",
      "Added ['un', '##bot', '##hered'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##bot        18,384\n",
      "##hered      27,190\n",
      "Added ['un', '##car', '##ing'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##car        10,010\n",
      "##ing         2,075\n",
      "Added ['uncertain'] to the tokenized_text array.\n",
      "uncertain     9,662\n",
      "Added ['uncertain', '##ly'] to the tokenized_text array.\n",
      "uncertain     9,662\n",
      "##ly          2,135\n",
      "Added ['uncertainty'] to the tokenized_text array.\n",
      "uncertainty  12,503\n",
      "Added ['un', '##ci', '##vil'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##ci          6,895\n",
      "##vil        14,762\n",
      "Added ['uncomfortable'] to the tokenized_text array.\n",
      "uncomfortable  8,796\n",
      "Added ['un', '##com', '##mit', '##ted'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##com         9,006\n",
      "##mit        22,930\n",
      "##ted         3,064\n",
      "Added ['un', '##com', '##mun', '##icative'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##com         9,006\n",
      "##mun        23,041\n",
      "##icative    25,184\n",
      "Added ['un', '##com', '##pre', '##hend', '##ing'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##com         9,006\n",
      "##pre        28,139\n",
      "##hend       22,342\n",
      "##ing         2,075\n",
      "Added ['un', '##com', '##promising'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##com         9,006\n",
      "##promising  25,013\n",
      "Added ['un', '##con', '##cer', '##ned'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##con         8,663\n",
      "##cer        17,119\n",
      "##ned         7,228\n",
      "Added ['un', '##con', '##fide', '##nt'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##con         8,663\n",
      "##fide       20,740\n",
      "##nt          3,372\n",
      "Added ['un', '##con', '##vin', '##ced'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##con         8,663\n",
      "##vin         6,371\n",
      "##ced        11,788\n",
      "Added ['un', '##co', '##oper', '##ative'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##co          3,597\n",
      "##oper       25,918\n",
      "##ative       8,082\n",
      "Added ['un', '##cu', '##rio', '##us'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##cu         10,841\n",
      "##rio         9,488\n",
      "##us          2,271\n",
      "Added ['und', '##ec', '##ided'] to the tokenized_text array.\n",
      "und           6,151\n",
      "##ec          8,586\n",
      "##ided       14,097\n",
      "Added ['under', '##hand', '##ed'] to the tokenized_text array.\n",
      "under         2,104\n",
      "##hand       11,774\n",
      "##ed          2,098\n",
      "Added ['understanding'] to the tokenized_text array.\n",
      "understanding  4,824\n",
      "Added ['und', '##es', '##ira', '##ble'] to the tokenized_text array.\n",
      "und           6,151\n",
      "##es          2,229\n",
      "##ira         7,895\n",
      "##ble         3,468\n",
      "Added ['unease'] to the tokenized_text array.\n",
      "unease       27,880\n",
      "Added ['une', '##asi', '##ly'] to the tokenized_text array.\n",
      "une          16,655\n",
      "##asi        21,369\n",
      "##ly          2,135\n",
      "Added ['une', '##asi', '##ness'] to the tokenized_text array.\n",
      "une          16,655\n",
      "##asi        21,369\n",
      "##ness        2,791\n",
      "Added ['uneasy'] to the tokenized_text array.\n",
      "uneasy       15,491\n",
      "Added ['une', '##mot', '##ional'] to the tokenized_text array.\n",
      "une          16,655\n",
      "##mot        18,938\n",
      "##ional      19,301\n",
      "Added ['une', '##nt', '##hus', '##ias', '##tic'] to the tokenized_text array.\n",
      "une          16,655\n",
      "##nt          3,372\n",
      "##hus         9,825\n",
      "##ias         7,951\n",
      "##tic         4,588\n",
      "Added ['une', '##x', '##cite', '##d'] to the tokenized_text array.\n",
      "une          16,655\n",
      "##x           2,595\n",
      "##cite       17,847\n",
      "##d           2,094\n",
      "Added ['unexpected'] to the tokenized_text array.\n",
      "unexpected    9,223\n",
      "Added ['unfamiliar'] to the tokenized_text array.\n",
      "unfamiliar   16,261\n",
      "Added ['un', '##fat', '##hom', '##able'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##fat        27,753\n",
      "##hom        23,393\n",
      "##able        3,085\n",
      "Added ['un', '##fa', '##zed'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##fa          7,011\n",
      "##zed         5,422\n",
      "Added ['un', '##fe', '##elin', '##g'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##fe          7,959\n",
      "##elin       18,809\n",
      "##g           2,290\n",
      "Added ['un', '##fo', '##cus', '##ed'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##fo         14,876\n",
      "##cus         7,874\n",
      "##ed          2,098\n",
      "Added ['un', '##for', '##ese', '##en'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##for        29,278\n",
      "##ese         6,810\n",
      "##en          2,368\n",
      "Added ['un', '##for', '##giving'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##for        29,278\n",
      "##giving     23,795\n",
      "Added ['un', '##forth', '##coming'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##forth      15,628\n",
      "##coming     18,935\n",
      "Added ['unfortunate'] to the tokenized_text array.\n",
      "unfortunate  15,140\n",
      "Added ['un', '##fr', '##ien', '##dly'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##fr         19,699\n",
      "##ien         9,013\n",
      "##dly        18,718\n",
      "Added ['unhappy'] to the tokenized_text array.\n",
      "unhappy      12,511\n",
      "Added ['un', '##hing', '##ed'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##hing       12,053\n",
      "##ed          2,098\n",
      "Added ['un', '##im', '##pressed'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##im          5,714\n",
      "##pressed    19,811\n",
      "Added ['un', '##in', '##formed'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##in          2,378\n",
      "##formed     29,021\n",
      "Added ['un', '##ins', '##pired'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##ins         7,076\n",
      "##pired      21,649\n",
      "Added ['un', '##int', '##eres', '##ted'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##int        18,447\n",
      "##eres       18,702\n",
      "##ted         3,064\n",
      "Added ['un', '##in', '##vo', '##lved'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##in          2,378\n",
      "##vo          6,767\n",
      "##lved       26,832\n",
      "Added ['unique'] to the tokenized_text array.\n",
      "unique        4,310\n",
      "Added ['unlike', '##able'] to the tokenized_text array.\n",
      "unlike        4,406\n",
      "##able        3,085\n",
      "Added ['un', '##mo', '##ved'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##mo          5,302\n",
      "##ved         7,178\n",
      "Added ['un', '##ner', '##ved'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##ner         3,678\n",
      "##ved         7,178\n",
      "Added ['unpleasant'] to the tokenized_text array.\n",
      "unpleasant   16,010\n",
      "Added ['un', '##pre', '##par', '##ed'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##pre        28,139\n",
      "##par        19,362\n",
      "##ed          2,098\n",
      "Added ['un', '##qui', '##et'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##qui        15,549\n",
      "##et          3,388\n",
      "Added ['un', '##rea', '##ctive'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##rea        16,416\n",
      "##ctive      15,277\n",
      "Added ['un', '##res', '##olved'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##res         6,072\n",
      "##olved      16,116\n",
      "Added ['unrest', '##rained'] to the tokenized_text array.\n",
      "unrest       16,591\n",
      "##rained     27,361\n",
      "Added ['un', '##ruff', '##led'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##ruff       26,919\n",
      "##led         3,709\n",
      "Added ['un', '##sat', '##is', '##fied'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##sat        16,846\n",
      "##is          2,483\n",
      "##fied       10,451\n",
      "Added ['un', '##sett', '##led'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##sett       21,678\n",
      "##led         3,709\n",
      "Added ['un', '##so', '##cia', '##ble'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##so          6,499\n",
      "##cia         7,405\n",
      "##ble         3,468\n",
      "Added ['un', '##sp', '##eak', '##ing'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##sp         13,102\n",
      "##eak        25,508\n",
      "##ing         2,075\n",
      "Added ['unspoken'] to the tokenized_text array.\n",
      "unspoken     25,982\n",
      "Added ['un', '##st', '##run', '##g'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##st          3,367\n",
      "##run        15,532\n",
      "##g           2,290\n",
      "Added ['unsuccessful'] to the tokenized_text array.\n",
      "unsuccessful  7,736\n",
      "Added ['unsure'] to the tokenized_text array.\n",
      "unsure       12,422\n",
      "Added ['un', '##sur', '##pr', '##ised'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##sur        26,210\n",
      "##pr         18,098\n",
      "##ised        5,084\n",
      "Added ['un', '##sus', '##pe', '##cting'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##sus        13,203\n",
      "##pe          5,051\n",
      "##cting      11,873\n",
      "Added ['un', '##sw', '##ay', '##ed'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##sw         26,760\n",
      "##ay          4,710\n",
      "##ed          2,098\n",
      "Added ['un', '##sy', '##mp', '##ath', '##etic'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##sy          6,508\n",
      "##mp          8,737\n",
      "##ath         8,988\n",
      "##etic       16,530\n",
      "Added ['untouched'] to the tokenized_text array.\n",
      "untouched    22,154\n",
      "Added ['un', '##tro', '##ub', '##led'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##tro        13,181\n",
      "##ub         12,083\n",
      "##led         3,709\n",
      "Added ['un', '##trust', '##ing'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##trust      24,669\n",
      "##ing         2,075\n",
      "Added ['unwanted'] to the tokenized_text array.\n",
      "unwanted     18,162\n",
      "Added ['un', '##wave', '##ring'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##wave       16,535\n",
      "##ring        4,892\n",
      "Added ['un', '##we', '##lco', '##ming'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##we          8,545\n",
      "##lco        22,499\n",
      "##ming        6,562\n",
      "Added ['un', '##well'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##well        4,381\n",
      "Added ['unwilling'] to the tokenized_text array.\n",
      "unwilling    15,175\n",
      "Added ['un', '##yi', '##eld', '##ing'] to the tokenized_text array.\n",
      "un            4,895\n",
      "##yi         10,139\n",
      "##eld        14,273\n",
      "##ing         2,075\n",
      "Added ['up'] to the tokenized_text array.\n",
      "up            2,039\n",
      "Added ['upbeat'] to the tokenized_text array.\n",
      "upbeat       27,999\n",
      "Added ['up', '##lifting'] to the tokenized_text array.\n",
      "up            2,039\n",
      "##lifting    26,644\n",
      "Added ['up', '##pit', '##y'] to the tokenized_text array.\n",
      "up            2,039\n",
      "##pit        23,270\n",
      "##y           2,100\n",
      "Added ['upset'] to the tokenized_text array.\n",
      "upset         6,314\n",
      "Added ['up', '##tight'] to the tokenized_text array.\n",
      "up            2,039\n",
      "##tight      26,143\n",
      "Added ['useless'] to the tokenized_text array.\n",
      "useless      11,809\n",
      "Added ['vacant'] to the tokenized_text array.\n",
      "vacant       10,030\n",
      "Added ['va', '##cu', '##ous'] to the tokenized_text array.\n",
      "va           12,436\n",
      "##cu         10,841\n",
      "##ous         3,560\n",
      "Added ['van', '##qui', '##shed'] to the tokenized_text array.\n",
      "van           3,158\n",
      "##qui        15,549\n",
      "##shed       14,740\n",
      "Added ['ve', '##hem', '##ent'] to the tokenized_text array.\n",
      "ve            2,310\n",
      "##hem        29,122\n",
      "##ent         4,765\n",
      "Added ['ve', '##nge', '##ful'] to the tokenized_text array.\n",
      "ve            2,310\n",
      "##nge        15,465\n",
      "##ful         3,993\n",
      "Added ['venom', '##ous'] to the tokenized_text array.\n",
      "venom        15,779\n",
      "##ous         3,560\n",
      "Added ['ve', '##x'] to the tokenized_text array.\n",
      "ve            2,310\n",
      "##x           2,595\n",
      "Added ['ve', '##xa', '##tion'] to the tokenized_text array.\n",
      "ve            2,310\n",
      "##xa         18,684\n",
      "##tion        3,508\n",
      "Added ['ve', '##xed'] to the tokenized_text array.\n",
      "ve            2,310\n",
      "##xed        19,068\n",
      "Added ['vicious'] to the tokenized_text array.\n",
      "vicious      13,925\n",
      "Added ['victorious'] to the tokenized_text array.\n",
      "victorious   13,846\n",
      "Added ['vi', '##gil', '##ant'] to the tokenized_text array.\n",
      "vi            6,819\n",
      "##gil        20,142\n",
      "##ant         4,630\n",
      "Added ['vile'] to the tokenized_text array.\n",
      "vile         25,047\n",
      "Added ['villain', '##ous'] to the tokenized_text array.\n",
      "villain      12,700\n",
      "##ous         3,560\n",
      "Added ['vin', '##dict', '##ive'] to the tokenized_text array.\n",
      "vin          19,354\n",
      "##dict       29,201\n",
      "##ive         3,512\n",
      "Added ['violence'] to the tokenized_text array.\n",
      "violence      4,808\n",
      "Added ['violent'] to the tokenized_text array.\n",
      "violent       6,355\n",
      "Added ['viper', '##ous'] to the tokenized_text array.\n",
      "viper        17,947\n",
      "##ous         3,560\n",
      "Added ['vi', '##tu', '##per', '##ative'] to the tokenized_text array.\n",
      "vi            6,819\n",
      "##tu          8,525\n",
      "##per         4,842\n",
      "##ative       8,082\n",
      "Added ['vocal'] to the tokenized_text array.\n",
      "vocal         5,554\n",
      "Added ['vocal', '##ized'] to the tokenized_text array.\n",
      "vocal         5,554\n",
      "##ized        3,550\n",
      "Added ['vulgar'] to the tokenized_text array.\n",
      "vulgar       29,364\n",
      "Added ['vulnerability'] to the tokenized_text array.\n",
      "vulnerability 18,130\n",
      "Added ['vulnerable'] to the tokenized_text array.\n",
      "vulnerable    8,211\n",
      "Added ['wa', '##cky'] to the tokenized_text array.\n",
      "wa           11,333\n",
      "##cky        17,413\n",
      "Added ['waiting'] to the tokenized_text array.\n",
      "waiting       3,403\n",
      "Added ['wanted'] to the tokenized_text array.\n",
      "wanted        2,359\n",
      "Added ['wanting'] to the tokenized_text array.\n",
      "wanting       5,782\n",
      "Added ['want', '##on'] to the tokenized_text array.\n",
      "want          2,215\n",
      "##on          2,239\n",
      "Added ['war', '##iness'] to the tokenized_text array.\n",
      "war           2,162\n",
      "##iness       9,961\n",
      "Added ['warm'] to the tokenized_text array.\n",
      "warm          4,010\n",
      "Added ['wary'] to the tokenized_text array.\n",
      "wary         15,705\n",
      "Added ['wasted'] to the tokenized_text array.\n",
      "wasted       13,842\n",
      "Added ['watch'] to the tokenized_text array.\n",
      "watch         3,422\n",
      "Added ['watch', '##ful'] to the tokenized_text array.\n",
      "watch         3,422\n",
      "##ful         3,993\n",
      "Added ['watching'] to the tokenized_text array.\n",
      "watching      3,666\n",
      "Added ['wave', '##ring'] to the tokenized_text array.\n",
      "wave          4,400\n",
      "##ring        4,892\n",
      "Added ['wear', '##iness'] to the tokenized_text array.\n",
      "wear          4,929\n",
      "##iness       9,961\n",
      "Added ['weary'] to the tokenized_text array.\n",
      "weary        16,040\n",
      "Added ['weeping'] to the tokenized_text array.\n",
      "weeping      19,750\n",
      "Added ['weird'] to the tokenized_text array.\n",
      "weird         6,881\n",
      "Added ['welcome'] to the tokenized_text array.\n",
      "welcome       6,160\n",
      "Added ['welcoming'] to the tokenized_text array.\n",
      "welcoming    18,066\n",
      "Added ['whatever'] to the tokenized_text array.\n",
      "whatever      3,649\n",
      "Added ['whimper', '##ing'] to the tokenized_text array.\n",
      "whimper      28,544\n",
      "##ing         2,075\n",
      "Added ['w', '##him', '##sic', '##al'] to the tokenized_text array.\n",
      "w             1,059\n",
      "##him        14,341\n",
      "##sic        19,570\n",
      "##al          2,389\n",
      "Added ['whisper'] to the tokenized_text array.\n",
      "whisper       7,204\n",
      "Added ['whistle'] to the tokenized_text array.\n",
      "whistle      13,300\n",
      "Added ['white'] to the tokenized_text array.\n",
      "white         2,317\n",
      "Added ['wicked'] to the tokenized_text array.\n",
      "wicked       10,433\n",
      "Added ['wild'] to the tokenized_text array.\n",
      "wild          3,748\n",
      "Added ['will', '##ful'] to the tokenized_text array.\n",
      "will          2,097\n",
      "##ful         3,993\n",
      "Added ['willing'] to the tokenized_text array.\n",
      "willing       5,627\n",
      "Added ['wil', '##y'] to the tokenized_text array.\n",
      "wil          19,863\n",
      "##y           2,100\n",
      "Added ['wink'] to the tokenized_text array.\n",
      "wink         16,837\n",
      "Added ['wired'] to the tokenized_text array.\n",
      "wired        17,502\n",
      "Added ['wish', '##ful'] to the tokenized_text array.\n",
      "wish          4,299\n",
      "##ful         3,993\n",
      "Added ['wi', '##st', '##ful'] to the tokenized_text array.\n",
      "wi           15,536\n",
      "##st          3,367\n",
      "##ful         3,993\n",
      "Added ['wi', '##st', '##fully'] to the tokenized_text array.\n",
      "wi           15,536\n",
      "##st          3,367\n",
      "##fully       7,699\n",
      "Added ['withdraw'] to the tokenized_text array.\n",
      "withdraw     10,632\n",
      "Added ['withdrawn'] to the tokenized_text array.\n",
      "withdrawn     9,633\n",
      "Added ['with', '##held'] to the tokenized_text array.\n",
      "with          2,007\n",
      "##held       24,850\n",
      "Added ['with', '##holding'] to the tokenized_text array.\n",
      "with          2,007\n",
      "##holding    23,410\n",
      "Added ['wo', '##e'] to the tokenized_text array.\n",
      "wo           24,185\n",
      "##e           2,063\n",
      "Added ['wo', '##ef', '##ul'] to the tokenized_text array.\n",
      "wo           24,185\n",
      "##ef         12,879\n",
      "##ul          5,313\n",
      "Added ['wonder'] to the tokenized_text array.\n",
      "wonder        4,687\n",
      "Added ['wondering'] to the tokenized_text array.\n",
      "wondering     6,603\n",
      "Added ['wonder', '##ment'] to the tokenized_text array.\n",
      "wonder        4,687\n",
      "##ment        3,672\n",
      "Added ['wool', '##y'] to the tokenized_text array.\n",
      "wool         12,121\n",
      "##y           2,100\n",
      "Added ['woo', '##zy'] to the tokenized_text array.\n",
      "woo          15,854\n",
      "##zy          9,096\n",
      "Added ['worn'] to the tokenized_text array.\n",
      "worn          6,247\n",
      "Added ['worried'] to the tokenized_text array.\n",
      "worried       5,191\n",
      "Added ['wo', '##rri', '##some'] to the tokenized_text array.\n",
      "wo           24,185\n",
      "##rri        18,752\n",
      "##some       14,045\n",
      "Added ['worry'] to the tokenized_text array.\n",
      "worry         4,737\n",
      "Added ['worrying'] to the tokenized_text array.\n",
      "worrying     15,366\n",
      "Added ['worrying', '##ly'] to the tokenized_text array.\n",
      "worrying     15,366\n",
      "##ly          2,135\n",
      "Added ['wounded'] to the tokenized_text array.\n",
      "wounded       5,303\n",
      "Added ['wow'] to the tokenized_text array.\n",
      "wow          10,166\n",
      "Added ['wrath', '##ful'] to the tokenized_text array.\n",
      "wrath        14,532\n",
      "##ful         3,993\n",
      "Added ['wrath', '##fully'] to the tokenized_text array.\n",
      "wrath        14,532\n",
      "##fully       7,699\n",
      "Added ['wrecked'] to the tokenized_text array.\n",
      "wrecked      18,480\n",
      "Added ['wr', '##etched'] to the tokenized_text array.\n",
      "wr           23,277\n",
      "##etched     29,574\n",
      "Added ['wrong', '##ed'] to the tokenized_text array.\n",
      "wrong         3,308\n",
      "##ed          2,098\n",
      "Added ['wr', '##oth'] to the tokenized_text array.\n",
      "wr           23,277\n",
      "##oth        14,573\n",
      "Added ['wry'] to the tokenized_text array.\n",
      "wry          24,639\n",
      "Added ['ya', '##wn'] to the tokenized_text array.\n",
      "ya            8,038\n",
      "##wn          7,962\n",
      "Added ['ya', '##wn', '##ing'] to the tokenized_text array.\n",
      "ya            8,038\n",
      "##wn          7,962\n",
      "##ing         2,075\n",
      "Added ['yearning'] to the tokenized_text array.\n",
      "yearning     29,479\n",
      "Added ['yell'] to the tokenized_text array.\n",
      "yell         14,315\n",
      "Added ['yelling'] to the tokenized_text array.\n",
      "yelling      13,175\n",
      "Added ['yielding'] to the tokenized_text array.\n",
      "yielding     21,336\n",
      "Added ['yu', '##ck'] to the tokenized_text array.\n",
      "yu            9,805\n",
      "##ck          3,600\n",
      "Added ['za', '##ny'] to the tokenized_text array.\n",
      "za           23,564\n",
      "##ny          4,890\n",
      "Added ['ze', '##alo', '##us'] to the tokenized_text array.\n",
      "ze           27,838\n",
      "##alo        23,067\n",
      "##us          2,271\n",
      "Added ['zen'] to the tokenized_text array.\n",
      "zen          16,729\n",
      "Added ['zone', '##d'] to the tokenized_text array.\n",
      "zone          4,224\n",
      "##d           2,094\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = []\n",
    "indexed_tokens = []\n",
    "for word in vocab:\n",
    "    # Add the special tokens.\n",
    "#     marked_text = \"[CLS] \" + word + \" [SEP]\"\n",
    "    marked_text = word\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    # tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    tokenized_text.append(tokenizer.tokenize(marked_text))\n",
    "    print(f'Added {tokenized_text[-1]} to the tokenized_text array.')\n",
    "\n",
    "    \n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens.append(tokenizer.convert_tokens_to_ids(tokenized_text[-1]))\n",
    "\n",
    "    # Display the words with their indeces.\n",
    "#     print(f'The word {tokenized_text[-1][1]} is at index {indexed_tokens[-1]}.')\n",
    "    for tup in zip(tokenized_text[-1], indexed_tokens[-1]):\n",
    "        print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1]\n",
      "[11113, 16368, 5596]\n",
      "['ab', '##hor', '##red']\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the tokens as belonging to sentence \"0\" or \"1\".\n",
    "segments_ids = [1] * len(tokenized_text[3])\n",
    "# segments_ids = [0,0,0]\n",
    "print (segments_ids)\n",
    "print(indexed_tokens[3])\n",
    "print(tokenized_text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', '##hor', '##red'] [11113, 16368, 5596]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "print(tokenized_text[3], indexed_tokens[3])\n",
    "tokens_tensor = torch.tensor([indexed_tokens[3]])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n",
      "Number of batches: 1\n",
      "Number of tokens: 3\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Type of encoded_layers:  <class 'list'>\n",
      "Tensor shape for each layer:  torch.Size([1, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# `encoded_layers` is a Python list.\n",
    "print('     Type of encoded_layers: ', type(encoded_layers))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', encoded_layers[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 3, 768])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 3, 768])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12, 768])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 3 x 3072\n",
      "tensor([-0.1539,  0.0500,  0.2618,  ..., -1.0918, -0.8056,  0.2805])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2189\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0475, -0.0816,  0.2937,  ..., -0.9062, -0.0488,  0.1440])\n"
     ]
    }
   ],
   "source": [
    "embedding_mean = sum(token_vecs_cat) / len(token_vecs_cat)\n",
    "print(embedding_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abhorred\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "this_word = ''\n",
    "for i in tokenized_text[3]:\n",
    "    this_word += i.strip('#')\n",
    "print(this_word)\n",
    "print(type(this_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(token_vecs_cat[0]))\n",
    "# print((token_vecs_cat[0]).data[0].item())\n",
    "# print(type((token_vecs_cat[0]).data[0].item()))\n",
    "# for value in token_vecs_cat[0]:\n",
    "#     print(f'Value is {value.item()} of type {type(value.item())}')\n",
    "path = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/vocabulary_embeddings/BERT.txt'\n",
    "with open(path, 'w') as f:\n",
    "    f.write(tokenized_text[8][0])\n",
    "    for value in token_vecs_cat[0]:\n",
    "        f.write(' ' + str(value.item()))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of ['absent'] and ['absent'] in token_vecs_cat is: 1.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "# Test the similarity of a word with itself.\n",
    "# For words trained contextually, self-synonymy is less than 1.\n",
    "similarity = 1 - cosine(token_vecs_cat[0], token_vecs_cat[0])\n",
    "print(f'Similarity of {tokenized_text[8]} and {tokenized_text[8]} in token_vecs_cat is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum[4], token_vecs_sum[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_first[4], token_vecs_cat_first[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_first is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_first[4], token_vecs_sum_first[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_first is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_middle1[4], token_vecs_cat_middle1[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_middle1 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_middle1[4], token_vecs_sum_middle1[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_middle1 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_middle2[4], token_vecs_cat_middle2[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_middle2 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_middle2[4], token_vecs_sum_middle2[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_middle2 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_all[4], token_vecs_cat_all[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_all is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_all[4], token_vecs_sum_all[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_all is: {similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal-venv-3.6",
   "language": "python",
   "name": "crystal-venv-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
