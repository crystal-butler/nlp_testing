{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/Notebooks/crystal/NLP/nlp_testing'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disgusted'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(tokenizer.vocab.keys())[5000:5020]\n",
    "list(tokenizer.vocab.keys())[17733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"disgusted\"\n",
    "text = \"[CLS] She made a disgusted pout [SEP] Her disgusted expression was contagious [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recreate vocabulary words from their tokenized representations.\n",
    "for t in tokenized_text:\n",
    "    this_word = ''\n",
    "    for token in t:\n",
    "        this_word += token.strip('#')\n",
    "#     print(this_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mark each of the tokens as belonging to sentence \"0\" or \"1\".\n",
    "\n",
    "segments_ids = [1] * len(tokenized_text[3])\n",
    "# segments_ids = [0,0,0]\n",
    "print (segments_ids)\n",
    "print(indexed_tokens)\n",
    "print(tokenized_text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens[3]])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 1\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For our token, select its feature values from layer 5.\n",
    "token_i = 1\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "# print(vec)\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# `encoded_layers` is a Python list.\n",
    "print('     Type of encoded_layers: ', type(encoded_layers))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', encoded_layers[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 1 x 3072\n",
      "tensor([-0.2373,  0.8259, -0.6190,  ..., -0.3836, -0.5039,  0.6153])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_last = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_last.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_last), len(token_vecs_cat_last[0])))\n",
    "print(token_vecs_cat_last[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 1 x 768\n"
     ]
    }
   ],
   "source": [
    "# Sum the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_last = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_last.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_last), len(token_vecs_sum_last[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the first 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_first = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[0], token[1], token[2], token[3]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_first.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_first), len(token_vecs_cat_first[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the first 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_first = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[:4], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_first.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_first), len(token_vecs_sum_first[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the middle 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_middle1 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[4], token[5], token[6], token[7]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_middle1.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_middle1), len(token_vecs_cat_middle1[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the middle 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_middle1 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[4:8], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_middle1.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_middle1), len(token_vecs_sum_middle1[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the middle 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_middle2 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[8], token[9], token[10], token[11]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_middle2.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_middle2), len(token_vecs_cat_middle2[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the middle 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_middle2 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[8:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_middle2.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_middle2), len(token_vecs_sum_middle2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate all hidden layers to create word embeddings.\n",
    "token_vecs_cat_all = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[0], token[1], token[2], token[3], token[4], token[5], token[6], token[7], token[8], token[9], token[10], token[11]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_all.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_all), len(token_vecs_cat_all[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum all hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_all = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_all.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_all), len(token_vecs_sum_all[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a single vector to represent the pair of sentences by averaging across tokens.\n",
    "# `encoded_layers` has shape [12 x 1 x 22 x 768]\n",
    "sentences_vec = []\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = encoded_layers[11][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "for s in sentence_embedding:\n",
    "    sentences_vec.append(s)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n",
    "print(sentence_embedding[767])\n",
    "print(sentence_embedding[-1])\n",
    "print(f'Shape of sentences vector is: {len(sentences_vec)}')\n",
    "print(sentences_vec[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "# Test the similarity of a word with itself.\n",
    "# For words trained contextually, self-synonymy is less than 1.\n",
    "similarity = 1 - cosine(token_vecs_cat[0], token_vecs_cat[0])\n",
    "print(f'Similarity of {tokenized_text[8]} and {tokenized_text[8]} in token_vecs_cat is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum[4], token_vecs_sum[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_first[4], token_vecs_cat_first[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_first is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_first[4], token_vecs_sum_first[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_first is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_middle1[4], token_vecs_cat_middle1[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_middle1 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_middle1[4], token_vecs_sum_middle1[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_middle1 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_middle2[4], token_vecs_cat_middle2[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_middle2 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_middle2[4], token_vecs_sum_middle2[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_middle2 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_all[4], token_vecs_cat_all[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_all is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_all[4], token_vecs_sum_all[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_all is: {similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "############## BEGIN TESTING STATIC CONTEXTUAL EMBEDDING CREATION ####################\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    # start = timer()\n",
    "    vocab = []\n",
    "    # vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/vocab_files/vocab_checked.txt'\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    # end = timer()\n",
    "    # run_time = end - start\n",
    "#     print(f'There are {len(vocab)} words in the vocabulary.\\n')\n",
    "#     print(f'It took {run_time} seconds to read the vocabulary file into memory.')\n",
    "#     print(f'Test word is {vocab[2]}.')\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(vocab):\n",
    "    tokenized_text = []\n",
    "    indexed_tokens = []\n",
    "    for word in vocab:\n",
    "        # Add the special tokens.\n",
    "    #     marked_text = \"[CLS] \" + word + \" [SEP]\"\n",
    "        marked_text = word\n",
    "\n",
    "        # Split the sentence into tokens.\n",
    "        # tokenized_text = tokenizer.tokenize(marked_text)\n",
    "        tokenized_text.append(tokenizer.tokenize(marked_text))\n",
    "#         print(f'Added {tokenized_text[-1]} to the tokenized_text array.')\n",
    "\n",
    "\n",
    "        # Map the token strings to their vocabulary indeces.\n",
    "        indexed_tokens.append(tokenizer.convert_tokens_to_ids(tokenized_text[-1]))\n",
    "\n",
    "        # Display the words with their indeces.\n",
    "    #     print(f'The word {tokenized_text[-1][1]} is at index {indexed_tokens[-1]}.')\n",
    "#         for tup in zip(tokenized_text[-1], indexed_tokens[-1]):\n",
    "#             print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
    "    return tokenized_text, indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_segments_IDs(tokenized_text):\n",
    "    # Create segment IDs for sentence 1 (there can be a sentence 0 to compare to\n",
    "    # sentence 1, but we're not doing that).\n",
    "    # Check that indices and token indices look correct.\n",
    "    segments_IDs = []\n",
    "    for i in range(len(tokenized_text)):\n",
    "        segments_IDs.append([1] * len(tokenized_text[i]))\n",
    "#     for i in range(len(segments_IDs)):\n",
    "#         print (segments_IDs[i])\n",
    "#         print(tokenized_text[i])\n",
    "    return segments_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(indexed_tokens, segments_IDs):\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_IDs])\n",
    "\n",
    "    # Load pre-trained model (weights)\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "\n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "#         print('Type of encoded_layers: ', type(encoded_layers))\n",
    "        # Each layer in the list is a torch tensor.\n",
    "#         print('Tensor shape for each layer: ', encoded_layers[0].size())\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to\n",
    "    # create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "#     print(token_embeddings.size())\n",
    "\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "#     print(token_embeddings.size())\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "#     print(token_embeddings.size())\n",
    "    \n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_last_four(token_embeddings): \n",
    "    # Concatenate the last 4 hidden layers to create contextual embeddings.\n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    token_vecs_cat_last = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat_last.append(cat_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_cat_last), len(token_vecs_cat_last[0])))\n",
    "    print(token_vecs_cat_last[0])\n",
    "    \n",
    "    return token_vecs_cat_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_middle_four2(token_embeddings):\n",
    "    # Concatenate the middle 4 hidden layers to create word embeddings.\n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    token_vecs_cat_middle2 = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last \n",
    "        # four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        cat_vec = torch.cat((token[8], token[9], token[10], token[11]), dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat_middle2.append(cat_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_cat_middle2), len(token_vecs_cat_middle2[0])))\n",
    "    print(token_vecs_cat_middle2[0])\n",
    "    \n",
    "    return token_vecs_cat_middle2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_middle_four1(token_embeddings):\n",
    "    # Concatenate the middle 4 hidden layers to create word embeddings.\n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    token_vecs_cat_middle1 = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last \n",
    "        # four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        cat_vec = torch.cat((token[4], token[5], token[6], token[7]), dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat_middle1.append(cat_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_cat_middle1), len(token_vecs_cat_middle1[0])))\n",
    "    print(token_vecs_cat_middle1[0])\n",
    "    \n",
    "    return token_vecs_cat_middle1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_first_four(token_embeddings):\n",
    "    token_vecs_cat_first = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last \n",
    "        # four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        cat_vec = torch.cat((token[0], token[1], token[2], token[3]), dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat_first.append(cat_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_cat_first), len(token_vecs_cat_first[0])))\n",
    "    print(token_vecs_cat_first[0])\n",
    "    \n",
    "    return token_vecs_cat_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_first_four(token_embeddings): \n",
    "    # Sum the first 4 hidden layers to create word embeddings.\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum_first = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[:4], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_first.append(sum_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_sum_first), len(token_vecs_sum_first[0])))\n",
    "    print(token_vecs_sum_first[0])\n",
    "    \n",
    "    return token_vecs_sum_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_embedding(token_embeddings):\n",
    "    mean_embedding = sum(token_embeddings) / len(token_embeddings)\n",
    "    print(mean_embedding)\n",
    "    \n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct_tokens(tokenized_text):\n",
    "    vocab_word = ''\n",
    "    for i in tokenized_text:\n",
    "        vocab_word += i.strip('#')\n",
    "    print(vocab_word)\n",
    "\n",
    "    return vocab_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "        print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aback'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 768\n",
      "tensor([ 1.3098e+00,  1.8307e+00, -9.1022e-01, -9.0292e-01, -2.1998e+00,\n",
      "         6.3040e-01, -1.1588e+00, -8.7911e-01, -1.1964e+00, -1.9318e+00,\n",
      "         1.3116e+00, -1.1063e+00,  2.7830e+00, -8.6465e-02, -1.4349e+00,\n",
      "         1.1972e+00,  3.4477e-01,  5.1449e-01, -9.9386e-01, -1.3216e+00,\n",
      "        -3.2423e+00, -2.5107e+00,  2.5035e+00,  1.2549e-01,  9.5864e-01,\n",
      "        -1.1997e+00,  1.5864e-01,  2.0923e-01, -2.2787e+00,  4.2496e+00,\n",
      "        -7.6541e-01,  1.2488e+00, -1.2207e+00,  1.5731e+00, -4.2927e+00,\n",
      "         4.8802e-01, -1.0646e+00,  5.2440e-01,  9.0669e-01, -2.5054e-01,\n",
      "         5.2654e-01,  1.0493e+00, -2.1174e+00,  1.2490e+00, -2.0319e+00,\n",
      "        -1.5065e+00, -8.0729e+00,  2.7015e+00,  4.5749e-01, -3.1602e+00,\n",
      "         1.0981e+00,  1.6076e+00, -1.5988e+00,  3.7263e+00,  1.3033e+00,\n",
      "        -5.7730e-02, -6.1124e-01, -9.1227e-01,  1.6515e+00, -2.8915e-01,\n",
      "        -1.5198e+00, -9.6111e-02,  1.5513e+00, -3.3837e+00,  2.8860e-01,\n",
      "         1.8514e+00,  2.4644e+00, -6.0153e-01,  8.4121e-01,  1.8662e+00,\n",
      "        -1.9172e+00,  2.9991e-01, -2.5165e+00,  4.9094e-01,  1.5261e-01,\n",
      "        -4.2697e-01, -2.2302e+00,  7.2034e-01,  1.8940e+00,  1.2958e+00,\n",
      "        -5.6190e-01, -3.3459e-01,  3.0052e+00, -2.8961e-01,  1.6098e+00,\n",
      "        -2.3840e+00, -9.7441e-01, -8.8908e-02, -2.3016e+00, -2.8970e-01,\n",
      "         1.6579e-01, -3.4084e-02,  1.7309e-01,  3.8940e-01, -1.8352e+00,\n",
      "        -6.1696e-01,  8.6030e-01, -1.3477e+00, -1.2627e-01, -2.8547e+00,\n",
      "         4.7255e-02, -8.0760e-01, -1.0685e+00, -4.6893e-01, -1.5517e+00,\n",
      "         2.8153e+00, -1.4085e+00, -1.2457e+00,  2.4694e+00,  1.0756e+00,\n",
      "         1.4235e+00,  1.8340e+00, -1.4347e+00, -1.1621e+00,  1.8771e+00,\n",
      "         4.4211e-01, -9.1854e-01,  1.5533e+00,  9.4504e-01, -2.9243e+00,\n",
      "         2.2694e+00,  1.4973e+00, -1.4159e+00, -6.7271e-01,  6.8663e-01,\n",
      "         1.2775e+00, -2.8870e-01, -2.0970e+00,  6.0662e-01,  1.4401e+00,\n",
      "        -1.2613e+00,  2.5256e+00,  1.7885e+00,  9.2609e-01, -2.3936e+00,\n",
      "        -3.2400e+00,  1.2340e+00, -1.4583e+00,  4.3589e-01,  7.2189e+00,\n",
      "        -2.1873e+00,  1.7203e+00, -9.4262e-01,  1.5536e+00,  5.5149e-01,\n",
      "         1.7913e+00, -2.0339e+00, -3.9307e-01, -1.2563e+00,  3.0751e-01,\n",
      "        -2.6726e+00,  2.8063e+00,  8.0829e-01,  9.0674e-01,  1.6483e+00,\n",
      "         1.1835e+00,  1.5907e+00,  2.5873e-01, -8.7369e-01,  1.0633e+00,\n",
      "        -1.6260e+00, -2.2615e-01, -1.5515e+00,  1.2929e+00,  1.5934e-01,\n",
      "         9.5394e-01,  3.4787e-01,  1.0966e+00,  3.9343e+00,  1.8805e+00,\n",
      "         1.8492e-01,  8.3006e-01,  1.5020e+00, -2.0796e-01, -8.2938e-01,\n",
      "         5.8785e-01, -1.8970e-01, -6.6262e-01,  6.6966e-01,  2.5821e-01,\n",
      "         5.0589e-01,  1.4386e+00,  1.0248e+00,  2.2646e+00,  9.4761e-01,\n",
      "        -2.5533e+00, -3.3950e+00,  1.5513e-01, -3.8706e+00, -1.1110e+00,\n",
      "        -8.7083e-01, -1.2070e+00, -1.7164e+00,  1.9699e+00, -1.6439e+00,\n",
      "        -1.1234e+00, -4.4927e-01,  1.1830e+00,  1.7279e-01, -8.5574e-01,\n",
      "        -1.6173e+00, -9.1891e-01,  2.0743e-02, -5.4017e-01, -4.5911e-01,\n",
      "        -1.7081e+00,  2.2636e+00,  1.6768e+00,  2.0839e+00,  1.9177e+00,\n",
      "         2.1944e-01, -1.4418e+00,  1.8560e+00,  1.8318e+00, -9.3075e-01,\n",
      "        -6.8330e-01,  2.8608e+00,  8.8537e-01,  7.0670e-01,  3.4276e-01,\n",
      "         1.6059e+00,  1.5920e+00, -2.4559e+00, -1.1939e+00,  2.0740e+00,\n",
      "        -4.2507e+00,  1.6878e+00,  3.1879e+00, -2.0684e+00, -9.2770e-01,\n",
      "         2.1216e-01,  8.9092e-02,  2.5229e-01, -2.9397e-01,  7.6200e-01,\n",
      "        -1.1680e+00,  1.7327e+00, -5.4056e-02, -1.6569e+00, -9.4519e-01,\n",
      "        -4.9751e-01,  1.1014e+00, -1.9530e+00, -9.2764e-01, -1.2315e-01,\n",
      "        -3.1069e+00, -3.3849e-04, -1.5297e-01, -8.5018e-01, -2.5384e-01,\n",
      "        -1.6977e+00, -3.7555e-01, -9.6920e-02, -7.6748e-01,  5.3972e-01,\n",
      "         5.8376e-01,  1.4152e+00,  4.8270e-02,  3.1690e+00,  1.9928e+00,\n",
      "         6.1832e-01,  5.5228e-01,  1.4812e+00,  4.1792e-01,  9.0819e-01,\n",
      "         7.1602e-01, -7.1909e-01, -1.0810e-01, -1.8560e+00,  2.4004e+00,\n",
      "        -1.8889e+00,  3.1698e-01,  2.3351e+00,  1.4676e+00, -1.1024e+00,\n",
      "        -2.3573e-01, -1.1965e+00, -3.6461e+00,  7.5246e-01,  4.9392e-01,\n",
      "         3.8853e-01, -9.4517e-01, -7.0795e-02, -7.9415e-01,  2.0986e+00,\n",
      "         1.4188e+00,  9.4024e-01, -1.2108e+00, -1.6441e+00,  2.2393e+00,\n",
      "        -2.0631e+00,  2.3796e+00, -1.2602e+00,  2.5584e-01,  5.5508e-01,\n",
      "         1.2540e+00,  2.1133e-01,  1.5136e+00, -2.5015e+00, -7.7505e-01,\n",
      "        -1.7699e+00,  1.0341e-01,  2.4852e+00, -1.8854e+00, -3.1251e+00,\n",
      "         1.0896e+00, -1.0685e+00, -1.3276e+00, -4.0023e+01, -3.4650e-01,\n",
      "        -8.2898e-01,  2.5865e+00, -3.1879e-01, -8.2901e-01, -2.6275e+00,\n",
      "         1.7805e+00, -7.1844e-01,  1.3603e+00,  4.3944e-01,  2.3076e+00,\n",
      "        -2.5678e+00,  3.5511e+00,  1.3498e+00, -1.3872e+00,  3.8854e-01,\n",
      "        -1.9333e+00, -1.7032e+00,  1.4105e+00,  5.4138e-01,  2.2876e+00,\n",
      "        -1.8624e+00, -2.9063e+00,  1.3649e+00, -2.2736e+00, -1.0227e-01,\n",
      "         1.2957e+00, -1.2829e+00,  4.3776e-01, -4.3145e-01,  7.8336e-01,\n",
      "         3.5133e-02,  6.3688e-01,  5.2811e-01, -1.3222e+00, -1.6901e+00,\n",
      "         5.4561e+00, -8.6585e-02,  1.9783e+00, -2.0074e+00, -3.3450e-01,\n",
      "        -8.2438e-01,  6.8547e-01,  1.0698e+00,  1.2436e+00, -1.3788e+00,\n",
      "        -2.6262e+00,  1.4094e+00,  5.1807e-01,  1.0933e+00,  9.1527e-01,\n",
      "        -4.0472e-01,  2.8306e+00, -7.2998e-01,  4.2010e-01,  2.8150e+00,\n",
      "         7.7795e-01, -8.2363e-02,  2.2038e+00, -2.1011e+00,  9.7580e-01,\n",
      "        -1.5873e+00,  1.2538e+00,  2.4754e+00, -8.2010e-01, -2.9530e+00,\n",
      "        -1.1297e+00,  8.0146e-01,  8.5342e-02, -4.0198e-01, -2.4173e+00,\n",
      "         4.1651e-02, -1.5368e+00,  8.6816e-01,  4.1475e-01,  2.9021e+00,\n",
      "        -1.1082e+00,  4.9446e-02,  8.3246e-01, -1.8677e+00, -1.3183e-01,\n",
      "        -9.8758e-01, -2.4590e+00,  2.3998e-01, -9.0769e-01,  1.0083e+00,\n",
      "        -1.1628e+00,  4.6994e-01, -7.7509e-01,  2.3732e+00, -1.3664e+00,\n",
      "        -1.0378e+00,  2.1711e+00, -8.8918e-02, -2.9143e-01, -1.1831e-01,\n",
      "        -8.3349e-01, -1.0543e+00,  7.2210e-01, -2.2260e+00,  2.3992e-01,\n",
      "         1.0674e+00,  1.6201e-01, -7.9445e-01,  3.2172e-01, -7.9941e-01,\n",
      "        -1.1321e+00, -2.4167e-01,  1.4526e+00,  2.9795e+00,  3.2969e-01,\n",
      "         6.4471e-02, -2.8320e+00,  1.9689e+00,  4.0044e-01, -1.2839e+00,\n",
      "         3.1641e-01, -1.8412e+00,  1.8743e+00, -2.2219e+00,  3.6613e-01,\n",
      "         1.4573e+00, -1.1730e-02, -1.2601e+00,  1.8484e+00,  4.9705e-01,\n",
      "        -7.7756e-01, -3.1851e-01, -4.1427e+00,  1.0364e+00, -3.3294e-01,\n",
      "         1.8770e-01,  1.7627e+00,  1.7824e+00,  2.2291e+00,  8.4661e-01,\n",
      "        -1.8007e+00, -2.7799e+00,  2.8105e+00, -1.8723e-01,  4.4163e-02,\n",
      "        -1.1558e+00, -5.6846e-01,  1.2080e+00,  1.5468e+00, -1.1122e+00,\n",
      "         1.0494e+00,  9.5276e-02,  6.8652e-01, -5.7013e-01, -8.3698e-01,\n",
      "         9.6242e-01, -2.1077e+00,  2.5477e+00, -6.5283e-01,  5.7463e-01,\n",
      "         3.1235e+00,  1.7927e+00, -1.1320e+00,  5.4559e-01,  1.0535e+00,\n",
      "        -1.5883e+00,  2.1955e-01, -8.1895e-01,  6.9769e-01, -1.3948e+00,\n",
      "         4.7086e-01,  2.5541e-01,  1.0252e+00, -2.5862e+00, -6.6974e-01,\n",
      "         3.6960e-01,  6.5302e-01, -7.8741e-01, -2.1586e+00,  1.3437e+00,\n",
      "        -5.9924e-01,  4.2648e-01, -8.6829e-01,  1.1395e+00, -5.0641e-01,\n",
      "        -9.3887e-01,  4.2294e+00, -8.0738e-01, -2.8824e-01,  2.4356e-01,\n",
      "         1.5299e+00, -7.9604e-01, -1.1175e+00,  1.3768e+00, -2.0154e+00,\n",
      "         2.6761e-01, -7.4315e-01,  1.3060e+00,  1.6178e+00,  2.6318e+00,\n",
      "         1.2721e+00, -3.1232e-01, -2.3978e+00, -2.6001e+00,  4.0297e-01,\n",
      "         1.1537e+00, -2.6574e+00, -1.9104e+00, -4.0081e-01, -1.9913e+00,\n",
      "         9.5038e-01, -4.0193e+00,  3.0551e+00, -9.6913e-02, -4.3378e-01,\n",
      "         6.9447e-01,  7.3058e-01,  2.6200e+00,  5.9678e-02, -1.7855e+00,\n",
      "        -1.9694e+00,  8.3837e-01,  8.7397e-01,  1.2077e+00,  2.6764e+00,\n",
      "         1.7502e+00, -3.9435e+00, -2.5239e-01, -2.9962e+00,  2.1289e+00,\n",
      "        -7.1469e-01, -1.0101e+00, -1.6068e+00,  1.2885e-01,  9.8074e-01,\n",
      "        -1.7250e+00,  4.9680e-01, -1.7432e+00, -1.1529e-01,  3.3572e-01,\n",
      "         5.3648e+00, -6.4240e-01,  1.2784e+00,  2.7757e-01,  1.1556e+00,\n",
      "        -6.0094e-01, -9.1308e-01,  1.7291e-01,  1.1339e+00,  7.0880e-01,\n",
      "        -1.0211e+00,  8.1738e-02,  1.4185e+00, -1.1676e-03, -2.0703e-01,\n",
      "        -1.3974e+00, -3.1955e-01, -2.0022e+00,  5.2073e-01,  2.4255e+00,\n",
      "        -1.3584e+00, -1.4423e+00, -1.2012e+00,  6.8448e-01,  1.4812e-01,\n",
      "        -2.5616e-01,  1.5405e-01, -1.7845e+00, -1.7255e+00,  6.4128e-01,\n",
      "         3.0908e-01,  4.6049e-01, -4.1021e-01,  6.6986e-01, -9.9114e-01,\n",
      "         7.3224e-02, -3.6282e+00,  8.8514e-01, -1.7069e+00, -2.0705e-01,\n",
      "         1.5859e-02,  4.3599e-01, -1.1481e+00, -7.1302e-01, -2.4984e-01,\n",
      "        -2.2295e-01,  2.2797e-01, -2.7983e-01,  7.6544e-01, -1.5431e+00,\n",
      "        -1.5604e+00, -2.6907e+00,  8.5325e-02, -2.2562e+00, -8.4649e-02,\n",
      "        -8.5841e-01,  1.8897e+00, -6.9116e-01,  2.3853e+00, -2.7141e+00,\n",
      "         3.8383e-01,  6.1782e-01, -2.0670e-01, -1.8664e+00,  1.8220e+00,\n",
      "         2.0721e+00, -3.4164e+00,  3.4991e-01, -3.5054e-01, -8.3496e-01,\n",
      "        -2.0005e+00, -6.8443e-01, -5.6406e-01, -1.4783e-01,  5.7295e-01,\n",
      "        -1.3442e+00, -5.1758e-02,  6.1150e-01,  1.3712e+00, -2.9354e-01,\n",
      "         1.9400e-01,  2.5919e+00, -8.3409e-01,  1.1604e+00, -6.1948e-01,\n",
      "         9.7399e-01, -3.3440e-01,  1.6080e+00, -2.5181e+00,  8.9848e-01,\n",
      "         3.1994e-01,  7.3601e-02,  6.5041e-01, -1.8721e+00, -2.4921e+00,\n",
      "         1.3997e+00,  2.0895e-01, -4.7269e-02, -4.8847e-01,  9.6855e-01,\n",
      "         1.6278e+00,  1.3594e-01, -1.3517e+00, -2.8139e-01,  1.6326e-01,\n",
      "         6.0447e-01,  2.2293e+00, -4.0157e+00,  3.5845e-01, -5.0073e-02,\n",
      "        -5.0911e-02,  3.3356e-01,  6.7168e-01,  1.5815e+00,  2.2708e-01,\n",
      "         5.7510e-01,  6.9311e-01,  5.7781e-01, -1.7647e+00, -1.7531e+00,\n",
      "        -2.8664e+00, -1.0629e+00, -1.0603e+00, -5.9306e-01,  1.2464e-01,\n",
      "        -3.0852e+00,  1.0048e+00, -3.8767e-01, -1.6118e+00,  1.8833e+00,\n",
      "         8.2084e-01,  2.2649e+00, -1.0647e+00, -4.6572e-01, -9.3030e-01,\n",
      "         6.0110e-01,  1.3276e+00, -1.7797e+00, -8.0234e-02,  2.2759e+00,\n",
      "         6.5430e-01, -1.2835e+00,  8.2469e-01, -1.7346e+00, -7.0448e-01,\n",
      "         2.4745e+00, -1.6318e-01, -2.7014e-01,  1.9301e+00, -3.9899e-01,\n",
      "         2.0653e-01, -1.6988e+00, -5.7397e-01,  1.5880e-01,  8.7015e-01,\n",
      "         4.7733e-02,  3.8417e-01, -3.6228e+00, -4.3034e-01,  6.2513e-01,\n",
      "         1.1320e-02,  1.0207e+00, -1.2058e+00, -8.8562e-01,  1.2267e+00,\n",
      "        -1.8889e+00,  2.4280e+00, -4.5066e-02,  4.5744e-02,  7.3032e-02,\n",
      "         1.7796e+00, -1.7853e-01, -2.0161e+00, -2.0327e+00,  4.9258e-01,\n",
      "        -9.9215e-01,  1.2221e+00, -2.5214e+00, -1.2761e-01, -9.1510e-01,\n",
      "         7.2897e-01,  2.6778e-01, -2.2098e+00, -4.9916e-01, -5.3657e-01,\n",
      "        -2.0596e-01,  2.1570e-01, -8.1742e-01, -3.9156e-01, -1.9728e+00,\n",
      "         9.1921e-01,  1.9125e+00,  3.5458e-01, -7.9541e-01, -2.5097e-02,\n",
      "        -1.3285e+00, -2.5826e+00, -5.8170e-02,  5.1529e-01,  6.5479e-01,\n",
      "        -7.3419e-01,  1.6252e+00,  9.7213e-01,  4.9215e-01,  1.2733e+00,\n",
      "        -5.5337e-02,  1.2437e+00,  5.6362e-01,  2.1941e+00, -1.3267e+00,\n",
      "         2.1870e-01,  7.3487e-02, -1.6681e+00,  8.3310e-01, -5.7719e-01,\n",
      "        -9.2265e-01,  1.3105e+00, -1.2691e+00, -2.1735e+00, -4.3694e-01,\n",
      "        -2.7501e+00,  2.4374e+00,  2.1270e+00])\n",
      "aback\n",
      "Saved the embedding for aback.\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "# Set up input and output paths.\n",
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/vocab_files/vocab_checked.txt'\n",
    "layer_combining_function = sum_first_four\n",
    "embeddings_file = os.path.join('/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab', layer_combining_function.__name__ + '.txt')\n",
    "if os.path.exists(embeddings_file):\n",
    "    os.remove(embeddings_file)\n",
    "\n",
    "    # Create a list of vocabulary words we want embeddings for.\n",
    "vocab = make_vocab(vocab_file)\n",
    "\n",
    "# Tokenize the vocabulary and look up the BERT token indices.\n",
    "tokenized_text, indexed_tokens = tokenize_text(vocab)\n",
    "\n",
    "# Generate segment IDs for each token.\n",
    "segments_IDs = generate_segments_IDs(tokenized_text)\n",
    "\n",
    "# Generate and write out the contextual embeddings for the vocabulary words.\n",
    "# Embeddings are saved in a standard format that can be used for calcualting\n",
    "# the cosine distances between word vectors.\n",
    "for i in range(len(tokenized_text)):\n",
    "    # Convert indexed tokens and segments to tensors.\n",
    "    # Create a BERT model for the tokens.\n",
    "    # Get the encoded model layers and reshape them.\n",
    "    token_embeddings = generate_embeddings(indexed_tokens[i], segments_IDs[i])\n",
    "    print(f'{tokenized_text[i]} has a token embedding of size {token_embeddings.size()}')\n",
    "\n",
    "    # Extract the contextual embedding for a token.\n",
    "    contextual_embedding = layer_combining_function(token_embeddings)\n",
    "\n",
    "    # Write the embedding to a text file, with the vocabulary word prepended.\n",
    "    vocab_word = reconstruct_tokens(tokenized_text[i])\n",
    "    # Make sure we've got the correct vocabulary word.\n",
    "    assert vocab[i] == vocab_word\n",
    "    write_embedding(embeddings_file, vocab[i], contextual_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal-venv-3.6",
   "language": "python",
   "name": "crystal-venv-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
