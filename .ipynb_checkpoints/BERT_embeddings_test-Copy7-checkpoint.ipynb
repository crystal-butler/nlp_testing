{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/Notebooks/crystal/NLP/nlp_testing'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disgusted'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(tokenizer.vocab.keys())[5000:5020]\n",
    "list(tokenizer.vocab.keys())[17733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"disgusted\"\n",
    "text = \"[CLS] She made a disgusted pout [SEP] Her disgusted expression was contagious [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recreate vocabulary words from their tokenized representations.\n",
    "for t in tokenized_text:\n",
    "    this_word = ''\n",
    "    for token in t:\n",
    "        this_word += token.strip('#')\n",
    "#     print(this_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mark each of the tokens as belonging to sentence \"0\" or \"1\".\n",
    "\n",
    "segments_ids = [1] * len(tokenized_text[3])\n",
    "# segments_ids = [0,0,0]\n",
    "print (segments_ids)\n",
    "print(indexed_tokens)\n",
    "print(tokenized_text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens[3]])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 1\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For our token, select its feature values from layer 5.\n",
    "token_i = 1\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "# print(vec)\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# `encoded_layers` is a Python list.\n",
    "print('     Type of encoded_layers: ', type(encoded_layers))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', encoded_layers[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 1 x 3072\n",
      "tensor([-0.2373,  0.8259, -0.6190,  ..., -0.3836, -0.5039,  0.6153])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_last = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_last.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_last), len(token_vecs_cat_last[0])))\n",
    "print(token_vecs_cat_last[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 1 x 768\n"
     ]
    }
   ],
   "source": [
    "# Sum the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_last = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_last.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_last), len(token_vecs_sum_last[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the first 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_first = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[0], token[1], token[2], token[3]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_first.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_first), len(token_vecs_cat_first[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the first 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_first = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[:4], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_first.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_first), len(token_vecs_sum_first[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the middle 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_middle1 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[4], token[5], token[6], token[7]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_middle1.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_middle1), len(token_vecs_cat_middle1[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the middle 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_middle1 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[4:8], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_middle1.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_middle1), len(token_vecs_sum_middle1[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the middle 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_middle2 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[8], token[9], token[10], token[11]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_middle2.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_middle2), len(token_vecs_cat_middle2[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the middle 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_middle2 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[8:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_middle2.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_middle2), len(token_vecs_sum_middle2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate all hidden layers to create word embeddings.\n",
    "token_vecs_cat_all = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[0], token[1], token[2], token[3], token[4], token[5], token[6], token[7], token[8], token[9], token[10], token[11]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_all.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_all), len(token_vecs_cat_all[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum all hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_all = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_all.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_all), len(token_vecs_sum_all[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a single vector to represent the pair of sentences by averaging across tokens.\n",
    "# `encoded_layers` has shape [12 x 1 x 22 x 768]\n",
    "sentences_vec = []\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = encoded_layers[11][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "for s in sentence_embedding:\n",
    "    sentences_vec.append(s)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n",
    "print(sentence_embedding[767])\n",
    "print(sentence_embedding[-1])\n",
    "print(f'Shape of sentences vector is: {len(sentences_vec)}')\n",
    "print(sentences_vec[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "# Test the similarity of a word with itself.\n",
    "# For words trained contextually, self-synonymy is less than 1.\n",
    "similarity = 1 - cosine(token_vecs_cat[0], token_vecs_cat[0])\n",
    "print(f'Similarity of {tokenized_text[8]} and {tokenized_text[8]} in token_vecs_cat is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum[4], token_vecs_sum[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_first[4], token_vecs_cat_first[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_first is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_first[4], token_vecs_sum_first[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_first is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_middle1[4], token_vecs_cat_middle1[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_middle1 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_middle1[4], token_vecs_sum_middle1[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_middle1 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_middle2[4], token_vecs_cat_middle2[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_middle2 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_middle2[4], token_vecs_sum_middle2[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_middle2 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_all[4], token_vecs_cat_all[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_all is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_all[4], token_vecs_sum_all[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_all is: {similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "############## BEGIN TESTING STATIC CONTEXTUAL EMBEDDING CREATION ####################\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# and the paper (under review)\n",
    "# bert_wears_gloves_distilling_static_embeddings_from_pretrained_contextual_representations-Original Pdf.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/Notebooks/crystal/NLP/nlp_testing'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    # start = timer()\n",
    "    vocab = []\n",
    "    # vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/vocab_files/vocab_checked.txt'\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    # end = timer()\n",
    "    # run_time = end - start\n",
    "#     print(f'There are {len(vocab)} words in the vocabulary.\\n')\n",
    "#     print(f'It took {run_time} seconds to read the vocabulary file into memory.')\n",
    "#     print(f'Test word is {vocab[2]}.')\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(vocab):\n",
    "    tokenized_text = []\n",
    "    indexed_tokens = []\n",
    "    for word in vocab:\n",
    "        # Add the special tokens.\n",
    "    #     marked_text = \"[CLS] \" + word + \" [SEP]\"\n",
    "        marked_text = word\n",
    "\n",
    "        # Split the sentence into tokens.\n",
    "        # tokenized_text = tokenizer.tokenize(marked_text)\n",
    "        tokenized_text.append(tokenizer.tokenize(marked_text))\n",
    "#         print(f'Added {tokenized_text[-1]} to the tokenized_text array.')\n",
    "\n",
    "\n",
    "        # Map the token strings to their vocabulary indeces.\n",
    "        indexed_tokens.append(tokenizer.convert_tokens_to_ids(tokenized_text[-1]))\n",
    "\n",
    "        # Display the words with their indeces.\n",
    "    #     print(f'The word {tokenized_text[-1][1]} is at index {indexed_tokens[-1]}.')\n",
    "#         for tup in zip(tokenized_text[-1], indexed_tokens[-1]):\n",
    "#             print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
    "    return tokenized_text, indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_segments_IDs(tokenized_text):\n",
    "    # Create segment IDs for sentence 1 (there can be a sentence 0 to compare to\n",
    "    # sentence 1, but we're not doing that).\n",
    "    # Check that indices and token indices look correct.\n",
    "    segments_IDs = []\n",
    "    for i in range(len(tokenized_text)):\n",
    "        segments_IDs.append([1] * len(tokenized_text[i]))\n",
    "#     for i in range(len(segments_IDs)):\n",
    "#         print (segments_IDs[i])\n",
    "#         print(tokenized_text[i])\n",
    "    return segments_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(indexed_tokens, segments_IDs):\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_IDs])\n",
    "\n",
    "    # Load pre-trained model (weights)\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "\n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "#         print('Type of encoded_layers: ', type(encoded_layers))\n",
    "        # Each layer in the list is a torch tensor.\n",
    "#         print('Tensor shape for each layer: ', encoded_layers[0].size())\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to\n",
    "    # create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "#     print(token_embeddings.size())\n",
    "\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "#     print(token_embeddings.size())\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "#     print(token_embeddings.size())\n",
    "    \n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_last_four(token_embeddings): \n",
    "    # Concatenate the last 4 hidden layers to create contextual embeddings.\n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    token_vecs_cat_last = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat_last.append(cat_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_cat_last), len(token_vecs_cat_last[0])))\n",
    "    print(token_vecs_cat_last[0])\n",
    "    \n",
    "    return token_vecs_cat_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_middle_four2(token_embeddings):\n",
    "    # Concatenate the middle 4 hidden layers to create word embeddings.\n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    token_vecs_cat_middle2 = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last \n",
    "        # four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        cat_vec = torch.cat((token[8], token[9], token[10], token[11]), dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat_middle2.append(cat_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_cat_middle2), len(token_vecs_cat_middle2[0])))\n",
    "    print(token_vecs_cat_middle2[0])\n",
    "    \n",
    "    return token_vecs_cat_middle2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_middle_four1(token_embeddings):\n",
    "    # Concatenate the middle 4 hidden layers to create word embeddings.\n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    token_vecs_cat_middle1 = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last \n",
    "        # four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        cat_vec = torch.cat((token[4], token[5], token[6], token[7]), dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat_middle1.append(cat_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_cat_middle1), len(token_vecs_cat_middle1[0])))\n",
    "    print(token_vecs_cat_middle1[0])\n",
    "    \n",
    "    return token_vecs_cat_middle1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_first_four(token_embeddings):\n",
    "    token_vecs_cat_first = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last \n",
    "        # four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        cat_vec = torch.cat((token[0], token[1], token[2], token[3]), dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat_first.append(cat_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_cat_first), len(token_vecs_cat_first[0])))\n",
    "    print(token_vecs_cat_first[0])\n",
    "    \n",
    "    return token_vecs_cat_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_last_four(token_embeddings):\n",
    "    # Sum the last 4 hidden layers to create word embeddings.\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum_last = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_last.append(sum_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_sum_last), len(token_vecs_sum_last[0])))\n",
    "    print(token_vecs_sum_last[0])\n",
    "    \n",
    "    return token_vecs_sum_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_middle_four2(token_embeddings):\n",
    "    # Sum the middle 4 hidden layers to create word embeddings.\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum_middle2 = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[8:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_middle2.append(sum_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_sum_middle2), len(token_vecs_sum_middle2[0])))\n",
    "    print(token_vecs_sum_middle2[0])\n",
    "    \n",
    "    return token_vecs_sum_middle2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_middle_four1(token_embeddings):    \n",
    "    # Sum the middle 4 hidden layers to create word embeddings.\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum_middle1 = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[4:8], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_middle1.append(sum_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_sum_middle1), len(token_vecs_sum_middle1[0])))\n",
    "    print(token_vecs_sum_middle1[0])\n",
    "    \n",
    "    return token_vecs_sum_middle1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_first_four(token_embeddings): \n",
    "    # Sum the first 4 hidden layers to create word embeddings.\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum_first = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[:4], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_first.append(sum_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_sum_first), len(token_vecs_sum_first[0])))\n",
    "    print(token_vecs_sum_first[0])\n",
    "    \n",
    "    return token_vecs_sum_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_embedding(token_embeddings):\n",
    "    mean_embedding = sum(token_embeddings) / len(token_embeddings)\n",
    "    print(mean_embedding)\n",
    "    \n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct_tokens(tokenized_text):\n",
    "    vocab_word = ''\n",
    "    for i in tokenized_text:\n",
    "        vocab_word += i.strip('#')\n",
    "    print(vocab_word)\n",
    "\n",
    "    return vocab_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "        print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aback'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 768\n",
      "tensor([-1.1752e+00,  1.6355e+00,  3.1701e-01,  4.2604e-01, -1.8822e+00,\n",
      "        -4.5753e-01, -2.5937e+00,  4.7774e-01,  1.6182e+00,  1.2391e+00,\n",
      "        -1.7148e+00,  3.4380e+00,  2.2214e+00,  1.7032e+00,  2.3263e+00,\n",
      "         1.0625e+00, -2.8077e-01,  5.3699e-01,  4.4309e-01,  2.3522e+00,\n",
      "         1.0541e+00, -2.3613e+00,  5.7413e+00,  1.5299e+00,  2.4016e+00,\n",
      "        -6.3551e-01, -1.6419e+00, -2.0198e+00, -2.9064e+00,  6.8646e-01,\n",
      "         9.7107e-01, -1.0402e+00,  2.3892e+00,  2.3063e+00, -1.2446e+00,\n",
      "        -1.9551e+00, -3.0504e+00, -2.9872e+00,  5.0333e+00,  1.3730e+00,\n",
      "         1.5402e-01, -6.6993e-01, -6.1393e-01,  3.2613e+00,  2.4385e-01,\n",
      "         3.3128e+00, -6.9301e+00,  3.0743e+00,  2.9201e+00, -2.8332e+00,\n",
      "         3.4047e+00,  3.7639e+00, -3.4852e-02, -1.8639e+00,  1.6925e+00,\n",
      "         5.9461e+00, -1.1384e+00, -4.9767e+00,  3.7974e-01,  2.9951e+00,\n",
      "         2.1746e+00, -7.6856e-01,  4.5185e+00, -5.6350e+00,  1.0951e+00,\n",
      "         1.4331e+00,  2.0227e+00, -5.3181e-01, -1.3445e+00, -2.3204e+00,\n",
      "        -3.4185e+00,  1.7676e+00,  1.4737e-01, -9.9615e-01,  4.3268e-01,\n",
      "         3.4397e+00,  3.2838e+00,  3.8576e+00, -6.8386e-01,  2.4792e+00,\n",
      "        -1.5722e+00,  8.9625e-01,  3.8051e+00, -1.1964e+00,  1.7251e+00,\n",
      "        -7.9621e-03, -4.5672e-01, -1.8275e+00, -4.2550e+00, -1.6261e+00,\n",
      "        -2.8681e+00, -3.1206e+00, -1.8735e+00,  1.2359e+00,  1.8895e+00,\n",
      "         6.9517e-01, -4.2714e+00, -7.1540e-01,  4.0056e+00,  5.7539e-01,\n",
      "         7.8103e-01,  1.0210e+00, -5.0739e-01, -2.2072e+00, -2.3249e+00,\n",
      "        -7.8674e-01, -2.8948e+00, -1.3126e+00,  1.5697e+00, -2.6569e+00,\n",
      "         1.1804e+00,  1.5493e+00, -5.8512e+00, -7.4381e-02, -4.4801e-01,\n",
      "        -1.0548e+00, -1.6205e+00, -8.1671e-01, -2.4595e-01, -2.3514e+00,\n",
      "         1.8336e+00, -8.8055e-01, -2.2759e+00,  2.6803e+00,  4.0839e+00,\n",
      "         3.7291e+00,  2.7529e-01, -2.5599e+00, -2.6105e-01,  5.2048e+00,\n",
      "        -8.1089e-01,  1.5778e+00,  2.6302e+00, -1.7998e-01, -1.6220e+00,\n",
      "         1.0080e+00, -1.0983e+00, -4.5866e+00, -1.6920e+00, -1.0559e+00,\n",
      "        -4.5248e+00,  1.4327e+00, -4.7218e-01,  3.1382e+00,  1.1838e+00,\n",
      "         2.8897e+00, -1.9354e+00,  3.1171e+00, -4.0406e+00,  2.3461e+00,\n",
      "        -2.3291e+00,  2.4037e+00, -1.9570e+00,  4.5792e+00,  2.3399e+00,\n",
      "         2.3589e-01,  3.8780e+00, -9.5406e-01,  1.6033e+00,  1.2983e+00,\n",
      "         7.3329e-01, -2.6251e+00,  5.3964e-01,  3.6138e+00, -1.7305e+00,\n",
      "        -6.4595e-01, -1.8400e+00, -3.3150e-01, -8.9107e-01,  2.4280e+00,\n",
      "         3.0076e+00, -6.7839e+00,  1.4728e+00,  1.6835e+00, -1.3390e+00,\n",
      "         2.0149e+00, -2.0111e+00, -2.7895e+00, -7.8083e-01,  2.6286e+00,\n",
      "         1.2019e+00,  1.2385e+00, -2.5430e+00, -1.7037e-01,  2.5810e+00,\n",
      "         1.7129e+00, -2.3353e+00,  3.7156e+00, -1.6051e+00,  3.2333e+00,\n",
      "        -6.1352e+00, -1.6493e+00, -2.3935e+00,  3.9205e+00,  2.4667e+00,\n",
      "        -1.3415e+00, -2.3256e+00, -1.8537e-01, -6.6888e-01, -2.1967e+00,\n",
      "         7.4470e-01,  1.5535e+00,  2.8874e+00,  1.8433e+00, -2.7122e+00,\n",
      "        -1.5211e+00,  2.3669e+00,  1.5537e-01, -3.6988e-01,  2.1406e+00,\n",
      "        -2.2084e+00, -1.4673e+00,  2.2515e+00,  5.4694e+00,  9.8319e-01,\n",
      "         8.1565e-01, -9.5108e-01, -1.3488e+00,  5.1321e-01, -3.4572e+00,\n",
      "         1.0414e+00,  1.8607e+00, -1.2578e+00,  3.5607e+00, -1.3836e+00,\n",
      "         7.9931e-01, -1.1401e+00,  3.3786e-01, -1.2654e+00, -1.1060e+00,\n",
      "         3.2166e+00,  3.1099e-01,  3.1527e+00,  1.8188e-01, -6.9221e-01,\n",
      "         8.1661e-02, -1.0543e+00, -1.7759e+00,  1.4014e-01, -2.1039e+00,\n",
      "        -2.9673e+00, -2.1922e-01,  1.2181e+00,  8.7928e-01, -1.1255e+00,\n",
      "        -3.3370e+00,  7.8758e-01,  5.6619e-01, -3.3150e-01,  6.0160e-01,\n",
      "        -1.1836e+00, -1.2604e+00,  1.9495e+00,  7.7342e-01, -6.8349e-02,\n",
      "         2.3096e+00, -1.7603e+00, -1.1548e+00,  4.1537e+00,  3.0525e+00,\n",
      "         1.0486e+00,  1.0528e+00,  5.3719e+00, -1.9676e+00,  3.0478e+00,\n",
      "         1.2334e+00, -3.9154e+00,  3.7419e-01, -1.2379e+00,  2.6757e+00,\n",
      "        -3.1690e-01,  2.0164e+00, -5.9632e-01,  1.4878e+00,  1.1161e+00,\n",
      "         1.3220e-01, -1.9563e+00, -2.1214e+00, -4.1959e-02, -4.1419e-01,\n",
      "         8.5231e-01, -7.4645e-01, -1.2253e+00, -9.0215e-01,  2.4342e+00,\n",
      "         8.9712e-01,  7.3929e-01, -1.7441e+00,  1.1455e+00,  3.3194e+00,\n",
      "        -4.5574e+00,  5.9470e-01, -2.0735e+00,  1.1883e+00, -1.7566e+00,\n",
      "        -4.0093e-02, -1.6953e+00,  1.1589e+00, -1.0558e+00, -1.5201e+00,\n",
      "        -2.6548e+00, -1.9962e+00, -3.4001e-01, -1.0480e+00, -7.9341e-01,\n",
      "         2.6193e-01, -3.3606e-01, -6.9241e+00, -3.6153e+01, -3.7274e-01,\n",
      "        -1.8832e+00, -1.0472e+00,  8.4105e-01, -2.2687e-01, -1.6534e+00,\n",
      "         2.6226e+00, -3.5003e+00,  1.1886e+00,  2.9768e+00,  2.4825e+00,\n",
      "        -2.0557e+00,  9.0407e-02,  3.2711e+00, -3.1009e+00, -3.1954e+00,\n",
      "        -1.2898e+00, -2.1616e+00, -4.4608e-01, -5.1512e-01,  2.3496e-01,\n",
      "         1.9459e+00, -7.1532e-01,  2.4391e+00,  1.7732e+00,  8.0915e-01,\n",
      "         1.8629e+00, -3.0379e+00, -2.3920e+00,  3.9828e-01, -1.4951e-01,\n",
      "        -4.0923e-01,  2.3345e+00, -2.7849e+00, -1.7838e+00, -3.4583e+00,\n",
      "         3.2798e+00, -1.8778e-01, -2.5257e+00, -5.8717e-02,  1.3800e+00,\n",
      "         7.8140e-01,  5.7497e-01, -1.5291e+00,  3.8399e+00, -1.8856e+00,\n",
      "         3.5584e-01, -3.2607e+00,  9.2779e-02,  2.6357e+00,  1.2770e+00,\n",
      "        -2.1233e-01, -5.3805e-01,  8.3198e-03,  4.9909e+00, -1.9357e+00,\n",
      "        -3.6601e+00,  2.9900e+00,  2.9049e-01,  1.6613e+00,  1.1922e-01,\n",
      "        -1.8849e+00,  4.8575e+00, -3.4555e-01,  1.7473e+00, -4.9838e+00,\n",
      "         4.5750e-01,  3.3927e+00,  2.2763e+00, -4.1759e+00, -9.6942e-02,\n",
      "        -3.8244e+00, -7.1690e+00, -9.1169e-01, -7.6806e+00,  5.1484e+00,\n",
      "        -1.6475e+00, -2.0480e+00,  3.8886e+00, -3.7378e+00, -1.3758e+00,\n",
      "        -5.9790e-01,  9.6552e-01, -1.0818e+00,  3.9760e-02,  3.7663e-02,\n",
      "        -2.7927e+00, -4.2507e-01, -3.1457e+00,  1.2551e+00, -3.3867e+00,\n",
      "         5.2850e+00,  2.0696e+00,  1.5743e+00, -1.5231e+00, -4.5860e-02,\n",
      "        -4.6839e+00, -1.9444e+00,  2.4195e+00, -7.9861e-01, -2.5929e+00,\n",
      "         8.5886e-01, -1.6720e+00, -5.6801e-01,  1.7836e+00, -2.1520e+00,\n",
      "        -1.0389e+00, -1.0361e+00,  2.2508e+00,  3.7660e+00, -2.0478e+00,\n",
      "         5.1281e-01, -3.4502e+00,  1.6584e+00, -2.3750e+00, -1.9511e+00,\n",
      "         9.4823e-01, -2.4374e-01, -4.6717e+00, -1.2347e+00, -1.1860e+00,\n",
      "        -1.4417e+00, -3.0160e+00,  6.1026e-01, -6.8989e-01, -8.6002e-01,\n",
      "        -1.2025e+00, -9.4783e-01, -2.7170e+00,  1.7003e+00, -1.4804e-01,\n",
      "        -3.0820e+00, -3.9620e-01,  4.1312e+00, -2.3264e+00,  1.6094e+00,\n",
      "        -1.1144e+00, -2.3631e+00, -1.8062e+00, -6.6816e-01, -2.3600e+00,\n",
      "        -1.5283e+00, -4.3692e+00,  7.6613e-01,  2.2064e+00, -4.0520e+00,\n",
      "        -5.2664e-01,  2.4439e-01,  4.0185e+00, -2.6318e+00, -5.0683e+00,\n",
      "         2.2322e+00, -3.1814e+00,  5.0840e-01,  3.2453e+00,  9.1717e-01,\n",
      "         2.2549e+00, -5.1954e-01, -4.4179e+00, -1.4374e-01,  2.0326e+00,\n",
      "        -1.9965e+00,  1.6304e-01, -1.5191e+00,  3.5666e+00, -2.1600e+00,\n",
      "         2.1488e+00,  3.2206e-01, -2.7170e-01, -4.1803e+00,  3.8591e+00,\n",
      "        -7.8454e-01, -1.6688e+00, -5.3794e+00,  8.7234e-01,  5.6930e+00,\n",
      "        -2.0246e+00, -2.0195e+00,  1.9436e+00,  9.6504e-01,  1.0681e+00,\n",
      "        -3.1697e+00,  1.3979e+00, -3.7600e+00, -1.2680e+00, -2.2575e-01,\n",
      "         2.0045e-01,  7.7426e-01,  3.0660e+00, -1.9486e+00, -2.3139e+00,\n",
      "        -5.5935e-01, -1.4047e+00, -8.7614e-01,  1.6074e-01,  3.5755e+00,\n",
      "         1.0558e+00,  1.3172e+00, -3.8832e+00, -6.7661e+00, -2.8402e+00,\n",
      "        -1.1905e+00,  1.2833e-01, -1.9909e+00,  2.9226e+00, -5.9567e-01,\n",
      "        -4.2493e-01, -3.4524e+00,  8.5894e-01, -5.4359e-01,  1.4081e+00,\n",
      "         1.0628e+00, -2.3936e+00,  2.5415e+00, -1.6526e+00, -7.8172e-01,\n",
      "         1.8337e+00, -1.1393e+00,  1.6873e+00, -9.7308e-01,  6.0650e-01,\n",
      "         5.5892e+00, -6.7261e-01, -7.9532e-01, -1.2374e+00,  1.8269e+00,\n",
      "         1.6043e-01, -1.5670e+00,  1.8516e+00,  1.2485e+00,  8.9917e-01,\n",
      "        -1.4279e+00, -1.8147e+00, -2.1078e+00,  2.7471e-01,  1.8596e+00,\n",
      "         1.1239e+00, -7.5237e-01, -2.4917e+00,  7.3846e-01,  1.2240e+00,\n",
      "         1.3843e+00, -4.7342e+00, -1.3973e+00, -1.4644e+00,  4.7937e-01,\n",
      "        -1.7404e+00,  3.6300e-01,  8.5911e-01, -1.4872e+00, -9.9365e-01,\n",
      "        -1.2725e+00, -4.4560e+00, -1.5373e+00, -7.0398e-01,  8.0451e-01,\n",
      "        -2.0564e+00, -3.4387e-01,  2.8878e-01, -1.2701e+00,  2.7298e+00,\n",
      "         2.1409e-02, -2.1897e+00, -4.5026e+00, -1.7921e+00, -5.4027e-01,\n",
      "         3.1974e-01,  3.1750e+00,  2.7105e+00,  2.6869e+00, -1.9743e+00,\n",
      "         2.3981e+00, -1.5393e+00,  1.7943e+00, -3.1069e+00, -1.1338e+00,\n",
      "        -3.8683e+00, -1.7684e+00,  6.8629e-01, -5.6907e+00, -7.2475e-01,\n",
      "         1.8349e+00,  1.4784e+00,  2.9022e+00,  7.3286e-01, -3.3444e+00,\n",
      "        -3.5950e+00, -2.9887e+00,  1.5561e+00, -1.6001e+00, -1.0495e+00,\n",
      "         2.9410e+00,  1.0474e+00, -1.3519e+00, -5.3019e-01,  4.1384e+00,\n",
      "         5.1665e+00, -2.5206e+00,  1.9908e+00,  6.6191e-02,  1.0098e+00,\n",
      "         3.8321e+00, -2.7861e+00, -2.5306e+00,  3.5960e-01, -2.1464e+00,\n",
      "         2.2164e+00,  7.4020e-01, -1.0502e+00, -9.1371e-01, -2.5248e+00,\n",
      "         5.3728e+00, -8.1200e-01,  1.7769e+00, -4.7864e-01, -1.4173e+00,\n",
      "         1.1813e+00,  2.0597e+00, -4.7187e+00,  4.6933e-01,  8.4040e-01,\n",
      "        -2.2430e-01,  3.2372e+00,  2.9206e+00, -2.6103e+00, -1.2529e-01,\n",
      "        -1.6736e+00,  5.6562e-01,  1.2365e+00, -1.0731e+00, -2.8127e+00,\n",
      "         2.6500e+00, -8.0351e-02,  1.3498e+00,  1.6215e+00,  8.2153e-03,\n",
      "        -3.2064e+00,  1.7376e+00, -1.2275e-02, -6.7438e-02,  1.6746e+00,\n",
      "         1.3413e-01,  4.6559e+00,  4.9754e-01,  4.7886e+00, -4.9784e+00,\n",
      "         1.5568e+00,  1.8360e+00,  1.1480e+00,  3.0882e+00,  1.9810e+00,\n",
      "        -2.1398e-01,  1.9586e+00, -1.1235e+00,  7.3316e-01,  3.4040e+00,\n",
      "        -3.2437e+00,  8.1146e-01,  5.2340e+00, -7.7173e-01,  1.2692e+00,\n",
      "        -1.6691e+00,  1.9818e+00,  1.1466e+00, -9.0653e-01,  7.0183e-01,\n",
      "         1.6751e+00,  2.2179e+00, -1.6450e-01, -1.3230e+00, -1.1452e+00,\n",
      "        -3.1350e-01,  2.2845e+00, -2.3849e+00,  1.9438e+00,  7.8184e-01,\n",
      "        -5.1627e-01, -2.4212e+00, -2.4821e+00, -1.2289e+00, -1.5514e+00,\n",
      "         1.8383e+00, -3.2011e+00,  2.9410e-01,  1.9173e+00, -3.2176e+00,\n",
      "         1.2785e+00,  5.1532e-01,  5.3878e-01,  5.1610e-01,  6.2638e-01,\n",
      "        -9.6055e-02,  1.8295e-01, -2.5249e+00,  1.6018e+00,  9.9215e-01,\n",
      "        -4.0802e+00, -2.4203e-01, -1.1460e+00, -1.1891e+00,  4.0421e+00,\n",
      "         2.1190e+00,  1.8886e+00,  2.8300e+00, -1.3882e-01,  1.1806e+00,\n",
      "         3.9000e+00, -2.8176e-01, -7.7242e-01, -2.0464e-01, -5.2511e-02,\n",
      "        -2.5995e+00,  1.0507e-01, -6.1025e+00, -2.5241e+00,  9.8922e-01,\n",
      "         1.9944e+00, -1.2318e+00, -2.1620e-01,  1.0542e+00, -5.6464e+00,\n",
      "         1.4318e+00,  7.7715e-01,  2.8082e+00,  8.2367e-01,  3.6525e-01,\n",
      "         2.2918e-01,  2.6156e+00,  5.1972e+00, -2.6839e+00, -2.6535e-01,\n",
      "         4.0275e-01, -1.4163e+00,  8.3317e-01, -1.1861e+00,  1.7275e+00,\n",
      "         2.1878e+00,  6.0360e+00,  1.5900e+00,  2.2415e+00,  1.9099e+00,\n",
      "        -3.0410e-01,  4.6200e-02, -4.6073e+00,  1.5769e+00,  3.2856e-01,\n",
      "         5.1996e-02, -4.3821e+00, -1.8874e+00,  4.2537e+00, -9.5405e-02,\n",
      "        -2.0418e-01,  2.3739e+00, -2.3681e+00, -4.0128e+00,  1.2245e+00,\n",
      "        -1.7335e+00, -1.1571e+00,  8.3822e-01])\n",
      "aback\n",
      "Saved the embedding for aback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aba', '##shed'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 768\n",
      "tensor([ 9.6546e-02,  9.2929e-01, -3.2142e-01,  1.7481e+00, -7.9370e-02,\n",
      "         2.1331e+00,  3.6665e-01, -9.6653e-01, -2.5492e-01, -3.0540e+00,\n",
      "        -4.4038e-02,  1.9377e+00,  4.7627e-01,  3.6633e-01, -1.3055e-01,\n",
      "         3.6278e+00, -3.4152e-01, -9.0565e-01,  3.0239e-01, -9.1025e-02,\n",
      "         1.3477e+00, -1.8584e+00, -4.4135e-01, -7.0441e-01,  2.2739e+00,\n",
      "         6.2448e-01,  8.4341e-02, -5.3411e+00,  1.4804e+00, -5.1637e-01,\n",
      "         5.6736e-01, -2.4585e+00,  1.9108e+00, -6.1768e-01, -1.6921e+00,\n",
      "         6.5342e-01, -3.4238e+00,  3.6130e+00, -6.0206e-01,  6.1197e-01,\n",
      "         7.7411e-01, -1.5190e+00,  1.0063e+00,  5.6596e-01,  2.1014e-01,\n",
      "         1.6790e+00, -5.3652e+00,  6.3886e-01,  8.3258e-01,  5.3383e-01,\n",
      "        -1.4711e-01, -1.8461e+00,  2.8669e+00, -1.9642e+00,  8.3422e-01,\n",
      "         1.1250e-01,  3.7282e+00, -2.6304e-01,  7.7691e-01,  3.4549e-01,\n",
      "         2.4298e+00, -9.5807e-01,  1.7687e+00, -8.7065e-01, -3.9082e+00,\n",
      "         1.6309e+00,  1.4802e+00,  4.3852e-01, -3.6579e+00, -8.7554e-02,\n",
      "         8.1404e-01, -4.8149e-01, -4.1860e+00, -2.5248e+00, -4.2963e-01,\n",
      "         9.8911e-01,  8.3688e-01,  3.0059e+00, -2.0409e-01,  6.6481e-01,\n",
      "         3.5494e+00, -1.5321e+00,  3.4138e+00, -5.6095e-01,  4.2687e+00,\n",
      "        -6.0375e-01,  9.7725e-01,  5.8686e-02, -1.9815e+00,  1.9011e-01,\n",
      "        -1.7514e+00, -6.3355e-01, -2.2014e+00,  1.0358e+00, -1.3315e+00,\n",
      "         1.7903e+00, -3.6624e+00, -2.0581e+00,  2.6479e-01, -9.3636e-01,\n",
      "         1.3758e-01, -1.2923e+00,  7.7128e-01, -9.0622e-01, -9.5752e-01,\n",
      "         6.8531e-01, -1.6162e+00, -9.8236e-01,  4.8067e+00, -1.6630e+00,\n",
      "        -3.3063e-01,  2.9951e+00, -6.5851e-01,  6.4937e-01,  2.6548e+00,\n",
      "         6.2797e-01, -3.5674e-01, -1.3356e+00, -5.8801e-01, -1.1032e+00,\n",
      "         4.0952e+00, -3.3148e-01, -2.4728e+00,  2.2593e+00,  8.3729e-01,\n",
      "         2.4340e+00, -6.6254e-01, -2.1133e+00, -3.9184e+00,  3.3615e+00,\n",
      "         2.2638e+00,  4.3260e+00, -1.1942e+00,  3.1100e+00, -5.9442e-01,\n",
      "        -1.7850e+00, -1.5239e-01,  3.8895e+00, -2.0889e+00,  5.7015e-01,\n",
      "        -9.5447e-01,  5.6721e-01, -6.2952e-02,  9.4738e-01, -4.4111e+00,\n",
      "         7.6145e-01, -3.1365e+00,  3.2378e+00, -6.7131e+00,  3.4760e+00,\n",
      "        -1.1177e+00,  2.0339e+00,  4.4608e-01,  8.3523e-01,  2.2794e+00,\n",
      "        -2.9853e+00,  2.8116e+00, -2.0412e+00,  3.4674e-01,  3.9926e+00,\n",
      "        -9.3164e-01, -5.9803e+00,  2.8563e+00, -2.6468e+00, -1.6223e+00,\n",
      "        -1.7610e+00, -2.1576e+00, -1.7700e+00,  1.7063e+00,  2.4495e+00,\n",
      "        -2.1800e+00, -4.0443e+00,  5.7459e-01,  1.0246e+00, -1.0553e+00,\n",
      "         8.6024e-01,  5.1223e-01, -2.3818e+00, -1.3233e+00,  2.7695e+00,\n",
      "        -1.1893e-01, -1.4693e+00, -3.4276e+00,  1.9545e+00, -6.4146e-01,\n",
      "         1.4377e+00,  1.8198e-01, -6.3821e-01,  2.1432e+00,  2.7877e+00,\n",
      "         2.5337e-01,  1.3400e+00, -9.3787e-01,  1.0987e+00,  2.4485e+00,\n",
      "        -4.6975e+00,  2.4563e+00, -1.7876e+00,  1.2640e+00,  5.7165e-01,\n",
      "        -1.5960e+00,  1.8891e-01,  8.2592e-02,  1.7863e+00, -6.3285e-01,\n",
      "         1.8637e+00, -1.9907e+00,  1.9603e-01, -3.1906e+00,  1.0782e+00,\n",
      "        -7.7743e-01,  2.3631e+00, -1.8724e+00, -9.3362e-02,  1.7070e+00,\n",
      "        -3.2893e+00,  3.3803e+00, -3.9636e+00,  2.1981e-01, -3.6391e+00,\n",
      "         1.3837e+00, -7.1284e-01,  4.0921e+00,  1.4089e+00, -3.8336e-01,\n",
      "        -2.0214e+00,  1.4574e+00,  2.4635e+00,  7.7647e-01, -2.3814e+00,\n",
      "         1.7612e+00,  1.3609e+00,  1.6301e+00,  7.0252e-01,  8.7641e-01,\n",
      "        -1.2121e+00, -1.5760e+00, -1.4670e+00,  1.0805e+00, -4.4636e-01,\n",
      "        -1.3890e+00,  1.7203e+00,  3.8485e+00, -1.9110e+00, -3.3530e-01,\n",
      "        -1.4085e+00,  1.1665e+00,  1.2004e+00, -2.3658e+00,  3.5231e-02,\n",
      "        -3.6431e+00, -8.8252e-02,  1.1749e+00, -2.9342e+00,  9.2429e-01,\n",
      "        -3.3475e+00,  2.3243e+00, -5.9272e-01, -1.3601e+00,  4.6272e+00,\n",
      "        -2.0891e+00,  1.5968e+00,  3.2004e+00, -1.2575e+00,  2.6944e+00,\n",
      "         4.5996e-01, -6.4109e-01, -6.7793e-01, -2.4980e-01, -2.1692e-01,\n",
      "         1.8357e+00,  2.5359e+00, -4.2414e+00, -7.7700e-01,  1.1026e+00,\n",
      "         2.3918e+00,  1.4263e+00,  3.2739e+00,  1.0091e+00,  1.6122e+00,\n",
      "        -1.3837e+00, -3.6888e+00, -2.2977e+00, -3.9819e+00,  2.6405e+00,\n",
      "        -7.5499e-01, -2.4727e+00, -1.3339e+00,  4.6178e-02,  1.3331e+00,\n",
      "        -2.4849e+00,  2.0502e-01, -2.5502e-01, -8.4706e-01,  1.4161e+00,\n",
      "        -3.4837e+00, -3.1709e-01,  1.9877e+00, -2.5467e+00, -4.4084e+00,\n",
      "        -3.7028e+00,  2.1956e+00,  2.3061e+00,  7.7758e-01, -4.5588e+00,\n",
      "        -6.0218e-01, -3.2023e+00,  1.0092e+00, -4.6732e+01, -3.5703e-01,\n",
      "        -1.8976e+00,  1.3695e+00, -9.0760e-01, -1.1004e+00, -1.0406e+00,\n",
      "        -2.0045e-01, -1.7516e+00,  9.3483e-01,  1.4967e-01,  1.7347e-01,\n",
      "        -3.8848e+00, -1.7694e+00,  1.6155e-01, -1.5848e+00,  3.3674e+00,\n",
      "         6.8593e-02,  1.1754e+00,  4.7717e-01, -2.6518e-01, -2.3649e+00,\n",
      "         2.1905e+00,  1.2238e+00,  1.7765e+00, -1.6371e+00,  3.5238e+00,\n",
      "        -6.7870e-01, -2.7230e+00, -1.6030e+00, -2.8106e+00, -5.2598e-01,\n",
      "         2.2334e+00, -3.2993e+00, -4.3856e-01,  9.7418e-02,  1.1107e+00,\n",
      "         1.6112e-01,  2.8015e+00,  3.3163e-01, -4.6308e-01, -6.5214e-01,\n",
      "         9.9746e-01,  1.3796e+00,  1.6372e+00, -2.0368e+00,  1.8237e-01,\n",
      "         1.4497e+00, -3.6033e-01,  2.8002e+00,  1.1841e+00, -5.4511e-02,\n",
      "         9.8422e-01, -7.5396e-01, -4.1280e-01,  2.9690e+00, -1.3431e-01,\n",
      "        -1.1717e+00,  1.7813e+00,  8.4219e-02,  4.9695e+00,  3.7787e+00,\n",
      "         3.3843e-01,  2.6897e+00, -1.2652e+00,  2.5911e+00,  2.4517e-01,\n",
      "         7.3065e-01, -3.1191e+00, -7.4232e-01, -1.7955e+00, -1.2682e-01,\n",
      "        -9.6803e-01, -1.1626e+01,  2.2702e+00, -1.4360e+00, -2.4963e-01,\n",
      "        -1.0329e+00, -9.8060e-01,  3.7445e+00, -3.3445e+00, -2.2178e+00,\n",
      "         3.3228e+00,  1.7905e+00, -9.7915e-01, -8.2449e-01, -5.0963e+00,\n",
      "        -9.8034e-01, -1.2125e+00,  6.6263e-01, -2.0051e+00, -3.6851e+00,\n",
      "        -6.6737e-01, -6.2357e-01,  4.9828e+00, -2.2473e+00, -1.7516e-02,\n",
      "         3.2316e-01, -1.5915e+00,  1.9563e-01,  1.5501e+00, -4.6797e+00,\n",
      "         2.3672e+00, -3.9288e-01, -4.5314e-01,  2.1570e-01, -5.1020e-01,\n",
      "         2.4690e+00, -8.5867e-01, -9.5285e-01,  4.5052e+00,  1.1945e+00,\n",
      "        -1.2871e+00,  2.4369e+00,  4.1553e+00,  1.7851e+00, -4.3044e+00,\n",
      "         3.4646e+00,  3.7622e+00, -3.1200e+00, -3.4528e+00,  1.4962e+00,\n",
      "         2.9401e+00,  1.3806e+00,  2.8703e+00,  3.8824e-01,  3.0329e+00,\n",
      "        -3.1684e-01, -4.5297e+00, -3.3708e+00,  2.6014e+00,  2.7639e-01,\n",
      "        -9.6510e-01, -8.2736e+00, -2.3027e+00,  7.2673e-01,  1.6223e+00,\n",
      "        -6.1479e-01,  1.3498e+00,  7.9902e-01,  4.1006e+00, -2.2211e+00,\n",
      "        -4.2486e-01, -5.3106e-01,  3.1956e-01,  5.2556e+00, -7.9203e-01,\n",
      "        -1.5854e+00,  1.0372e+00,  5.7734e+00, -3.1665e+00, -2.3652e+00,\n",
      "         2.3677e+00, -1.8005e+00, -1.6542e+00,  6.5880e-01, -8.5801e-01,\n",
      "        -3.3512e-01, -2.6276e+00, -2.8086e-01, -9.6089e-01,  1.4968e+00,\n",
      "        -3.2839e+00,  8.0448e-01,  1.6088e+00,  2.6184e-01, -1.0778e+00,\n",
      "         9.9000e-01, -2.4724e+00,  1.6522e+00,  7.0429e-01,  1.3496e+00,\n",
      "        -3.7888e-01, -2.5599e+00, -2.0399e+00,  2.3432e+00,  3.8207e+00,\n",
      "        -1.9092e+00,  5.3332e-01,  2.2856e+00, -2.7222e+00,  2.3129e+00,\n",
      "        -3.5535e+00,  1.5425e-02, -1.8295e+00, -1.6015e+00, -8.9658e-01,\n",
      "         1.1495e+00,  1.3664e+00,  5.4889e-01,  4.2273e+00, -5.6732e+00,\n",
      "        -7.5641e-01,  2.5080e-01,  2.2608e+00, -2.9472e+00,  7.6319e-01,\n",
      "         2.3539e+00,  1.3648e+00, -8.1874e-01, -3.9566e+00, -9.8048e-01,\n",
      "        -9.4636e-01, -1.9645e+00, -2.4077e+00,  3.9512e-01,  2.4492e+00,\n",
      "        -1.2781e+00, -3.1296e+00,  1.9645e+00,  3.4310e+00,  1.8976e+00,\n",
      "        -1.7820e+00,  2.2658e+00,  9.2056e-01, -6.3026e-01,  4.6677e+00,\n",
      "         6.2579e+00,  3.4468e-01, -6.1241e-01, -2.7217e+00, -1.1930e+00,\n",
      "        -2.1226e+00, -1.6988e+00, -1.3311e+00, -1.3111e-01, -9.2252e-03,\n",
      "        -6.0328e-01, -1.6898e+00, -1.2078e-01,  1.8489e+00,  2.4378e-02,\n",
      "        -2.0809e+00,  8.2499e-01,  2.9247e+00,  3.4598e-01,  5.7616e-01,\n",
      "        -5.6744e-01, -2.5260e+00, -2.8191e-01,  9.4874e-01, -1.0671e+00,\n",
      "         1.1075e+00,  6.1002e-01, -1.5627e+00,  2.2045e+00,  1.7875e+00,\n",
      "        -1.0614e+00,  2.4285e+00,  1.6247e+00, -2.0419e+00, -1.4261e+00,\n",
      "        -2.3421e+00, -1.4914e+00, -1.8743e+00, -1.8322e+00, -9.3959e-01,\n",
      "        -1.6974e+00,  4.0480e-01,  1.6604e-01, -1.4614e+00,  3.1392e+00,\n",
      "         3.0816e+00,  7.8175e-02, -1.0050e+00, -5.5394e-01,  4.3622e+00,\n",
      "        -1.1328e+00,  4.8929e-01, -1.1821e+00,  1.4387e+00,  7.0921e-01,\n",
      "         1.2453e+00,  2.1024e+00,  1.1379e+00, -1.2812e-01, -1.2174e+00,\n",
      "         1.4264e+00, -2.0283e+00,  1.2627e+00, -3.5829e+00, -1.6504e+00,\n",
      "         8.9139e-01, -1.8395e+00,  1.8500e+00, -2.2897e+00, -3.4931e+00,\n",
      "        -2.9585e+00,  6.1157e-01, -1.3041e+00, -2.6605e+00, -2.1666e+00,\n",
      "        -2.0211e-01, -2.5500e-01,  2.0357e+00,  7.1677e-01, -3.1699e+00,\n",
      "         2.7710e+00, -3.4191e+00,  3.6743e-01, -1.8189e+00,  3.5141e+00,\n",
      "         3.3666e+00, -3.1590e+00, -5.6632e-01,  1.3442e-02,  1.9804e-01,\n",
      "        -3.5367e-01, -9.2879e-01,  4.2762e+00,  1.5171e+00,  7.4224e-01,\n",
      "         1.5078e+00,  2.1071e+00, -2.3888e+00, -8.5146e-01,  1.6920e+00,\n",
      "         2.9044e+00, -5.2749e-01, -2.1689e+00, -1.9296e+00,  8.9741e-01,\n",
      "         1.6191e+00,  7.6239e-01,  8.8164e-01, -2.1164e+00,  5.7528e-01,\n",
      "        -1.0627e+00,  2.6818e+00, -1.0440e+00, -2.3404e+00,  3.5193e-01,\n",
      "        -1.3884e+00,  5.3452e-01, -1.0730e+00,  5.0196e-01,  1.3283e+00,\n",
      "        -1.6793e+00, -1.0808e+00,  1.7678e+00,  1.9587e+00,  9.2623e-01,\n",
      "        -1.9594e+00, -1.7245e+00, -9.3327e-01,  3.9104e+00, -2.0954e+00,\n",
      "         1.5084e+00, -4.4856e-02,  7.7727e-01,  4.6746e-01, -7.7301e-01,\n",
      "         3.2092e+00, -3.8184e-01, -7.4351e-01, -1.7934e+00,  2.0883e+00,\n",
      "        -2.1167e+00, -2.1172e-01,  2.4505e+00, -2.7746e+00,  5.5145e-01,\n",
      "         3.1001e+00,  3.3802e+00,  1.1749e+00, -3.8089e-02, -2.2319e+00,\n",
      "         7.6869e+00, -1.3713e+00,  2.3227e+00,  1.9963e+00,  1.7856e+00,\n",
      "        -1.7049e+00, -3.7586e-01, -5.3030e-01,  1.3059e+00, -1.1271e+00,\n",
      "        -1.1867e+00, -4.2361e+00, -2.5384e+00,  7.3152e-01,  3.4280e+00,\n",
      "        -9.5957e-01, -7.5780e-01,  3.8797e+00,  1.2104e+00, -2.9433e+00,\n",
      "         4.7608e-01, -1.2156e+00,  2.1358e+00,  9.6707e-01, -1.2600e-01,\n",
      "        -3.5359e+00,  4.2684e+00, -9.9345e-01,  6.3473e-02, -6.4192e-01,\n",
      "         9.7420e-02,  1.8520e+00, -4.6738e-01,  8.8466e-01,  2.8819e+00,\n",
      "         3.8542e+00, -1.8041e+00,  2.3881e+00, -1.1403e+00, -1.6544e+00,\n",
      "         5.1065e-01, -3.1420e+00,  2.5615e-01, -3.0117e-01, -1.1038e+00,\n",
      "        -2.6321e+00, -4.0259e-01, -3.6861e+00, -2.4990e-01, -1.2124e+00,\n",
      "        -3.4059e-01, -4.5251e-01, -2.2198e+00, -1.2336e+00,  1.7316e-01,\n",
      "        -5.6886e-02,  3.4381e+00,  2.1575e+00,  2.9072e+00,  6.6818e-01,\n",
      "         2.1590e+00,  4.3287e+00,  3.0906e+00, -1.0997e+00, -9.6458e-01,\n",
      "         2.1402e+00,  1.1116e+00, -6.3178e-01, -5.1920e-02, -1.3789e+00,\n",
      "        -2.2873e+00,  2.2567e+00, -1.0648e+00,  5.1095e+00, -2.5258e+00,\n",
      "         1.4609e+00, -4.5052e-01, -3.9449e+00, -2.0214e+00, -1.5776e+00,\n",
      "        -3.0125e-01, -4.1375e+00,  2.7965e+00,  7.0794e-01, -3.2509e+00,\n",
      "        -3.5304e+00, -3.5886e-01, -2.7409e+00, -2.4581e+00,  9.6514e-02,\n",
      "        -3.4741e+00, -3.6211e+00, -2.8867e-01])\n",
      "abashed\n",
      "Saved the embedding for abashed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', '##hor'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 768\n",
      "tensor([-4.6097e-01,  1.5608e+00,  2.9614e+00,  6.6388e-01, -1.7265e-01,\n",
      "        -2.5630e-01,  7.0395e-01,  2.0900e+00,  5.9422e-01, -3.3758e+00,\n",
      "        -1.1736e+00,  1.4834e+00,  3.9120e+00, -2.0419e+00,  7.8520e-01,\n",
      "         3.1959e+00,  1.8540e+00,  1.0744e+00, -1.7597e+00, -3.6821e+00,\n",
      "        -1.1397e+00, -1.4454e+00,  1.3715e+00, -2.5372e+00, -2.6178e+00,\n",
      "        -4.2230e-01,  3.7442e-02, -7.3414e-03,  1.0962e+00,  1.9446e+00,\n",
      "        -2.9891e+00, -6.5630e-01,  7.3697e-01, -6.4695e-01, -4.0081e+00,\n",
      "        -1.0567e+00, -2.2713e+00,  5.6767e-01, -1.1906e+00, -1.9548e+00,\n",
      "        -1.9565e+00, -4.7388e+00,  2.2107e+00,  2.1096e+00, -1.6655e+00,\n",
      "         1.5465e+00, -9.8925e+00, -1.7099e-01,  1.7980e+00,  6.9004e-01,\n",
      "         1.0929e+00,  3.4908e-01, -9.0564e-01,  1.3598e+00, -6.9507e-01,\n",
      "         1.6031e+00, -5.0976e-01, -3.5644e+00,  2.1891e+00,  1.2719e+00,\n",
      "        -7.9241e-01,  3.4593e+00,  2.7021e+00, -4.8201e-01, -5.8983e-01,\n",
      "        -2.8248e-01, -1.4528e-01,  3.0073e-01, -3.6912e+00, -2.7478e+00,\n",
      "         2.7998e+00, -1.7168e+00, -2.3248e+00, -2.1299e+00, -1.1455e+00,\n",
      "        -8.9921e-01,  6.9196e-01,  4.7272e+00,  1.2689e+00,  3.2765e+00,\n",
      "         2.4466e+00,  7.4813e-02,  4.4876e+00,  1.6466e+00,  2.9734e+00,\n",
      "         1.1648e+00, -5.3062e-01, -1.1616e+00, -1.7489e+00,  7.2280e-02,\n",
      "        -2.4809e+00,  1.8782e+00, -2.3452e+00,  1.3087e+00,  5.3860e+00,\n",
      "         2.5461e+00, -8.9655e-02, -7.6834e-01,  8.8651e-01,  1.5806e+00,\n",
      "        -8.6926e-01,  1.3749e-01,  3.9626e+00,  1.3763e+00, -4.8930e+00,\n",
      "         9.0481e-01, -7.8363e-01, -2.7410e+00,  1.1737e+00, -1.2216e-01,\n",
      "        -1.3777e+00,  2.5218e+00,  2.0141e-01, -9.2700e-01,  4.7905e-01,\n",
      "         2.2907e+00,  3.5619e+00,  3.2011e-01, -6.3914e-01, -3.7405e+00,\n",
      "        -9.0697e-01,  6.3413e-01,  1.6393e-01,  3.7925e-01,  1.3338e+00,\n",
      "         9.3024e-01, -2.6918e+00,  3.1573e+00, -4.8135e+00,  1.1913e-01,\n",
      "         4.8967e+00,  2.4528e+00,  1.3988e+00,  2.0778e+00, -2.3532e+00,\n",
      "        -1.0536e+00,  4.7314e-01,  1.7953e+00, -3.0855e+00, -3.5585e-01,\n",
      "        -1.9190e+00,  1.2299e+00, -1.0913e+00,  2.8809e+00, -7.7456e-01,\n",
      "        -2.4099e-01, -3.2360e+00, -8.8736e-01, -1.6499e+00, -8.4319e-01,\n",
      "        -1.8940e+00,  2.5634e+00, -1.2313e+00,  7.2229e-01,  2.6449e+00,\n",
      "        -3.7525e+00,  2.3132e+00,  2.4176e+00,  2.4149e+00, -6.1245e-02,\n",
      "        -2.0384e+00, -1.1574e+00,  2.7343e+00, -1.4288e+00,  1.9838e+00,\n",
      "        -1.7262e+00, -9.7339e-01,  1.1877e+00,  5.1680e+00,  3.2540e+00,\n",
      "         4.4898e-01, -1.1474e-01, -3.8359e-01, -2.1597e+00,  2.5038e+00,\n",
      "        -7.5179e-01, -1.2336e+00, -2.4762e+00, -3.0360e+00,  3.8987e-01,\n",
      "        -2.6539e-01, -1.6281e+00, -1.0454e+00, -1.8565e+00,  1.0438e+00,\n",
      "         1.9000e-01,  1.2459e+00, -4.0012e+00, -2.0629e+00,  1.5086e+00,\n",
      "        -2.1593e-01, -7.6074e-01, -1.2588e+00,  1.6581e+00,  1.1017e+00,\n",
      "        -1.8981e+00,  2.5873e-01, -2.7848e-01,  1.6470e+00,  1.8541e+00,\n",
      "        -2.3747e+00,  2.1020e+00,  6.2158e-01,  1.8562e+00, -5.5057e-01,\n",
      "         2.7624e+00, -2.2319e+00, -5.6121e-02,  1.6606e+00, -9.9006e-01,\n",
      "        -3.4277e-02, -1.9091e+00,  4.3037e+00,  3.5658e+00,  4.2189e-01,\n",
      "        -2.2028e+00,  1.9006e+00,  1.8923e+00,  1.5835e+00, -4.0920e-01,\n",
      "         2.9209e+00,  1.4673e-01, -6.9880e-01,  2.1869e+00,  8.7982e-01,\n",
      "        -9.7922e-01,  4.2940e+00,  1.3008e+00, -3.3038e+00,  5.2614e-01,\n",
      "        -1.6103e+00,  2.0754e+00,  3.3418e+00,  1.1109e+00, -1.9878e+00,\n",
      "        -3.9798e-01, -7.3830e-01, -4.7628e+00,  1.2934e+00, -1.2588e+00,\n",
      "        -1.8861e+00, -9.8551e-01,  2.4117e+00, -9.7008e-01, -1.0937e+00,\n",
      "        -8.5352e-01,  1.1640e+00,  3.7863e+00, -3.2679e+00, -7.0475e-01,\n",
      "         1.6234e+00, -2.3191e+00,  2.3769e+00, -2.0228e+00,  1.0147e+00,\n",
      "        -3.5833e-01,  8.3560e-02,  1.8129e+00, -2.9009e+00,  3.4141e+00,\n",
      "         1.2006e+00,  5.5661e-02,  3.1495e+00, -2.9766e+00, -4.7701e-01,\n",
      "        -2.2088e+00, -1.7540e+00, -1.0447e+00, -2.8380e+00, -1.5820e+00,\n",
      "        -1.3472e+00,  3.1293e+00, -1.5466e+00,  1.7556e+00,  5.1074e-01,\n",
      "         2.7084e-01,  1.7652e+00,  3.7738e+00,  3.2204e+00,  7.9139e-01,\n",
      "        -1.2470e+00, -1.2641e+00,  9.2266e-01,  4.0023e-01,  1.3286e-01,\n",
      "         1.3096e+00,  4.1139e+00, -2.1931e+00,  1.7202e+00,  4.7492e+00,\n",
      "         1.8899e+00,  1.2359e+00,  5.6347e-01,  5.0392e-02,  1.2953e+00,\n",
      "        -7.6514e-01, -3.0259e+00,  1.0305e+00, -2.1607e+00, -6.5835e-01,\n",
      "         1.0680e+00,  4.2869e+00,  1.8498e-01,  3.9953e+00, -6.8456e-01,\n",
      "        -1.5419e+00, -1.3992e+00,  2.4610e-01, -3.9513e+01,  2.7371e-01,\n",
      "        -2.6289e+00, -2.4849e+00, -3.8504e-01,  4.7770e-01, -2.8511e+00,\n",
      "        -8.1733e-01, -5.0644e+00,  8.5342e-01,  2.2901e+00,  6.5688e-01,\n",
      "        -1.8754e+00,  2.0511e-01, -2.8118e+00, -2.9031e+00,  1.2820e+00,\n",
      "         1.8989e+00,  2.4004e+00, -1.5029e+00, -8.2507e-01, -3.1074e+00,\n",
      "        -5.2926e-01, -1.7964e-01,  1.7884e+00,  8.3972e-01, -8.0663e-01,\n",
      "        -1.0151e+00, -4.5600e+00,  1.6499e+00,  4.2213e-01,  1.4134e+00,\n",
      "         2.8753e+00, -1.5127e+00,  2.8261e+00,  2.5769e-01,  2.2055e+00,\n",
      "        -5.4812e-01,  3.0669e-01, -5.8133e-02,  9.9959e-02,  5.8632e-01,\n",
      "        -3.3109e-01,  6.6615e+00, -1.0448e-01, -1.9072e+00,  1.0901e-01,\n",
      "        -1.8835e-02,  1.8823e+00,  2.0803e+00, -2.3353e+00,  2.1426e-01,\n",
      "        -1.1660e-01,  4.3773e-01,  1.2627e-01,  3.8736e+00,  1.4224e+00,\n",
      "         3.7100e+00,  3.9617e+00,  3.4292e-01,  4.1864e-01,  8.8885e-01,\n",
      "        -1.7821e+00,  3.7649e+00, -1.7764e+00, -2.7117e-01, -5.9871e-01,\n",
      "         1.9935e+00,  1.7377e+00,  2.5435e+00, -8.9940e-01, -3.0640e-01,\n",
      "         3.6138e+00, -1.3317e+01,  1.2636e+00,  5.7788e-01,  1.5014e-01,\n",
      "        -3.9981e+00,  6.1446e-02,  3.2796e+00, -1.5695e+00,  7.4702e-01,\n",
      "        -1.3220e+00,  1.0184e+00, -2.6456e+00, -2.3668e-01, -1.9694e+00,\n",
      "        -3.5183e+00,  2.3927e-01,  7.5702e-01, -2.7811e-01, -2.3517e+00,\n",
      "        -2.8306e+00, -1.0039e+00,  7.9953e-01,  9.8414e-01, -6.4778e-01,\n",
      "        -4.7050e-01,  2.1718e+00,  1.6267e+00,  3.2483e-02, -2.6325e+00,\n",
      "        -1.2976e-01, -1.1984e-01,  1.4295e+00, -1.7104e+00,  1.9600e+00,\n",
      "         2.8297e+00, -2.7037e+00,  2.2926e+00,  2.9270e+00,  2.1472e+00,\n",
      "         2.7308e+00,  2.0057e+00,  2.6329e+00, -4.1859e-01, -5.2607e+00,\n",
      "         2.5478e+00, -8.2968e-01, -1.7735e+00, -2.2007e+00,  9.5359e-01,\n",
      "        -8.8685e-02, -3.3202e+00, -1.3871e+00, -1.2735e+00, -5.5362e-01,\n",
      "        -4.1332e+00, -2.8555e+00, -3.8915e-01,  3.9550e+00,  7.9032e-01,\n",
      "        -1.8009e+00, -6.9720e+00,  8.2797e-01, -4.8760e+00,  8.0750e-02,\n",
      "        -1.8040e+00, -2.6183e+00,  6.4338e-02,  1.6821e+00, -2.2724e-01,\n",
      "        -1.7979e+00, -1.2484e+00,  5.5425e-01,  1.8659e+00, -1.8626e+00,\n",
      "        -9.8027e-01, -9.9926e-01,  2.3943e+00, -1.0304e+00,  1.7648e-01,\n",
      "         1.9285e+00, -4.3987e+00,  2.6465e-01,  8.3162e-01, -1.0111e+00,\n",
      "         2.4834e+00, -1.3994e+00,  2.4543e+00, -1.9036e+00,  5.1614e-01,\n",
      "        -7.9926e+00,  1.2768e+00, -2.8041e-01, -1.5130e+00, -1.8218e-01,\n",
      "        -1.5510e+00,  1.0413e+00, -6.9771e-01,  8.0093e-01,  6.4891e-01,\n",
      "         2.8570e-02,  1.2533e+00, -1.8188e-01,  2.0006e+00,  2.3853e+00,\n",
      "        -1.6963e+00,  2.6396e-01, -1.6683e-01, -1.0610e+00,  1.5079e+00,\n",
      "        -1.8586e+00,  1.3645e+00, -3.1083e+00, -3.9511e+00, -1.5754e-01,\n",
      "        -2.5667e-01,  2.0952e+00,  4.6053e-01,  1.4798e+00, -3.1840e+00,\n",
      "        -1.6191e+00, -2.6335e+00,  2.8144e+00,  1.9002e+00,  1.0467e+00,\n",
      "        -2.3784e-01,  7.9831e-01,  3.3298e+00, -2.5837e+00, -1.1106e+00,\n",
      "        -3.7089e+00,  5.7678e-01, -2.1313e+00,  1.9535e+00, -8.6999e-01,\n",
      "         1.1989e+00, -2.7025e+00,  3.9705e+00,  1.4571e+00,  2.2504e+00,\n",
      "         5.8268e-01,  1.8549e+00,  8.4899e-01, -3.6414e+00,  1.5784e+00,\n",
      "         1.2269e+00,  2.2334e+00,  5.5166e-01, -1.8953e+00, -1.0212e+00,\n",
      "        -5.2839e+00,  4.8513e-01, -3.5139e+00,  9.1194e-01,  3.0584e+00,\n",
      "        -1.1660e+00,  4.3039e+00, -3.3770e+00, -2.4561e+00, -6.2634e-01,\n",
      "         2.2126e+00, -1.7909e+00,  1.1595e+00, -1.2335e+00,  2.8861e+00,\n",
      "         3.4366e+00, -1.3871e+00,  2.1663e+00,  2.9284e+00, -2.9214e+00,\n",
      "         3.7329e-01, -9.6831e-02, -1.9192e+00,  5.6984e-01,  9.9009e-02,\n",
      "        -2.8856e+00, -1.7532e+00,  6.0073e-01, -2.1530e+00, -3.4388e+00,\n",
      "         4.2492e-01, -3.3008e+00, -4.8752e+00, -2.4460e+00,  1.8025e+00,\n",
      "         2.8118e+00,  8.9236e-01,  8.3485e-01,  9.6599e-01,  2.2985e+00,\n",
      "        -1.6430e+00, -9.2915e-01, -4.1478e-01, -2.4049e+00,  4.9488e+00,\n",
      "        -1.7802e+00,  8.0336e-01, -2.1891e+00,  7.1139e-01, -1.7419e+00,\n",
      "         1.3988e-02,  2.9548e+00,  6.7422e-01, -3.1702e+00, -1.5036e+00,\n",
      "         1.6049e+00, -2.2987e+00, -2.6614e-01, -4.1477e-01, -2.2293e+00,\n",
      "         6.4992e-01,  2.2816e+00, -1.1028e+00,  9.6151e-01, -3.1407e+00,\n",
      "        -3.2375e+00,  1.5026e+00,  1.9736e+00, -2.6228e+00, -7.3534e-01,\n",
      "         7.5100e-02,  1.2648e+00,  3.1622e-01,  2.5325e+00, -1.9949e+00,\n",
      "        -8.7953e-01, -8.2213e-01, -1.2151e+00, -1.6887e+00,  1.5446e+00,\n",
      "        -5.2699e-01,  6.3778e-02,  2.4197e+00,  3.9648e+00, -5.9264e-01,\n",
      "        -1.2163e+00, -8.6651e-01,  2.4716e+00, -1.1360e-01, -3.2086e+00,\n",
      "         5.1666e-01,  1.7075e+00, -2.6089e+00,  1.1200e+00,  6.3728e-01,\n",
      "         6.5573e-02,  1.2841e+00,  6.4867e-02,  1.5833e+00, -1.3768e+00,\n",
      "         3.2348e+00,  1.7522e+00,  1.3308e+00, -2.5339e+00,  1.2429e+00,\n",
      "        -2.5097e+00, -2.8208e-01, -3.0759e-01,  2.6060e-01, -1.2157e+00,\n",
      "        -2.0210e+00,  1.8093e+00, -3.0520e+00,  1.7021e+00,  1.8965e+00,\n",
      "        -1.6572e+00,  1.9296e+00,  2.1844e+00,  2.4720e+00, -1.1672e+00,\n",
      "        -2.8680e+00, -7.4429e-01,  1.1302e-01, -2.9463e-01, -3.7081e+00,\n",
      "        -1.0210e+00, -3.2202e+00, -2.1781e+00, -1.7161e-01,  2.2851e+00,\n",
      "         9.0393e-01, -3.4174e+00, -1.1095e+00, -1.2586e+00,  2.9978e+00,\n",
      "        -3.0658e+00, -2.3489e+00,  1.2568e+00, -1.2472e+00,  3.8290e+00,\n",
      "         3.7899e+00,  2.5202e+00,  1.3109e+00, -3.1471e+00,  6.8495e-01,\n",
      "        -3.3028e-01,  6.6247e-01,  1.3899e+00, -6.2480e-01, -1.6506e-01,\n",
      "         1.4479e+00,  1.9488e+00,  2.9036e+00,  1.3687e+00, -2.6808e+00,\n",
      "        -2.1898e+00, -1.1050e+00,  1.4509e+00, -1.2517e+00,  3.0754e-01,\n",
      "        -3.4019e-01, -1.0849e+00,  4.7019e+00, -6.4445e-01, -3.4802e+00,\n",
      "        -3.2526e+00, -8.4543e-02,  4.6180e+00, -1.3559e+00,  1.5954e-01,\n",
      "        -1.5514e+00, -1.0162e+00, -1.9138e+00, -3.4065e+00, -8.4509e-01,\n",
      "         1.6944e+00, -7.5163e-02, -4.4543e+00,  9.8997e-01,  3.6696e+00,\n",
      "        -1.1272e+00,  1.7324e+00,  2.7363e+00,  1.3735e+00, -6.6414e-01,\n",
      "         2.0593e+00, -1.1344e+00,  1.5494e+00, -8.6640e-01, -2.5829e+00,\n",
      "         2.6465e+00,  5.3725e+00, -8.0776e-01, -7.1393e-01, -2.4598e+00,\n",
      "        -2.8394e+00,  1.3152e+00, -2.9298e+00, -3.1819e+00, -1.2612e+00,\n",
      "         1.4210e+00,  4.9012e-02, -8.3150e-01,  6.5301e-01,  4.0949e-01,\n",
      "        -3.2962e-01,  5.1955e+00,  1.7147e+00, -3.1228e+00,  2.4248e+00,\n",
      "         9.0065e-01, -1.8402e+00, -1.7107e+00,  1.1495e+00, -4.4391e-01,\n",
      "         2.1514e-01,  3.1311e+00, -2.1719e+00,  3.0231e+00, -2.2106e+00,\n",
      "        -1.3629e+00, -3.9873e-01, -3.5912e+00, -2.1723e+00, -1.1749e+00,\n",
      "         3.8540e-03, -6.3407e+00, -1.2353e+00,  3.9017e+00, -4.2695e+00,\n",
      "        -4.9043e+00,  1.7868e+00, -2.8252e-01, -4.3550e-01,  1.4997e+00,\n",
      "        -1.7026e+00, -3.9370e+00,  9.9637e-01])\n",
      "abhor\n",
      "Saved the embedding for abhor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', '##hor', '##red'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 768\n",
      "tensor([-1.0018e+00,  1.6727e+00,  1.0812e+00,  4.7101e-01, -2.3002e+00,\n",
      "         1.3166e+00, -3.4731e-01,  1.8777e+00,  1.2252e+00, -2.8995e+00,\n",
      "        -2.1372e-01,  1.8903e+00,  4.7215e+00, -2.0196e+00,  1.9660e+00,\n",
      "         2.9272e+00,  9.9477e-01, -1.1296e+00,  3.9193e-01, -2.4596e+00,\n",
      "        -1.6319e-01,  9.0574e-02,  1.0923e+00,  7.3312e-01, -1.2848e+00,\n",
      "         1.0026e+00, -1.2924e+00, -2.2912e+00,  7.0858e-01, -2.3059e-01,\n",
      "        -2.3351e+00, -3.8204e-01,  1.2498e-01, -1.1012e+00, -1.2625e+00,\n",
      "        -2.2932e+00, -3.1123e+00,  2.5604e+00, -1.2811e+00, -1.0581e+00,\n",
      "        -9.6761e-01, -3.2963e+00,  3.9100e-01,  5.0886e+00, -1.7870e+00,\n",
      "         9.7251e-01, -1.0500e+01,  2.1496e+00,  2.0222e+00, -9.5180e-01,\n",
      "         6.8093e-01,  8.5480e-01, -1.9256e+00,  1.1784e+00,  9.6726e-01,\n",
      "         1.6476e+00,  1.9354e-01, -3.1750e+00,  3.1787e+00,  8.9831e-01,\n",
      "         2.8670e+00,  1.4719e+00,  9.8402e-01, -2.8969e+00, -1.2697e+00,\n",
      "        -4.0269e-01, -3.5021e-01, -1.3999e+00, -4.0857e+00, -3.8080e-01,\n",
      "         2.0721e+00, -3.2685e+00, -1.2381e+00, -2.6823e+00, -1.4117e+00,\n",
      "         3.4193e-01,  9.0851e-01,  2.3616e+00,  1.9586e+00,  1.8005e+00,\n",
      "         9.7107e-01,  1.3994e+00,  3.5101e+00,  1.9216e+00,  2.1355e+00,\n",
      "         1.4845e+00, -1.2327e+00, -2.8424e+00, -2.0739e+00, -3.5236e-01,\n",
      "        -1.1401e+00,  1.7975e-01, -9.1538e-01,  2.2887e-01,  2.6355e+00,\n",
      "         1.8096e+00, -1.3511e+00, -6.4224e-01,  2.0071e+00, -9.1588e-02,\n",
      "        -1.2642e+00, -4.4234e-01,  5.0084e+00, -2.0990e+00, -2.3154e+00,\n",
      "         1.3557e+00, -1.1799e+00, -3.2147e+00,  1.7728e+00,  2.0723e+00,\n",
      "        -1.1247e-01,  1.2146e+00, -1.5437e+00, -3.8535e-01, -7.0455e-01,\n",
      "         3.2488e+00,  3.2015e+00,  7.7913e-01,  2.0253e+00, -3.8754e+00,\n",
      "        -3.3619e-01,  3.5919e-01,  2.8081e-01,  2.2324e+00,  9.8657e-01,\n",
      "         1.3371e+00, -1.3590e+00, -6.2265e-01, -3.0617e+00,  5.0478e-01,\n",
      "         4.1876e+00,  2.3611e+00,  1.7524e+00,  8.7387e-01, -1.7019e+00,\n",
      "        -1.0371e+00,  7.7903e-01, -2.8517e-02, -2.2275e+00,  9.6306e-01,\n",
      "        -1.8973e+00, -8.0305e-01, -8.3705e-01,  3.3058e+00, -1.0499e+00,\n",
      "        -2.2558e+00, -4.5669e-01, -5.3380e-01, -3.2676e+00, -5.8076e-01,\n",
      "        -1.2819e+00,  2.6127e+00, -6.3341e-01, -1.3244e+00,  9.7566e-01,\n",
      "         6.6922e-01,  1.7271e+00, -7.7948e-02,  9.6003e-01,  1.0720e+00,\n",
      "        -4.0947e-01, -2.5281e+00,  3.8872e+00, -3.1940e+00,  2.2650e+00,\n",
      "        -1.1240e+00, -1.4023e+00,  1.6169e+00,  2.7821e+00,  2.4177e+00,\n",
      "        -1.0702e+00, -1.8145e+00, -9.1872e-01, -9.5570e-01,  2.4181e+00,\n",
      "        -2.3261e+00,  8.3077e-01, -2.5727e+00,  1.0034e+00,  5.1789e-01,\n",
      "        -9.9259e-01, -1.5515e+00,  6.3048e-01, -2.7461e+00, -1.8183e+00,\n",
      "        -3.2564e-01,  7.1076e-01, -1.5074e+00, -2.3015e+00,  1.3285e+00,\n",
      "        -2.5480e+00, -8.6443e-01,  2.5275e+00,  4.0422e-01,  3.3437e+00,\n",
      "        -2.8075e+00, -1.4127e-01, -9.9323e-01,  4.3297e-01,  3.5883e+00,\n",
      "         3.9933e-01,  9.2239e-01, -1.1351e+00,  1.8978e-01, -1.1733e+00,\n",
      "         4.7323e+00, -1.3595e+00, -2.3331e+00, -4.4291e-01,  4.2218e-01,\n",
      "        -7.2069e-03, -3.0432e+00,  2.6659e+00,  4.0620e+00, -4.7039e-01,\n",
      "        -2.7197e+00, -3.3648e-01,  3.8524e+00, -1.8155e+00, -7.9330e-01,\n",
      "         2.0278e-01,  1.6878e+00, -5.6831e-01,  1.3975e+00, -2.8041e-02,\n",
      "        -2.3391e+00,  2.2343e+00,  1.0652e+00, -1.4446e+00, -1.5433e+00,\n",
      "         1.2810e+00,  1.6992e+00,  3.0957e+00,  1.3707e+00, -1.8834e+00,\n",
      "        -5.2012e-01, -1.7141e+00, -2.1995e+00,  7.1559e-01, -3.8285e-01,\n",
      "        -2.7881e+00, -1.1908e+00,  1.6274e+00,  6.1210e-01, -3.1459e+00,\n",
      "         1.0239e+00, -1.2322e+00,  7.6521e-01, -2.4705e+00, -3.8004e-01,\n",
      "         9.1159e-01, -2.3684e+00,  3.7168e+00, -1.1220e+00, -1.6775e-01,\n",
      "        -1.3946e+00, -2.1989e-01,  2.6357e+00, -3.6417e+00,  7.2155e-01,\n",
      "        -5.6611e-03, -4.5367e-01,  1.8190e+00, -3.0979e+00, -8.8412e-01,\n",
      "        -4.0178e+00, -5.8360e-01, -1.0790e+00, -1.2225e+00, -1.2820e+00,\n",
      "        -3.0266e+00,  3.6321e+00, -2.6525e+00,  1.0379e+00,  3.7461e-01,\n",
      "         5.4767e-01,  6.3254e-01,  4.5647e+00,  9.7160e-03,  3.0866e+00,\n",
      "         8.2250e-01, -6.1347e-01,  1.3725e+00, -1.8031e-01,  6.2852e-01,\n",
      "         1.7915e-01,  1.4995e+00, -1.1827e+00,  1.2333e+00,  2.3707e+00,\n",
      "        -4.3421e-01,  8.2628e-02, -1.6428e+00, -1.3874e+00,  8.2130e-01,\n",
      "        -6.4524e-01, -2.1555e+00,  1.3175e+00, -3.6602e+00, -3.6718e+00,\n",
      "        -1.0640e+00,  5.5061e+00,  9.1167e-01,  2.1798e+00, -6.8902e-01,\n",
      "        -1.4522e+00, -2.9421e+00, -2.1758e+00, -4.6080e+01,  6.1321e-01,\n",
      "        -2.5277e+00,  9.4050e-01, -1.1472e+00, -9.5610e-01, -3.1022e+00,\n",
      "         8.8432e-01, -4.2022e+00, -4.8818e-01,  3.4641e+00, -5.1192e-01,\n",
      "        -2.3580e+00,  2.2166e-01, -3.1681e+00, -1.5682e+00, -3.4997e-01,\n",
      "         1.7509e+00, -9.7560e-01, -6.2712e-03, -1.0966e+00, -1.7981e+00,\n",
      "         7.8384e-01,  1.4692e+00,  1.3798e+00,  2.3631e+00,  6.9884e-01,\n",
      "         1.1456e+00, -4.3665e+00,  2.3062e+00,  8.7554e-01,  3.3578e-01,\n",
      "         3.0387e+00, -3.7434e-01,  1.1813e+00,  9.3386e-01,  2.2293e+00,\n",
      "         3.4558e-02,  1.1008e+00,  2.7714e+00, -2.7529e+00, -1.5096e-01,\n",
      "         6.3905e-01,  4.9452e+00, -9.7596e-01, -1.4616e+00,  2.0902e+00,\n",
      "         1.1460e+00,  3.0916e+00,  2.0914e+00, -1.6174e+00,  2.1959e+00,\n",
      "         6.0223e-03, -2.7925e+00,  2.8596e-01,  3.0544e+00,  1.1391e+00,\n",
      "         2.2597e+00, -7.8455e-01, -3.8093e-01, -6.3735e-01,  2.6661e+00,\n",
      "        -9.0788e-01,  3.4447e+00, -1.0899e-01,  3.6690e+00, -8.1897e-01,\n",
      "        -1.0486e-01,  1.1367e-01,  2.4117e+00,  1.3934e+00,  1.7647e+00,\n",
      "         2.7030e+00, -1.4080e+01,  2.8729e+00, -1.7920e-01, -1.5802e+00,\n",
      "         1.3075e+00,  1.7285e+00,  3.2767e+00, -3.2123e+00, -4.0017e-02,\n",
      "        -1.5955e+00,  1.0105e+00, -9.9462e-01, -9.1880e-01, -2.9570e+00,\n",
      "        -1.2619e+00, -6.7979e-01,  1.3653e+00,  1.2175e+00, -2.0083e+00,\n",
      "        -4.5286e-01, -4.0031e-01,  3.1035e+00,  3.2592e-01, -1.4773e+00,\n",
      "        -8.1063e-01,  7.4023e-01,  2.9131e+00,  7.3516e-01, -1.4645e+00,\n",
      "         4.0947e-01,  1.0650e+00, -1.3704e+00, -9.8216e-01,  1.8567e+00,\n",
      "         4.6168e+00, -1.7250e+00,  1.8546e+00,  2.0194e+00,  1.0049e+00,\n",
      "         8.6865e-01,  3.7314e+00,  2.8157e+00,  1.6671e+00, -2.9971e+00,\n",
      "         3.3133e+00,  1.6557e+00, -1.4440e+00, -1.9362e-01,  3.0690e+00,\n",
      "         1.9558e-01, -8.0636e-01, -7.4575e-01, -1.8516e+00, -1.4313e+00,\n",
      "        -1.4422e+00, -2.0172e+00, -1.6471e+00,  2.2288e+00, -6.9210e-01,\n",
      "        -8.1900e-01, -7.8769e+00,  3.5652e-01, -3.6677e+00,  7.2115e-01,\n",
      "         4.0354e-01, -2.1310e+00, -1.3103e+00,  3.3162e+00,  8.0627e-01,\n",
      "         3.8804e-01,  9.6397e-02,  1.9021e+00,  4.1538e-01, -3.4819e+00,\n",
      "        -1.7757e+00, -6.3928e-01,  2.7430e+00, -1.1349e+00, -1.2711e-01,\n",
      "         2.4023e-01, -2.8006e+00, -2.2777e-01, -5.7061e-01, -2.6788e+00,\n",
      "         5.8812e-01,  1.8039e-03,  6.1206e-01, -1.7274e+00,  2.2190e+00,\n",
      "        -9.1126e+00,  2.5775e-01, -4.6400e-01,  1.2637e+00,  6.6560e-01,\n",
      "         9.3224e-01,  2.6759e-01,  1.2971e-01,  1.1712e+00, -3.9407e-02,\n",
      "        -8.8556e-01,  1.0777e+00, -1.5393e+00,  2.5747e+00,  3.3881e+00,\n",
      "        -1.5325e+00,  1.1143e+00,  1.2582e+00,  8.4301e-01,  3.1526e+00,\n",
      "        -2.3719e+00,  1.4395e+00, -8.8601e-01, -2.6296e+00,  2.4342e+00,\n",
      "        -6.1001e-01,  1.3258e-01,  1.2577e+00,  7.6474e-01, -2.9521e+00,\n",
      "        -7.5019e-01, -1.5614e+00,  1.1484e+00,  6.9611e-01,  2.4706e+00,\n",
      "         1.5925e+00,  1.8474e+00,  2.9840e-01, -5.1258e-01,  5.0303e-01,\n",
      "        -3.8098e+00,  1.7483e+00, -2.7768e+00,  1.2505e-01,  2.8273e+00,\n",
      "         4.5463e-01, -2.1677e+00,  2.0566e+00,  4.5041e+00,  1.0733e+00,\n",
      "        -3.0650e-01,  1.3227e+00,  2.6380e+00, -1.8214e+00,  1.6320e+00,\n",
      "         1.1804e+00,  1.9302e+00,  5.3305e-01,  3.1545e-02, -1.9384e+00,\n",
      "        -2.5827e+00, -1.3373e-02,  4.0341e-01,  1.4846e+00,  1.1357e+00,\n",
      "        -1.3045e+00,  2.8312e+00, -2.3925e+00, -7.1637e-01,  9.8748e-01,\n",
      "         1.3934e+00, -7.8617e-02,  1.8659e+00, -1.1768e+00,  5.5146e-01,\n",
      "         4.3607e+00, -6.9815e-01,  7.5263e-01,  1.9428e+00, -1.9917e+00,\n",
      "         4.9178e-01, -2.2757e+00, -2.8213e+00, -9.1869e-01, -7.2747e-02,\n",
      "        -2.3482e+00,  1.3876e+00,  6.6187e-01, -3.0524e+00, -1.9310e+00,\n",
      "         6.6126e-01, -2.2255e+00, -1.3853e+00, -3.3552e+00, -5.2779e-01,\n",
      "         4.6974e-01, -1.2233e+00, -3.7499e-01, -5.5395e-01,  3.2472e+00,\n",
      "        -9.0969e-01, -7.2648e-01, -1.6722e+00, -6.8429e-01,  5.0520e+00,\n",
      "        -2.6699e+00, -3.2652e-02,  3.2338e-01,  2.8909e-02,  4.7047e-02,\n",
      "        -7.8640e-01,  2.8153e+00, -1.4430e+00,  2.3462e-01, -2.4062e+00,\n",
      "         3.0319e+00, -2.1185e+00,  7.9011e-01, -2.1988e+00, -1.2102e+00,\n",
      "         2.9110e+00,  9.2755e-01,  8.0681e-01,  1.0442e+00, -7.7072e-01,\n",
      "        -2.6761e+00, -2.7668e-01,  6.2040e-01, -3.6328e+00, -1.6611e-01,\n",
      "        -1.2779e+00, -2.3145e+00,  2.2201e-01, -7.8008e-01, -5.7549e+00,\n",
      "         6.8901e-01,  5.9923e-01, -5.4242e-01, -1.0503e+00,  2.1939e+00,\n",
      "        -1.7709e+00, -1.3175e+00,  2.1364e-01,  1.6421e-04, -8.9844e-01,\n",
      "        -9.5516e-01, -4.8864e-01,  2.7272e+00,  4.9084e-01, -2.7243e-01,\n",
      "        -2.0931e-01,  1.3995e+00, -2.2505e+00, -4.5762e-01,  3.8698e+00,\n",
      "         1.7814e+00,  2.0593e+00, -1.5294e+00,  1.8954e+00, -7.7968e-01,\n",
      "         2.3893e+00,  1.4189e-01,  4.7384e-01,  1.6376e-01,  5.6553e-01,\n",
      "        -1.1932e+00,  1.5956e-01, -3.5652e-01, -8.1022e-01, -3.2335e-01,\n",
      "        -1.4913e+00, -4.5807e-02, -2.0659e+00, -1.7230e-02,  9.9869e-01,\n",
      "         5.1743e-01,  4.6767e-01,  3.0925e+00,  4.3529e+00, -1.2107e+00,\n",
      "        -1.4317e+00,  1.8091e+00, -5.4874e-01,  7.8790e-01, -1.2275e+00,\n",
      "        -8.9091e-01, -2.8861e+00, -3.2402e+00,  4.1055e-01,  2.3192e+00,\n",
      "         2.2229e+00, -1.8945e+00, -1.6426e+00, -2.3397e+00,  3.7602e+00,\n",
      "        -9.2746e-01, -2.2522e+00,  4.3676e-01, -2.2104e+00,  3.8262e+00,\n",
      "         2.9646e+00,  3.1529e+00,  8.6926e-01, -1.0659e+00,  3.6404e-01,\n",
      "         8.4526e-01, -3.6348e-01,  1.6274e+00, -5.6156e-01,  3.3339e-01,\n",
      "        -1.2140e+00,  1.7295e+00,  2.3958e+00,  2.5107e+00, -1.0987e+00,\n",
      "        -1.6324e+00,  8.3018e-01, -1.4236e+00, -2.7491e+00, -1.4938e+00,\n",
      "        -1.2122e+00, -2.0446e-01,  2.1820e+00, -5.6318e-01, -3.3766e+00,\n",
      "        -1.5278e+00, -1.1021e+00,  9.4658e-01,  1.5177e+00,  2.2481e-01,\n",
      "        -1.8576e+00,  6.4121e-01, -3.9161e+00, -2.8560e+00,  1.3866e-01,\n",
      "         1.4814e-01, -4.2755e-01, -6.0966e+00, -1.1848e+00,  3.9838e+00,\n",
      "         5.9884e-01,  1.6204e-01,  1.4923e+00,  2.5904e+00, -2.5021e+00,\n",
      "         1.8796e+00, -1.1466e+00, -2.2174e-01, -9.7683e-02, -2.2838e+00,\n",
      "         7.7184e-02,  4.9880e+00, -1.6534e+00,  1.2143e-01, -2.8397e+00,\n",
      "        -3.6619e+00,  7.6557e-01, -2.2523e+00, -1.3444e-01, -7.5320e-01,\n",
      "         2.5148e+00,  2.0555e+00, -1.6516e+00, -1.1631e+00, -3.5853e-01,\n",
      "         1.4643e+00,  5.0103e+00,  4.4964e-01, -4.3919e+00,  2.1281e+00,\n",
      "         1.9877e+00,  6.9658e-01,  1.4918e-01,  5.3489e-01, -5.7075e-01,\n",
      "        -1.8661e+00,  1.1515e+00,  3.4228e-01,  2.9353e-01, -1.0425e+00,\n",
      "        -2.8293e-01,  3.1238e-02, -2.6548e+00, -2.9138e+00, -1.1738e+00,\n",
      "        -3.6569e+00, -6.0036e+00,  1.7827e+00,  6.5892e+00, -3.5885e+00,\n",
      "        -3.8648e+00,  2.9887e+00, -1.0436e+00, -9.4463e-01,  1.0693e+00,\n",
      "        -2.8722e+00, -2.5603e+00,  5.9091e-01])\n",
      "abhorred\n",
      "Saved the embedding for abhorred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', '##hor', '##rence'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 768\n",
      "tensor([ 1.2273e+00,  3.8250e-01, -1.5134e+00,  1.4927e-01, -2.7415e+00,\n",
      "         1.1806e+00, -3.1730e-01,  1.5033e+00,  9.6468e-01, -4.6290e+00,\n",
      "        -4.5177e+00,  2.2696e+00,  3.4126e+00,  2.2782e+00,  1.2181e+00,\n",
      "         8.6961e-01,  1.9495e+00,  2.4563e+00, -1.7877e-01, -1.3199e+00,\n",
      "         3.0858e+00,  1.9671e+00,  6.5549e-01, -5.1091e-01, -1.5099e+00,\n",
      "        -5.2710e-01, -2.6631e+00, -2.6074e+00,  9.7937e-01, -4.9702e-01,\n",
      "        -2.1131e+00, -1.3368e-01,  7.1467e-01, -6.3433e-02, -1.4457e+00,\n",
      "        -2.0914e+00, -3.1629e+00,  1.2466e+00,  4.7013e-01,  2.5598e-01,\n",
      "        -1.2451e+00, -2.9973e+00,  1.3332e+00,  2.1870e+00, -6.4745e-01,\n",
      "         2.3777e+00, -1.2604e+01,  3.3635e+00,  1.4632e+00, -1.5439e+00,\n",
      "         1.5831e+00, -5.9646e-01,  7.5398e-01,  2.4395e+00,  1.7005e+00,\n",
      "         1.5699e+00,  2.3977e+00, -4.4748e+00,  6.7060e-01, -5.9457e-01,\n",
      "         3.7660e+00,  2.3534e+00, -2.2751e+00, -2.9933e+00, -4.3339e-01,\n",
      "        -1.7025e+00,  3.1233e-01,  2.9724e+00, -4.1792e+00, -1.2256e+00,\n",
      "         1.3596e+00, -3.5617e+00, -2.4991e-01, -1.1343e+00,  4.1726e-01,\n",
      "         6.6734e-01, -2.8841e+00,  2.2614e+00,  2.0064e+00, -9.5087e-01,\n",
      "        -3.4539e-01,  1.8086e+00,  4.3045e+00, -1.0002e+00,  2.6182e+00,\n",
      "         1.4300e+00,  4.4121e-01, -3.4337e+00, -1.0553e-01, -1.2504e+00,\n",
      "        -3.5137e+00,  1.1969e+00,  8.6288e-01, -1.1387e+00,  4.0773e-01,\n",
      "         1.2005e+00, -1.8150e+00, -1.9366e+00, -1.6571e+00, -1.2935e-01,\n",
      "         1.6380e+00, -4.9178e-02,  5.4883e+00, -3.3254e-03,  3.8158e-01,\n",
      "        -7.1944e-01, -1.0015e+00, -2.0424e+00, -6.2162e-01,  2.1051e+00,\n",
      "        -7.4976e-01,  1.2577e+00, -6.8918e-02,  6.1578e-01, -4.3744e+00,\n",
      "         1.7303e+00, -1.4252e-01,  1.0247e-01,  2.0518e+00, -1.4798e+00,\n",
      "         8.9318e-01,  1.5410e+00,  3.6511e-03,  5.2656e+00, -1.6505e+00,\n",
      "         1.9069e+00, -1.6926e-01, -1.8441e+00, -1.3229e+00,  1.5706e+00,\n",
      "         2.2467e+00,  1.1353e+00, -3.8581e-01,  1.4185e+00, -2.0038e+00,\n",
      "         8.7347e-01,  2.6725e+00,  1.4494e+00, -8.6780e-01,  1.9199e+00,\n",
      "        -1.7782e+00, -4.6925e+00, -3.3765e+00,  9.4538e-03,  4.5050e-01,\n",
      "        -4.2524e-01, -3.1895e+00,  2.3758e+00, -3.3307e+00, -4.9143e-01,\n",
      "        -7.9974e-01,  2.3602e+00, -1.2268e+00, -2.4003e-02,  7.8542e-01,\n",
      "        -1.5124e-01,  3.0928e+00, -4.4286e-01,  1.8642e-01,  1.6137e+00,\n",
      "        -3.2787e-01, -2.8782e+00,  4.0012e+00, -2.2504e+00,  2.6855e+00,\n",
      "        -3.0640e+00,  6.9732e-01,  1.6079e+00,  1.2901e+00,  2.4224e+00,\n",
      "         1.1350e-01, -1.8696e+00,  9.8824e-01, -2.5662e+00,  3.6138e+00,\n",
      "        -1.1869e+00, -1.6815e-01, -1.0324e+00,  1.8832e+00,  2.6832e+00,\n",
      "        -7.6518e-01, -1.4610e+00,  2.9401e+00, -1.5786e+00,  4.3317e-01,\n",
      "        -1.9415e+00,  1.2812e+00, -1.2946e+00, -2.2251e+00, -6.0878e-01,\n",
      "        -2.2193e+00,  4.2856e-01,  7.6360e-02, -4.7518e-01,  2.5863e+00,\n",
      "         5.0284e-01, -2.8292e-01, -3.4628e+00, -1.3630e-01,  1.1637e+00,\n",
      "        -2.5816e+00,  1.0815e+00, -5.1064e-01, -1.0592e+00, -1.4779e+00,\n",
      "         6.2091e+00, -6.2017e-01, -6.5652e-01, -2.0479e+00,  9.7556e-01,\n",
      "        -1.8580e+00, -3.0908e+00, -4.3388e-01,  2.9281e+00,  8.4614e-01,\n",
      "        -2.2072e+00,  8.4421e-01,  3.1487e+00,  1.4061e+00, -1.1239e+00,\n",
      "         4.9790e-01, -3.9754e-02,  1.6858e+00,  9.3330e-01, -7.1889e-01,\n",
      "        -4.6508e-01,  2.8707e+00,  1.0396e+00, -1.3062e+00, -1.0565e+00,\n",
      "         2.7292e+00,  2.8298e+00,  1.2472e+00, -4.9457e-01, -3.3211e+00,\n",
      "        -1.6266e+00, -1.9352e+00, -3.1869e+00,  4.3991e-01,  3.4659e-01,\n",
      "        -1.8721e+00,  4.2634e-01,  2.6689e+00, -7.0792e-01, -3.9778e-01,\n",
      "         3.3286e+00, -1.3100e-01,  2.6083e+00, -6.7491e-01,  1.0667e+00,\n",
      "         2.1339e+00, -3.3362e-02,  2.0160e+00, -4.6762e+00, -1.7803e-01,\n",
      "        -1.7082e-01, -5.3900e-01, -6.5138e-01, -2.9205e+00, -1.6170e-01,\n",
      "         1.8183e-01,  1.8071e+00,  2.6502e+00, -5.0113e+00, -3.4344e+00,\n",
      "        -5.5253e+00, -1.4215e-01,  6.7784e-01, -1.0580e+00, -2.4431e+00,\n",
      "        -3.9902e+00,  1.7761e+00, -3.4684e+00,  1.7495e-01, -1.3197e+00,\n",
      "        -6.8421e-02, -1.1885e+00,  5.3547e+00,  8.3324e-01,  2.7802e+00,\n",
      "         2.2996e+00,  8.3198e-03,  4.3787e-01,  3.5698e-01,  9.4259e-01,\n",
      "         1.5707e+00,  3.8018e-01,  8.5538e-01,  3.2800e+00,  2.6827e+00,\n",
      "        -1.2700e+00, -2.0984e-01,  1.9234e+00, -2.1621e+00,  4.3091e-02,\n",
      "        -1.4662e+00,  4.3123e-02,  1.0259e+00, -2.3986e+00, -1.4464e+00,\n",
      "         9.0765e-01,  3.1995e+00,  1.9004e+00, -1.2333e+00, -4.0002e+00,\n",
      "        -1.0776e+00, -9.1547e-01, -2.4864e+00, -4.0261e+01,  1.2654e+00,\n",
      "        -2.4504e+00,  7.9886e-01,  2.1830e+00, -1.3239e+00, -2.4216e+00,\n",
      "         3.7382e+00, -4.5410e+00,  1.8892e+00,  3.4235e+00, -2.1268e+00,\n",
      "         1.9764e-01,  1.7696e+00, -2.3494e+00, -3.3483e+00,  4.5687e-01,\n",
      "        -2.1468e-01, -1.0962e-01,  2.4199e+00, -2.0348e+00, -2.3149e+00,\n",
      "        -3.5544e-01,  2.2762e+00, -5.1714e-01,  8.6598e-01,  1.8228e-01,\n",
      "        -1.8129e-01, -2.5333e+00,  1.3845e+00,  7.7622e-01, -6.5864e-02,\n",
      "         1.4921e+00,  4.6635e-02,  5.6506e-01,  1.5160e+00,  7.9901e-01,\n",
      "         8.8161e-01,  1.3405e+00,  3.5498e+00, -2.4072e+00,  1.4980e+00,\n",
      "        -5.4849e-01,  5.0075e+00,  4.0687e+00,  6.1780e-01,  1.0495e-01,\n",
      "        -1.3743e+00,  1.0101e+00,  3.1359e+00, -3.4795e-01,  1.9373e+00,\n",
      "         1.2511e+00,  1.8546e+00,  4.2782e+00,  1.1358e+00,  1.3584e+00,\n",
      "         6.6537e-01, -6.5342e-01, -1.4985e+00,  1.8926e+00, -5.5091e-01,\n",
      "        -4.4742e-01,  3.6958e+00,  6.4067e-01,  6.0596e-01, -2.7164e+00,\n",
      "        -2.6276e+00, -1.5371e-01,  3.3703e+00, -1.0238e-01,  1.4541e+00,\n",
      "        -6.6217e-01, -1.3655e+01,  1.9850e+00, -5.7875e-01, -1.6170e+00,\n",
      "         1.3031e-01,  1.1370e-01,  1.7595e+00, -4.7398e+00,  1.7160e-04,\n",
      "        -2.1926e+00, -8.0477e-01, -8.9009e-01,  5.6513e-01, -3.7711e+00,\n",
      "        -4.8094e-01, -3.5965e+00, -1.4367e-01, -1.9872e-01, -1.2057e+00,\n",
      "         1.2324e-01,  1.6871e+00,  1.5351e+00, -2.6824e-01, -9.2317e-01,\n",
      "         1.8904e+00,  9.3138e-01,  2.6895e+00, -1.8123e+00, -1.3567e+00,\n",
      "         4.7586e-01,  2.8431e+00, -2.0940e+00, -1.4669e+00,  7.8099e-01,\n",
      "         3.7162e+00, -3.1243e-01, -1.2646e+00, -7.2857e-03,  2.4994e+00,\n",
      "         9.4193e-01,  4.2945e+00,  4.4921e+00,  3.1838e+00, -3.7920e+00,\n",
      "         3.8640e+00,  4.2273e+00, -5.3607e-01, -2.5169e+00,  3.2543e+00,\n",
      "        -2.1477e-01, -2.5999e+00,  1.3109e+00, -1.2542e+00, -9.8965e-01,\n",
      "        -1.7242e-01, -5.8309e-01, -3.3755e+00,  1.7879e+00,  1.6810e-01,\n",
      "        -8.8687e-01, -5.0365e+00,  5.8073e-01, -3.3914e+00,  1.2768e+00,\n",
      "         3.4965e+00, -6.6411e-01,  4.6640e-01,  2.9554e+00,  2.3666e+00,\n",
      "        -7.9059e-01,  2.0257e-01,  1.5825e+00,  1.2377e-01, -1.1876e+00,\n",
      "        -1.5548e+00,  2.0757e+00,  4.9085e+00, -1.6428e+00, -2.0181e+00,\n",
      "        -7.8888e-02, -4.1368e+00,  7.0963e-01,  6.8770e-01, -1.0573e+00,\n",
      "         6.0140e-01, -9.5757e-01, -9.0027e-01, -1.0250e-01,  5.1653e-01,\n",
      "        -7.5593e+00, -2.2601e+00, -7.7923e-01, -1.4777e+00, -5.6967e-01,\n",
      "        -9.1980e-01,  1.9865e-01, -6.3338e-01,  5.1916e-01, -9.9906e-01,\n",
      "        -9.2053e-01,  9.7755e-01,  3.5323e-01,  1.6177e+00,  5.3351e+00,\n",
      "        -6.2794e-01,  1.1351e+00, -9.4559e-01, -4.8612e-01,  3.9424e+00,\n",
      "        -3.4886e+00,  4.1666e+00, -2.9030e+00, -1.7781e+00,  2.1268e+00,\n",
      "        -4.9230e-01,  2.3246e-01,  1.4004e+00,  9.5845e-01, -2.0840e+00,\n",
      "        -2.6250e-01, -2.8053e+00, -4.1726e-02,  2.4774e-02,  3.2274e+00,\n",
      "         6.1680e-01,  1.3167e+00, -4.6995e+00,  8.7501e-01,  2.7301e-01,\n",
      "        -1.8498e+00, -4.1949e-01, -9.7648e-01, -1.6798e+00,  1.6231e+00,\n",
      "         1.2819e+00, -3.5410e+00,  8.0697e-01,  3.2406e+00,  1.2893e+00,\n",
      "        -7.3572e-01,  2.3872e+00,  3.8988e+00, -4.1018e+00,  8.5769e-01,\n",
      "         4.9079e-01,  4.3170e-01,  1.4708e+00,  2.8801e-01, -1.7932e+00,\n",
      "        -6.1442e-01,  8.3066e-01, -1.9923e-01, -4.7585e-02, -1.3928e+00,\n",
      "        -3.1364e+00,  2.8324e+00, -1.3195e+00, -1.3462e+00, -8.6536e-01,\n",
      "         1.0809e+00,  1.6994e+00,  2.1428e+00,  5.9198e-01,  1.6231e+00,\n",
      "         3.8499e+00, -2.9192e-01, -9.8574e-01,  1.9885e+00,  1.7246e+00,\n",
      "        -2.5936e+00, -7.2916e-01, -2.1702e+00, -3.2230e-01, -2.2077e+00,\n",
      "        -3.3795e+00,  1.4701e+00,  1.5588e+00, -3.5000e+00, -2.1352e+00,\n",
      "         1.6884e+00, -1.2907e+00, -1.9986e+00, -3.4412e+00, -2.5945e+00,\n",
      "        -6.5556e-01, -7.7343e-01,  8.5269e-01, -1.0254e+00,  4.2276e+00,\n",
      "        -2.8740e+00, -2.3609e+00, -1.4202e+00, -1.7854e+00,  2.4340e+00,\n",
      "        -8.2170e-01,  1.1765e+00, -1.8327e+00,  4.7659e-03, -8.6932e-01,\n",
      "         6.4449e-01,  3.4102e+00, -1.0682e+00,  1.8764e-01, -1.8538e+00,\n",
      "        -1.2962e+00, -2.1449e+00, -2.1094e+00, -2.3445e+00, -2.6690e+00,\n",
      "         8.0494e-01,  9.7487e-02,  1.5213e+00, -1.7104e+00, -7.5843e-01,\n",
      "        -4.0137e+00, -1.2759e+00,  8.0962e-01, -1.2332e+00,  1.3308e+00,\n",
      "         1.1929e-01, -2.0380e+00,  1.6580e+00,  7.9080e-02, -3.8590e+00,\n",
      "        -6.5875e-01,  4.7540e-01, -1.5536e+00, -7.3044e-01,  3.8988e+00,\n",
      "        -7.5091e-01, -2.1878e+00, -1.1736e+00, -4.3699e-02, -5.9686e-01,\n",
      "        -1.8362e+00,  1.7414e+00,  1.1304e+00,  1.1959e+00,  7.6932e-01,\n",
      "         6.9816e-01,  3.0726e+00, -2.6016e+00, -9.9917e-02,  3.1063e+00,\n",
      "         2.0486e+00,  1.4726e+00,  2.6533e+00, -2.9671e+00, -1.0105e+00,\n",
      "         3.5816e+00,  1.4618e-01, -2.8296e-01, -1.3008e+00,  7.3113e-01,\n",
      "        -8.7589e-01,  2.9206e+00, -4.6724e+00,  1.3873e+00, -5.4056e-01,\n",
      "        -2.0705e-03,  6.7909e-01, -7.7174e-01, -8.3657e-01, -1.3085e-02,\n",
      "        -4.9585e-01, -2.9733e+00,  5.9314e-02,  1.4725e+00, -1.6407e+00,\n",
      "        -4.8170e-01,  2.1670e+00, -1.0410e+00,  1.5778e+00, -8.5104e-01,\n",
      "         2.5092e+00, -4.4237e+00, -2.3567e+00, -1.4333e+00,  1.6714e+00,\n",
      "         8.5888e-01, -4.5917e-01,  4.2725e-01, -2.5081e+00,  3.3754e+00,\n",
      "         3.4459e-01, -9.0252e-01,  1.1029e+00, -2.5985e+00,  3.9190e+00,\n",
      "        -4.6261e-01,  2.0213e+00,  2.7976e+00, -2.2707e+00, -5.7234e-02,\n",
      "         3.7082e-01,  2.0810e+00,  2.2452e+00,  1.3435e+00, -5.9314e-01,\n",
      "        -1.2252e+00,  2.4717e+00,  3.0873e+00,  1.7538e+00, -1.4647e+00,\n",
      "        -3.1352e+00, -1.2621e+00, -6.5449e-01, -3.3494e+00, -1.4352e+00,\n",
      "        -4.1416e-01, -6.4216e-01,  2.1599e+00, -5.7283e-02, -1.3800e+00,\n",
      "        -6.7485e-01, -3.9331e-01, -3.5754e-01,  2.4408e+00,  1.1732e+00,\n",
      "        -1.9381e+00,  1.4773e+00, -2.8386e+00, -1.7153e+00, -1.3496e+00,\n",
      "        -4.0658e-01, -4.4954e-01, -8.4785e+00,  1.3720e-01,  4.0334e+00,\n",
      "         1.3475e+00,  1.1859e+00,  2.8617e+00, -3.7165e-01, -9.7457e-01,\n",
      "         1.2909e+00, -1.0482e+00,  1.3370e+00,  1.6865e-01, -1.7214e+00,\n",
      "         4.7247e-01,  3.4141e+00, -2.4396e+00,  3.8187e-01, -1.6558e+00,\n",
      "        -1.2335e+00, -6.9596e-01, -3.4508e+00, -2.5647e+00, -1.2573e+00,\n",
      "         3.7770e+00,  1.6424e+00, -3.2159e+00,  4.5166e-03,  2.6185e-01,\n",
      "         7.0288e-01,  4.2627e+00,  5.4347e-01, -4.0284e+00,  1.0530e+00,\n",
      "        -5.2030e-01,  6.7926e-01, -2.6043e+00, -2.4817e-01,  3.9529e+00,\n",
      "         1.2331e-01,  2.1175e+00,  6.5904e-01,  3.7252e-01,  1.6062e-01,\n",
      "         1.2401e-02,  4.9755e-01, -3.5259e-02, -3.6256e+00, -3.2168e+00,\n",
      "         1.4020e-01, -4.7606e+00,  1.5060e+00,  6.4146e+00, -4.5022e-01,\n",
      "        -5.3409e+00,  3.7953e+00, -2.1825e+00, -2.4822e+00,  1.9775e+00,\n",
      "        -3.7351e+00, -8.6994e-01,  1.2025e+00])\n",
      "abhorrence\n",
      "Saved the embedding for abhorrence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', '##hor', '##rent'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 768\n",
      "tensor([-2.6595e-01,  1.7159e+00,  1.5281e+00,  5.0462e-01, -2.3388e-01,\n",
      "        -4.4043e-01, -1.8914e+00,  1.1382e+00,  5.0572e-01, -2.7821e+00,\n",
      "        -1.4823e+00,  2.0087e+00,  4.5182e+00, -1.5696e+00,  2.4695e+00,\n",
      "         2.5036e+00,  3.7351e+00,  6.2487e-01, -9.5104e-01, -3.0007e+00,\n",
      "        -6.5531e-01,  5.2558e-01,  1.6019e+00,  4.3991e-02, -2.1309e+00,\n",
      "        -6.8878e-01, -1.4728e+00,  7.1215e-03,  2.2928e+00,  1.7888e-01,\n",
      "        -1.9871e+00, -1.8363e+00,  1.5607e+00, -6.6447e-01, -2.0956e+00,\n",
      "        -1.5763e+00, -2.9804e+00, -2.5807e-01,  1.4370e-01, -5.9045e-01,\n",
      "        -1.3469e+00, -4.3343e+00,  1.7083e+00,  3.7607e+00, -1.2759e+00,\n",
      "         2.6963e+00, -1.1404e+01,  2.3544e+00,  3.6220e+00, -1.0041e+00,\n",
      "        -3.9489e-01, -7.4348e-01,  1.1626e+00,  2.0524e+00,  1.1759e+00,\n",
      "         3.6898e+00,  3.9464e-01, -3.3193e+00,  2.1369e+00, -4.9158e-01,\n",
      "         2.3200e+00,  1.2297e+00,  2.3571e+00, -6.9797e-01,  5.9077e-02,\n",
      "         9.0297e-01,  6.9547e-02, -2.9075e-01, -3.7369e+00, -2.0378e+00,\n",
      "         2.6311e+00, -2.5388e+00, -1.5917e+00,  3.6865e-01,  1.0608e+00,\n",
      "        -1.5951e+00,  9.8159e-01,  4.5197e+00,  2.1414e+00,  2.4324e+00,\n",
      "         1.2342e+00,  7.2263e-01,  5.1007e+00,  1.7184e-02,  8.2031e-01,\n",
      "         2.8580e+00, -1.2778e+00, -2.2301e+00, -4.9489e-01, -2.1216e+00,\n",
      "        -2.9138e+00,  1.2732e+00, -2.4830e+00, -1.3255e-01,  4.5061e+00,\n",
      "         1.4901e+00, -1.4259e+00, -7.7004e-01,  2.1516e+00, -2.0794e-01,\n",
      "        -1.0130e+00,  6.0324e-01,  4.6931e+00,  1.2177e+00, -2.6324e+00,\n",
      "        -6.3279e-01, -1.3210e-01, -2.3972e+00, -2.7177e-01, -5.6907e-02,\n",
      "        -2.4676e-01,  2.7084e+00, -2.2951e+00, -2.3346e-01, -4.9179e-01,\n",
      "         1.9863e+00,  2.0801e+00,  9.6624e-01, -2.8882e-01, -1.8645e+00,\n",
      "        -1.1173e+00,  1.1879e+00, -5.0202e-01,  4.1663e+00,  8.0114e-01,\n",
      "         5.7368e-01, -1.7107e+00,  7.1749e-01, -2.9512e+00,  2.9074e+00,\n",
      "         5.0601e+00,  1.7554e+00,  1.6833e+00,  1.0581e+00, -3.3548e+00,\n",
      "         4.4232e-01,  1.0224e+00,  4.1240e-01, -1.1914e+00,  2.7794e+00,\n",
      "        -1.3921e+00, -3.8096e-01, -1.2059e+00,  7.7203e-01, -9.8365e-01,\n",
      "        -6.2568e-01, -1.0098e+00, -8.5663e-02, -3.7489e+00, -3.9318e-01,\n",
      "        -2.2932e+00,  2.4425e+00, -2.1587e+00,  1.6898e+00,  1.7920e+00,\n",
      "        -2.5815e+00,  4.3247e+00, -4.3782e-01,  3.0275e+00,  2.6279e+00,\n",
      "        -1.0538e+00, -7.8979e-01,  3.3322e+00, -7.6419e-01,  7.3318e-01,\n",
      "        -3.7908e+00, -9.2892e-01,  1.6312e+00,  3.4028e+00,  3.9738e+00,\n",
      "        -2.5254e+00, -1.7242e+00,  3.3527e-01, -2.0278e+00,  6.3680e-03,\n",
      "        -9.1418e-01, -1.3545e+00, -4.6417e-01, -1.0802e+00, -1.5558e+00,\n",
      "        -7.0824e-02, -1.0554e+00,  5.7332e-01, -2.4590e+00, -9.4340e-01,\n",
      "        -1.4954e+00,  2.2502e+00, -5.8784e-01, -2.6049e+00,  8.7423e-01,\n",
      "        -2.2104e+00, -1.1422e+00,  1.1854e+00,  2.0680e+00,  4.3520e+00,\n",
      "        -2.3662e+00, -1.3672e+00, -7.3689e-01,  5.1172e-01,  6.2378e-01,\n",
      "        -5.1895e-01, -5.0160e-02, -1.0345e+00,  1.4348e+00, -2.9601e+00,\n",
      "         3.6939e+00,  4.8391e-01, -3.4905e+00,  1.7402e+00, -2.2127e-01,\n",
      "        -3.1101e-01, -2.5191e+00,  2.6280e+00,  2.6723e+00,  1.9541e+00,\n",
      "        -9.1360e-01,  6.5449e-03,  2.1262e+00, -3.0383e-02, -6.8174e-01,\n",
      "         1.2667e+00,  7.9915e-01, -1.1198e+00,  1.5365e+00, -9.3459e-01,\n",
      "         2.3606e-01,  3.3827e+00,  1.2577e+00, -6.5397e-01, -2.5375e+00,\n",
      "         1.8773e+00, -3.2815e-01,  3.1128e+00, -7.1975e-02, -2.7126e+00,\n",
      "        -4.8994e-01, -9.6087e-01, -3.4320e+00,  1.9582e+00, -1.5981e+00,\n",
      "        -1.5757e+00, -3.8532e+00,  1.3806e+00, -4.4936e-01, -1.7166e+00,\n",
      "        -5.6149e-01,  4.5885e-01,  4.4410e+00, -2.1271e+00, -1.3519e+00,\n",
      "         1.9708e+00, -2.0785e+00,  1.8795e+00, -2.5302e+00, -1.3270e+00,\n",
      "         1.2235e+00,  8.5369e-01,  5.3434e-01, -3.6352e+00,  3.7295e+00,\n",
      "         1.0551e+00,  3.0675e-01,  2.4587e+00, -2.7130e+00,  3.3451e-02,\n",
      "        -3.9212e+00,  2.7920e-01,  1.3007e+00, -1.9354e+00, -5.0939e-01,\n",
      "        -2.3849e+00,  2.6098e+00, -1.8667e+00,  3.5805e-01,  7.4802e-01,\n",
      "         1.3464e+00, -7.0104e-01,  2.8609e+00,  2.2287e+00,  2.4249e+00,\n",
      "        -1.8952e+00,  1.4075e+00,  2.2891e+00,  5.8657e-01, -1.2307e+00,\n",
      "         1.1925e+00,  2.3088e+00, -1.3919e+00,  3.1714e+00,  4.2020e+00,\n",
      "        -8.6831e-01, -9.6223e-01, -8.1378e-01, -5.4980e-01, -3.9044e-01,\n",
      "         1.6345e-01, -3.2438e+00, -1.3292e+00, -2.9699e+00, -1.9890e-01,\n",
      "         1.4697e-02,  3.9244e+00, -2.2931e+00,  3.0993e+00, -3.5432e+00,\n",
      "        -1.1842e-01, -1.4854e+00, -2.2426e-01, -3.8843e+01,  1.4568e-01,\n",
      "        -2.1695e+00, -2.0376e+00,  1.4846e+00,  1.5033e+00, -3.3028e+00,\n",
      "         4.7021e-02, -4.0859e+00, -9.1017e-01,  2.5402e+00, -1.0490e+00,\n",
      "        -2.0775e+00, -1.7077e+00, -3.2719e+00, -2.3011e+00, -2.0642e-01,\n",
      "         5.2061e-01,  9.3573e-01,  1.9194e+00,  1.2673e+00, -2.4658e+00,\n",
      "        -6.2563e-01,  1.3811e+00, -4.8956e-01,  4.4891e-01, -1.1172e+00,\n",
      "         1.6996e+00, -3.5397e+00,  7.4007e-01,  2.9636e-01,  1.1385e+00,\n",
      "         3.9201e-01,  4.9024e-01,  1.1233e+00, -1.1510e+00,  3.3322e+00,\n",
      "         1.6744e+00,  1.8693e-01,  8.5295e-01, -2.8051e-01, -4.2810e-01,\n",
      "         1.1052e+00,  5.1709e+00,  7.5782e-01, -9.6045e-01,  1.7018e+00,\n",
      "         1.1635e+00, -1.5027e-01,  3.0553e+00, -1.1173e+00,  4.4519e-01,\n",
      "         8.4082e-01, -6.6676e-01,  4.1944e-01,  2.1966e+00,  1.2925e+00,\n",
      "         7.1266e-01,  2.0510e+00, -4.3975e-01,  1.6634e+00,  5.9498e-01,\n",
      "        -1.2662e+00,  4.7814e+00, -1.1031e+00,  2.9974e-01, -2.3212e+00,\n",
      "         7.5421e-02,  1.8144e+00,  3.3233e+00,  5.4776e-01,  6.5463e-01,\n",
      "         2.5459e+00, -1.5903e+01,  1.6261e+00, -1.4627e+00, -3.8428e-01,\n",
      "        -1.7278e+00, -5.2168e-01,  3.3323e+00, -9.8218e-01, -6.3423e-01,\n",
      "        -6.0069e-01, -2.2164e-02, -2.0362e+00, -1.7215e+00, -1.9210e+00,\n",
      "        -1.0253e+00, -2.1700e+00,  8.2987e-01, -5.2602e-01, -9.6289e-01,\n",
      "        -7.5984e-01, -1.9178e+00,  2.1882e+00,  7.1905e-01, -2.0163e-01,\n",
      "        -8.4624e-01, -5.7095e-02,  2.7846e+00, -6.5959e-02, -4.1384e+00,\n",
      "        -2.3459e+00, -1.3877e+00,  1.3276e-01, -7.6659e-01,  3.1228e+00,\n",
      "         4.1869e+00, -4.7938e-01, -1.9692e-01,  2.7769e+00,  4.1553e+00,\n",
      "         1.6137e+00,  2.6517e+00,  1.0174e+00,  1.3974e+00, -4.4871e+00,\n",
      "         2.0450e+00, -1.1704e-01, -3.1527e+00, -2.0975e+00,  2.0279e+00,\n",
      "         1.0298e+00, -2.2793e+00,  7.3812e-01, -1.1271e+00, -2.2565e+00,\n",
      "        -3.5774e+00, -6.2183e-01,  3.4656e-01,  2.7363e+00, -1.1029e+00,\n",
      "         8.3536e-01, -6.5560e+00, -9.3032e-02, -2.4322e+00,  4.6658e-01,\n",
      "         2.8623e-01, -3.2118e-01, -6.6362e-02,  2.3653e+00,  1.0599e+00,\n",
      "        -1.5212e+00, -7.5060e-01, -1.4192e-01,  1.3795e+00, -1.4992e+00,\n",
      "        -1.6377e+00, -9.8420e-01,  3.2530e+00, -1.3466e+00, -7.6989e-01,\n",
      "         4.6125e-01, -3.1801e+00, -2.3371e-01,  6.7207e-01, -1.0134e+00,\n",
      "         1.4349e+00, -6.8505e-01,  2.1669e+00, -1.2291e+00,  6.7073e-01,\n",
      "        -8.4175e+00, -2.6209e+00, -1.3057e+00,  6.4773e-01, -5.5730e-02,\n",
      "        -9.0235e-01, -2.2480e-01,  9.8160e-01, -5.8075e-01,  4.8897e-01,\n",
      "        -7.2453e-01,  3.5044e-02, -1.0978e+00,  1.8727e+00,  2.5663e+00,\n",
      "        -1.2918e+00,  8.3515e-01,  3.9047e-01, -1.6641e+00,  2.7725e+00,\n",
      "        -3.6219e+00,  2.6915e+00, -5.1089e+00, -1.9148e+00,  1.1302e+00,\n",
      "        -2.2625e+00,  1.6523e+00,  2.3256e+00,  1.3632e+00, -2.4039e+00,\n",
      "        -3.0303e-01, -2.8618e+00,  2.4393e-01,  1.2086e+00,  1.3932e+00,\n",
      "         8.7038e-01,  7.8126e-01,  1.0309e+00, -4.3074e+00, -8.4438e-02,\n",
      "        -2.3749e+00,  1.4570e-01, -2.4714e+00,  2.7024e+00,  5.7523e-01,\n",
      "         4.7158e-01, -3.2472e+00,  1.8939e+00,  3.5556e+00,  2.3850e+00,\n",
      "         3.0609e-01,  2.4011e+00,  2.6133e+00, -2.5385e+00,  7.3885e-01,\n",
      "         1.8413e+00,  2.5592e+00, -6.7763e-01, -1.2413e+00, -1.0890e+00,\n",
      "        -2.1520e+00,  7.4624e-01, -8.6352e-02,  1.3187e+00,  1.4472e+00,\n",
      "        -1.0910e+00,  2.5226e+00, -1.0872e+00,  8.6420e-01,  1.8444e-01,\n",
      "         1.6291e+00,  4.0422e-01,  1.7044e+00, -8.0269e-01,  1.3850e+00,\n",
      "         3.6388e+00, -2.6469e+00,  2.6715e+00,  3.7495e+00, -2.5365e+00,\n",
      "        -9.8589e-02, -8.7013e-01, -3.2210e+00, -9.7513e-01, -6.2290e-01,\n",
      "        -2.7193e+00,  9.1660e-02,  2.1230e-01, -3.7306e+00, -2.1857e+00,\n",
      "         2.3143e+00, -2.3470e+00, -2.2115e+00, -2.7389e+00,  1.0497e+00,\n",
      "         1.1267e+00,  2.5008e-02,  9.4952e-01, -3.8280e-01,  3.4253e+00,\n",
      "        -2.2240e+00, -2.2016e+00, -2.0015e+00, -1.7267e+00,  1.0943e-01,\n",
      "        -1.2334e+00,  6.6625e-01, -1.1756e+00,  9.5097e-01,  4.8811e-01,\n",
      "         2.9275e-01,  2.8029e+00, -1.3984e+00, -4.0839e+00, -1.7689e+00,\n",
      "         2.3809e+00, -2.3249e+00, -7.6055e-01, -1.3904e+00,  2.4208e-01,\n",
      "         5.7204e-02,  8.1738e-01, -4.4108e-01,  4.8795e-01, -1.1806e+00,\n",
      "        -1.1542e+00, -5.0108e-01,  3.9281e-01, -2.5865e+00, -3.2122e-01,\n",
      "         7.5953e-01, -2.6472e-02, -7.3149e-01,  1.2317e+00, -1.9289e+00,\n",
      "         6.5759e-01,  5.6661e-01,  1.1239e+00, -7.8657e-01,  1.7610e+00,\n",
      "         1.1652e+00, -1.1731e+00,  8.0767e-01,  1.9876e+00, -9.2552e-01,\n",
      "        -2.5148e-01,  1.4551e-01,  2.2062e+00,  1.4372e+00, -1.8162e+00,\n",
      "         8.6803e-01,  2.2483e+00, -1.7069e+00,  2.5622e+00,  2.5089e+00,\n",
      "         1.3281e+00,  2.3671e+00, -1.3330e+00,  1.3449e+00, -1.8714e-01,\n",
      "         2.1986e+00,  5.2826e-01,  1.7032e+00, -1.7536e+00,  4.3135e-01,\n",
      "        -7.4966e-01,  6.5390e-01, -2.7119e+00,  3.2073e-01,  1.1890e+00,\n",
      "         3.1399e-01,  1.3699e+00, -3.9641e-02,  1.1524e-01,  1.3774e+00,\n",
      "        -1.0366e+00,  1.5445e+00,  2.5705e+00,  2.1906e-01, -1.4782e+00,\n",
      "        -2.6640e+00,  1.5548e+00,  1.5182e+00,  7.5167e-01, -4.2521e+00,\n",
      "         1.3876e+00, -3.3335e+00, -3.0665e+00, -4.2737e-01,  1.0425e+00,\n",
      "         1.6502e+00, -1.6892e+00, -3.7552e+00, -3.7478e+00,  4.1241e+00,\n",
      "        -2.1999e+00, -5.3058e-01,  2.1239e+00, -2.1159e+00,  4.5447e+00,\n",
      "         1.2752e-01,  2.5442e+00,  2.8137e+00, -1.4861e+00, -6.3703e-01,\n",
      "         1.7973e+00,  1.6878e+00,  7.3871e-01, -1.7090e-01, -2.4189e+00,\n",
      "         8.6207e-01,  1.7603e+00,  2.4328e+00,  1.7358e+00, -1.1350e-01,\n",
      "        -3.3133e+00, -2.4275e+00,  7.4489e-01, -1.4550e+00, -6.0605e-01,\n",
      "        -1.0208e+00, -1.3470e+00,  2.1385e+00,  3.0793e-01, -3.0855e+00,\n",
      "        -1.2012e+00,  5.1909e-01,  5.5669e-01,  1.2076e+00,  8.3606e-03,\n",
      "        -1.8629e+00, -8.1781e-01, -3.0291e+00, -3.3750e+00, -1.4716e+00,\n",
      "        -5.1843e-02, -2.7813e-01, -6.6332e+00, -1.4670e+00,  4.5999e+00,\n",
      "        -1.5781e-01,  1.6095e+00,  2.4900e+00,  1.6448e+00, -6.8688e-01,\n",
      "         2.1541e+00, -2.8605e+00, -2.2185e+00,  1.2579e+00, -2.7720e+00,\n",
      "         1.9091e+00,  3.3794e+00, -4.2844e+00, -9.7329e-01, -3.3324e+00,\n",
      "        -6.2705e-01,  1.0132e+00, -8.9910e-01, -1.7621e+00, -1.3645e+00,\n",
      "         2.4210e+00,  8.9568e-02, -1.0698e+00, -1.6260e+00, -4.7147e-01,\n",
      "         1.4655e+00,  4.4944e+00,  1.3605e+00, -3.4434e+00,  1.7015e+00,\n",
      "        -1.6226e+00, -8.1945e-03, -7.1955e-01, -8.3347e-01,  2.2439e+00,\n",
      "         1.8832e+00,  2.8707e+00, -1.2744e+00,  1.9696e+00,  4.6978e-01,\n",
      "         3.5538e-01,  1.0276e+00, -3.7401e+00, -1.8760e+00, -2.2142e+00,\n",
      "        -2.3089e+00, -5.6477e+00,  2.6180e-02,  5.6934e+00, -4.9790e+00,\n",
      "        -3.6477e+00,  1.3763e+00, -1.1365e+00, -1.2328e+00,  2.7176e+00,\n",
      "        -1.9685e+00, -3.2496e+00,  9.6209e-01])\n",
      "abhorrent\n",
      "Saved the embedding for abhorrent.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', '##omi', '##nable'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 768\n",
      "tensor([-2.4412e-01,  3.6524e+00,  1.0266e+00,  7.7521e-01, -1.1941e+00,\n",
      "        -1.3787e+00,  1.0720e+00,  4.4871e-01,  1.4765e+00, -2.4590e+00,\n",
      "        -1.2064e+00,  1.4762e+00,  5.7491e+00, -5.2510e-01,  7.2839e-02,\n",
      "         3.8979e+00,  3.2738e+00,  4.4911e-02, -1.7781e+00, -4.7455e+00,\n",
      "         1.1707e+00,  1.6170e-01,  1.2482e+00, -1.5398e+00,  8.3421e-01,\n",
      "         1.2716e+00, -6.7534e-01,  4.9867e-01,  2.9359e+00,  2.5944e+00,\n",
      "        -2.3070e+00, -1.0377e+00,  5.7682e-01, -4.1282e-01, -1.6022e+00,\n",
      "        -7.4054e-03, -3.9714e+00,  2.2868e+00, -1.3709e+00, -1.4296e+00,\n",
      "        -3.0254e+00, -3.6659e+00,  1.3646e+00,  4.5213e+00, -6.3216e-01,\n",
      "         4.0550e+00, -1.1022e+01,  1.1411e+00,  5.0968e-01, -2.3063e+00,\n",
      "        -4.1313e-01, -1.3264e+00, -8.0042e-01,  2.9774e+00, -1.1163e+00,\n",
      "         2.1150e+00,  1.0282e+00, -1.6231e+00,  1.6143e+00,  1.3194e-01,\n",
      "         3.9308e-01,  1.5380e+00,  5.4201e-01, -2.1342e+00, -1.5501e-01,\n",
      "         1.6120e+00,  8.5355e-01, -2.0588e+00, -1.4100e+00, -1.5692e+00,\n",
      "         1.5846e+00, -1.3544e-01, -1.8295e+00, -7.7805e-01, -4.3022e-02,\n",
      "         2.1377e-01, -6.3582e-01,  3.0654e+00, -2.3905e-01,  2.7136e+00,\n",
      "         6.6610e-02, -1.2237e+00,  4.0328e+00, -1.3634e-01,  1.4409e+00,\n",
      "         3.2213e+00,  2.7819e+00, -2.7023e+00,  7.7921e-02,  1.5206e-01,\n",
      "        -2.5027e+00,  1.8364e+00, -4.7720e+00,  1.8671e-01,  2.9919e+00,\n",
      "         1.4151e+00, -2.5178e+00, -3.0805e+00,  2.1240e+00, -1.7599e-01,\n",
      "        -1.0413e+00,  5.9545e-01,  3.1999e+00,  2.1189e-01, -4.1839e+00,\n",
      "         2.0835e-02,  2.5487e-01, -5.5733e+00,  7.8787e-03, -9.8536e-01,\n",
      "        -1.3381e+00,  1.8475e+00, -1.3571e+00, -1.0186e-01, -2.0785e+00,\n",
      "         1.7362e+00,  1.2423e+00,  1.3393e-01, -4.9406e-01, -2.9914e+00,\n",
      "        -3.0486e+00,  1.3662e+00, -8.6418e-01,  2.5572e+00,  3.4726e-01,\n",
      "         2.3425e+00, -9.6081e-01, -2.9958e-01, -2.2200e+00,  1.0652e+00,\n",
      "         2.2175e+00,  7.1700e-01,  1.3817e+00,  1.4047e+00, -3.1297e+00,\n",
      "         1.1284e+00,  1.7185e-01,  1.1790e+00, -1.3405e+00,  6.5330e-01,\n",
      "        -4.8627e-01, -7.8178e-01, -8.8196e-01,  6.4904e-01, -6.7991e-01,\n",
      "         2.4256e+00, -3.0716e+00,  6.0223e-01, -2.5828e+00,  3.2026e+00,\n",
      "        -3.6435e+00,  1.2606e+00, -1.4483e+00,  2.3735e+00,  1.6825e+00,\n",
      "        -1.8264e+00,  3.1173e+00, -1.9320e-01,  1.6038e+00,  9.7178e-01,\n",
      "        -3.1154e+00,  2.6315e-01,  3.9021e+00, -1.7652e+00,  6.8663e-01,\n",
      "        -4.5071e+00,  4.5069e-01,  1.3079e+00,  2.2692e+00,  4.0449e+00,\n",
      "        -2.7233e+00, -2.0522e+00,  1.4962e+00, -1.6595e+00,  1.8803e+00,\n",
      "        -5.4412e-01, -2.4702e+00, -1.4932e+00, -1.1665e+00,  1.2674e+00,\n",
      "        -9.4162e-01, -2.2847e+00,  1.0690e+00, -3.3838e-01,  8.3712e-01,\n",
      "        -2.0145e+00,  1.7350e+00, -2.1450e-01, -2.2643e+00,  1.9388e+00,\n",
      "        -6.5175e-01, -1.2856e+00,  6.3932e-01,  2.9007e+00,  3.3224e+00,\n",
      "        -1.4602e+00, -1.2790e+00,  1.2316e+00,  1.5547e+00,  3.6545e+00,\n",
      "        -1.0012e+00, -2.7369e-01, -2.2074e+00, -1.6857e+00, -1.1848e+00,\n",
      "         3.9789e+00,  9.2305e-01, -1.5294e+00,  1.1650e+00, -1.3924e+00,\n",
      "        -4.0189e-01, -1.5668e+00,  3.8882e+00,  3.4047e+00, -5.5225e-01,\n",
      "        -1.5881e+00, -9.5361e-01,  1.0173e+00,  4.3789e-03, -2.8762e+00,\n",
      "         1.1409e+00,  1.3455e+00, -2.5913e-01,  9.9874e-01, -1.8399e+00,\n",
      "         4.3087e-01,  4.2880e+00, -2.9160e-01,  2.4961e-01, -2.5664e+00,\n",
      "         2.0013e-01, -1.3774e+00,  1.9514e+00,  1.2707e+00, -3.2470e+00,\n",
      "        -2.0095e+00,  6.1148e-01, -3.9926e+00,  8.3307e-01, -3.8623e+00,\n",
      "        -1.0692e+00, -4.2061e+00,  2.5094e-02, -1.0385e-01, -1.4269e+00,\n",
      "         4.5492e-01,  2.7832e+00,  4.5082e+00, -2.9646e+00, -1.6045e+00,\n",
      "         1.5080e+00, -1.7085e+00,  2.1698e+00, -3.0226e+00, -2.5687e+00,\n",
      "         1.9709e+00,  1.4763e+00, -8.0791e-01, -3.8258e+00,  1.5783e+00,\n",
      "         7.0224e-01,  8.2958e-01,  1.6997e+00, -1.2410e+00,  5.1628e-01,\n",
      "         2.8209e+00, -9.3640e-01,  4.6197e-01, -4.4344e-01, -3.5285e-01,\n",
      "        -2.4973e+00,  1.6404e+00, -1.2034e+00, -8.2166e-01, -6.7176e-01,\n",
      "         4.3403e-01, -8.4178e-02,  2.0067e+00,  2.0943e+00,  2.3081e+00,\n",
      "        -1.4666e-01,  1.7002e+00,  1.6568e+00, -1.0076e+00,  1.9391e+00,\n",
      "        -9.5423e-01,  3.0852e+00, -2.3393e+00,  2.0526e+00,  1.6819e+00,\n",
      "        -7.5205e-01,  1.8777e-01, -1.6242e+00,  1.3881e+00,  1.5401e+00,\n",
      "         1.2840e+00, -9.5686e-01, -6.0891e-01, -9.2042e-01,  5.1039e-01,\n",
      "        -3.6621e-01,  6.6783e+00, -1.3717e+00,  9.9203e-01, -8.6111e-01,\n",
      "        -3.7528e-01,  6.8860e-01, -3.1597e+00, -3.9117e+01, -1.3543e-01,\n",
      "        -1.8884e+00, -6.5449e-01,  9.3322e-01,  7.5701e-01, -1.4811e+00,\n",
      "        -1.1809e+00, -4.6159e+00, -9.3140e-01,  1.9458e+00, -4.9707e-01,\n",
      "        -3.9122e+00, -6.8977e-01, -2.9341e+00, -2.6617e+00,  1.6819e+00,\n",
      "         8.5845e-01,  9.1439e-01,  1.1099e+00,  2.1963e-01, -3.6527e+00,\n",
      "        -1.4900e-01,  4.4230e-01,  3.6822e-02,  1.4312e+00, -2.5633e+00,\n",
      "         1.1794e+00, -3.7800e+00,  2.4004e+00, -7.2697e-01, -7.3104e-01,\n",
      "         1.1876e+00,  6.8960e-01,  1.9297e+00, -2.7584e-01,  2.8049e+00,\n",
      "         7.7177e-01,  1.5535e+00,  2.0570e+00, -1.4505e+00, -1.3920e+00,\n",
      "         3.5885e-02,  3.1010e+00, -7.8345e-01, -2.5965e-02, -8.7707e-01,\n",
      "        -8.7245e-01,  7.9561e-01, -2.3676e-01, -1.1934e+00,  6.2247e-01,\n",
      "         2.9578e+00, -2.8262e+00,  6.4199e-01,  3.3330e+00, -3.0488e-01,\n",
      "         3.0244e+00,  2.5541e+00,  4.8851e-01,  7.9904e-01,  2.6151e+00,\n",
      "        -2.5783e+00,  3.0901e+00, -2.1142e-01, -4.1543e-01,  7.0769e-01,\n",
      "         1.8598e+00,  5.8919e-01,  3.1114e+00, -1.8932e-01, -1.3603e+00,\n",
      "         3.7430e+00, -1.6510e+01, -3.0107e-01, -6.1929e-01, -8.0805e-01,\n",
      "        -2.3055e+00,  3.0126e-01,  1.8420e+00,  1.5882e-01, -6.5155e-01,\n",
      "        -4.8969e-01,  9.0213e-01,  2.6418e-02,  1.9004e-01, -2.5886e+00,\n",
      "        -1.1712e+00,  2.2320e+00,  2.0589e+00,  9.5363e-01, -2.0447e-01,\n",
      "         4.6172e-01, -2.5476e+00,  7.4264e-01, -4.1850e-03, -5.2207e-01,\n",
      "         2.6197e-01, -5.8372e-01,  3.0775e+00,  1.9139e+00, -6.5256e-01,\n",
      "        -2.7867e+00, -1.0039e+00, -6.2762e-01,  1.8419e+00,  3.0136e+00,\n",
      "         4.1184e+00, -2.1920e+00, -3.2628e-01,  2.5451e+00,  9.7117e-01,\n",
      "         1.3018e+00,  9.6463e-01,  1.5876e+00,  1.6629e+00, -4.5897e+00,\n",
      "         1.0370e+00, -1.3073e+00, -5.2141e+00, -2.5172e+00,  8.2218e-01,\n",
      "         1.6069e-01, -1.1041e+00, -2.2542e-01, -2.0496e+00, -4.5092e+00,\n",
      "        -4.8828e+00, -8.9017e-01,  1.0469e+00,  3.7520e+00,  2.4806e-01,\n",
      "         5.5015e-01, -4.0583e+00, -9.3695e-01, -2.2019e+00,  1.0875e+00,\n",
      "        -1.9854e+00, -5.1970e-01, -3.9933e-01,  2.6629e+00,  1.1447e+00,\n",
      "        -9.2641e-02,  1.3036e+00,  9.4174e-01,  2.7130e+00, -7.3789e-01,\n",
      "        -1.2636e+00, -7.9557e-01,  1.7966e+00,  4.2523e-01, -1.9294e+00,\n",
      "         1.7456e+00, -3.9024e+00, -1.7183e-01,  1.9643e+00,  1.3722e+00,\n",
      "        -1.1801e-01,  6.6558e-02,  3.6324e+00, -3.9739e-01, -7.5630e-01,\n",
      "        -8.2474e+00,  6.0786e-01,  7.2174e-01, -3.5168e-01, -9.5306e-01,\n",
      "         6.4460e-01, -3.9421e-02,  4.7742e-01, -1.4345e+00,  6.6096e-01,\n",
      "        -4.5756e-01,  6.4477e-01, -1.5735e+00,  2.3897e+00,  2.7522e+00,\n",
      "        -1.4839e+00,  1.1307e+00, -4.4273e-01,  6.9966e-01,  2.7008e+00,\n",
      "        -2.9436e+00,  1.9928e+00, -5.7969e+00, -5.7165e-01,  3.3583e-01,\n",
      "        -3.2222e+00,  1.5092e+00,  2.8959e+00,  2.5042e-01, -2.7197e+00,\n",
      "        -1.8635e+00, -1.8694e+00,  3.7844e+00,  9.3477e-01,  2.1479e+00,\n",
      "         2.4313e+00, -3.1418e-01,  1.5646e+00, -2.2825e+00,  8.5263e-01,\n",
      "        -3.3660e+00,  1.8549e+00, -7.2003e-01,  1.4405e+00,  1.4562e+00,\n",
      "         1.5561e+00, -2.8756e+00,  3.9108e+00,  4.1682e+00,  1.8313e+00,\n",
      "         5.0230e-01, -1.4613e+00,  2.3165e+00, -3.0796e+00, -2.6835e-01,\n",
      "         1.9418e+00,  2.4093e+00,  4.7949e-01, -2.2271e+00, -1.5535e+00,\n",
      "        -1.7171e+00, -5.7790e-01, -3.2717e-01,  1.6428e+00,  1.3100e+00,\n",
      "        -2.5214e-01,  1.6027e+00, -3.0753e+00, -1.3462e+00, -1.0833e+00,\n",
      "         5.4417e-01,  1.8452e-01,  1.2295e+00, -2.7071e-01, -1.0149e+00,\n",
      "         2.2192e+00, -1.0267e+00,  2.5988e+00,  3.3935e+00, -2.2144e+00,\n",
      "         1.1748e+00, -1.3576e+00, -1.7641e+00, -1.4064e+00,  9.8996e-01,\n",
      "        -2.2788e+00, -1.9771e-01, -4.5161e-02, -1.7810e+00, -3.1493e+00,\n",
      "         6.3865e-01, -1.9923e+00, -3.0104e+00, -1.0119e+00,  2.0530e+00,\n",
      "         1.1178e+00, -3.7541e-01, -9.6636e-02,  9.9057e-01,  2.3867e+00,\n",
      "        -1.7526e+00, -2.4196e+00, -9.7820e-01, -1.6793e+00,  1.9152e+00,\n",
      "        -2.2087e+00,  2.6377e+00, -1.3882e+00,  9.7403e-01,  2.8677e-01,\n",
      "         1.6572e+00,  3.8609e+00,  1.1208e+00, -2.9154e+00, -1.6036e+00,\n",
      "         1.8903e+00, -3.5159e+00, -8.8682e-01, -7.1372e-01, -9.1573e-01,\n",
      "        -9.9384e-01, -6.8308e-01,  1.0749e+00,  1.7545e+00, -2.0625e+00,\n",
      "        -2.9060e+00,  2.1745e+00, -3.6304e-02, -2.1895e+00, -7.4072e-01,\n",
      "        -1.0816e+00, -1.6962e+00, -2.9237e-01,  1.2317e+00, -2.0818e+00,\n",
      "        -3.7303e-01,  1.6686e+00,  3.2322e+00, -2.1178e+00,  1.5078e+00,\n",
      "         3.3094e-01, -2.1618e+00, -1.0500e+00,  1.2539e+00, -2.7643e+00,\n",
      "        -1.8366e+00,  4.7088e-01,  2.1051e+00,  6.3501e-01, -2.8769e+00,\n",
      "         6.0482e-01,  2.0864e+00, -1.4642e+00,  2.7833e+00,  2.1851e+00,\n",
      "         3.9976e-01,  2.3839e+00, -1.2111e+00,  7.3114e-01, -1.0015e+00,\n",
      "         2.4940e+00,  6.0179e-01,  2.7834e+00, -2.0010e+00,  4.6399e-01,\n",
      "         1.0228e-01,  1.1392e+00, -1.4496e+00, -2.8018e-01, -1.5248e+00,\n",
      "         6.1525e-01,  4.7983e-01,  9.6776e-01,  9.8167e-01,  1.3200e+00,\n",
      "        -1.3203e+00, -7.8763e-02,  1.7491e+00,  1.0175e+00, -1.3681e+00,\n",
      "        -2.2881e+00,  3.2279e-01,  2.4865e+00, -8.2936e-02, -3.0565e+00,\n",
      "        -1.9107e-01, -2.7907e+00, -2.2393e+00,  1.4558e-01,  9.6477e-01,\n",
      "         1.3988e+00, -2.3504e+00, -1.3469e+00, -1.8652e+00,  2.7325e+00,\n",
      "        -1.7257e+00, -1.0191e+00,  1.1403e+00, -3.3028e+00,  5.1854e+00,\n",
      "         8.3353e-01,  5.2709e+00,  2.4130e+00, -1.9268e+00,  5.5252e-02,\n",
      "         1.2244e+00, -3.1517e-01,  1.8622e+00, -9.3084e-01, -7.5830e-01,\n",
      "         1.2091e+00,  2.8887e+00,  2.0718e+00,  2.0702e+00, -1.2004e+00,\n",
      "        -1.9512e+00, -2.5391e+00,  1.4595e+00, -9.0377e-01,  4.7342e-01,\n",
      "         2.2286e+00, -5.6619e-02,  3.0405e+00,  7.8625e-01, -1.7275e+00,\n",
      "        -1.6656e+00,  1.2969e+00,  4.3396e-01,  2.2272e+00, -2.2441e+00,\n",
      "        -2.0768e+00, -1.1035e+00, -1.3762e+00, -4.4696e+00,  1.2265e+00,\n",
      "        -4.7955e-01,  1.1137e+00, -3.3623e+00,  7.1066e-01,  4.2776e+00,\n",
      "        -6.9172e-01,  4.6025e-01,  2.7116e+00,  1.7234e-01,  2.3696e-02,\n",
      "         1.2094e+00,  8.3370e-01, -4.6564e-01, -1.9511e-01, -4.9625e+00,\n",
      "        -8.6679e-03,  3.2440e+00, -2.3544e+00, -1.1451e+00, -1.5277e+00,\n",
      "        -1.9752e+00,  1.6991e-01, -2.8495e+00, -3.3162e+00,  4.6612e-01,\n",
      "         2.4111e+00, -8.7320e-02, -1.8935e+00, -1.0837e+00,  1.1585e+00,\n",
      "         2.0420e+00,  4.0713e+00,  7.0945e-01, -3.6623e+00,  1.9236e+00,\n",
      "        -1.0129e+00,  1.7445e+00, -9.4192e-01, -1.0640e+00,  8.4135e-01,\n",
      "         5.6901e-01,  2.4511e+00, -2.6691e+00,  1.0136e+00, -1.0104e+00,\n",
      "         7.2929e-01,  1.1696e-01, -2.2415e+00, -1.6972e+00, -3.1064e+00,\n",
      "        -2.9563e+00, -5.7647e+00, -1.3065e+00,  4.8372e+00, -3.8534e+00,\n",
      "        -3.2710e+00,  1.5820e+00, -3.5759e-01, -1.4692e+00,  1.8990e+00,\n",
      "        -2.5560e+00, -2.4363e+00,  1.0885e+00])\n",
      "abominable\n",
      "Saved the embedding for abominable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', '##ound'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 768\n",
      "tensor([ 3.7244e-01,  1.8365e+00,  8.1073e-01,  1.3501e+00, -2.0478e-01,\n",
      "        -2.9591e-01,  3.1523e-01,  1.3150e+00,  1.0632e+00, -1.7436e+00,\n",
      "        -1.3497e+00,  6.6125e-01,  2.7406e+00,  2.1021e-01,  1.3746e+00,\n",
      "         2.6388e+00,  1.4325e+00,  1.4436e+00, -1.1495e+00, -3.3246e+00,\n",
      "        -1.1052e-01,  1.1951e+00,  9.3251e-01, -1.6217e+00, -3.4235e-01,\n",
      "         1.1569e+00, -7.2331e-02, -1.2717e+00,  1.7234e+00, -7.1375e-01,\n",
      "        -2.6071e+00,  1.4657e+00,  7.1340e-01,  3.0259e-01, -2.6930e+00,\n",
      "        -1.6348e+00, -3.2435e+00,  5.5825e-01,  1.5903e+00, -2.1145e+00,\n",
      "        -3.5911e+00, -3.4565e+00,  1.4356e+00,  2.3069e+00, -8.5711e-01,\n",
      "         2.0550e+00, -9.8743e+00,  2.5320e+00,  2.7361e+00,  8.2331e-01,\n",
      "         1.0359e+00, -8.4824e-01, -2.5239e+00,  4.1485e-01,  6.7848e-01,\n",
      "         1.8896e+00,  1.3897e+00, -3.1456e+00,  2.1544e+00,  1.0054e+00,\n",
      "        -2.7669e-02,  2.8430e+00,  4.3898e-01,  4.7059e-01, -7.9780e-01,\n",
      "         3.7857e-01, -1.5723e+00, -1.5985e-01, -4.4776e+00, -1.4038e+00,\n",
      "         2.1540e+00, -5.3038e-01, -2.0825e+00, -9.3251e-01, -7.4613e-01,\n",
      "        -1.1938e-01,  4.1100e-01,  3.4453e+00, -1.3365e+00,  2.5803e+00,\n",
      "         1.9272e+00, -1.0411e+00,  4.4094e+00,  1.6679e+00,  2.3041e+00,\n",
      "         1.5946e-01,  2.5248e+00, -2.8895e+00, -1.0217e+00, -4.7445e-01,\n",
      "        -1.8753e+00, -8.3072e-02, -1.1574e+00,  2.4335e-01,  2.4631e+00,\n",
      "        -3.1178e-01, -1.9926e+00, -2.8417e+00,  1.5614e+00, -9.2242e-02,\n",
      "         4.9569e-01, -4.4033e-01,  2.2249e+00, -3.8094e-01, -4.7812e+00,\n",
      "        -3.3370e-01, -1.0653e+00, -2.6639e+00,  1.0850e+00, -2.9606e-01,\n",
      "        -3.0534e-01,  1.3508e+00, -1.7156e+00, -4.6879e-01,  1.5220e+00,\n",
      "         1.8897e+00,  8.8218e-01,  3.5320e-01,  4.2880e-01, -2.0017e+00,\n",
      "        -8.1273e-02,  1.5416e+00,  3.4886e-01,  2.9728e+00,  2.4377e+00,\n",
      "         5.0297e-01, -2.1081e+00, -1.2691e+00, -3.8672e+00,  2.9472e+00,\n",
      "         1.9825e+00,  1.7655e+00,  4.7406e-01,  1.7834e+00, -2.0113e+00,\n",
      "        -1.3509e+00,  5.4779e-02,  2.5857e+00, -6.8843e-01,  1.2334e+00,\n",
      "        -2.3469e+00,  5.9854e-01, -2.1745e+00,  1.3722e+00, -2.0070e+00,\n",
      "         6.9499e-01, -1.5934e+00, -5.2621e-01, -1.7022e+00,  1.7391e+00,\n",
      "        -3.4279e+00,  1.8162e+00, -6.3137e-01,  6.6492e-01,  1.7395e+00,\n",
      "        -1.3211e+00,  1.4504e+00, -1.0382e+00, -5.4062e-01,  1.9077e+00,\n",
      "        -8.8730e-01, -8.4471e-01,  3.4923e+00, -1.0662e+00,  1.0345e+00,\n",
      "        -3.6072e+00,  1.2367e+00,  5.2425e-01,  3.7596e+00,  2.0655e+00,\n",
      "        -9.6915e-01, -4.6862e+00, -8.4729e-01, -8.5519e-01,  1.2035e+00,\n",
      "        -4.5083e-01, -1.1819e+00, -1.9273e+00, -1.9070e+00,  1.5735e+00,\n",
      "        -5.7656e-01, -1.1380e+00,  2.9398e-01, -1.9967e+00,  5.9777e-01,\n",
      "         2.2306e-01, -1.1544e+00, -1.7442e+00, -1.4988e+00,  1.6653e-01,\n",
      "         8.2596e-01,  2.0502e-01, -1.0881e+00,  1.3381e+00,  3.9611e+00,\n",
      "        -1.3963e+00,  4.1814e-02,  1.4034e-01,  1.8119e+00,  2.9741e+00,\n",
      "        -3.4377e+00,  1.7643e-01, -6.3227e-01, -9.2191e-03, -1.6137e+00,\n",
      "         4.8804e+00, -5.8172e-01, -1.1825e+00,  8.9869e-02,  1.3045e+00,\n",
      "        -3.6679e-03, -1.8559e+00,  1.4974e+00,  2.5489e+00,  2.0236e+00,\n",
      "        -4.7831e+00,  9.2913e-01,  1.2866e-01,  5.6084e-01, -1.0087e+00,\n",
      "         1.3936e+00, -1.1986e+00,  1.2020e-01,  7.7944e-01,  5.9930e-01,\n",
      "        -3.6123e-01,  4.3984e+00,  1.4206e+00, -7.0037e-01, -2.4009e+00,\n",
      "         1.5685e+00,  3.1227e+00,  2.5019e+00,  6.2226e-02, -2.5931e+00,\n",
      "        -1.1090e+00,  1.4638e-02, -2.5188e+00,  3.7020e-01, -7.6300e-01,\n",
      "        -1.2235e+00, -9.1843e-01,  9.0425e-02, -9.2261e-01,  3.2131e-01,\n",
      "        -1.2006e+00,  2.8335e+00,  2.9740e+00, -4.0464e+00, -3.1330e-01,\n",
      "         1.5436e+00, -1.1182e+00,  1.3050e+00, -1.8717e+00, -1.3027e+00,\n",
      "         7.8296e-01,  1.1109e+00,  1.0868e+00, -2.6948e+00,  2.7379e+00,\n",
      "         2.0694e+00, -1.6821e+00,  4.6236e+00, -7.7143e-01,  8.2327e-01,\n",
      "        -7.7048e-01, -2.0070e+00,  2.4084e+00, -2.3097e+00, -1.1233e+00,\n",
      "        -1.7487e+00,  2.4094e+00, -2.3021e+00,  1.0279e+00,  1.8508e+00,\n",
      "        -2.3949e-01,  9.5071e-01,  7.7653e-01,  3.0281e+00,  2.8698e+00,\n",
      "        -1.2895e+00, -1.1789e+00, -1.0787e+00, -1.6790e+00, -3.7776e-01,\n",
      "         2.1309e-01,  1.1414e+00, -1.8271e+00,  2.3155e+00,  4.0669e+00,\n",
      "         1.5524e-01,  9.5830e-01, -4.2543e-03,  1.6470e-01,  6.9781e-01,\n",
      "        -6.1664e-02, -2.1504e+00,  1.9156e+00, -2.0652e+00, -3.6601e-01,\n",
      "         1.7574e+00,  5.3635e+00,  3.0659e-01,  1.3179e+00, -1.9444e+00,\n",
      "         4.9707e-02, -2.3288e+00,  8.3923e-01, -4.5102e+01, -1.5294e-01,\n",
      "        -2.1326e+00, -1.4033e+00,  6.0634e-01,  7.2129e-01, -1.4495e+00,\n",
      "         1.9829e-01, -4.8294e+00,  1.2088e-01,  8.2581e-01,  1.1015e+00,\n",
      "        -4.2200e+00, -6.7325e-01, -2.0889e+00, -2.5778e+00,  1.2539e+00,\n",
      "         1.0156e+00,  1.6971e+00,  1.9946e+00,  7.8126e-02, -3.0500e+00,\n",
      "         1.5496e+00, -5.6236e-02,  2.8282e+00,  6.9647e-01,  2.1554e-01,\n",
      "         1.3697e-01, -2.2968e+00,  4.3540e-01,  2.2317e-01,  1.3252e+00,\n",
      "         4.5507e-01, -1.8833e+00,  1.5978e+00, -6.4186e-01,  1.7708e+00,\n",
      "        -4.3571e-01, -1.1638e-01,  2.4401e+00, -1.3179e+00, -4.4472e-01,\n",
      "         9.5627e-01,  4.7248e+00, -5.8885e-01, -1.4328e+00,  2.3351e-01,\n",
      "         3.5128e-01,  5.5076e-01,  2.1480e+00, -1.1476e+00,  5.4936e-01,\n",
      "         3.6458e-01, -2.5421e+00,  2.4253e+00,  4.2655e+00,  1.7814e+00,\n",
      "         2.5778e+00,  2.2482e+00, -4.6643e-02,  1.7235e+00,  7.9366e-01,\n",
      "        -1.2783e+00,  3.1962e+00, -9.8706e-01,  8.5193e-01,  6.6117e-01,\n",
      "         3.1832e+00, -3.0593e-01,  3.0887e+00, -2.5851e+00,  9.8622e-02,\n",
      "         2.6872e+00, -1.4939e+01, -9.5216e-01, -3.0051e-01, -7.6541e-01,\n",
      "        -2.9748e+00, -9.2317e-02,  2.9231e+00, -2.3866e+00, -1.2045e-01,\n",
      "        -9.1531e-01,  4.6426e-01, -3.9238e+00, -3.5933e-01, -2.2363e+00,\n",
      "        -2.3871e+00, -1.0502e+00,  2.1522e+00,  2.0263e+00, -3.4717e+00,\n",
      "        -6.6761e-01,  1.0666e+00,  3.3977e+00,  1.3700e+00, -6.9965e-01,\n",
      "        -1.8071e+00,  5.5200e-02,  1.4501e+00,  4.5773e-01, -2.8140e+00,\n",
      "         2.3030e-01, -5.2282e-01, -1.2352e-01, -6.0827e-01, -4.4453e-01,\n",
      "         3.6153e+00, -5.7236e-01,  1.4450e+00,  1.9141e+00,  6.9120e-01,\n",
      "        -4.4926e-01,  1.2578e+00,  2.4070e+00,  1.0402e+00, -4.0121e+00,\n",
      "         4.1739e+00,  3.9654e-01, -3.0716e+00, -3.4629e+00,  2.2687e+00,\n",
      "        -8.1070e-01, -2.8739e+00, -6.6125e-01, -1.1510e+00, -9.2864e-01,\n",
      "        -3.2276e+00, -2.1702e+00, -3.1421e-01,  3.3876e+00, -9.3821e-01,\n",
      "         1.7777e-01, -7.4517e+00, -1.0746e+00, -1.3350e+00, -3.8506e-01,\n",
      "         5.2587e-01, -1.6166e-01, -1.5477e+00,  3.4462e+00, -4.1354e-02,\n",
      "         1.5830e+00,  5.4334e-01, -2.7329e-01,  2.8327e+00, -1.8510e+00,\n",
      "        -1.2871e+00, -2.5214e-01,  3.0958e+00, -1.0509e-01, -2.2837e+00,\n",
      "         1.2914e+00, -4.6232e+00,  7.7621e-01,  2.1533e+00, -1.2333e+00,\n",
      "         2.7497e+00, -3.1587e+00,  1.2005e+00, -2.1081e+00,  9.9667e-01,\n",
      "        -8.5012e+00, -1.2145e+00, -2.7534e-01, -7.1141e-01, -8.2248e-01,\n",
      "        -9.6586e-01,  8.3906e-01,  1.6403e-01,  8.5908e-01,  7.0601e-01,\n",
      "        -1.2691e+00,  6.2432e-01, -6.8772e-01,  2.6204e+00,  3.7640e+00,\n",
      "        -1.9484e+00,  9.8519e-01, -4.8953e-01, -1.4468e+00,  2.9358e+00,\n",
      "        -2.8875e+00,  2.4606e+00, -4.1738e+00, -4.2403e+00, -4.2508e-01,\n",
      "        -6.8480e-01,  2.5742e+00,  2.1852e+00,  8.7886e-01, -4.6265e-01,\n",
      "        -5.4999e-01, -2.1664e+00,  2.2724e+00,  2.1396e-01,  1.4458e+00,\n",
      "         5.5848e-01, -1.3288e+00,  1.9540e+00, -1.1747e+00,  1.1842e-01,\n",
      "        -1.8989e+00, -1.5973e+00, -6.4981e-01, -4.1657e-01,  2.3331e-01,\n",
      "        -7.6538e-02, -4.2235e+00,  3.1828e+00,  3.5714e+00,  3.6474e+00,\n",
      "        -2.7604e-01,  2.1601e+00,  3.0390e+00, -2.0186e+00,  2.0052e+00,\n",
      "         1.9307e+00,  3.2219e+00, -7.7059e-01, -1.6110e+00, -2.4638e+00,\n",
      "        -5.2733e+00,  1.4155e+00, -1.9761e+00,  1.0120e+00,  2.0966e+00,\n",
      "        -1.8406e+00,  2.5994e+00, -1.5351e+00,  1.6262e+00, -1.5258e+00,\n",
      "         1.0008e-01, -1.6165e-01, -1.1491e+00, -5.6848e-01,  1.9676e+00,\n",
      "         4.4411e+00, -2.1040e+00,  1.5312e+00,  3.6816e-01, -1.5191e+00,\n",
      "         1.0017e+00,  1.3288e+00, -2.2775e+00,  1.2206e+00,  7.1130e-01,\n",
      "        -3.2903e+00,  5.5131e-01,  1.6772e+00, -2.1224e+00, -4.5810e+00,\n",
      "         1.6808e+00, -2.1048e+00, -2.6726e+00, -1.0079e+00,  1.7528e+00,\n",
      "         3.6396e-01,  9.8132e-01,  1.9603e-01, -7.1428e-01,  2.8165e+00,\n",
      "        -1.9444e+00, -9.4459e-01,  1.0284e-01, -1.8113e+00,  2.0607e+00,\n",
      "        -7.5382e-01,  1.3101e+00, -1.6748e+00, -2.4075e-01, -6.3040e-02,\n",
      "        -5.1375e-01, -2.1314e-01,  7.8025e-02, -2.2023e+00, -4.0970e+00,\n",
      "         2.0781e+00, -3.2120e+00, -1.0810e+00,  1.3959e+00,  3.4679e-01,\n",
      "         1.5311e-01, -3.5506e-01, -1.2489e-01,  2.6585e-01, -3.3070e+00,\n",
      "        -2.4414e+00,  7.5786e-01,  1.5032e+00, -3.4906e+00,  3.7125e-01,\n",
      "         9.2958e-01, -1.4872e+00, -2.1920e+00,  2.5900e+00, -1.1999e+00,\n",
      "         3.7373e+00,  9.7463e-01, -1.6426e+00, -3.3900e+00,  4.9864e-01,\n",
      "         6.8593e-01, -1.8031e-01,  1.6543e-02,  8.1111e-01,  5.3043e-01,\n",
      "        -1.7544e+00, -2.2770e+00,  2.6570e+00, -1.5836e+00, -8.2586e-01,\n",
      "        -1.1868e-02,  1.9705e+00, -1.3786e+00,  8.0207e-02,  2.5919e+00,\n",
      "         2.1037e-01,  2.9751e+00, -1.7699e-02,  7.8067e-01, -7.7778e-01,\n",
      "         3.7091e+00,  6.6661e-01,  1.2265e+00, -2.0419e+00,  1.0543e-01,\n",
      "        -1.4630e-01,  6.3982e-01, -1.3189e+00,  6.8235e-01, -2.4662e+00,\n",
      "         1.7523e-01,  1.6375e+00, -8.3786e-01,  3.5278e-01,  5.2505e-01,\n",
      "        -2.4787e+00,  1.0747e+00,  9.7847e-02,  1.9678e+00, -6.8944e-01,\n",
      "        -2.2428e+00, -7.2130e-01,  9.3572e-01,  5.9511e-01, -2.4944e+00,\n",
      "         1.9699e+00, -3.4214e+00, -1.9364e+00, -2.3031e+00, -1.3316e+00,\n",
      "         7.9326e-01, -2.3030e+00, -3.8136e-01, -3.1559e+00,  4.0209e+00,\n",
      "        -1.8816e+00, -1.2296e+00,  2.4548e+00, -3.0188e+00,  3.6588e+00,\n",
      "         2.6977e+00,  2.9076e+00,  2.3628e+00, -1.3313e-01, -1.4077e+00,\n",
      "         1.5364e+00,  2.1588e+00,  1.2200e+00,  4.2927e-02, -1.3362e+00,\n",
      "         1.3809e+00,  2.3655e+00,  8.0591e-01,  2.3424e+00, -1.6966e+00,\n",
      "        -2.6441e+00, -2.3129e+00,  4.8684e-01, -3.0621e+00,  4.9870e-01,\n",
      "        -4.8505e-01,  4.1198e-01,  2.9711e+00,  1.6447e+00, -3.0654e+00,\n",
      "        -5.7338e-01,  2.5106e-01,  3.8788e+00,  1.2268e+00, -5.5931e-01,\n",
      "        -2.9587e+00, -1.0389e-01, -1.6868e+00, -2.6794e+00, -2.5185e+00,\n",
      "         1.2039e+00, -3.7317e-01, -5.0922e+00, -3.3246e-01,  3.0816e+00,\n",
      "         7.3908e-01,  1.2439e+00,  1.8039e+00,  6.1939e-01,  5.9675e-01,\n",
      "         1.3186e+00, -9.5538e-01,  7.1955e-01,  6.6464e-01, -1.6613e+00,\n",
      "        -1.0437e+00,  3.6702e+00, -2.3224e+00, -1.2475e-01, -7.2960e-01,\n",
      "        -1.4204e+00,  1.7170e+00, -2.5347e+00, -2.5457e+00, -3.9967e-02,\n",
      "         1.8933e+00,  2.0615e+00, -2.9743e-01,  4.7585e-01,  1.1581e+00,\n",
      "         1.9910e+00,  6.1240e+00,  1.7273e-01, -3.1840e+00,  1.6210e+00,\n",
      "         4.3642e-01, -1.7735e+00, -1.7529e+00,  9.3151e-01,  9.5457e-01,\n",
      "        -1.7849e-01,  1.2138e+00,  2.0621e-01,  1.4030e+00, -1.5504e+00,\n",
      "         6.7935e-01, -9.7759e-01, -3.0193e+00, -3.6293e+00, -2.4184e+00,\n",
      "         5.7765e-01, -5.5912e+00, -3.4540e-01,  6.1348e+00, -4.5936e+00,\n",
      "        -3.9871e+00,  3.5635e+00, -1.3433e+00, -1.3716e+00,  2.2225e+00,\n",
      "        -1.3811e+00, -1.9132e+00,  1.4053e+00])\n",
      "abound\n",
      "Saved the embedding for abound.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absent'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 768\n",
      "tensor([-7.3119e-02,  5.9319e-01,  3.7638e-01, -6.2817e-01, -3.3395e-01,\n",
      "        -6.9703e-01,  1.2632e+00, -1.6457e+00,  1.1321e+00,  1.2770e+00,\n",
      "        -4.6780e+00,  1.6328e+00, -2.0579e+00, -1.8364e+00,  8.6914e-01,\n",
      "         2.7275e+00, -2.0525e+00,  3.1216e+00, -1.5273e+00, -6.4661e-02,\n",
      "        -6.8326e-01, -2.8119e+00,  4.5670e-01,  1.3045e+00,  1.3026e+00,\n",
      "         7.8920e-01,  2.5370e+00, -1.8399e+00,  1.4350e+00, -2.1484e+00,\n",
      "         5.4540e+00,  1.9827e+00,  1.2019e+00, -2.3644e+00, -2.3784e+00,\n",
      "        -1.4820e+00, -1.5754e+00,  1.4522e+00, -2.1389e+00,  1.4480e+00,\n",
      "        -9.9529e-01, -2.3936e+00,  3.2725e+00,  4.7248e+00,  1.7247e+00,\n",
      "         5.0671e-01, -3.8084e+00,  3.6129e+00,  1.8924e+00, -3.5819e+00,\n",
      "        -1.6110e+00,  9.4986e-01,  2.7025e+00, -2.0831e-01,  3.7803e+00,\n",
      "         3.5807e+00,  6.6356e-01, -3.9993e+00,  3.8660e-02,  1.8465e+00,\n",
      "        -3.4037e+00,  1.7026e+00,  4.5263e-01, -1.5249e+00, -2.9528e+00,\n",
      "        -2.1398e+00,  1.2382e+00,  4.6301e-03,  4.2469e-01, -3.9204e+00,\n",
      "        -1.2545e+00, -1.4836e-02, -1.1146e+00, -3.9035e+00,  8.2239e-01,\n",
      "        -2.2910e+00,  1.3672e+00,  3.6661e+00,  2.8501e+00, -1.4853e-01,\n",
      "         4.1733e+00, -1.6064e+00,  2.3192e+00,  1.0303e-01,  2.0686e+00,\n",
      "         5.4022e-01,  1.1998e+00, -9.5150e-01, -3.0490e+00, -1.1546e+00,\n",
      "        -4.5117e+00,  3.2729e+00, -6.4579e-01,  4.4653e-01,  3.1264e+00,\n",
      "         2.4237e+00, -8.3799e-01,  2.7116e-01, -5.3150e-01,  1.3217e+00,\n",
      "         1.9285e+00, -1.6864e+00, -1.8830e+00, -2.3109e+00, -1.3020e+00,\n",
      "        -1.7542e+00, -1.9235e+00, -8.6757e-01,  3.4549e+00, -9.5785e-01,\n",
      "        -1.9436e+00,  6.4675e-01, -5.3997e+00,  9.7898e-01, -3.8457e-01,\n",
      "         2.7701e+00, -1.2274e+00,  3.7791e+00, -3.5406e+00, -2.7086e+00,\n",
      "        -2.6751e-01,  6.8836e-01, -2.5345e+00,  4.4800e-01, -2.0291e-01,\n",
      "         2.1960e+00, -3.2960e+00, -1.8028e+00,  4.2164e-02,  1.3152e+00,\n",
      "         1.3972e+00,  2.3992e+00,  2.1024e+00, -5.2913e-01, -8.1126e-01,\n",
      "        -1.2339e+00, -2.3756e-01,  3.4790e+00, -2.2541e+00, -2.0867e+00,\n",
      "        -2.6276e+00,  2.3484e+00,  1.9535e-01,  1.3803e+00, -1.0422e+00,\n",
      "        -1.3913e-01,  2.4110e+00,  1.3577e-01, -5.8635e+00,  6.2293e-01,\n",
      "        -1.5594e+00,  4.9756e+00, -3.7257e+00, -9.5480e-01,  2.5652e-01,\n",
      "        -6.7914e-01,  2.9601e+00,  1.2684e+00,  4.7870e-01,  9.6201e-01,\n",
      "         1.7923e+00,  3.3409e-01, -8.6486e-01, -1.5693e-01, -3.5279e-01,\n",
      "        -1.7986e+00, -3.7822e-01,  1.2447e+00, -1.2168e+00,  2.1259e+00,\n",
      "        -3.5946e-01,  8.0821e-01,  3.0685e+00, -7.0308e-02,  3.4047e+00,\n",
      "        -6.8847e-01,  1.6125e+00, -5.6079e-01, -3.9934e+00,  3.1737e+00,\n",
      "         3.6310e+00,  1.7950e+00, -3.1284e+00,  1.1472e+00,  1.2060e+00,\n",
      "         3.9930e-01,  2.1355e+00,  2.3713e-01,  3.7189e-01, -7.0722e-01,\n",
      "        -1.6793e+00, -2.1210e+00, -9.1869e-01,  2.5601e+00,  2.2639e+00,\n",
      "         7.5008e-02,  2.8490e-01, -1.4220e-01, -1.0596e+00, -1.0561e+00,\n",
      "         1.7489e+00, -1.3021e+00, -2.5540e+00,  1.0367e+00, -1.8998e+00,\n",
      "         1.0282e-01,  5.7002e-01, -2.0007e+00, -6.7279e-01,  1.6804e+00,\n",
      "         9.2099e-01, -3.3668e+00,  3.4261e+00,  4.8274e+00,  4.0089e-01,\n",
      "        -2.5577e+00,  4.4658e-01, -1.5027e-01,  2.3851e-01,  5.1692e-01,\n",
      "         2.1830e+00,  7.8148e-01, -3.9723e+00,  4.7829e+00,  2.7966e+00,\n",
      "        -1.9922e-02,  2.2120e+00,  3.2790e+00, -3.0046e+00, -2.4427e+00,\n",
      "         3.7913e-01,  2.6161e+00,  1.8365e+00, -8.9518e-01,  1.2601e-01,\n",
      "         2.0325e+00,  7.5814e-01, -1.6733e+00,  2.3972e+00, -1.2266e+00,\n",
      "        -2.4367e+00, -2.8928e+00,  5.3316e-01, -7.1560e-01,  1.0671e+00,\n",
      "        -1.0948e+00,  8.3209e-01, -2.2292e-01, -2.6478e+00,  5.8798e-01,\n",
      "         2.6254e-02, -6.6519e-01, -2.6659e+00, -9.8191e-01,  2.1949e+00,\n",
      "        -2.6014e-01,  2.3166e-01, -1.8533e+00, -2.2742e+00,  4.1478e+00,\n",
      "         1.8158e+00, -1.0714e+00,  5.2463e+00, -6.5268e+00,  7.5388e-01,\n",
      "        -4.9089e+00, -1.1399e+00,  1.6654e+00, -1.9437e+00,  1.9826e+00,\n",
      "         1.0803e+00,  4.0745e+00, -3.8941e+00,  3.5232e-01, -1.7279e+00,\n",
      "         9.9516e-01, -5.0655e-01,  5.0457e-01,  2.9314e+00, -3.0482e+00,\n",
      "        -1.5390e+00, -3.0904e+00, -2.6641e-02, -8.5953e-01, -3.9882e+00,\n",
      "         2.7139e+00,  2.6563e+00, -2.4000e+00, -8.3836e-02,  2.6215e+00,\n",
      "        -1.1793e+00, -1.6911e+00, -2.5414e-01, -2.7479e+00,  6.8637e-01,\n",
      "        -3.6584e-03, -6.7316e-01,  3.8673e-01, -1.3071e+00, -2.0986e+00,\n",
      "        -2.6835e+00,  3.2043e+00, -1.5244e+00,  1.5242e-02,  2.2159e+00,\n",
      "         8.1652e-01,  2.5421e-01, -3.8166e+00, -3.6622e+01,  5.9730e-01,\n",
      "        -1.2331e+00,  1.0180e+00,  2.7471e+00,  4.7356e-02, -1.6715e+00,\n",
      "        -2.4929e-01, -1.6841e-01,  5.1023e-01,  1.1262e+00,  2.1511e+00,\n",
      "        -2.0873e+00,  1.8006e+00,  1.7189e+00, -3.5795e+00, -6.9822e-01,\n",
      "         1.7260e+00, -7.1114e-01,  1.5927e-01, -1.1695e+00,  2.1800e-01,\n",
      "         1.3257e+00,  8.2926e-01,  1.7781e+00,  4.1422e-01,  2.9095e+00,\n",
      "        -7.2029e-01, -1.0422e+00,  2.2097e+00, -5.0413e+00, -4.8373e-01,\n",
      "         5.5971e-01,  2.7117e-01, -1.6792e+00, -3.9221e-01,  1.6115e+00,\n",
      "        -4.2248e-01, -1.3375e+00, -1.6916e+00,  4.5275e-01, -6.9002e-01,\n",
      "        -2.2367e-01,  3.3560e+00, -4.5100e-01,  4.5326e+00,  2.1367e+00,\n",
      "         5.9474e-02, -2.4348e+00,  1.1983e+00,  1.5734e+00,  1.3004e+00,\n",
      "         1.8383e+00, -2.9702e+00,  1.3481e+00,  1.6987e-01, -2.2118e+00,\n",
      "        -1.0962e+00, -1.4208e+00, -2.0824e+00,  1.3437e+00,  1.1448e+00,\n",
      "        -1.9141e+00,  4.5403e+00,  1.4084e+00,  1.4861e+00,  9.4718e-01,\n",
      "        -6.9027e-01,  2.5149e-01,  1.6342e+00, -3.4546e+00,  3.5119e-01,\n",
      "        -8.6139e-01, -1.0033e+01, -1.7860e+00, -1.9716e+00,  1.3113e+00,\n",
      "        -8.0747e-01, -4.3165e+00, -7.0369e-02, -1.6488e+00, -1.5936e+00,\n",
      "        -7.9900e-01,  2.5758e+00, -5.0515e+00,  1.4608e+00, -1.0921e+00,\n",
      "        -4.4322e-01, -3.2682e+00,  2.4211e+00, -4.7728e-01, -4.7281e+00,\n",
      "        -1.4294e-01, -9.8469e-01,  8.4481e-01,  2.4819e+00,  1.5031e+00,\n",
      "        -4.1174e+00,  2.4481e+00,  4.6400e+00, -1.3156e+00, -2.4495e+00,\n",
      "         5.6572e-01,  3.7991e-01, -8.3546e-02,  1.5836e+00, -1.6649e+00,\n",
      "         1.4835e+00, -1.7558e+00,  1.6765e+00,  1.5306e+00,  3.6427e+00,\n",
      "        -3.3634e-01,  1.1944e+00,  5.8707e-02, -1.6639e+00, -4.0640e+00,\n",
      "         2.8427e+00, -1.0016e-02, -2.6667e+00, -2.3271e+00, -4.9354e-01,\n",
      "         1.1721e+00, -1.8139e+00, -1.9494e-02, -1.4120e+00,  3.6642e-01,\n",
      "        -8.8051e-01, -9.8962e-01,  1.8880e+00, -9.0258e-01, -3.7416e-01,\n",
      "        -1.2295e+00, -2.1536e+00,  6.3908e-01, -4.3453e+00, -1.3857e+00,\n",
      "        -7.7759e-01, -3.6759e+00,  9.8133e-01,  3.5478e+00, -1.1069e+00,\n",
      "        -7.4833e-01,  6.5958e-01,  8.2185e-01,  2.5029e+00, -5.1333e+00,\n",
      "        -8.6130e-01,  3.4747e+00,  7.2175e+00, -1.6060e+00, -3.1766e+00,\n",
      "         3.6236e+00, -5.6602e+00, -3.2881e-01,  4.3875e-01, -4.1840e+00,\n",
      "         8.7128e-01,  9.0903e-01, -1.1547e+00, -6.9081e-01,  6.8071e-01,\n",
      "        -1.8411e+00, -1.1433e+00, -5.1162e-02,  3.0564e+00, -2.1452e+00,\n",
      "        -4.2898e-01,  8.9866e-01,  1.4856e+00, -2.6904e+00,  1.5297e+00,\n",
      "         4.1072e-01,  1.6610e+00, -1.5926e+00,  2.7753e+00,  4.1213e+00,\n",
      "         8.2720e-01, -2.7728e+00, -1.1273e+00, -5.2525e-01,  6.5290e-01,\n",
      "        -1.7632e+00, -4.0335e-01, -9.4218e-01, -2.2814e-01,  6.8783e-01,\n",
      "        -6.8339e-01,  5.2126e+00, -6.7387e-01,  1.8259e+00, -1.1221e+00,\n",
      "         1.4743e+00, -2.0559e+00,  1.3457e+00, -1.4312e+00,  2.2931e+00,\n",
      "         3.8882e+00,  4.7432e+00, -1.9291e+00, -5.3989e+00, -3.1418e+00,\n",
      "        -2.7616e+00, -1.0406e+00, -1.5324e+00,  6.4615e-01, -7.5990e-01,\n",
      "         1.5567e+00, -2.7374e+00,  1.8591e+00, -1.6863e+00,  1.6895e+00,\n",
      "         1.5033e-01, -2.0242e+00,  3.2272e+00, -2.1638e+00,  9.4300e-01,\n",
      "         5.6952e+00,  1.1670e+00,  8.1309e-01, -6.6551e+00,  1.7334e+00,\n",
      "        -3.8889e-01, -8.2384e-01, -2.9929e+00,  8.6222e-01,  2.9775e+00,\n",
      "         1.3041e+00,  1.3402e+00, -2.6168e+00,  2.9348e-01, -2.2191e+00,\n",
      "         3.5695e-01, -1.6587e+00, -7.0882e-01,  1.8199e+00,  2.6440e+00,\n",
      "         4.0235e+00, -1.2627e-02, -6.1318e-01,  2.8123e+00,  1.0716e+00,\n",
      "         1.9811e+00, -2.8643e-02, -1.4739e+00,  1.9271e+00, -1.2453e+00,\n",
      "        -3.5187e+00,  6.3879e-01,  6.1676e-01,  1.5855e+00,  1.0461e+00,\n",
      "        -1.3732e+00, -2.6921e+00,  2.9109e-01, -2.1501e+00,  5.7663e-02,\n",
      "         5.1725e-01,  1.8415e+00, -1.1503e+00,  2.6228e+00,  1.1998e+00,\n",
      "        -3.5563e+00, -3.2267e+00, -2.1297e+00, -3.0218e+00,  4.1881e+00,\n",
      "        -2.0092e+00,  5.6611e+00,  4.4958e+00, -8.7341e-01, -1.1464e-01,\n",
      "         2.4014e+00, -2.1711e+00, -1.8537e+00,  1.3005e+00, -1.1246e+00,\n",
      "        -1.4425e+00,  4.2923e-01, -4.6516e-01, -2.4426e+00,  1.1717e-02,\n",
      "        -6.8042e-01, -2.8564e-01,  1.2280e+00, -1.9528e+00, -1.4252e+00,\n",
      "        -4.9542e+00,  8.6624e-01,  2.2577e+00, -1.4801e+00, -1.7236e+00,\n",
      "         7.7019e-01, -8.5200e-01,  1.4030e+00,  3.6942e-01,  9.6414e-01,\n",
      "         4.9620e+00, -2.8464e+00,  5.2770e-02,  2.6287e+00,  5.6429e-01,\n",
      "         4.8057e+00, -2.4086e+00,  2.5604e+00,  2.4735e+00, -2.6911e-01,\n",
      "        -4.7042e+00, -4.3092e-01,  1.5436e+00, -2.0021e+00, -2.8757e+00,\n",
      "         1.0025e+00,  2.2625e+00, -2.4435e+00, -2.8681e-03,  1.7029e+00,\n",
      "        -6.6942e-02,  2.9746e+00, -3.3529e+00,  1.8477e+00, -1.9499e-01,\n",
      "         3.9704e+00,  2.4086e-01,  1.8232e+00,  1.4806e+00, -1.8585e-01,\n",
      "         2.8999e-01,  2.8980e+00,  4.6247e-01,  3.1700e+00, -2.8054e+00,\n",
      "        -2.1892e+00,  2.2400e+00,  2.6776e+00, -6.4802e-01,  5.3615e-01,\n",
      "         6.6635e-01,  2.1020e+00, -7.5904e-01,  1.0998e-01, -2.8288e+00,\n",
      "        -4.1385e-01,  3.2939e+00, -2.1679e-01,  1.3242e+00, -3.4057e+00,\n",
      "         1.6419e+00,  1.5837e+00, -1.8617e+00,  6.8758e-01,  2.3177e+00,\n",
      "         3.0576e-01, -2.4742e-01, -1.9026e+00, -1.3964e+00,  4.7354e+00,\n",
      "        -4.0646e+00, -2.4510e+00,  2.7308e+00, -2.8787e+00, -1.4673e+00,\n",
      "         2.2215e+00, -1.0905e+00,  1.7207e+00, -3.1918e+00,  5.4267e-01,\n",
      "        -3.1109e+00,  9.5494e-01,  2.4223e+00,  3.7856e-01, -1.1968e+00,\n",
      "        -5.3568e-01,  6.6756e-01,  3.8704e+00,  2.6679e+00, -3.4629e-01,\n",
      "        -2.8363e+00, -4.7609e+00,  2.0391e+00, -2.9324e+00,  9.6621e-01,\n",
      "        -7.4511e-01,  1.0964e+00,  4.9626e+00, -1.4869e+00, -3.6306e+00,\n",
      "        -1.3058e-01,  2.2246e+00,  4.4015e+00, -2.1969e+00, -7.2020e-01,\n",
      "        -2.4318e+00,  2.9785e+00, -2.6697e+00,  7.9105e-01, -4.0676e+00,\n",
      "        -1.7926e+00, -1.5149e+00, -3.5528e+00, -2.7019e-01,  3.8434e+00,\n",
      "         1.6402e+00,  9.0872e-01, -2.3480e+00, -1.6374e+00, -9.3427e-01,\n",
      "         3.4033e+00, -7.6359e-01, -4.2286e-01, -5.5103e-01,  5.9636e-01,\n",
      "         1.2464e+00,  2.3985e+00, -1.2359e+00, -5.6721e-01, -7.8622e-01,\n",
      "        -3.9142e-02, -7.0451e-01, -7.7986e-01,  1.4137e+00, -7.1620e-01,\n",
      "        -1.6935e-01,  1.2198e+00, -2.4531e-02, -2.3027e+00, -3.3000e+00,\n",
      "        -5.6879e-01,  3.2205e+00,  3.3121e+00, -1.3546e+00,  1.3120e+00,\n",
      "        -6.2681e-01, -2.7080e+00,  3.3434e-01, -2.3751e+00,  1.2177e+00,\n",
      "         5.4954e-01, -5.6282e-01, -2.3223e+00,  2.8179e+00,  1.1000e+00,\n",
      "        -4.8817e-02, -2.3302e-01, -1.8279e+00, -2.4661e+00, -2.6535e+00,\n",
      "         7.4363e-01, -4.9952e+00, -1.0236e+00,  3.5737e+00, -4.4885e+00,\n",
      "        -3.1739e+00, -1.3554e+00,  6.6775e-01, -2.4762e+00,  9.1708e-01,\n",
      "        -6.3266e-01, -1.0711e+00, -9.8269e-01])\n",
      "absent\n",
      "Saved the embedding for absent.\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "# Set up input and output paths.\n",
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/vocab_files/vocab_checked.txt'\n",
    "layer_combining_function = sum_middle_four2\n",
    "embeddings_file = os.path.join('/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab', layer_combining_function.__name__ + '.txt')\n",
    "if os.path.exists(embeddings_file):\n",
    "    os.remove(embeddings_file)\n",
    "\n",
    "    # Create a list of vocabulary words we want embeddings for.\n",
    "vocab = make_vocab(vocab_file)\n",
    "\n",
    "# Tokenize the vocabulary and look up the BERT token indices.\n",
    "tokenized_text, indexed_tokens = tokenize_text(vocab)\n",
    "\n",
    "# Generate segment IDs for each token.\n",
    "segments_IDs = generate_segments_IDs(tokenized_text)\n",
    "\n",
    "# Generate and write out the contextual embeddings for the vocabulary words.\n",
    "# Embeddings are saved in a standard format that can be used for calcualting\n",
    "# the cosine distances between word vectors.\n",
    "for i in range(len(tokenized_text)):\n",
    "    # Convert indexed tokens and segments to tensors.\n",
    "    # Create a BERT model for the tokens.\n",
    "    # Get the encoded model layers and reshape them.\n",
    "    token_embeddings = generate_embeddings(indexed_tokens[i], segments_IDs[i])\n",
    "    print(f'{tokenized_text[i]} has a token embedding of size {token_embeddings.size()}')\n",
    "\n",
    "    # Extract the contextual embedding for a token.\n",
    "    contextual_embedding = layer_combining_function(token_embeddings)\n",
    "\n",
    "    # Write the embedding to a text file, with the vocabulary word prepended.\n",
    "    vocab_word = reconstruct_tokens(tokenized_text[i])\n",
    "    # Make sure we've got the correct vocabulary word.\n",
    "    assert vocab[i] == vocab_word\n",
    "    write_embedding(embeddings_file, vocab[i], contextual_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal-venv-3.6",
   "language": "python",
   "name": "crystal-venv-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
