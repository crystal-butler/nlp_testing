{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, RobertaConfig\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/Notebooks/crystal/NLP/transformers/examples'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we're in the transformers directory with fine-tuned model output.\n",
    "os.chdir('/home/jupyter/Notebooks/crystal/NLP/transformers/examples/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# and Transformers documentation: https://huggingface.co/transformers/model_doc/roberta.html#robertaformaskedlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################### BEGIN TESTING #####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aback', 'abashed', 'abhor', 'abhorred', 'abhorrence', 'abhorrent', 'abominable', 'abound', 'absent', 'absorbed']\n",
      "2187\n"
     ]
    }
   ],
   "source": [
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/data/vocab_files/vocab_checked.txt'\n",
    "vocab = make_vocab(vocab_file)\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('./output_CC-ab/')\n",
    "config = RobertaConfig.from_pretrained('./output_CC-ab/')\n",
    "model = RobertaForMaskedLM.from_pretrained('./output_CC-ab/', config=config)\n",
    "# Outputting hidden states allows direct access to hidden layers of the model.\n",
    "config.output_hidden_states = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She             264\n",
      "made            156\n",
      "an               41\n",
      "abhor        35,350\n",
      "red           2,050\n",
      "expression    8,151\n",
      "as               25\n",
      "he               37\n",
      "began           880\n",
      "making          442\n",
      "excuses      19,791\n",
      ".                 4\n",
      "He               91\n",
      "wanted          770\n",
      "to                7\n",
      "keep            489\n",
      "playing         816\n",
      ",                 6\n",
      "but              53\n",
      "abhor        35,350\n",
      "red           2,050\n",
      "the               5\n",
      "poor          2,129\n",
      "sports        1,612\n",
      "manship      17,187\n",
      "of                9\n",
      "his              39\n",
      "fellows      36,304\n",
      ".                 4\n",
      "<s> She made an abhorred expression as he began making excuses. He wanted to keep playing, but abhorred the poor sportsmanship of his fellows.</s>\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple contexts for the word \"abhorred\"\n",
    "test_text = \"She made an abhorred expression as he began making excuses. He wanted to keep playing, but abhorred the poor sportsmanship of his fellows.\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_test_text = tokenizer.encode(test_text, add_special_tokens=True)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for t in tokenized_test_text[1:-1]:\n",
    "        print('{:<12} {:>6,}'.format(tokenizer.decode(t).strip(), t))\n",
    "        \n",
    "print(tokenizer.decode(tokenized_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokens_tensor = torch.tensor([tokenized_test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.1821, -0.0584, -0.0180,  ..., -0.0704,  0.1026, -0.0030],\n",
      "         [-0.1530, -0.0148,  0.1621,  ..., -0.0165,  0.0989,  0.2619],\n",
      "         [-0.1659, -0.2194,  0.6360,  ...,  0.0112, -0.6962,  0.0143],\n",
      "         ...,\n",
      "         [-0.0603, -0.6785,  0.3369,  ..., -0.1608, -0.1464,  0.4502],\n",
      "         [ 0.2217,  0.0367,  0.0914,  ..., -0.8357,  0.0687,  0.0556],\n",
      "         [ 0.0619, -0.0906, -0.0385,  ...,  0.4705,  0.0345, -0.1859]]]), tensor([[[-0.0529,  0.0224,  0.0254,  ...,  0.0015,  0.0220, -0.1526],\n",
      "         [ 0.0337,  0.0747,  0.1383,  ...,  0.2554,  0.2624,  0.6075],\n",
      "         [-0.2280, -0.4853,  0.9908,  ..., -0.1713, -0.9801,  0.2253],\n",
      "         ...,\n",
      "         [-0.3420, -0.4283, -0.2935,  ...,  0.0829,  0.0355,  0.2964],\n",
      "         [-0.1282,  0.1487,  0.2648,  ..., -0.2111, -0.0344, -0.1823],\n",
      "         [-0.2712,  0.4222,  0.0146,  ...,  0.4811, -0.1373, -0.6872]]]), tensor([[[ 4.9576e-02,  4.7782e-02,  1.0115e-02,  ...,  1.5301e-02,\n",
      "           7.0869e-04, -6.6151e-02],\n",
      "         [ 1.6585e-01,  3.7590e-01, -9.9868e-02,  ...,  6.9865e-01,\n",
      "           4.2312e-01,  6.4181e-01],\n",
      "         [-4.4737e-01, -6.1049e-01,  4.7846e-01,  ...,  7.0510e-02,\n",
      "          -8.3817e-01,  4.1901e-02],\n",
      "         ...,\n",
      "         [-4.2351e-01, -5.5416e-01, -3.4401e-01,  ..., -8.1274e-02,\n",
      "          -1.5230e-02,  1.1873e-01],\n",
      "         [-2.3876e-01,  1.5484e-01,  9.9842e-02,  ..., -4.5259e-03,\n",
      "          -4.1328e-02, -2.4828e-01],\n",
      "         [-1.9059e-01,  5.3967e-01,  1.0500e-01,  ...,  5.6551e-01,\n",
      "           1.6221e-01, -4.4047e-01]]]), tensor([[[ 0.0343,  0.0127, -0.0232,  ...,  0.0512,  0.0268,  0.0077],\n",
      "         [ 0.2296,  0.4484, -0.4307,  ...,  0.5636,  0.2837,  0.2249],\n",
      "         [-0.7053, -0.2004, -0.2603,  ..., -0.0009, -0.4574,  0.2746],\n",
      "         ...,\n",
      "         [-0.7456, -0.3884, -0.3885,  ...,  0.1130, -0.1825, -0.2425],\n",
      "         [ 0.0324,  0.2752,  0.0657,  ..., -0.0738,  0.0454, -0.0600],\n",
      "         [-0.0446,  0.3568, -0.1348,  ...,  0.4135, -0.0452, -0.2576]]]), tensor([[[-4.6944e-02,  4.1677e-02, -7.2794e-03,  ...,  5.1589e-02,\n",
      "          -4.1966e-02, -3.6552e-02],\n",
      "         [-7.6823e-02,  4.5732e-01, -2.4615e-01,  ...,  3.4740e-01,\n",
      "           1.2852e-01,  1.9139e-01],\n",
      "         [-6.2073e-01, -3.9625e-01, -8.4976e-01,  ..., -8.0510e-02,\n",
      "          -2.8904e-01,  9.1242e-01],\n",
      "         ...,\n",
      "         [-6.5824e-01, -4.8831e-01, -3.6025e-01,  ...,  5.7998e-03,\n",
      "          -3.6086e-01, -3.4245e-01],\n",
      "         [ 6.2542e-02,  1.1949e-01,  6.5635e-04,  ..., -2.0125e-04,\n",
      "           3.6982e-02, -7.9291e-02],\n",
      "         [-3.3988e-01, -3.3628e-02, -1.3579e-01,  ...,  4.3827e-01,\n",
      "          -9.2783e-02, -1.3392e-01]]]), tensor([[[-5.7115e-04,  4.6936e-02,  1.1553e-03,  ...,  2.5917e-02,\n",
      "          -5.6785e-03, -1.1482e-02],\n",
      "         [ 7.2537e-02,  8.9023e-01, -3.8852e-01,  ...,  1.6331e-01,\n",
      "          -2.7888e-03,  1.2692e-01],\n",
      "         [-5.7811e-01,  6.9700e-02, -6.5168e-01,  ..., -2.9279e-01,\n",
      "          -1.9806e-01,  1.0228e+00],\n",
      "         ...,\n",
      "         [-1.6052e-01, -4.0856e-01, -5.8593e-01,  ...,  2.0237e-02,\n",
      "          -2.2376e-01, -8.8026e-02],\n",
      "         [ 1.0511e-02,  7.9226e-02, -1.5857e-03,  ..., -9.1052e-03,\n",
      "           1.3586e-02,  2.0635e-02],\n",
      "         [-2.0993e-01, -3.4107e-02, -3.0318e-01,  ...,  5.1079e-01,\n",
      "          -1.5782e-01, -7.8787e-02]]]), tensor([[[ 0.0275,  0.0580,  0.0865,  ...,  0.0469, -0.0816, -0.0292],\n",
      "         [ 0.1486,  1.0228, -0.4403,  ...,  0.2388, -0.1499,  0.1832],\n",
      "         [-0.5069, -0.3001, -1.0076,  ..., -0.1250, -0.1729,  0.4713],\n",
      "         ...,\n",
      "         [ 0.0385, -0.1791, -0.3765,  ...,  0.0750, -0.2847, -0.0740],\n",
      "         [ 0.0186,  0.0609, -0.0322,  ...,  0.0103,  0.0428, -0.0116],\n",
      "         [-0.2084,  0.1593, -0.0740,  ...,  0.4177, -0.3367, -0.2290]]]), tensor([[[-4.1080e-02,  6.5282e-02, -8.7687e-03,  ...,  1.1872e-01,\n",
      "          -1.0300e-01, -3.3917e-02],\n",
      "         [ 4.8437e-02,  1.1295e+00, -2.7506e-01,  ...,  3.9332e-01,\n",
      "          -5.2481e-02,  1.2625e-01],\n",
      "         [-4.7661e-02, -1.1313e-01, -4.0228e-01,  ..., -7.5156e-02,\n",
      "           1.2755e-01,  5.3442e-01],\n",
      "         ...,\n",
      "         [ 2.0817e-03, -8.5437e-02, -2.2546e-01,  ...,  4.7579e-01,\n",
      "          -3.4109e-01,  5.0127e-01],\n",
      "         [ 8.9517e-03,  7.6832e-02, -1.1781e-02,  ..., -3.8862e-04,\n",
      "           1.9482e-02,  4.1657e-02],\n",
      "         [ 1.6245e-01,  1.5243e-01,  1.8577e-02,  ...,  6.2193e-01,\n",
      "          -5.0994e-01, -5.9492e-02]]]), tensor([[[-0.0306,  0.0921, -0.0260,  ...,  0.0444, -0.0680, -0.0680],\n",
      "         [ 0.1138,  0.8001, -0.1782,  ...,  0.3452, -0.5355, -0.0098],\n",
      "         [ 0.1875, -0.1305, -0.2541,  ..., -0.2437,  0.2583,  0.3931],\n",
      "         ...,\n",
      "         [-0.0161,  0.1853, -0.1760,  ...,  0.4472, -0.2150,  0.2223],\n",
      "         [ 0.0138,  0.0175, -0.0009,  ...,  0.0358, -0.0215,  0.0069],\n",
      "         [-0.0396, -0.1572, -0.0876,  ...,  0.6679, -0.3987, -0.0692]]]), tensor([[[-0.0949,  0.0382, -0.0118,  ...,  0.0817,  0.0048, -0.0579],\n",
      "         [-0.1291,  0.6376,  0.2347,  ...,  0.2725, -0.4482, -0.0045],\n",
      "         [-0.1528,  0.1061, -0.0781,  ..., -0.3867,  0.0214,  0.2348],\n",
      "         ...,\n",
      "         [ 0.0422,  0.1032, -0.1622,  ...,  0.4890, -0.1377,  0.0784],\n",
      "         [-0.0019,  0.0016,  0.0107,  ...,  0.0058,  0.0210, -0.0013],\n",
      "         [-0.1504, -0.2551, -0.0437,  ...,  0.6457, -0.4360, -0.2977]]]), tensor([[[-8.4519e-03,  6.5337e-02, -3.2542e-02,  ...,  3.6497e-02,\n",
      "          -1.8756e-02, -1.5326e-02],\n",
      "         [ 2.5593e-01,  5.6262e-01, -1.5146e-01,  ...,  1.9684e-01,\n",
      "          -2.9838e-01,  3.3251e-02],\n",
      "         [ 2.9675e-02,  2.4595e-01,  7.2762e-02,  ..., -2.8592e-01,\n",
      "          -1.5922e-01,  2.6295e-01],\n",
      "         ...,\n",
      "         [ 2.0706e-01,  1.9917e-02, -1.9788e-01,  ...,  5.5681e-01,\n",
      "           1.4991e-01, -2.9951e-02],\n",
      "         [-5.8052e-03,  5.7513e-02, -1.6433e-04,  ...,  5.1288e-03,\n",
      "          -1.0641e-02, -1.1278e-02],\n",
      "         [ 1.7328e-01, -5.7643e-02, -1.2278e-01,  ...,  5.6461e-01,\n",
      "          -2.9252e-01,  1.2464e-01]]]), tensor([[[ 0.0006,  0.0337,  0.0218,  ..., -0.0104, -0.0146, -0.0027],\n",
      "         [ 0.5146,  0.4068, -0.1556,  ...,  0.3775, -0.4213, -0.0272],\n",
      "         [ 0.0973,  0.2013,  0.1921,  ..., -0.2859, -0.0461,  0.0926],\n",
      "         ...,\n",
      "         [ 0.3314,  0.0187, -0.1044,  ...,  0.5209,  0.0055, -0.0034],\n",
      "         [-0.0055,  0.0318,  0.0265,  ..., -0.0134, -0.0146, -0.0021],\n",
      "         [ 0.2641, -0.1255, -0.1237,  ...,  0.4026, -0.1057, -0.0276]]]), tensor([[[-0.0641,  0.1020, -0.0035,  ..., -0.1924, -0.0491, -0.0135],\n",
      "         [ 0.1558,  0.1855, -0.0532,  ...,  0.0266, -0.1110,  0.0330],\n",
      "         [ 0.1358,  0.1934,  0.0841,  ..., -0.1331,  0.0219, -0.0678],\n",
      "         ...,\n",
      "         [-0.0844,  0.1910, -0.0006,  ..., -0.1056, -0.0051, -0.0224],\n",
      "         [-0.0658,  0.1010, -0.0042,  ..., -0.1962, -0.0502, -0.0153],\n",
      "         [ 0.0463,  0.0420,  0.0284,  ...,  0.0353, -0.0138,  0.0538]]]))\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(tokenized_test_text).unsqueeze(0)  # Batch size 1\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "    # Documentation for the \"forward\" method of RobertaForMaskedLM\n",
    "    # details what is returned for each index of \"outputs.\"\n",
    "    # print(outputs[0])  # masked_lm_loss\n",
    "    # print(outputs[1])  # prediction_scores\n",
    "    print(outputs[2])  # hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13\n",
      "Number of batches: 1\n",
      "Number of tokens: 32\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(outputs[2]))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(outputs[2][layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(outputs[2][layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(outputs[2][layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_layers = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For our token, select its feature values from layer 5.\n",
    "token_i = 4\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "# print(vec)\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Type of encoded_layers:  <class 'list'>\n",
      "Tensor shape for each layer:  torch.Size([1, 32, 768])\n"
     ]
    }
   ],
   "source": [
    "# `encoded_layers` is a tuple.\n",
    "print('     Type of encoded_layers: ', type(list(encoded_layers)))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', encoded_layers[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 32, 768])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 32, 768])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 13, 768])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  disgusted\n",
      "21  disgusted\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_test_text):\n",
    "      if tokenizer.decode(token_str).strip() == \"disgusted\":\n",
    "        print (i, tokenizer.decode(token_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 32 x 768\n"
     ]
    }
   ],
   "source": [
    "# Create token vectors by summing the last 4 layers of the model.\n",
    "# Stores the token vectors, with shape [32 x 768]\n",
    "token_vecs_sum = []\n",
    "# `token_embeddings` is a [32 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # `token` is a [13 x 768] tensor\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "# Shape is: 32 x 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 32 x 768\n"
     ]
    }
   ],
   "source": [
    "# Extract the last layer's features\n",
    "token_vecs_last = []\n",
    "# `token_embeddings` is a [32 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # `token` is a [13 x 768] tensor\n",
    "    # Extract the vector from the last layer.\n",
    "    last_vec = token[-1]\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_last.append(last_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_last), len(token_vecs_last[0])))\n",
    "# Shape is: 32 x 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 32 x 768\n"
     ]
    }
   ],
   "source": [
    "# Extract the embedding layer's features (layer 0 is the embedding layer)\n",
    "token_vecs_embed = []\n",
    "# `token_embeddings` is a [32 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # `token` is a [13 x 768] tensor\n",
    "    # Extract the vector from the last layer.\n",
    "    embed_vec = token[0]\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_embed.append(last_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_embed), len(token_vecs_embed[0])))\n",
    "# Shape is: 32 x 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 21]\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "for i, token_str in enumerate(tokenized_test_text):\n",
    "      if tokenizer.decode(token_str).strip() == \"disgusted\":\n",
    "        indices.append(i)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 32 x 768\n",
      "disgusted at index 4: tensor([-0.7637, -1.0375, -0.4840, -0.2251, -0.1792])\n",
      "disgusted at index 21: tensor([-0.8459, -0.2973, -0.3336, -0.6372, -0.7399])\n"
     ]
    }
   ],
   "source": [
    "token_vecs_sum_last_four = sum_last_four_token_vecs(token_embeddings)\n",
    "for i in range(len(indices)):\n",
    "    print(f'disgusted at index {indices[i]}: {str(token_vecs_sum_last_four[indices[i]][:5])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 open\n",
      "1 openness\n",
      "2 opposed\n",
      "3 oppositional\n",
      "4 oppressed\n",
      "5 optimism\n",
      "6 optimistic\n",
      "7 ordering\n",
      "8 orgasmic\n",
      "9 ornery\n",
      "10 ouch\n",
      "11 out\n",
      "12 outburst\n",
      "13 outcry\n",
      "14 outed\n",
      "15 outlandish\n",
      "16 outrage\n",
      "17 outraged\n",
      "18 outspoken\n",
      "19 overbearing\n",
      "20 overexcited\n",
      "21 overjoyed\n",
      "22 overshadowed\n",
      "23 overstrung\n",
      "24 overwhelmed\n",
      "25 overworked\n",
      "26 overwrought\n",
      "27 pain\n",
      "28 pained\n",
      "29 painful\n",
      "30 painfully\n",
      "31 panic\n",
      "32 panicked\n",
      "33 panicky\n",
      "34 paralyzed\n",
      "35 paranoid\n",
      "36 passionate\n",
      "37 passive\n",
      "38 patience\n",
      "39 patient\n",
      "40 patronizing\n",
      "41 pause\n",
      "42 pausing\n",
      "43 peaceful\n",
      "44 peculiar\n",
      "45 peering\n",
      "46 peeved\n",
      "47 peevish\n",
      "48 pensive\n",
      "49 peppy\n",
      "50 perceptive\n",
      "51 perfidious\n",
      "52 perky\n",
      "53 perplexed\n",
      "54 perplexing\n",
      "55 persistent\n",
      "56 personable\n",
      "57 perturbed\n",
      "58 perverse\n",
      "59 pesky\n",
      "60 pessimism\n",
      "61 pessimistic\n",
      "62 pestered\n",
      "63 petitioning\n",
      "64 petrified\n",
      "65 petty\n",
      "66 petulant\n",
      "67 picked\n",
      "68 piercing\n",
      "69 pinched\n",
      "70 pious\n",
      "71 piqued\n",
      "72 pissed\n",
      "73 pitiable\n",
      "74 pitiful\n",
      "75 pity\n",
      "76 pitying\n",
      "77 placated\n",
      "78 placation\n",
      "79 placid\n",
      "80 plain\n",
      "81 plaintive\n",
      "82 planning\n",
      "83 playful\n",
      "84 playfully\n",
      "85 pleading\n",
      "86 pleasant\n",
      "87 pleased\n",
      "88 pleasing\n",
      "89 pleasurable\n",
      "90 pleasure\n",
      "91 pleasured\n",
      "92 pliant\n",
      "93 plotting\n",
      "94 poignant\n",
      "95 pointed\n",
      "96 poised\n",
      "97 polite\n",
      "98 pompous\n",
      "99 ponder\n",
      "100 pondering\n",
      "101 pooping\n",
      "102 pop\n",
      "103 posing\n",
      "104 positive\n",
      "105 positivity\n",
      "106 possibly\n",
      "107 pout\n",
      "108 pouting\n",
      "109 pouty\n",
      "110 powerful\n",
      "111 powerless\n",
      "112 pranking\n",
      "113 precarious\n",
      "114 predatory\n",
      "115 prejudiced\n",
      "116 preoccupied\n",
      "117 prepared\n",
      "118 preparing\n",
      "119 pretending\n",
      "120 pretentious\n",
      "121 prideful\n",
      "122 priggish\n",
      "123 primed\n",
      "124 private\n",
      "125 processing\n",
      "126 propositioning\n",
      "127 proud\n",
      "128 provocative\n",
      "129 provoke\n",
      "130 provoked\n",
      "131 provoking\n",
      "132 prying\n",
      "133 psycho\n",
      "134 psychotic\n",
      "135 puckish\n",
      "136 puerile\n",
      "137 pugnacious\n",
      "138 punished\n",
      "139 punishing\n",
      "140 punitive\n",
      "141 punk\n",
      "142 puppyish\n",
      "143 purposeful\n",
      "144 pursed\n",
      "145 put\n",
      "146 putting\n",
      "147 puzzled\n",
      "148 puzzlement\n",
      "149 qualms\n",
      "150 quarrelsome\n",
      "151 queasy\n",
      "152 quenched\n",
      "153 questionable\n",
      "154 questioning\n",
      "155 questioningly\n",
      "156 quiet\n",
      "157 quietness\n",
      "158 quilt\n",
      "159 quirky\n",
      "160 quizzical\n",
      "161 rabid\n",
      "162 racked\n",
      "163 radiant\n",
      "164 rage\n",
      "165 raged\n",
      "166 ragged\n",
      "167 raging\n",
      "168 rancorous\n",
      "169 randy\n",
      "170 rapt\n",
      "171 rattled\n",
      "172 raving\n",
      "173 reactive\n",
      "174 ready\n",
      "175 realization\n",
      "176 reassured\n",
      "177 rebellious\n",
      "178 rebuke\n",
      "179 recalling\n",
      "180 receptive\n",
      "181 reckless\n",
      "182 recoil\n",
      "183 recoiling\n",
      "184 reflecting\n",
      "185 reflection\n",
      "186 reflective\n",
      "187 refulgent\n",
      "188 refusing\n",
      "189 regret\n",
      "190 regretful\n",
      "191 rejected\n",
      "192 rejecting\n",
      "193 rejection\n",
      "194 rejoicing\n",
      "195 relaxation\n",
      "196 relaxed\n",
      "197 relentless\n",
      "198 relief\n",
      "199 relieved\n",
      "200 relived\n",
      "201 reluctant\n",
      "202 reluctantly\n",
      "203 remorse\n",
      "204 remorseful\n",
      "205 repelled\n",
      "206 repressed\n",
      "207 reproach\n",
      "208 reproachful\n",
      "209 repugnance\n",
      "210 repugnant\n",
      "211 repulsed\n",
      "212 repulsion\n",
      "213 resent\n",
      "214 resentful\n",
      "215 resenting\n",
      "216 resentment\n",
      "217 reserved\n",
      "218 resignation\n",
      "219 resigned\n",
      "220 resilience\n",
      "221 resistance\n",
      "222 resistant\n",
      "223 resistent\n",
      "224 resisting\n",
      "225 resolute\n",
      "226 resolved\n",
      "227 responsive\n",
      "228 restful\n",
      "229 resting\n",
      "230 restless\n",
      "231 restlessness\n",
      "232 restrained\n",
      "233 restraint\n",
      "234 retaliating\n",
      "235 retaliatory\n",
      "236 rethinking\n",
      "237 reticence\n",
      "238 reticent\n",
      "239 revengeful\n",
      "240 reverent\n",
      "241 revolted\n",
      "242 revulsion\n",
      "243 righteous\n",
      "244 rigid\n",
      "245 riled\n",
      "246 riotous\n",
      "247 riveted\n",
      "248 roar\n",
      "249 roguish\n",
      "250 roiled\n",
      "251 rough\n",
      "252 roused\n",
      "253 rude\n",
      "254 rueful\n",
      "255 ruffled\n",
      "256 ruminating\n",
      "257 rustled\n",
      "258 ruthless\n",
      "259 sad\n",
      "260 sadden\n",
      "261 saddened\n",
      "262 sadistic\n",
      "263 sadness\n",
      "264 salacious\n",
      "265 salivating\n",
      "266 sanctimonious\n",
      "267 sane\n",
      "268 sanguine\n",
      "269 sappy\n",
      "270 sarcasm\n",
      "271 sarcastic\n",
      "272 sardonic\n",
      "273 sassy\n",
      "274 sated\n",
      "275 satiated\n",
      "276 satirical\n",
      "277 satisfaction\n",
      "278 satisfied\n",
      "279 satisfy\n",
      "280 saturnine\n",
      "281 saucy\n",
      "282 savage\n",
      "283 scandalized\n",
      "284 scare\n",
      "285 scared\n",
      "286 scary\n",
      "287 scattered\n",
      "288 schadenfreude\n",
      "289 scheming\n",
      "290 scoffer\n",
      "291 scoffing\n",
      "292 scorn\n",
      "293 scorned\n",
      "294 scornful\n",
      "295 scowl\n",
      "296 scowling\n",
      "297 scream\n",
      "298 screaming\n",
      "299 scrutinizing\n",
      "300 sealed\n",
      "301 searching\n",
      "302 secretive\n",
      "303 secretively\n",
      "304 secure\n",
      "305 sedate\n",
      "306 seduction\n",
      "307 seductive\n",
      "308 seething\n",
      "309 self\n",
      "310 sensual\n",
      "311 sentimental\n",
      "312 serene\n",
      "313 serious\n",
      "314 seriousness\n",
      "315 servile\n",
      "316 set\n",
      "317 severe\n",
      "318 shabby\n",
      "319 shady\n",
      "320 shaken\n",
      "321 shaky\n",
      "322 shame\n",
      "323 shamed\n",
      "324 shamefaced\n",
      "325 shameful\n",
      "326 shameless\n",
      "327 sharp\n",
      "328 sheepish\n",
      "329 sheepishness\n",
      "330 shelled\n",
      "331 shifty\n",
      "332 shock\n",
      "333 shocked\n",
      "334 shocking\n",
      "335 shockingly\n",
      "336 shook\n",
      "337 shout\n",
      "338 shouting\n",
      "339 shrewd\n",
      "340 shy\n",
      "341 shyness\n",
      "342 sick\n",
      "343 sicken\n",
      "344 sickened\n",
      "345 sigh\n",
      "346 silenced\n",
      "347 silent\n",
      "348 silliness\n",
      "349 silly\n",
      "350 simmering\n",
      "351 simper\n",
      "352 simpering\n",
      "353 simple\n",
      "354 simplicity\n",
      "355 sincere\n",
      "356 sinful\n",
      "357 singing\n",
      "358 sinister\n",
      "359 sinisterly\n",
      "360 sizing\n",
      "361 skeptic\n",
      "362 skeptical\n",
      "363 skeptically\n",
      "364 skepticism\n",
      "365 sketchy\n",
      "366 skittish\n",
      "367 slack\n",
      "368 sleazy\n",
      "369 sleepy\n",
      "370 slick\n",
      "371 slothful\n",
      "372 slow\n",
      "373 sluggish\n",
      "374 sly\n",
      "375 smarmy\n",
      "376 smart\n",
      "377 smashed\n",
      "378 smile\n",
      "379 smiley\n",
      "380 smiling\n",
      "381 smirk\n",
      "382 smirking\n",
      "383 smoldering\n",
      "384 smooching\n",
      "385 smooth\n",
      "386 smug\n",
      "387 smugness\n",
      "388 snake\n",
      "389 snappy\n",
      "390 snarky\n",
      "391 snarl\n",
      "392 snarled\n",
      "393 snarling\n",
      "394 snarly\n",
      "395 sneaky\n",
      "396 sneer\n",
      "397 sneering\n",
      "398 sneeze\n",
      "399 sneezing\n",
      "400 snicker\n",
      "401 snickering\n",
      "402 snide\n",
      "403 sniggering\n",
      "404 sniveling\n",
      "405 snobbish\n",
      "406 snobby\n",
      "407 snooty\n",
      "408 snotty\n",
      "409 sociable\n",
      "410 soft\n",
      "411 solemn\n",
      "412 solicitous\n",
      "413 solitary\n",
      "414 solitude\n",
      "415 somber\n",
      "416 somberly\n",
      "417 somnolent\n",
      "418 soothed\n",
      "419 sore\n",
      "420 sorrow\n",
      "421 sorrowful\n",
      "422 sorry\n",
      "423 sour\n",
      "424 spaced\n",
      "425 spacing\n",
      "426 spastic\n",
      "427 speaking\n",
      "428 specious\n",
      "429 speculative\n",
      "430 speechless\n",
      "431 spent\n",
      "432 spirited\n",
      "433 spiritless\n",
      "434 spite\n",
      "435 spiteful\n",
      "436 spoiled\n",
      "437 spooked\n",
      "438 squeamish\n",
      "439 staggered\n",
      "440 stalker\n",
      "441 stare\n",
      "442 staring\n",
      "443 starstruck\n",
      "444 started\n",
      "445 startled\n",
      "446 stately\n",
      "447 steadfast\n",
      "448 steady\n",
      "449 stealthy\n",
      "450 steamed\n",
      "451 steaming\n",
      "452 steeling\n",
      "453 steely\n",
      "454 stern\n",
      "455 stiff\n",
      "456 stifled\n",
      "457 stifling\n",
      "458 still\n",
      "459 stillness\n",
      "460 stimulated\n",
      "461 stinky\n",
      "462 stirred\n",
      "463 stoic\n",
      "464 stoical\n",
      "465 stolid\n",
      "466 stoned\n",
      "467 storming\n",
      "468 stormy\n",
      "469 stout\n",
      "470 straight\n",
      "471 strained\n",
      "472 strange\n",
      "473 stressed\n",
      "474 stricken\n",
      "475 strict\n",
      "476 strong\n",
      "477 struck\n",
      "478 stubborn\n",
      "479 stubbornness\n",
      "480 studious\n",
      "481 studying\n",
      "482 stumped\n",
      "483 stung\n",
      "484 stunned\n",
      "485 stupefaction\n",
      "486 stupefied\n",
      "487 stupefy\n",
      "488 stupid\n",
      "489 stuporous\n",
      "490 suave\n",
      "491 subdued\n",
      "492 sublime\n",
      "493 submissive\n",
      "494 suffering\n",
      "495 suggestive\n",
      "496 sulking\n",
      "497 sulky\n",
      "498 sullen\n",
      "499 sullenness\n",
      "500 sunny\n",
      "501 superior\n",
      "502 superiority\n",
      "503 suppressed\n",
      "504 suppressing\n",
      "505 suppression\n",
      "506 sure\n",
      "507 surly\n",
      "508 surprise\n",
      "509 surprised\n",
      "510 surprising\n",
      "511 surprisingly\n",
      "512 surreptitious\n",
      "513 suspect\n",
      "514 suspecting\n",
      "515 suspense\n",
      "516 suspicion\n",
      "517 suspicious\n",
      "518 suspiciously\n",
      "519 suspiciousness\n",
      "520 swaggering\n",
      "521 swearing\n",
      "522 sympathetic\n",
      "523 sympathizing\n",
      "524 sympathy\n",
      "525 taciturn\n",
      "526 talkative\n",
      "527 talking\n",
      "528 tantalized\n",
      "529 tart\n",
      "530 tasteful\n",
      "531 tattling\n",
      "532 taunt\n",
      "533 taunting\n",
      "534 taut\n",
      "535 tearful\n",
      "536 teary\n",
      "537 tease\n",
      "538 teasing\n",
      "539 tempered\n",
      "540 tempest\n",
      "541 tempestuous\n",
      "542 tempted\n",
      "543 tenacious\n",
      "544 tender\n",
      "545 tenderness\n",
      "546 tense\n",
      "547 tensed\n",
      "548 tension\n",
      "549 tentative\n",
      "550 terrified\n",
      "551 terror\n",
      "552 terrorized\n",
      "553 terrorizing\n",
      "554 terse\n",
      "555 testy\n",
      "556 tetchy\n",
      "557 thankful\n",
      "558 thinking\n",
      "559 thought\n",
      "560 thoughtful\n",
      "561 thoughtfulness\n",
      "562 threat\n",
      "563 threatened\n",
      "564 threatening\n",
      "565 thrilled\n",
      "566 thrown\n",
      "567 thunderstruck\n",
      "568 thwarted\n",
      "569 ticked\n",
      "570 tickled\n",
      "571 tied\n",
      "572 tiered\n",
      "573 tight\n",
      "574 tightlipped\n",
      "575 timid\n",
      "576 timidly\n",
      "577 timidness\n",
      "578 tired\n",
      "579 tiredly\n",
      "580 tiredness\n",
      "581 titillated\n",
      "582 tolerant\n",
      "583 tongue\n",
      "584 tormented\n",
      "585 touched\n",
      "586 tough\n",
      "587 toying\n",
      "588 tragic\n",
      "589 tragical\n",
      "590 tranquil\n",
      "591 tranquility\n",
      "592 transfixed\n",
      "593 traumatized\n",
      "594 trembling\n",
      "595 trepid\n",
      "596 trepidation\n",
      "597 trickster\n",
      "598 tricky\n",
      "599 triumphant\n",
      "600 troubled\n",
      "601 troublesome\n",
      "602 troubling\n",
      "603 trusting\n",
      "604 trustworthy\n",
      "605 tumultuous\n",
      "606 turbulent\n",
      "607 twinkly\n",
      "608 umbrage\n",
      "609 umbrageous\n",
      "610 unaffected\n",
      "611 unagitated\n",
      "612 unamused\n",
      "613 unappreciative\n",
      "614 unapproachable\n",
      "615 unassertive\n",
      "616 unassuming\n",
      "617 unaware\n",
      "618 unbelief\n",
      "619 unbelievable\n",
      "620 unbelieving\n",
      "621 unbothered\n",
      "622 uncaring\n",
      "623 uncertain\n",
      "624 uncertainly\n",
      "625 uncertainty\n",
      "626 uncivil\n",
      "627 uncomfortable\n",
      "628 uncommitted\n",
      "629 uncommunicative\n",
      "630 uncomprehending\n",
      "631 uncompromising\n",
      "632 unconcerned\n",
      "633 unconfident\n",
      "634 unconvinced\n",
      "635 uncooperative\n",
      "636 uncurious\n",
      "637 undecided\n",
      "638 underhanded\n",
      "639 understanding\n",
      "640 undesirable\n",
      "641 unease\n",
      "642 uneasily\n",
      "643 uneasiness\n",
      "644 uneasy\n",
      "645 unemotional\n",
      "646 unenthusiastic\n",
      "647 unexcited\n",
      "648 unexpected\n",
      "649 unfamiliar\n",
      "650 unfathomable\n",
      "651 unfazed\n",
      "652 unfeeling\n",
      "653 unfocused\n",
      "654 unforeseen\n",
      "655 unforgiving\n",
      "656 unforthcoming\n",
      "657 unfortunate\n",
      "658 unfriendly\n",
      "659 unhappy\n",
      "660 unhinged\n",
      "661 unimpressed\n",
      "662 uninformed\n",
      "663 uninspired\n",
      "664 uninterested\n",
      "665 uninvolved\n",
      "666 unique\n",
      "667 unlikeable\n",
      "668 unmoved\n",
      "669 unnerved\n",
      "670 unpleasant\n",
      "671 unprepared\n",
      "672 unquiet\n",
      "673 unreactive\n",
      "674 unresolved\n",
      "675 unrestrained\n",
      "676 unruffled\n",
      "677 unsatisfied\n",
      "678 unsettled\n",
      "679 unsociable\n",
      "680 unspeaking\n",
      "681 unspoken\n",
      "682 unstrung\n",
      "683 unsuccessful\n",
      "684 unsure\n",
      "685 unsurprised\n",
      "686 unsuspecting\n",
      "687 unswayed\n",
      "688 unsympathetic\n",
      "689 untouched\n",
      "690 untroubled\n",
      "691 untrusting\n",
      "692 unwanted\n",
      "693 unwavering\n",
      "694 unwelcoming\n",
      "695 unwell\n",
      "696 unwilling\n",
      "697 unyielding\n",
      "698 up\n",
      "699 upbeat\n",
      "700 uplifting\n",
      "701 uppity\n",
      "702 upset\n",
      "703 uptight\n",
      "704 useless\n",
      "705 vacant\n",
      "706 vacuous\n",
      "707 vanquished\n",
      "708 vehement\n",
      "709 vengeful\n",
      "710 venomous\n",
      "711 vex\n",
      "712 vexation\n",
      "713 vexed\n",
      "714 vicious\n",
      "715 victorious\n",
      "716 vigilant\n",
      "717 vile\n",
      "718 villainous\n",
      "719 vindictive\n",
      "720 violence\n",
      "721 violent\n",
      "722 viperous\n",
      "723 vituperative\n",
      "724 vocal\n",
      "725 vocalized\n",
      "726 vulgar\n",
      "727 vulnerability\n",
      "728 vulnerable\n",
      "729 wacky\n",
      "730 waiting\n",
      "731 wanted\n",
      "732 wanting\n",
      "733 wanton\n",
      "734 wariness\n",
      "735 warm\n",
      "736 wary\n",
      "737 wasted\n",
      "738 watch\n",
      "739 watchful\n",
      "740 watching\n",
      "741 wavering\n",
      "742 weariness\n",
      "743 weary\n",
      "744 weeping\n",
      "745 weird\n",
      "746 welcome\n",
      "747 welcoming\n",
      "748 whatever\n",
      "749 whimpering\n",
      "750 whimsical\n",
      "751 whisper\n",
      "752 whistle\n",
      "753 white\n",
      "754 wicked\n",
      "755 wild\n",
      "756 willful\n",
      "757 willing\n",
      "758 wily\n",
      "759 wink\n",
      "760 wired\n",
      "761 wishful\n",
      "762 wistful\n",
      "763 wistfully\n",
      "764 withdraw\n",
      "765 withdrawn\n",
      "766 withheld\n",
      "767 withholding\n",
      "768 woe\n",
      "769 woeful\n",
      "770 wonder\n",
      "771 wondering\n",
      "772 wonderment\n",
      "773 wooly\n",
      "774 woozy\n",
      "775 worn\n",
      "776 worried\n",
      "777 worrisome\n",
      "778 worry\n",
      "779 worrying\n",
      "780 worryingly\n",
      "781 wounded\n",
      "782 wow\n",
      "783 wrathful\n",
      "784 wrathfully\n",
      "785 wrecked\n",
      "786 wretched\n",
      "787 wronged\n",
      "788 wroth\n",
      "789 wry\n",
      "790 yawn\n",
      "791 yawning\n",
      "792 yearning\n",
      "793 yell\n",
      "794 yelling\n",
      "795 yielding\n",
      "796 yuck\n",
      "797 zany\n",
      "798 zealous\n",
      "799 zen\n",
      "800 zoned\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(vocab):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 outburst\n",
      "1 outcry\n",
      "2 outed\n",
      "3 outlandish\n",
      "4 outrage\n",
      "5 outraged\n",
      "6 outspoken\n",
      "7 overbearing\n",
      "8 overexcited\n",
      "9 overjoyed\n",
      "10 overshadowed\n",
      "11 overstrung\n",
      "12 overwhelmed\n",
      "13 overworked\n",
      "14 overwrought\n",
      "15 pain\n",
      "16 pained\n",
      "17 painful\n",
      "18 painfully\n",
      "19 panic\n",
      "20 panicked\n",
      "21 panicky\n",
      "22 paralyzed\n",
      "23 paranoid\n",
      "24 passionate\n",
      "25 passive\n",
      "26 patience\n",
      "27 patient\n",
      "28 patronizing\n",
      "29 pause\n",
      "30 pausing\n",
      "31 peaceful\n",
      "32 peculiar\n",
      "33 peering\n",
      "34 peeved\n",
      "35 peevish\n",
      "36 pensive\n",
      "37 peppy\n",
      "38 perceptive\n",
      "39 perfidious\n",
      "40 perky\n",
      "41 perplexed\n",
      "42 perplexing\n",
      "43 persistent\n",
      "44 personable\n",
      "45 perturbed\n",
      "46 perverse\n",
      "47 pesky\n",
      "48 pessimism\n",
      "49 pessimistic\n",
      "50 pestered\n",
      "51 petitioning\n",
      "52 petrified\n",
      "53 petty\n",
      "54 petulant\n",
      "55 picked\n",
      "56 piercing\n",
      "57 pinched\n",
      "58 pious\n",
      "59 piqued\n",
      "60 pissed\n",
      "61 pitiable\n",
      "62 pitiful\n",
      "63 pity\n",
      "64 pitying\n",
      "65 placated\n",
      "66 placation\n",
      "67 placid\n",
      "68 plain\n",
      "69 plaintive\n",
      "70 planning\n",
      "71 playful\n",
      "72 playfully\n",
      "73 pleading\n",
      "74 pleasant\n",
      "75 pleased\n",
      "76 pleasing\n",
      "77 pleasurable\n",
      "78 pleasure\n",
      "79 pleasured\n",
      "80 pliant\n",
      "81 plotting\n",
      "82 poignant\n",
      "83 pointed\n",
      "84 poised\n",
      "85 polite\n",
      "86 pompous\n",
      "87 ponder\n",
      "88 pondering\n",
      "89 pooping\n",
      "90 posing\n",
      "91 positive\n",
      "92 positivity\n",
      "93 possibly\n",
      "94 pout\n",
      "95 pouting\n",
      "96 pouty\n",
      "97 powerful\n",
      "98 powerless\n",
      "99 pranking\n",
      "100 precarious\n",
      "101 predatory\n",
      "102 prejudiced\n",
      "103 preoccupied\n",
      "104 prepared\n",
      "105 preparing\n",
      "106 pretending\n",
      "107 pretentious\n",
      "108 prideful\n",
      "109 priggish\n",
      "110 primed\n",
      "111 private\n",
      "112 processing\n",
      "113 propositioning\n",
      "114 proud\n",
      "115 provocative\n",
      "116 provoke\n",
      "117 provoked\n",
      "118 provoking\n",
      "119 prying\n",
      "120 psycho\n",
      "121 psychotic\n",
      "122 puckish\n",
      "123 puerile\n",
      "124 pugnacious\n",
      "125 punished\n",
      "126 punishing\n",
      "127 punitive\n",
      "128 punk\n",
      "129 puppyish\n",
      "130 purposeful\n",
      "131 pursed\n",
      "132 putting\n",
      "133 puzzled\n",
      "134 puzzlement\n",
      "135 qualms\n",
      "136 quarrelsome\n",
      "137 queasy\n",
      "138 quenched\n",
      "139 questionable\n",
      "140 questioning\n",
      "141 questioningly\n",
      "142 quiet\n",
      "143 quietness\n",
      "144 quilt\n",
      "145 quirky\n",
      "146 quizzical\n",
      "147 rabid\n",
      "148 racked\n",
      "149 radiant\n",
      "150 rage\n",
      "151 raged\n",
      "152 ragged\n",
      "153 raging\n",
      "154 rancorous\n",
      "155 randy\n",
      "156 rapt\n",
      "157 rattled\n",
      "158 raving\n",
      "159 reactive\n",
      "160 ready\n",
      "161 realization\n",
      "162 reassured\n",
      "163 rebellious\n",
      "164 rebuke\n",
      "165 recalling\n",
      "166 receptive\n",
      "167 reckless\n",
      "168 recoil\n",
      "169 recoiling\n",
      "170 reflecting\n",
      "171 reflection\n",
      "172 reflective\n",
      "173 refulgent\n",
      "174 refusing\n",
      "175 regret\n",
      "176 regretful\n",
      "177 rejected\n",
      "178 rejecting\n",
      "179 rejection\n",
      "180 rejoicing\n",
      "181 relaxation\n",
      "182 relaxed\n",
      "183 relentless\n",
      "184 relief\n",
      "185 relieved\n",
      "186 relived\n",
      "187 reluctant\n",
      "188 reluctantly\n",
      "189 remorse\n",
      "190 remorseful\n",
      "191 repelled\n",
      "192 repressed\n",
      "193 reproach\n",
      "194 reproachful\n",
      "195 repugnance\n",
      "196 repugnant\n",
      "197 repulsed\n",
      "198 repulsion\n",
      "199 resent\n",
      "200 resentful\n",
      "201 resenting\n",
      "202 resentment\n",
      "203 reserved\n",
      "204 resignation\n",
      "205 resigned\n",
      "206 resilience\n",
      "207 resistance\n",
      "208 resistant\n",
      "209 resistent\n",
      "210 resisting\n",
      "211 resolute\n",
      "212 resolved\n",
      "213 responsive\n",
      "214 restful\n",
      "215 resting\n",
      "216 restless\n",
      "217 restlessness\n",
      "218 restrained\n",
      "219 restraint\n",
      "220 retaliating\n",
      "221 retaliatory\n",
      "222 rethinking\n",
      "223 reticence\n",
      "224 reticent\n",
      "225 revengeful\n",
      "226 reverent\n",
      "227 revolted\n",
      "228 revulsion\n",
      "229 righteous\n",
      "230 rigid\n",
      "231 riled\n",
      "232 riotous\n",
      "233 riveted\n",
      "234 roar\n",
      "235 roguish\n",
      "236 roiled\n",
      "237 rough\n",
      "238 roused\n",
      "239 rude\n",
      "240 rueful\n",
      "241 ruffled\n",
      "242 ruminating\n",
      "243 rustled\n",
      "244 ruthless\n",
      "245 sad\n",
      "246 sadden\n",
      "247 saddened\n",
      "248 sadistic\n",
      "249 sadness\n",
      "250 salacious\n",
      "251 salivating\n",
      "252 sanctimonious\n",
      "253 sane\n",
      "254 sanguine\n",
      "255 sappy\n",
      "256 sarcasm\n",
      "257 sarcastic\n",
      "258 sardonic\n",
      "259 sassy\n",
      "260 sated\n",
      "261 satiated\n",
      "262 satirical\n",
      "263 satisfaction\n",
      "264 satisfied\n",
      "265 satisfy\n",
      "266 saturnine\n",
      "267 saucy\n",
      "268 savage\n",
      "269 scandalized\n",
      "270 scare\n",
      "271 scared\n",
      "272 scary\n",
      "273 scattered\n",
      "274 schadenfreude\n",
      "275 scheming\n",
      "276 scoffer\n",
      "277 scoffing\n",
      "278 scorn\n",
      "279 scorned\n",
      "280 scornful\n",
      "281 scowl\n",
      "282 scowling\n",
      "283 scream\n",
      "284 screaming\n",
      "285 scrutinizing\n",
      "286 sealed\n",
      "287 searching\n",
      "288 secretive\n",
      "289 secretively\n",
      "290 secure\n",
      "291 sedate\n",
      "292 seduction\n",
      "293 seductive\n",
      "294 seething\n",
      "295 sensual\n",
      "296 sentimental\n",
      "297 serene\n",
      "298 serious\n",
      "299 seriousness\n",
      "300 servile\n",
      "301 severe\n",
      "302 shabby\n",
      "303 shady\n",
      "304 shaken\n",
      "305 shaky\n",
      "306 shame\n",
      "307 shamed\n",
      "308 shamefaced\n",
      "309 shameful\n",
      "310 shameless\n",
      "311 sharp\n",
      "312 sheepish\n",
      "313 sheepishness\n",
      "314 shelled\n",
      "315 shifty\n",
      "316 shock\n",
      "317 shocked\n",
      "318 shocking\n",
      "319 shockingly\n",
      "320 shook\n",
      "321 shout\n",
      "322 shouting\n",
      "323 shrewd\n",
      "324 shy\n",
      "325 shyness\n",
      "326 sick\n",
      "327 sicken\n",
      "328 sickened\n",
      "329 sigh\n",
      "330 silenced\n",
      "331 silent\n",
      "332 silliness\n",
      "333 silly\n",
      "334 simmering\n",
      "335 simper\n",
      "336 simpering\n",
      "337 simple\n",
      "338 simplicity\n",
      "339 sincere\n",
      "340 sinful\n",
      "341 singing\n",
      "342 sinister\n",
      "343 sinisterly\n",
      "344 sizing\n",
      "345 skeptic\n",
      "346 skeptical\n",
      "347 skeptically\n",
      "348 skepticism\n",
      "349 sketchy\n",
      "350 skittish\n",
      "351 slack\n",
      "352 sleazy\n",
      "353 sleepy\n",
      "354 slick\n",
      "355 slothful\n",
      "356 slow\n",
      "357 sluggish\n",
      "358 sly\n",
      "359 smarmy\n",
      "360 smart\n",
      "361 smashed\n",
      "362 smile\n",
      "363 smiley\n",
      "364 smiling\n",
      "365 smirk\n",
      "366 smirking\n",
      "367 smoldering\n",
      "368 smooching\n",
      "369 smooth\n",
      "370 smug\n",
      "371 smugness\n",
      "372 snake\n",
      "373 snappy\n",
      "374 snarky\n",
      "375 snarl\n",
      "376 snarled\n",
      "377 snarling\n",
      "378 snarly\n",
      "379 sneaky\n",
      "380 sneer\n",
      "381 sneering\n",
      "382 sneeze\n",
      "383 sneezing\n",
      "384 snicker\n",
      "385 snickering\n",
      "386 snide\n",
      "387 sniggering\n",
      "388 sniveling\n",
      "389 snobbish\n",
      "390 snobby\n",
      "391 snooty\n",
      "392 snotty\n",
      "393 sociable\n",
      "394 solemn\n",
      "395 solicitous\n",
      "396 solitary\n",
      "397 solitude\n",
      "398 somber\n",
      "399 somberly\n",
      "400 somnolent\n",
      "401 soothed\n",
      "402 sore\n",
      "403 sorrow\n",
      "404 sorrowful\n",
      "405 sorry\n",
      "406 sour\n",
      "407 spaced\n",
      "408 spacing\n",
      "409 spastic\n",
      "410 speaking\n",
      "411 specious\n",
      "412 speculative\n",
      "413 speechless\n",
      "414 spent\n",
      "415 spirited\n",
      "416 spiritless\n",
      "417 spite\n",
      "418 spiteful\n",
      "419 spoiled\n",
      "420 spooked\n",
      "421 squeamish\n",
      "422 staggered\n",
      "423 stalker\n",
      "424 stare\n",
      "425 staring\n",
      "426 starstruck\n",
      "427 started\n",
      "428 startled\n",
      "429 stately\n",
      "430 steadfast\n",
      "431 steady\n",
      "432 stealthy\n",
      "433 steamed\n",
      "434 steaming\n",
      "435 steeling\n",
      "436 steely\n",
      "437 stern\n",
      "438 stiff\n",
      "439 stifled\n",
      "440 stifling\n",
      "441 still\n",
      "442 stillness\n",
      "443 stimulated\n",
      "444 stinky\n",
      "445 stirred\n",
      "446 stoic\n",
      "447 stoical\n",
      "448 stolid\n",
      "449 stoned\n",
      "450 storming\n",
      "451 stormy\n",
      "452 stout\n",
      "453 straight\n",
      "454 strained\n",
      "455 strange\n",
      "456 stressed\n",
      "457 stricken\n",
      "458 strict\n",
      "459 strong\n",
      "460 struck\n",
      "461 stubborn\n",
      "462 stubbornness\n",
      "463 studious\n",
      "464 studying\n",
      "465 stumped\n",
      "466 stung\n",
      "467 stunned\n",
      "468 stupefaction\n",
      "469 stupefied\n",
      "470 stupefy\n",
      "471 stupid\n",
      "472 stuporous\n",
      "473 suave\n",
      "474 subdued\n",
      "475 sublime\n",
      "476 submissive\n",
      "477 suffering\n",
      "478 suggestive\n",
      "479 sulking\n",
      "480 sulky\n",
      "481 sullen\n",
      "482 sullenness\n",
      "483 sunny\n",
      "484 superior\n",
      "485 superiority\n",
      "486 suppressed\n",
      "487 suppressing\n",
      "488 suppression\n",
      "489 sure\n",
      "490 surly\n",
      "491 surprise\n",
      "492 surprised\n",
      "493 surprising\n",
      "494 surprisingly\n",
      "495 surreptitious\n",
      "496 suspect\n",
      "497 suspecting\n",
      "498 suspense\n",
      "499 suspicion\n",
      "500 suspicious\n",
      "501 suspiciously\n",
      "502 suspiciousness\n",
      "503 swaggering\n",
      "504 swearing\n",
      "505 sympathetic\n",
      "506 sympathizing\n",
      "507 sympathy\n",
      "508 taciturn\n",
      "509 talkative\n",
      "510 talking\n",
      "511 tantalized\n",
      "512 tart\n",
      "513 tasteful\n",
      "514 tattling\n",
      "515 taunt\n",
      "516 taunting\n",
      "517 taut\n",
      "518 tearful\n",
      "519 teary\n",
      "520 tease\n",
      "521 teasing\n",
      "522 tempered\n",
      "523 tempest\n",
      "524 tempestuous\n",
      "525 tempted\n",
      "526 tenacious\n",
      "527 tender\n",
      "528 tenderness\n",
      "529 tense\n",
      "530 tensed\n",
      "531 tension\n",
      "532 tentative\n",
      "533 terrified\n",
      "534 terror\n",
      "535 terrorized\n",
      "536 terrorizing\n",
      "537 terse\n",
      "538 testy\n",
      "539 tetchy\n",
      "540 thankful\n",
      "541 thinking\n",
      "542 thought\n",
      "543 thoughtful\n",
      "544 thoughtfulness\n",
      "545 threat\n",
      "546 threatened\n",
      "547 threatening\n",
      "548 thrilled\n",
      "549 thrown\n",
      "550 thunderstruck\n",
      "551 thwarted\n",
      "552 ticked\n",
      "553 tickled\n",
      "554 tied\n",
      "555 tiered\n",
      "556 tight\n",
      "557 tightlipped\n",
      "558 timid\n",
      "559 timidly\n",
      "560 timidness\n",
      "561 tired\n",
      "562 tiredly\n",
      "563 tiredness\n",
      "564 titillated\n",
      "565 tolerant\n",
      "566 tongue\n",
      "567 tormented\n",
      "568 touched\n",
      "569 tough\n",
      "570 toying\n",
      "571 tragic\n",
      "572 tragical\n",
      "573 tranquil\n",
      "574 tranquility\n",
      "575 transfixed\n",
      "576 traumatized\n",
      "577 trembling\n",
      "578 trepid\n",
      "579 trepidation\n",
      "580 trickster\n",
      "581 tricky\n",
      "582 triumphant\n",
      "583 troubled\n",
      "584 troublesome\n",
      "585 troubling\n",
      "586 trusting\n",
      "587 trustworthy\n",
      "588 tumultuous\n",
      "589 turbulent\n",
      "590 twinkly\n",
      "591 umbrage\n",
      "592 umbrageous\n",
      "593 unaffected\n",
      "594 unagitated\n",
      "595 unamused\n",
      "596 unappreciative\n",
      "597 unapproachable\n",
      "598 unassertive\n",
      "599 unassuming\n",
      "600 unaware\n",
      "601 unbelief\n",
      "602 unbelievable\n",
      "603 unbelieving\n",
      "604 unbothered\n",
      "605 uncaring\n",
      "606 uncertain\n",
      "607 uncertainly\n",
      "608 uncertainty\n",
      "609 uncivil\n",
      "610 uncomfortable\n",
      "611 uncommitted\n",
      "612 uncommunicative\n",
      "613 uncomprehending\n",
      "614 uncompromising\n",
      "615 unconcerned\n",
      "616 unconfident\n",
      "617 unconvinced\n",
      "618 uncooperative\n",
      "619 uncurious\n",
      "620 undecided\n",
      "621 underhanded\n",
      "622 understanding\n",
      "623 undesirable\n",
      "624 unease\n",
      "625 uneasily\n",
      "626 uneasiness\n",
      "627 uneasy\n",
      "628 unemotional\n",
      "629 unenthusiastic\n",
      "630 unexcited\n",
      "631 unexpected\n",
      "632 unfamiliar\n",
      "633 unfathomable\n",
      "634 unfazed\n",
      "635 unfeeling\n",
      "636 unfocused\n",
      "637 unforeseen\n",
      "638 unforgiving\n",
      "639 unforthcoming\n",
      "640 unfortunate\n",
      "641 unfriendly\n",
      "642 unhappy\n",
      "643 unhinged\n",
      "644 unimpressed\n",
      "645 uninformed\n",
      "646 uninspired\n",
      "647 uninterested\n",
      "648 uninvolved\n",
      "649 unique\n",
      "650 unlikeable\n",
      "651 unmoved\n",
      "652 unnerved\n",
      "653 unpleasant\n",
      "654 unprepared\n",
      "655 unquiet\n",
      "656 unreactive\n",
      "657 unresolved\n",
      "658 unrestrained\n",
      "659 unruffled\n",
      "660 unsatisfied\n",
      "661 unsettled\n",
      "662 unsociable\n",
      "663 unspeaking\n",
      "664 unspoken\n",
      "665 unstrung\n",
      "666 unsuccessful\n",
      "667 unsure\n",
      "668 unsurprised\n",
      "669 unsuspecting\n",
      "670 unswayed\n",
      "671 unsympathetic\n",
      "672 untouched\n",
      "673 untroubled\n",
      "674 untrusting\n",
      "675 unwanted\n",
      "676 unwavering\n",
      "677 unwelcoming\n",
      "678 unwell\n",
      "679 unwilling\n",
      "680 unyielding\n",
      "681 upbeat\n",
      "682 uplifting\n",
      "683 uppity\n",
      "684 upset\n",
      "685 uptight\n",
      "686 useless\n",
      "687 vacant\n",
      "688 vacuous\n",
      "689 vanquished\n",
      "690 vehement\n",
      "691 vengeful\n",
      "692 venomous\n",
      "693 vex\n",
      "694 vexation\n",
      "695 vexed\n",
      "696 vicious\n",
      "697 victorious\n",
      "698 vigilant\n",
      "699 vile\n",
      "700 villainous\n",
      "701 vindictive\n",
      "702 violence\n",
      "703 violent\n",
      "704 viperous\n",
      "705 vituperative\n",
      "706 vocal\n",
      "707 vocalized\n",
      "708 vulgar\n",
      "709 vulnerability\n",
      "710 vulnerable\n",
      "711 wacky\n",
      "712 waiting\n",
      "713 wanted\n",
      "714 wanting\n",
      "715 wanton\n",
      "716 wariness\n",
      "717 warm\n",
      "718 wary\n",
      "719 wasted\n",
      "720 watch\n",
      "721 watchful\n",
      "722 watching\n",
      "723 wavering\n",
      "724 weariness\n",
      "725 weary\n",
      "726 weeping\n",
      "727 weird\n",
      "728 welcome\n",
      "729 welcoming\n",
      "730 whatever\n",
      "731 whimpering\n",
      "732 whimsical\n",
      "733 whisper\n",
      "734 whistle\n",
      "735 white\n",
      "736 wicked\n",
      "737 wild\n",
      "738 willful\n",
      "739 willing\n",
      "740 wily\n",
      "741 wink\n",
      "742 wired\n",
      "743 wishful\n",
      "744 wistful\n",
      "745 wistfully\n",
      "746 withdraw\n",
      "747 withdrawn\n",
      "748 withheld\n",
      "749 withholding\n",
      "750 woe\n",
      "751 woeful\n",
      "752 wonder\n",
      "753 wondering\n",
      "754 wonderment\n",
      "755 wooly\n",
      "756 woozy\n",
      "757 worn\n",
      "758 worried\n",
      "759 worrisome\n",
      "760 worry\n",
      "761 worrying\n",
      "762 worryingly\n",
      "763 wounded\n",
      "764 wow\n",
      "765 wrathful\n",
      "766 wrathfully\n",
      "767 wrecked\n",
      "768 wretched\n",
      "769 wronged\n",
      "770 wroth\n",
      "771 wry\n",
      "772 yawn\n",
      "773 yawning\n",
      "774 yearning\n",
      "775 yell\n",
      "776 yelling\n",
      "777 yielding\n",
      "778 yuck\n",
      "779 zany\n",
      "780 zealous\n",
      "781 zen\n",
      "782 zoned\n"
     ]
    }
   ],
   "source": [
    "v_indices = (102, 145, 309, 316, 410, 698)\n",
    "\n",
    "start = 12\n",
    "sections = []\n",
    "for end in v_indices:\n",
    "    for i in range(start, end):\n",
    "#         print(i)\n",
    "        sections.append(vocab[i])\n",
    "        start = end + 1\n",
    "for i in range(start, len(vocab)):\n",
    "    sections.append(vocab[i])\n",
    "#     print(i)\n",
    "\n",
    "for i, v in enumerate(sections):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity at the last layer:  0.91\n",
      "Vector similarity at the embed layer:  1.00\n",
      "Vector similarity at the embed layer:  0.89\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cosine similarity between the word disgusted \n",
    "# in \"disgusted expression\" vs \"too disgusted\" (different contexts)\n",
    "# using the last layer of the model.\n",
    "diff_disgusted_last = 1 - cosine(token_vecs_last[4], token_vecs_last[21])\n",
    "\n",
    "# Calculate the cosine similarity between the word disgusted \n",
    "# in \"disgusted expression\" vs \"too disgusted\" (different contexts)\n",
    "# using the embed layer of the model.\n",
    "diff_disgusted_embed = 1 - cosine(token_vecs_embed[4], token_vecs_embed[21])\n",
    "\n",
    "# Calculate the cosine similarity between the word disgusted \n",
    "# in \"disgusted expression\" vs \"too disgusted\" (different contexts)\n",
    "# using the sum of the last 4 layers of the model.\n",
    "diff_disgusted_sum = 1 - cosine(token_vecs_sum_last_four[4], token_vecs_sum_last_four[21])\n",
    "\n",
    "print('Vector similarity at the last layer:  %.2f' % diff_disgusted_last)\n",
    "print('Vector similarity at the embed layer:  %.2f' % diff_disgusted_embed)\n",
    "print('Vector similarity at the embed layer:  %.2f' % diff_disgusted_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################ END TESTING ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################ START PRODUCTION: GET CONTEXTUAL EMBEDDINGS ##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('./output_CC-ad/')\n",
    "config = RobertaConfig.from_pretrained('./output_CC-ad/')\n",
    "model = RobertaForMaskedLM.from_pretrained('./output_CC-ad/', config=config)\n",
    "# Outputting hidden states allows direct access to hidden layers of the model.\n",
    "config.output_hidden_states = True\n",
    "model.eval()\n",
    "\n",
    "context_file = \"/home/jupyter/Notebooks/crystal/NLP/transformers/examples/CC_WET_test_ad\"\n",
    "output_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_layer12_CC_ad.txt'\n",
    "count_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_layer12_CC_ad_counts.txt'\n",
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/data/vocab_files/vocab_checked.txt'\n",
    "vocab = make_vocab(vocab_file)\n",
    "# vocab = [\"mad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aback\n",
      "\n",
      "Instance 1 of aback.\n",
      "Looking for vocab token: aback\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "aback at index 9: [0.07251332700252533, 0.11217253655195236, -0.015011493116617203, -0.053007014095783234, 0.9514669179916382]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tensor_sum' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b6d138e9bab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{tokenizer.decode(v_tokens[v_index + 1]).strip()} at index {indices[i]}: {token_vecs_layer[indices[i]][:5].tolist()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                         \u001b[0mtensor_layer\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtoken_vecs_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Sum of tensors is: {tensor_sum[0][:5].tolist()} before taking the mean.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;31m# If our vocab word is broken into more than one token, we need to get the mean of the token embeddings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor_sum' is not defined"
     ]
    }
   ],
   "source": [
    "# Process vocabulary words in the outer loop.\n",
    "for v in vocab:\n",
    "    start = timer()\n",
    "    with open(context_file, 'r') as lines:\n",
    "        v_sum = torch.zeros([1, 768])\n",
    "        v_tokens = tokenizer.encode(v)\n",
    "        print(f'\\nThere are {len(v_tokens) - 2} tokens in tokenized vocabulary word:')\n",
    "        for t in v_tokens[1:-1]:\n",
    "            print(tokenizer.decode(t).strip())\n",
    "        count_sentence = 0\n",
    "        count_tensor = 0\n",
    "        \n",
    "        # Process all lines in the context file in the inner loop.\n",
    "        for line in lines:\n",
    "            # Check for this vocab word in this line; if found, split the line into individual sentences.\n",
    "            if v in line.lower().split():\n",
    "                for sentence in line.split('.'):\n",
    "                    if v in sentence.lower():\n",
    "                        line = sentence\n",
    "                        count_sentence += 1\n",
    "                        print(f'\\nInstance {count_sentence} of {tokenizer.decode(v_tokens[1:-1]).strip()}.')\n",
    "                        break\n",
    "                # Split the new sentence-based line into tokens.\n",
    "                # Use max_length to avoid overflowing the maximum sequence length for the model.\n",
    "                tokenized_text = tokenizer.encode(line, add_special_tokens=True, max_length=512)\n",
    "#                 print(f'The decoded sentence has {len(tokenized_text)} tokens and is: {tokenizer.decode(tokenized_text)}')\n",
    "                indices = []              \n",
    "\n",
    "                # Check to see whether the vocab word is found in this particular line.\n",
    "                # Initially, some lines may have comprised multiple sentences, which were\n",
    "                # broken out individually above.\n",
    "                for t in v_tokens[1:-1]:\n",
    "                    print(f'Looking for vocab token: {tokenizer.decode(t).strip()}')\n",
    "                    for i, token_str in enumerate(tokenized_text):\n",
    "#                         print(f'Next sentence token: {tokenizer.decode(token_str).strip()}')\n",
    "#                         print(tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip())\n",
    "                        if tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip():\n",
    "                            indices.append(i)               \n",
    "\n",
    "                ###################################################################################\n",
    "                # If the vocabulary word was found, process the containing line.\n",
    "                if indices:\n",
    "\n",
    "                    # The vocab word was found in this line/sentence, at the locations in indices.\n",
    "                    print(f'Indices are {indices}')\n",
    "                    # Get the feature vectors for all tokens in the line/sentence.\n",
    "                    token_embeddings = create_token_embeddings(tokenized_text)\n",
    "                    # Sum the last four layers to get embeddings for the line/sentence.\n",
    "#                         for t in v_tokens[1:-1]:\n",
    "#                             for i, token_str in enumerate(tokenized_text):\n",
    "#                                 if (tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip()):\n",
    "#                                     print(f'{tokenizer.decode(token_str).strip()} is index {i} in the sentence and {token_str} in the vocabulary.')\n",
    "                    token_vecs_layer = get_layer_token_vecs(token_embeddings, 12)\n",
    "\n",
    "                    # Get the vocab word's contextual embedding for this line.\n",
    "                    tensor_layer = torch.zeros([1, 768])\n",
    "                    for i in range(len(indices)):\n",
    "                        v_index = i % len(v_tokens[1:-1])\n",
    "                        print(f'{tokenizer.decode(v_tokens[v_index + 1]).strip()} at index {indices[i]}: {token_vecs_layer[indices[i]][:5].tolist()}')\n",
    "                        tensor_layer += token_vecs_layer[indices[i]]\n",
    "                        print(f'Sum of tensors is: {tensor_layer[0][:5].tolist()} before taking the mean.')\n",
    "\n",
    "                    # If our vocab word is broken into more than one token, we need to get the mean of the token embeddings.\n",
    "                    tensor_layer /= len(indices)\n",
    "                    print(f'Sum of tensors is: {tensor_layer[0][:5].tolist()} after taking the mean.')\n",
    "\n",
    "                    # Add the embedding distilled from this line to the sum of embeddings for all lines.\n",
    "                    v_sum += tensor_layer\n",
    "                    count_tensor += 1\n",
    "                    print(f'Grand sum of {count_tensor} tensor sets is: {v_sum[0][:5].tolist()}')\n",
    "                ###################################################################################\n",
    "            # Stop processing lines once we've found 2000 instances of our vocab word.\n",
    "            if count_tensor >= 2000:\n",
    "                break\n",
    "        \n",
    "        # We're done processing all lines of 512 tokens or less containing our vocab word.\n",
    "        # Get the mean embedding for the word.\n",
    "        v_mean = v_sum / count_tensor\n",
    "        print(f'Mean of tensors is: {v_mean[0][:5]} ({len(v_mean[0])} features in tensor)')\n",
    "        write_embedding(output_file, v, v_mean)\n",
    "        try:\n",
    "            with open(count_file, 'a') as counts:\n",
    "                counts.write(v + ', ' + str(count_tensor) + '\\n')\n",
    "            print(f'Saved the count of sentences used to create {v} embedding')\n",
    "        except:\n",
    "            print('Wha?! Could not write the sentence count.')\n",
    "    end = timer()\n",
    "    print(f'Run time for {v} was {end - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    vocab = []\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_token_embeddings(tokenized_text):\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)  # Batch size 1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "        encoded_layers = outputs[2]\n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        print(f'Size of token embeddings is {token_embeddings.size()}')\n",
    "        return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the last 4 layers' features\n",
    "def sum_last_four_token_vecs(token_embeddings):\n",
    "    token_vecs_sum_last_four = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_last_four.append(sum_vec)\n",
    "\n",
    "    print ('Shape of summed layers is: %d x %d' % (len(token_vecs_sum_last_four), len(token_vecs_sum_last_four[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_sum_last_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a single layer of the model.\n",
    "def get_layer_token_vecs(token_embeddings, layer_number):\n",
    "    token_vecs_layer = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        layer_vec = token[layer_number]\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_layer.append(layer_vec)\n",
    "\n",
    "    print ('Shape of summed layers is: %d x %d' % (len(token_vecs_layer), len(token_vecs_layer[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "        print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal-venv-3.6",
   "language": "python",
   "name": "crystal-venv-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
