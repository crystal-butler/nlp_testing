{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, RobertaConfig\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/Notebooks/crystal/NLP/transformers/examples'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we're in the transformers directory with fine-tuned model output.\n",
    "os.chdir('/home/jupyter/Notebooks/crystal/NLP/transformers/examples/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# and Transformers documentation: https://huggingface.co/transformers/model_doc/roberta.html#robertaformaskedlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aback', 'abashed', 'abhor', 'abhorred', 'abhorrence', 'abhorrent', 'abominable', 'abound', 'absent', 'absorbed']\n"
     ]
    }
   ],
   "source": [
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/vocab_files/vocab_checked.txt'\n",
    "vocab = make_vocab(vocab_file)\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('./output_CC-ab/')\n",
    "config = RobertaConfig.from_pretrained('./output_CC-ab/')\n",
    "model = RobertaForMaskedLM.from_pretrained('./output_CC-ab/', config=config)\n",
    "# Outputting hidden states allows direct access to hidden layers of the model.\n",
    "config.output_hidden_states = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She             264\n",
      "made            156\n",
      "an               41\n",
      "abhor        35,350\n",
      "red           2,050\n",
      "expression    8,151\n",
      "as               25\n",
      "he               37\n",
      "began           880\n",
      "making          442\n",
      "excuses      19,791\n",
      ".                 4\n",
      "He               91\n",
      "wanted          770\n",
      "to                7\n",
      "keep            489\n",
      "playing         816\n",
      ",                 6\n",
      "but              53\n",
      "abhor        35,350\n",
      "red           2,050\n",
      "the               5\n",
      "poor          2,129\n",
      "sports        1,612\n",
      "manship      17,187\n",
      "of                9\n",
      "his              39\n",
      "fellows      36,304\n",
      ".                 4\n",
      "<s> She made an abhorred expression as he began making excuses. He wanted to keep playing, but abhorred the poor sportsmanship of his fellows.</s>\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple contexts for the word \"abhorred\"\n",
    "test_text = \"She made an abhorred expression as he began making excuses. He wanted to keep playing, but abhorred the poor sportsmanship of his fellows.\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_test_text = tokenizer.encode(test_text, add_special_tokens=True)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for t in tokenized_test_text[1:-1]:\n",
    "        print('{:<12} {:>6,}'.format(tokenizer.decode(t).strip(), t))\n",
    "        \n",
    "print(tokenizer.decode(tokenized_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokens_tensor = torch.tensor([tokenized_test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.1821, -0.0584, -0.0180,  ..., -0.0704,  0.1026, -0.0030],\n",
      "         [-0.1530, -0.0148,  0.1621,  ..., -0.0165,  0.0989,  0.2619],\n",
      "         [-0.1659, -0.2194,  0.6360,  ...,  0.0112, -0.6962,  0.0143],\n",
      "         ...,\n",
      "         [-0.0603, -0.6785,  0.3369,  ..., -0.1608, -0.1464,  0.4502],\n",
      "         [ 0.2217,  0.0367,  0.0914,  ..., -0.8357,  0.0687,  0.0556],\n",
      "         [ 0.0619, -0.0906, -0.0385,  ...,  0.4705,  0.0345, -0.1859]]]), tensor([[[-0.0529,  0.0224,  0.0254,  ...,  0.0015,  0.0220, -0.1526],\n",
      "         [ 0.0337,  0.0747,  0.1383,  ...,  0.2554,  0.2624,  0.6075],\n",
      "         [-0.2280, -0.4853,  0.9908,  ..., -0.1713, -0.9801,  0.2253],\n",
      "         ...,\n",
      "         [-0.3420, -0.4283, -0.2935,  ...,  0.0829,  0.0355,  0.2964],\n",
      "         [-0.1282,  0.1487,  0.2648,  ..., -0.2111, -0.0344, -0.1823],\n",
      "         [-0.2712,  0.4222,  0.0146,  ...,  0.4811, -0.1373, -0.6872]]]), tensor([[[ 4.9576e-02,  4.7782e-02,  1.0115e-02,  ...,  1.5301e-02,\n",
      "           7.0869e-04, -6.6151e-02],\n",
      "         [ 1.6585e-01,  3.7590e-01, -9.9868e-02,  ...,  6.9865e-01,\n",
      "           4.2312e-01,  6.4181e-01],\n",
      "         [-4.4737e-01, -6.1049e-01,  4.7846e-01,  ...,  7.0510e-02,\n",
      "          -8.3817e-01,  4.1901e-02],\n",
      "         ...,\n",
      "         [-4.2351e-01, -5.5416e-01, -3.4401e-01,  ..., -8.1274e-02,\n",
      "          -1.5230e-02,  1.1873e-01],\n",
      "         [-2.3876e-01,  1.5484e-01,  9.9842e-02,  ..., -4.5259e-03,\n",
      "          -4.1328e-02, -2.4828e-01],\n",
      "         [-1.9059e-01,  5.3967e-01,  1.0500e-01,  ...,  5.6551e-01,\n",
      "           1.6221e-01, -4.4047e-01]]]), tensor([[[ 0.0343,  0.0127, -0.0232,  ...,  0.0512,  0.0268,  0.0077],\n",
      "         [ 0.2296,  0.4484, -0.4307,  ...,  0.5636,  0.2837,  0.2249],\n",
      "         [-0.7053, -0.2004, -0.2603,  ..., -0.0009, -0.4574,  0.2746],\n",
      "         ...,\n",
      "         [-0.7456, -0.3884, -0.3885,  ...,  0.1130, -0.1825, -0.2425],\n",
      "         [ 0.0324,  0.2752,  0.0657,  ..., -0.0738,  0.0454, -0.0600],\n",
      "         [-0.0446,  0.3568, -0.1348,  ...,  0.4135, -0.0452, -0.2576]]]), tensor([[[-4.6944e-02,  4.1677e-02, -7.2794e-03,  ...,  5.1589e-02,\n",
      "          -4.1966e-02, -3.6552e-02],\n",
      "         [-7.6823e-02,  4.5732e-01, -2.4615e-01,  ...,  3.4740e-01,\n",
      "           1.2852e-01,  1.9139e-01],\n",
      "         [-6.2073e-01, -3.9625e-01, -8.4976e-01,  ..., -8.0510e-02,\n",
      "          -2.8904e-01,  9.1242e-01],\n",
      "         ...,\n",
      "         [-6.5824e-01, -4.8831e-01, -3.6025e-01,  ...,  5.7998e-03,\n",
      "          -3.6086e-01, -3.4245e-01],\n",
      "         [ 6.2542e-02,  1.1949e-01,  6.5635e-04,  ..., -2.0125e-04,\n",
      "           3.6982e-02, -7.9291e-02],\n",
      "         [-3.3988e-01, -3.3628e-02, -1.3579e-01,  ...,  4.3827e-01,\n",
      "          -9.2783e-02, -1.3392e-01]]]), tensor([[[-5.7115e-04,  4.6936e-02,  1.1553e-03,  ...,  2.5917e-02,\n",
      "          -5.6785e-03, -1.1482e-02],\n",
      "         [ 7.2537e-02,  8.9023e-01, -3.8852e-01,  ...,  1.6331e-01,\n",
      "          -2.7888e-03,  1.2692e-01],\n",
      "         [-5.7811e-01,  6.9700e-02, -6.5168e-01,  ..., -2.9279e-01,\n",
      "          -1.9806e-01,  1.0228e+00],\n",
      "         ...,\n",
      "         [-1.6052e-01, -4.0856e-01, -5.8593e-01,  ...,  2.0237e-02,\n",
      "          -2.2376e-01, -8.8026e-02],\n",
      "         [ 1.0511e-02,  7.9226e-02, -1.5857e-03,  ..., -9.1052e-03,\n",
      "           1.3586e-02,  2.0635e-02],\n",
      "         [-2.0993e-01, -3.4107e-02, -3.0318e-01,  ...,  5.1079e-01,\n",
      "          -1.5782e-01, -7.8787e-02]]]), tensor([[[ 0.0275,  0.0580,  0.0865,  ...,  0.0469, -0.0816, -0.0292],\n",
      "         [ 0.1486,  1.0228, -0.4403,  ...,  0.2388, -0.1499,  0.1832],\n",
      "         [-0.5069, -0.3001, -1.0076,  ..., -0.1250, -0.1729,  0.4713],\n",
      "         ...,\n",
      "         [ 0.0385, -0.1791, -0.3765,  ...,  0.0750, -0.2847, -0.0740],\n",
      "         [ 0.0186,  0.0609, -0.0322,  ...,  0.0103,  0.0428, -0.0116],\n",
      "         [-0.2084,  0.1593, -0.0740,  ...,  0.4177, -0.3367, -0.2290]]]), tensor([[[-4.1080e-02,  6.5282e-02, -8.7687e-03,  ...,  1.1872e-01,\n",
      "          -1.0300e-01, -3.3917e-02],\n",
      "         [ 4.8437e-02,  1.1295e+00, -2.7506e-01,  ...,  3.9332e-01,\n",
      "          -5.2481e-02,  1.2625e-01],\n",
      "         [-4.7661e-02, -1.1313e-01, -4.0228e-01,  ..., -7.5156e-02,\n",
      "           1.2755e-01,  5.3442e-01],\n",
      "         ...,\n",
      "         [ 2.0817e-03, -8.5437e-02, -2.2546e-01,  ...,  4.7579e-01,\n",
      "          -3.4109e-01,  5.0127e-01],\n",
      "         [ 8.9517e-03,  7.6832e-02, -1.1781e-02,  ..., -3.8862e-04,\n",
      "           1.9482e-02,  4.1657e-02],\n",
      "         [ 1.6245e-01,  1.5243e-01,  1.8577e-02,  ...,  6.2193e-01,\n",
      "          -5.0994e-01, -5.9492e-02]]]), tensor([[[-0.0306,  0.0921, -0.0260,  ...,  0.0444, -0.0680, -0.0680],\n",
      "         [ 0.1138,  0.8001, -0.1782,  ...,  0.3452, -0.5355, -0.0098],\n",
      "         [ 0.1875, -0.1305, -0.2541,  ..., -0.2437,  0.2583,  0.3931],\n",
      "         ...,\n",
      "         [-0.0161,  0.1853, -0.1760,  ...,  0.4472, -0.2150,  0.2223],\n",
      "         [ 0.0138,  0.0175, -0.0009,  ...,  0.0358, -0.0215,  0.0069],\n",
      "         [-0.0396, -0.1572, -0.0876,  ...,  0.6679, -0.3987, -0.0692]]]), tensor([[[-0.0949,  0.0382, -0.0118,  ...,  0.0817,  0.0048, -0.0579],\n",
      "         [-0.1291,  0.6376,  0.2347,  ...,  0.2725, -0.4482, -0.0045],\n",
      "         [-0.1528,  0.1061, -0.0781,  ..., -0.3867,  0.0214,  0.2348],\n",
      "         ...,\n",
      "         [ 0.0422,  0.1032, -0.1622,  ...,  0.4890, -0.1377,  0.0784],\n",
      "         [-0.0019,  0.0016,  0.0107,  ...,  0.0058,  0.0210, -0.0013],\n",
      "         [-0.1504, -0.2551, -0.0437,  ...,  0.6457, -0.4360, -0.2977]]]), tensor([[[-8.4519e-03,  6.5337e-02, -3.2542e-02,  ...,  3.6497e-02,\n",
      "          -1.8756e-02, -1.5326e-02],\n",
      "         [ 2.5593e-01,  5.6262e-01, -1.5146e-01,  ...,  1.9684e-01,\n",
      "          -2.9838e-01,  3.3251e-02],\n",
      "         [ 2.9675e-02,  2.4595e-01,  7.2762e-02,  ..., -2.8592e-01,\n",
      "          -1.5922e-01,  2.6295e-01],\n",
      "         ...,\n",
      "         [ 2.0706e-01,  1.9917e-02, -1.9788e-01,  ...,  5.5681e-01,\n",
      "           1.4991e-01, -2.9951e-02],\n",
      "         [-5.8052e-03,  5.7513e-02, -1.6433e-04,  ...,  5.1288e-03,\n",
      "          -1.0641e-02, -1.1278e-02],\n",
      "         [ 1.7328e-01, -5.7643e-02, -1.2278e-01,  ...,  5.6461e-01,\n",
      "          -2.9252e-01,  1.2464e-01]]]), tensor([[[ 0.0006,  0.0337,  0.0218,  ..., -0.0104, -0.0146, -0.0027],\n",
      "         [ 0.5146,  0.4068, -0.1556,  ...,  0.3775, -0.4213, -0.0272],\n",
      "         [ 0.0973,  0.2013,  0.1921,  ..., -0.2859, -0.0461,  0.0926],\n",
      "         ...,\n",
      "         [ 0.3314,  0.0187, -0.1044,  ...,  0.5209,  0.0055, -0.0034],\n",
      "         [-0.0055,  0.0318,  0.0265,  ..., -0.0134, -0.0146, -0.0021],\n",
      "         [ 0.2641, -0.1255, -0.1237,  ...,  0.4026, -0.1057, -0.0276]]]), tensor([[[-0.0641,  0.1020, -0.0035,  ..., -0.1924, -0.0491, -0.0135],\n",
      "         [ 0.1558,  0.1855, -0.0532,  ...,  0.0266, -0.1110,  0.0330],\n",
      "         [ 0.1358,  0.1934,  0.0841,  ..., -0.1331,  0.0219, -0.0678],\n",
      "         ...,\n",
      "         [-0.0844,  0.1910, -0.0006,  ..., -0.1056, -0.0051, -0.0224],\n",
      "         [-0.0658,  0.1010, -0.0042,  ..., -0.1962, -0.0502, -0.0153],\n",
      "         [ 0.0463,  0.0420,  0.0284,  ...,  0.0353, -0.0138,  0.0538]]]))\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(tokenized_test_text).unsqueeze(0)  # Batch size 1\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "    # Documentation for the \"forward\" method of RobertaForMaskedLM\n",
    "    # details what is returned for each index of \"outputs.\"\n",
    "    # print(outputs[0])  # masked_lm_loss\n",
    "    # print(outputs[1])  # prediction_scores\n",
    "    print(outputs[2])  # hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13\n",
      "Number of batches: 1\n",
      "Number of tokens: 32\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(outputs[2]))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(outputs[2][layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(outputs[2][layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(outputs[2][layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_layers = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For our token, select its feature values from layer 5.\n",
    "token_i = 4\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "# print(vec)\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Type of encoded_layers:  <class 'list'>\n",
      "Tensor shape for each layer:  torch.Size([1, 32, 768])\n"
     ]
    }
   ],
   "source": [
    "# `encoded_layers` is a tuple.\n",
    "print('     Type of encoded_layers: ', type(list(encoded_layers)))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', encoded_layers[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 32, 768])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 32, 768])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 13, 768])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  disgusted\n",
      "21  disgusted\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_test_text):\n",
    "      if tokenizer.decode(token_str).strip() == \"disgusted\":\n",
    "        print (i, tokenizer.decode(token_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 32 x 768\n"
     ]
    }
   ],
   "source": [
    "# Create token vectors by summing the last 4 layers of the model.\n",
    "# Stores the token vectors, with shape [32 x 768]\n",
    "token_vecs_sum = []\n",
    "# `token_embeddings` is a [32 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # `token` is a [13 x 768] tensor\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "# Shape is: 32 x 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 32 x 768\n"
     ]
    }
   ],
   "source": [
    "# Extract the last layer's features\n",
    "token_vecs_last = []\n",
    "# `token_embeddings` is a [32 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # `token` is a [13 x 768] tensor\n",
    "    # Extract the vector from the last layer.\n",
    "    last_vec = token[-1]\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_last.append(last_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_last), len(token_vecs_last[0])))\n",
    "# Shape is: 32 x 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 32 x 768\n"
     ]
    }
   ],
   "source": [
    "# Extract the embedding layer's features (layer 0 is the embedding layer)\n",
    "token_vecs_embed = []\n",
    "# `token_embeddings` is a [32 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # `token` is a [13 x 768] tensor\n",
    "    # Extract the vector from the last layer.\n",
    "    embed_vec = token[0]\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_embed.append(last_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_embed), len(token_vecs_embed[0])))\n",
    "# Shape is: 32 x 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the last 4 layers' features\n",
    "def sum_last_four_token_vecs(token_embeddings):\n",
    "    token_vecs_sum_last_four = []\n",
    "    # `token_embeddings` is a [32 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_last_four.append(sum_vec)\n",
    "\n",
    "    print ('Shape of summed layers is: %d x %d' % (len(token_vecs_sum_last_four), len(token_vecs_sum_last_four[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_sum_last_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_token_embeddings(tokenized_text):\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)  # Batch size 1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "        encoded_layers = outputs[2]\n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        print(f'Size of token embeddings is {token_embeddings.size()}')\n",
    "        \n",
    "return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 21]\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "for i, token_str in enumerate(tokenized_test_text):\n",
    "      if tokenizer.decode(token_str).strip() == \"disgusted\":\n",
    "        indices.append(i)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 32 x 768\n",
      "disgusted at index 4: tensor([-0.7637, -1.0375, -0.4840, -0.2251, -0.1792])\n",
      "disgusted at index 21: tensor([-0.8459, -0.2973, -0.3336, -0.6372, -0.7399])\n"
     ]
    }
   ],
   "source": [
    "token_vecs_sum_last_four = sum_last_four_token_vecs(token_embeddings)\n",
    "for i in range(len(indices)):\n",
    "    print(f'disgusted at index {indices[i]}: {str(token_vecs_sum_last_four[indices[i]][:5])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity at the last layer:  0.91\n",
      "Vector similarity at the embed layer:  1.00\n",
      "Vector similarity at the embed layer:  0.89\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cosine similarity between the word disgusted \n",
    "# in \"disgusted expression\" vs \"too disgusted\" (different contexts)\n",
    "# using the last layer of the model.\n",
    "diff_disgusted_last = 1 - cosine(token_vecs_last[4], token_vecs_last[21])\n",
    "\n",
    "# Calculate the cosine similarity between the word disgusted \n",
    "# in \"disgusted expression\" vs \"too disgusted\" (different contexts)\n",
    "# using the embed layer of the model.\n",
    "diff_disgusted_embed = 1 - cosine(token_vecs_embed[4], token_vecs_embed[21])\n",
    "\n",
    "# Calculate the cosine similarity between the word disgusted \n",
    "# in \"disgusted expression\" vs \"too disgusted\" (different contexts)\n",
    "# using the sum of the last 4 layers of the model.\n",
    "diff_disgusted_sum = 1 - cosine(token_vecs_sum_last_four[4], token_vecs_sum_last_four[21])\n",
    "\n",
    "print('Vector similarity at the last layer:  %.2f' % diff_disgusted_last)\n",
    "print('Vector similarity at the embed layer:  %.2f' % diff_disgusted_embed)\n",
    "print('Vector similarity at the embed layer:  %.2f' % diff_disgusted_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab\n",
      "ashed\n",
      "\n",
      "Instance 1 of abashed is at sentence index:\n",
      "<s> abashed — If you are abashed, you feel embarrassed and ashamed</s>\n",
      "ab\n",
      "<s>\n",
      "ab\n",
      "ashed\n",
      "—\n",
      "If\n",
      "you\n",
      "are\n",
      "ab\n",
      "ashed\n",
      ",\n",
      "you\n",
      "feel\n",
      "embarrassed\n",
      "and\n",
      "ashamed\n",
      "</s>\n",
      "ashed\n",
      "<s>\n",
      "ab\n",
      "ashed\n",
      "—\n",
      "If\n",
      "you\n",
      "are\n",
      "ab\n",
      "ashed\n",
      ",\n",
      "you\n",
      "feel\n",
      "embarrassed\n",
      "and\n",
      "ashamed\n",
      "</s>\n",
      "[1, 7, 2, 8]\n",
      "ab is index 1 in the sentence and 4091 in the vocabulary.\n",
      "ab is index 7 in the sentence and 4091 in the vocabulary.\n",
      "ashed is index 2 in the sentence and 9512 in the vocabulary.\n",
      "ashed is index 8 in the sentence and 9512 in the vocabulary.\n",
      "Shape is: 16 x 768\n",
      "ab at index 1: [0.7572423219680786, 0.45225709676742554, -0.09246983379125595, -0.057879433035850525, 0.04058745875954628]\n",
      "Sum of tensors is: [0.7572423219680786, 0.45225709676742554, -0.09246983379125595, -0.057879433035850525, 0.04058745875954628]\n",
      "ashed at index 7: [0.28692275285720825, -0.5777364373207092, -0.279830664396286, -1.4166972637176514, -1.3615275621414185]\n",
      "Sum of tensors is: [1.0441651344299316, -0.1254793405532837, -0.37230050563812256, -1.474576711654663, -1.3209401369094849]\n",
      "</s> at index 2: [-0.11984765529632568, -2.0102732181549072, 0.19409692287445068, -0.4035230875015259, 3.8420047760009766]\n",
      "Sum of tensors is: [0.924317479133606, -2.1357526779174805, -0.17820358276367188, -1.878099799156189, 2.5210647583007812]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-64f6b9a9d56c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mtensor_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{tokenizer.decode(v_tokens[i + 1]).strip()} at index {indices[i]}: {token_vecs_sum_last_four[indices[i]][:5].tolist()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                         \u001b[0mtensor_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtoken_vecs_sum_last_four\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Sum of tensors is: {tensor_sum[0][:5].tolist()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "context_file = \"/home/jupyter/Notebooks/crystal/NLP/transformers/examples/CC_WET_test_aa\"\n",
    "output_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_lastfour_CC_ab.txt'\n",
    "for v in vocab[1:2]:\n",
    "    with open(context_file, 'r') as lines:\n",
    "        v_sum = torch.zeros([1, 768])\n",
    "        v_tokens = tokenizer.encode(v)\n",
    "        print('\\nTokenized vocabulary word is:')\n",
    "        for t in v_tokens[1:-1]:\n",
    "            print(tokenizer.decode(t).strip())\n",
    "        count_sentence = 0\n",
    "        count_tensor = 0\n",
    "        for line in lines:\n",
    "            if v in line.split():\n",
    "                for sentence in line.split('.'):\n",
    "                    if v in sentence:\n",
    "                        line = sentence\n",
    "                        count_sentence += 1\n",
    "                        print(f'Instance {count_sentence} of {tokenizer.decode(v_tokens[1:-1]).strip()} is at sentence index:')\n",
    "                        break\n",
    "                # Split the line into tokens.\n",
    "                # Use max_length to avoid overflowing the maximum sequence length for the model.\n",
    "                tokenized_text = tokenizer.encode(line, add_special_tokens=True, max_length=512)\n",
    "                print(f'The decoded sentence is: {tokenizer.decode(tokenized_text)}')\n",
    "                indices = []\n",
    "                for t in v_tokens[1:-1]:\n",
    "                    print(f'Looking for vocab token: {tokenizer.decode(t).strip()}')\n",
    "                    for i, token_str in enumerate(tokenized_text):\n",
    "                        print(f'Next sentence token: {tokenizer.decode(token_str).strip()}')\n",
    "#                         print(tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip())\n",
    "                        if tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip():\n",
    "                            indices.append(i)\n",
    "                if indices:\n",
    "                    print(indices)\n",
    "                    token_embeddings = create_token_embeddings(tokenized_text)\n",
    "                    for t in v_tokens[1:-1]:\n",
    "                        for i, token_str in enumerate(tokenized_text):\n",
    "                            if (tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip()):\n",
    "    #                         print(tokenizer.encode(v))\n",
    "    #                         print(tokenizer.decode(tokenizer.encode(v)).strip())\n",
    "                                print(f'{tokenizer.decode(token_str).strip()} is index {i} in the sentence and {token_str} in the vocabulary.')\n",
    "                    token_vecs_sum_last_four = sum_last_four_token_vecs(token_embeddings)\n",
    "                    tensor_sum = torch.zeros([1, 768])\n",
    "                    for i in range(len(indices)):\n",
    "                        print(f'{tokenizer.decode(v_tokens[i + 1]).strip()} at index {indices[i]}: {token_vecs_sum_last_four[indices[i]][:5].tolist()}')\n",
    "                        tensor_sum += token_vecs_sum_last_four[indices[i]]\n",
    "                        print(f'Sum of tensors is: {tensor_sum[0][:5].tolist()}')\n",
    "#                         print(tensor_sum)\n",
    "                    v_sum += tensor_sum\n",
    "                    count_tensor += 1\n",
    "                    print(f'Grand sum of {count_tensor} tensors is: {v_sum[0][:5].tolist()}')\n",
    "        v_mean = v_sum / count_tensor\n",
    "        print(f'Mean of tensors is: {v_mean[0][:5]} ({len(v_mean[0])} features in tensor)')\n",
    "        write_embedding(output_file, v, v_mean)\n",
    "    \n",
    "end = timer()\n",
    "print(f'Total run time was {end - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "        print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal-venv-3.6",
   "language": "python",
   "name": "crystal-venv-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
