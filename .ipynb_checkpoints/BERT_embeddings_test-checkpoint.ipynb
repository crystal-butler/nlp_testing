{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/Notebooks/crystal/NLP/nlp_testing'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disgusted'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(tokenizer.vocab.keys())[5000:5020]\n",
    "list(tokenizer.vocab.keys())[17733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"disgusted\"\n",
    "text = \"[CLS] She made a disgusted pout [SEP] Her disgusted expression was contagious [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recreate vocabulary words from their tokenized representations.\n",
    "for t in tokenized_text:\n",
    "    this_word = ''\n",
    "    for token in t:\n",
    "        this_word += token.strip('#')\n",
    "#     print(this_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mark each of the tokens as belonging to sentence \"0\" or \"1\".\n",
    "\n",
    "segments_ids = [1] * len(tokenized_text[3])\n",
    "# segments_ids = [0,0,0]\n",
    "print (segments_ids)\n",
    "print(indexed_tokens)\n",
    "print(tokenized_text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens[3]])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 1\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For our token, select its feature values from layer 5.\n",
    "token_i = 1\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "# print(vec)\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# `encoded_layers` is a Python list.\n",
    "print('     Type of encoded_layers: ', type(encoded_layers))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', encoded_layers[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 1 x 3072\n",
      "tensor([-0.2373,  0.8259, -0.6190,  ..., -0.3836, -0.5039,  0.6153])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_last = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_last.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_last), len(token_vecs_cat_last[0])))\n",
    "print(token_vecs_cat_last[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 1 x 768\n"
     ]
    }
   ],
   "source": [
    "# Sum the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_last = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_last.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_last), len(token_vecs_sum_last[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the first 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_first = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[0], token[1], token[2], token[3]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_first.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_first), len(token_vecs_cat_first[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the first 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_first = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[:4], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_first.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_first), len(token_vecs_sum_first[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the middle 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_middle1 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[4], token[5], token[6], token[7]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_middle1.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_middle1), len(token_vecs_cat_middle1[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the middle 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_middle1 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[4:8], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_middle1.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_middle1), len(token_vecs_sum_middle1[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the middle 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_middle2 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[8], token[9], token[10], token[11]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_middle2.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_middle2), len(token_vecs_cat_middle2[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the middle 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_middle2 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[8:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_middle2.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_middle2), len(token_vecs_sum_middle2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate all hidden layers to create word embeddings.\n",
    "token_vecs_cat_all = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[0], token[1], token[2], token[3], token[4], token[5], token[6], token[7], token[8], token[9], token[10], token[11]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_all.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_all), len(token_vecs_cat_all[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum all hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_all = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_all.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_all), len(token_vecs_sum_all[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a single vector to represent the pair of sentences by averaging across tokens.\n",
    "# `encoded_layers` has shape [12 x 1 x 22 x 768]\n",
    "sentences_vec = []\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = encoded_layers[11][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "for s in sentence_embedding:\n",
    "    sentences_vec.append(s)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n",
    "print(sentence_embedding[767])\n",
    "print(sentence_embedding[-1])\n",
    "print(f'Shape of sentences vector is: {len(sentences_vec)}')\n",
    "print(sentences_vec[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "# Test the similarity of a word with itself.\n",
    "# For words trained contextually, self-synonymy is less than 1.\n",
    "similarity = 1 - cosine(token_vecs_cat[0], token_vecs_cat[0])\n",
    "print(f'Similarity of {tokenized_text[8]} and {tokenized_text[8]} in token_vecs_cat is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum[4], token_vecs_sum[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_first[4], token_vecs_cat_first[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_first is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_first[4], token_vecs_sum_first[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_first is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_middle1[4], token_vecs_cat_middle1[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_middle1 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_middle1[4], token_vecs_sum_middle1[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_middle1 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_middle2[4], token_vecs_cat_middle2[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_middle2 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_middle2[4], token_vecs_sum_middle2[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_middle2 is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_cat_all[4], token_vecs_cat_all[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_all is: {similarity}')\n",
    "# similarity = 1 - cosine(token_vecs_sum_all[4], token_vecs_sum_all[9])\n",
    "# print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_all is: {similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "############## BEGIN TESTING STATIC CONTEXTUAL EMBEDDING CREATION ####################\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    # start = timer()\n",
    "    vocab = []\n",
    "    # vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/vocab_files/vocab_checked.txt'\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    # end = timer()\n",
    "    # run_time = end - start\n",
    "#     print(f'There are {len(vocab)} words in the vocabulary.\\n')\n",
    "#     print(f'It took {run_time} seconds to read the vocabulary file into memory.')\n",
    "#     print(f'Test word is {vocab[2]}.')\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(vocab):\n",
    "    tokenized_text = []\n",
    "    indexed_tokens = []\n",
    "    for word in vocab:\n",
    "        # Add the special tokens.\n",
    "    #     marked_text = \"[CLS] \" + word + \" [SEP]\"\n",
    "        marked_text = word\n",
    "\n",
    "        # Split the sentence into tokens.\n",
    "        # tokenized_text = tokenizer.tokenize(marked_text)\n",
    "        tokenized_text.append(tokenizer.tokenize(marked_text))\n",
    "#         print(f'Added {tokenized_text[-1]} to the tokenized_text array.')\n",
    "\n",
    "\n",
    "        # Map the token strings to their vocabulary indeces.\n",
    "        indexed_tokens.append(tokenizer.convert_tokens_to_ids(tokenized_text[-1]))\n",
    "\n",
    "        # Display the words with their indeces.\n",
    "    #     print(f'The word {tokenized_text[-1][1]} is at index {indexed_tokens[-1]}.')\n",
    "#         for tup in zip(tokenized_text[-1], indexed_tokens[-1]):\n",
    "#             print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
    "    return tokenized_text, indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_segments_IDs(tokenized_text):\n",
    "    # Create segment IDs for sentence 1 (there can be a sentence 0 to compare to\n",
    "    # sentence 1, but we're not doing that).\n",
    "    # Check that indices and token indices look correct.\n",
    "    segments_IDs = []\n",
    "    for i in range(len(tokenized_text)):\n",
    "        segments_IDs.append([1] * len(tokenized_text[i]))\n",
    "#     for i in range(len(segments_IDs)):\n",
    "#         print (segments_IDs[i])\n",
    "#         print(tokenized_text[i])\n",
    "    return segments_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(indexed_tokens, segments_IDs):\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_IDs])\n",
    "\n",
    "    # Load pre-trained model (weights)\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "\n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "#         print('Type of encoded_layers: ', type(encoded_layers))\n",
    "        # Each layer in the list is a torch tensor.\n",
    "#         print('Tensor shape for each layer: ', encoded_layers[0].size())\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to\n",
    "    # create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "#     print(token_embeddings.size())\n",
    "\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "#     print(token_embeddings.size())\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "#     print(token_embeddings.size())\n",
    "    \n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_last_four(token_embeddings): \n",
    "    # Concatenate the last 4 hidden layers to create contextual embeddings.\n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    token_vecs_cat_last = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat_last.append(cat_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_cat_last), len(token_vecs_cat_last[0])))\n",
    "    print(token_vecs_cat_last[0])\n",
    "    \n",
    "    return token_vecs_cat_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_first_four(token_embeddings):\n",
    "    token_vecs_cat_first = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last \n",
    "        # four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        cat_vec = torch.cat((token[0], token[1], token[2], token[3]), dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat_first.append(cat_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_cat_first), len(token_vecs_cat_first[0])))\n",
    "    print(token_vecs_cat_first[0])\n",
    "    \n",
    "    return token_vecs_cat_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_embedding(token_embeddings):\n",
    "    mean_embedding = sum(token_embeddings) / len(token_embeddings)\n",
    "    print(mean_embedding)\n",
    "    \n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct_tokens(tokenized_text):\n",
    "    vocab_word = ''\n",
    "    for i in tokenized_text:\n",
    "        vocab_word += i.strip('#')\n",
    "    print(vocab_word)\n",
    "\n",
    "    return vocab_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "        print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aback'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.5053,  0.4971,  0.0317,  ..., -0.9494,  0.5799,  0.7537])\n",
      "aback\n",
      "Saved the embedding for aback.\n",
      "['aba', '##shed'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.1385,  0.1992, -0.6109,  ..., -0.1529,  0.0258, -0.1901])\n",
      "abashed\n",
      "Saved the embedding for abashed.\n",
      "['ab', '##hor'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.0943,  0.3695, -1.2897,  ...,  0.1303,  0.2182,  0.5159])\n",
      "abhor\n",
      "Saved the embedding for abhor.\n",
      "['ab', '##hor', '##red'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.0665,  0.2880, -1.2760,  ...,  0.0154,  0.2874,  0.4358])\n",
      "abhorred\n",
      "Saved the embedding for abhorred.\n",
      "['ab', '##hor', '##rence'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.0444,  0.3661, -1.2157,  ..., -0.0860,  0.2170,  0.4015])\n",
      "abhorrence\n",
      "Saved the embedding for abhorrence.\n",
      "['ab', '##hor', '##rent'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.0299,  0.3516, -1.1975,  ..., -0.1055,  0.1379,  0.5008])\n",
      "abhorrent\n",
      "Saved the embedding for abhorrent.\n",
      "['ab', '##omi', '##nable'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([-0.1525,  0.3198, -1.2715,  ..., -0.0955,  0.4356,  0.5340])\n",
      "abominable\n",
      "Saved the embedding for abominable.\n",
      "['ab', '##ound'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.0638,  0.2629, -1.2453,  ...,  0.0225,  0.1847,  0.4092])\n",
      "abound\n",
      "Saved the embedding for abound.\n",
      "['absent'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.4521,  0.1061, -0.4779,  ...,  0.2767, -0.0463, -0.0110])\n",
      "absent\n",
      "Saved the embedding for absent.\n",
      "['absorbed'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.7795,  0.8153,  0.0398,  ..., -0.5581, -0.3897,  0.4518])\n",
      "absorbed\n",
      "Saved the embedding for absorbed.\n",
      "['acceptance'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1205,  0.5259, -0.0911,  ..., -0.2337, -0.0629,  0.7135])\n",
      "acceptance\n",
      "Saved the embedding for acceptance.\n",
      "['accepted'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 3.6161e-04,  4.9226e-01, -2.7979e-01,  ..., -9.9633e-02,\n",
      "         2.3448e-01,  5.4789e-01])\n",
      "accepted\n",
      "Saved the embedding for accepted.\n",
      "['accepting'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.0763,  0.3260, -0.4907,  ..., -0.1538, -0.0674,  0.4879])\n",
      "accepting\n",
      "Saved the embedding for accepting.\n",
      "['acc', '##om', '##mo', '##dating'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([-0.3355, -0.2063, -0.0745,  ...,  0.3892,  0.7441,  0.8218])\n",
      "accommodating\n",
      "Saved the embedding for accommodating.\n",
      "['accomplished'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.5944,  0.7593, -0.1734,  ..., -0.4961,  0.2025, -0.1727])\n",
      "accomplished\n",
      "Saved the embedding for accomplished.\n",
      "['accord', '##ant'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.3170, -0.1001, -0.0009,  ...,  0.7048,  0.8281,  0.2016])\n",
      "accordant\n",
      "Saved the embedding for accordant.\n",
      "['acc', '##urse', '##d'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([-0.3793, -0.1344,  0.1207,  ...,  0.5067,  0.7098,  0.9900])\n",
      "accursed\n",
      "Saved the embedding for accursed.\n",
      "['acc', '##usa', '##tory'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([-0.2960, -0.3214,  0.0889,  ...,  0.7691,  0.7250,  0.8146])\n",
      "accusatory\n",
      "Saved the embedding for accusatory.\n",
      "['accused'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1689,  0.2206,  0.2253,  ..., -0.1096,  0.1622,  0.0140])\n",
      "accused\n",
      "Saved the embedding for accused.\n",
      "['accusing'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.3663, -0.0284,  0.0976,  ..., -0.3368,  0.4107,  0.0374])\n",
      "accusing\n",
      "Saved the embedding for accusing.\n",
      "['ace', '##rb', '##ic'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([0.0707, 0.2103, 0.2219,  ..., 0.1299, 0.4903, 0.3016])\n",
      "acerbic\n",
      "Saved the embedding for acerbic.\n",
      "['acidic'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.2818,  0.6978, -0.2317,  ..., -0.2214, -0.2032,  0.1642])\n",
      "acidic\n",
      "Saved the embedding for acidic.\n",
      "['active'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.2326,  0.9250, -0.1191,  ..., -0.0390,  0.0167,  0.3701])\n",
      "active\n",
      "Saved the embedding for active.\n",
      "['acute'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.4748,  0.2792,  0.3337,  ..., -0.3865,  0.3020,  0.3140])\n",
      "acute\n",
      "Saved the embedding for acute.\n",
      "['adamant'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.3846, -0.0636, -0.6831,  ..., -0.5805,  0.6256, -0.0859])\n",
      "adamant\n",
      "Saved the embedding for adamant.\n",
      "['add', '##led'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.1636, -0.4818, -0.4597,  ...,  0.1920,  0.1332,  0.5782])\n",
      "addled\n",
      "Saved the embedding for addled.\n",
      "['admiration'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([1.1107, 0.5495, 0.0098,  ..., 0.3824, 0.7252, 0.2131])\n",
      "admiration\n",
      "Saved the embedding for admiration.\n",
      "['admit'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0801, -0.1251,  0.1254,  ...,  0.7289,  0.9093,  1.1929])\n",
      "admit\n",
      "Saved the embedding for admit.\n",
      "['ad', '##oration'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.0249,  0.0707, -0.0193,  ..., -0.3659,  0.1235,  1.1537])\n",
      "adoration\n",
      "Saved the embedding for adoration.\n",
      "['ad', '##orin', '##g'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.1172, -0.2434, -0.0824,  ...,  0.0838,  0.1723,  1.0008])\n",
      "adoring\n",
      "Saved the embedding for adoring.\n",
      "['ad', '##rift'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.2050, -0.3438, -0.0324,  ...,  0.1996,  0.3146,  1.0630])\n",
      "adrift\n",
      "Saved the embedding for adrift.\n",
      "['ad', '##vers', '##aria', '##l'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([ 0.0666, -0.2499, -0.1469,  ..., -0.2066, -0.0153,  1.1773])\n",
      "adversarial\n",
      "Saved the embedding for adversarial.\n",
      "['af', '##fa', '##bility'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.3977,  0.2998, -0.3589,  ..., -0.0217,  0.6209, -0.1415])\n",
      "affability\n",
      "Saved the embedding for affability.\n",
      "['affected'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.2981,  0.5739, -0.1980,  ..., -0.6815, -0.2972,  0.5221])\n",
      "affected\n",
      "Saved the embedding for affected.\n",
      "['affection', '##ate'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.1900,  0.3582,  0.1356,  ..., -0.4018,  0.5205,  0.6047])\n",
      "affectionate\n",
      "Saved the embedding for affectionate.\n",
      "['af', '##flict', '##ed'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.1690,  0.3284, -0.4128,  ..., -0.1809,  0.6392, -0.2326])\n",
      "afflicted\n",
      "Saved the embedding for afflicted.\n",
      "['af', '##front', '##ed'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.2510,  0.3494, -0.5215,  ...,  0.0173,  0.5817, -0.0752])\n",
      "affronted\n",
      "Saved the embedding for affronted.\n",
      "['afl', '##utter'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.9138,  0.4283, -0.3716,  ...,  0.0233,  0.0200,  0.2565])\n",
      "aflutter\n",
      "Saved the embedding for aflutter.\n",
      "['afraid'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0737,  0.2915, -0.1547,  ..., -0.2382, -0.0785,  0.6777])\n",
      "afraid\n",
      "Saved the embedding for afraid.\n",
      "['ag', '##ape'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.0769,  0.9567, -0.1925,  ..., -0.2054,  0.8228,  0.7902])\n",
      "agape\n",
      "Saved the embedding for agape.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aggravated'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1777,  0.4265, -0.1913,  ..., -0.1204, -0.2994,  0.2673])\n",
      "aggravated\n",
      "Saved the embedding for aggravated.\n",
      "['ag', '##gra', '##vation'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.0667,  1.0663, -0.3278,  ..., -0.1178,  0.6243,  0.8914])\n",
      "aggravation\n",
      "Saved the embedding for aggravation.\n",
      "['aggression'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1853,  1.0595, -0.3109,  ..., -0.1024, -0.0157,  0.5554])\n",
      "aggression\n",
      "Saved the embedding for aggression.\n",
      "['aggressive'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0088,  0.5032, -0.5507,  ..., -0.0484,  0.1403,  0.2706])\n",
      "aggressive\n",
      "Saved the embedding for aggressive.\n",
      "['ag', '##gr', '##ie', '##ve'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([ 0.1614,  0.7959, -0.3026,  ..., -0.2129,  0.8440,  0.9215])\n",
      "aggrieve\n",
      "Saved the embedding for aggrieve.\n",
      "['ag', '##gr', '##ie', '##ved'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([ 0.1654,  0.7993, -0.3148,  ..., -0.1785,  0.7905,  0.9246])\n",
      "aggrieved\n",
      "Saved the embedding for aggrieved.\n",
      "['ag', '##has', '##t'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.1019,  0.8534, -0.2746,  ..., -0.1449,  0.9297,  0.8703])\n",
      "aghast\n",
      "Saved the embedding for aghast.\n",
      "['agitated'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.0096,  0.9270, -0.4666,  ..., -0.3978,  0.0863,  0.7954])\n",
      "agitated\n",
      "Saved the embedding for agitated.\n",
      "['ago', '##g'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.4704,  0.7104,  0.0192,  ..., -0.9048,  0.0639,  1.0450])\n",
      "agog\n",
      "Saved the embedding for agog.\n",
      "['ago', '##ni', '##zed'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.4335,  0.6901, -0.1771,  ..., -0.8478, -0.0159,  1.1202])\n",
      "agonized\n",
      "Saved the embedding for agonized.\n",
      "['agree', '##able'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.4308,  0.4233,  0.2899,  ..., -0.0703,  0.8025,  0.5212])\n",
      "agreeable\n",
      "Saved the embedding for agreeable.\n",
      "['ag', '##ress', '##ive'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.1005,  1.0699, -0.3213,  ..., -0.2663,  0.7951,  0.7708])\n",
      "agressive\n",
      "Saved the embedding for agressive.\n",
      "['air', '##head'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.6639,  0.8847,  0.6294,  ..., -0.2565,  0.4113, -0.3863])\n",
      "airhead\n",
      "Saved the embedding for airhead.\n",
      "['alarm'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1222,  0.6413, -0.5854,  ..., -0.0542, -0.0137, -0.6760])\n",
      "alarm\n",
      "Saved the embedding for alarm.\n",
      "['alarmed'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.3262,  0.2881, -0.8592,  ..., -0.1767,  0.3700,  0.3378])\n",
      "alarmed\n",
      "Saved the embedding for alarmed.\n",
      "['alarm', '##ing'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.1122,  0.6816, -0.5081,  ..., -0.0787,  0.3642, -0.6778])\n",
      "alarming\n",
      "Saved the embedding for alarming.\n",
      "['alert'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.2504,  0.4888,  0.3502,  ..., -0.9463,  0.6425, -0.8461])\n",
      "alert\n",
      "Saved the embedding for alert.\n",
      "['alerted'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.3156,  0.5728, -0.5240,  ..., -0.2714,  0.3028, -0.0503])\n",
      "alerted\n",
      "Saved the embedding for alerted.\n",
      "['alien', '##ated'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.0818,  0.2480,  0.4116,  ..., -0.4162, -0.0440, -0.1612])\n",
      "alienated\n",
      "Saved the embedding for alienated.\n",
      "['allergic'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.2866,  0.2021,  0.0675,  ..., -0.3051, -0.3642, -0.2076])\n",
      "allergic\n",
      "Saved the embedding for allergic.\n",
      "['alleviate', '##d'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.2998,  0.2491,  0.1289,  ..., -0.6662, -0.1526,  0.1656])\n",
      "alleviated\n",
      "Saved the embedding for alleviated.\n",
      "['all', '##uring'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.2253,  0.0058, -0.2638,  ...,  0.4658,  0.1473, -0.0081])\n",
      "alluring\n",
      "Saved the embedding for alluring.\n",
      "['al', '##oof'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.5146, -0.0272, -0.0992,  ...,  0.6112,  0.1657,  0.4889])\n",
      "aloof\n",
      "Saved the embedding for aloof.\n",
      "['ama', '##tory'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.4161, -0.3371, -0.2572,  ..., -0.0369,  0.5298,  0.1601])\n",
      "amatory\n",
      "Saved the embedding for amatory.\n",
      "['amazed'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.5026,  0.1645, -0.4694,  ...,  0.1452,  0.2892,  0.8518])\n",
      "amazed\n",
      "Saved the embedding for amazed.\n",
      "['amazement'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.6219,  0.6121, -0.0926,  ..., -0.3612,  0.3343,  0.4862])\n",
      "amazement\n",
      "Saved the embedding for amazement.\n",
      "['amazing'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.4215, -0.0483, -0.1648,  ..., -0.0957,  0.1689, -0.1652])\n",
      "amazing\n",
      "Saved the embedding for amazing.\n",
      "['ambition'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.3765, -0.2162,  0.2079,  ..., -0.1689,  0.4423,  0.2911])\n",
      "ambition\n",
      "Saved the embedding for ambition.\n",
      "['ambitious'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.3806,  0.2907,  0.1204,  ..., -0.0781,  0.2049,  0.1305])\n",
      "ambitious\n",
      "Saved the embedding for ambitious.\n",
      "['am', '##bi', '##vale', '##nce'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([ 0.0596, -0.2629, -0.1610,  ..., -0.2080,  0.3961,  0.3136])\n",
      "ambivalence\n",
      "Saved the embedding for ambivalence.\n",
      "['am', '##bi', '##valent'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.0855, -0.3003, -0.0858,  ..., -0.1764,  0.5322,  0.2354])\n",
      "ambivalent\n",
      "Saved the embedding for ambivalent.\n",
      "['am', '##ena', '##ble'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.1398, -0.3007,  0.0014,  ..., -0.1737,  0.6472,  0.2476])\n",
      "amenable\n",
      "Saved the embedding for amenable.\n",
      "['ami', '##able'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.0493, -0.6528,  0.0585,  ..., -0.0295,  0.6878,  0.6993])\n",
      "amiable\n",
      "Saved the embedding for amiable.\n",
      "['ami', '##cable'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.1929, -0.6145,  0.3026,  ...,  0.1835,  0.6596,  0.5605])\n",
      "amicable\n",
      "Saved the embedding for amicable.\n",
      "['amused'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0135,  0.5777, -0.1599,  ..., -0.6088,  0.7464,  0.6337])\n",
      "amused\n",
      "Saved the embedding for amused.\n",
      "['amusement'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1948,  0.9442,  0.1725,  ..., -0.1322,  0.3062,  0.4754])\n",
      "amusement\n",
      "Saved the embedding for amusement.\n",
      "['analytical'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.0190,  1.4264, -0.0962,  ...,  0.2647, -0.1238,  0.2906])\n",
      "analytical\n",
      "Saved the embedding for analytical.\n",
      "['analyzing'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1899,  1.0511,  0.0322,  ..., -0.1180, -0.4331,  0.4083])\n",
      "analyzing\n",
      "Saved the embedding for analyzing.\n",
      "['anger'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0792,  1.2233, -0.1786,  ..., -0.0681, -0.1048,  0.4824])\n",
      "anger\n",
      "Saved the embedding for anger.\n",
      "['angered'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1072,  0.4118, -0.4205,  ..., -0.0934,  0.0183,  0.2755])\n",
      "angered\n",
      "Saved the embedding for angered.\n",
      "['angrily'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1583, -0.0754, -0.7228,  ..., -0.1329,  0.6738,  0.0101])\n",
      "angrily\n",
      "Saved the embedding for angrily.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1087,  0.5733, -0.3362,  ..., -0.0491,  0.3709,  0.4512])\n",
      "angry\n",
      "Saved the embedding for angry.\n",
      "['ang', '##st'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.3287,  0.1654, -0.1497,  ..., -0.2954,  0.2572,  0.5989])\n",
      "angst\n",
      "Saved the embedding for angst.\n",
      "['anguish'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.3491,  0.3474, -0.4197,  ...,  0.1957,  0.3298,  0.0772])\n",
      "anguish\n",
      "Saved the embedding for anguish.\n",
      "['anguish', '##ed'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.2998,  0.3180, -0.4168,  ...,  0.1504,  0.3565,  0.3088])\n",
      "anguished\n",
      "Saved the embedding for anguished.\n",
      "['animated'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1402,  0.2226, -0.3230,  ...,  0.6311,  0.7730,  0.6633])\n",
      "animated\n",
      "Saved the embedding for animated.\n",
      "['an', '##imo', '##sity'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([-0.1607,  0.3769, -0.2438,  ..., -0.3759,  0.1383,  0.7063])\n",
      "animosity\n",
      "Saved the embedding for animosity.\n",
      "['annoyance'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.2053,  0.1891,  0.1939,  ..., -0.6405,  0.0541,  0.4799])\n",
      "annoyance\n",
      "Saved the embedding for annoyance.\n",
      "['annoyed'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.1482,  0.2925,  0.0670,  ..., -0.4254,  0.1755,  0.5588])\n",
      "annoyed\n",
      "Saved the embedding for annoyed.\n",
      "['annoying'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.0753,  0.2327,  0.2018,  ..., -0.0659, -0.0038,  0.6942])\n",
      "annoying\n",
      "Saved the embedding for annoying.\n",
      "['antagonist', '##ic'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.2095,  0.4188, -0.3818,  ..., -0.1000,  0.1955,  0.7520])\n",
      "antagonistic\n",
      "Saved the embedding for antagonistic.\n",
      "['ant', '##ago', '##ni', '##zed'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([ 0.4141,  0.4296, -0.2522,  ..., -0.3733,  0.2298, -0.2282])\n",
      "antagonized\n",
      "Saved the embedding for antagonized.\n",
      "['anticipated'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0919,  0.5220, -0.4011,  ..., -0.0329,  0.3526, -0.3940])\n",
      "anticipated\n",
      "Saved the embedding for anticipated.\n",
      "['anticipating'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0259,  0.5039, -0.1975,  ..., -0.0035,  0.3437,  0.3652])\n",
      "anticipating\n",
      "Saved the embedding for anticipating.\n",
      "['anticipation'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.2147,  0.2693, -0.2175,  ..., -0.5441,  0.2814, -0.1097])\n",
      "anticipation\n",
      "Saved the embedding for anticipation.\n",
      "['anti', '##ci', '##pati', '##ve'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([-0.0593,  0.0568,  0.3754,  ..., -0.4786, -0.1632,  0.2918])\n",
      "anticipative\n",
      "Saved the embedding for anticipative.\n",
      "['anti', '##ci', '##pa', '##tory'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([ 0.0230, -0.0152,  0.3542,  ..., -0.2529, -0.1224,  0.3169])\n",
      "anticipatory\n",
      "Saved the embedding for anticipatory.\n",
      "['anti', '##pathy'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.0037,  0.0492,  0.2924,  ..., -0.1397, -0.3223,  0.0923])\n",
      "antipathy\n",
      "Saved the embedding for antipathy.\n",
      "['ants', '##y'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.0677,  0.3223,  0.0602,  ..., -0.1033,  0.2030, -0.0634])\n",
      "antsy\n",
      "Saved the embedding for antsy.\n",
      "['anxiety'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.5603,  0.6207, -0.2761,  ..., -0.5699, -0.4852,  0.3996])\n",
      "anxiety\n",
      "Saved the embedding for anxiety.\n",
      "['anxious'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.4838,  0.3643, -0.0884,  ..., -0.4135,  0.1575,  0.3860])\n",
      "anxious\n",
      "Saved the embedding for anxious.\n",
      "['anxiously'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.0205, -0.3760, -0.6429,  ..., -0.0343,  0.3875,  0.0926])\n",
      "anxiously\n",
      "Saved the embedding for anxiously.\n",
      "['ap', '##ath', '##etic'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.0879,  0.5116, -0.2899,  ..., -0.6720, -0.2232,  0.2488])\n",
      "apathetic\n",
      "Saved the embedding for apathetic.\n",
      "['ap', '##athy'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.0215,  0.5857, -0.3594,  ..., -0.7559, -0.1834, -0.0104])\n",
      "apathy\n",
      "Saved the embedding for apathy.\n",
      "['apologetic'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.3353,  0.4886,  0.0352,  ...,  0.5679,  0.6259,  0.4588])\n",
      "apologetic\n",
      "Saved the embedding for apologetic.\n",
      "['appalled'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.4226,  0.2816, -0.8448,  ..., -0.1593,  0.5125,  0.2620])\n",
      "appalled\n",
      "Saved the embedding for appalled.\n",
      "['app', '##all', '##ingly'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([-0.3095, -0.1918,  0.0156,  ..., -0.4695,  0.6407,  0.6907])\n",
      "appallingly\n",
      "Saved the embedding for appallingly.\n",
      "['app', '##eased'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.3802, -0.3281, -0.0518,  ..., -0.2110,  0.9099,  0.7724])\n",
      "appeased\n",
      "Saved the embedding for appeased.\n",
      "['app', '##ea', '##sing'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([-0.2316, -0.1585,  0.0556,  ..., -0.2788,  0.9482,  0.7155])\n",
      "appeasing\n",
      "Saved the embedding for appeasing.\n",
      "['app', '##re', '##cia', '##tive'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([-0.3863, -0.1259,  0.0493,  ..., -0.6438,  0.5908,  0.7257])\n",
      "appreciative\n",
      "Saved the embedding for appreciative.\n",
      "['apprehension'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.4275,  0.4010, -0.0921,  ..., -0.0405,  0.0957,  0.4274])\n",
      "apprehension\n",
      "Saved the embedding for apprehension.\n",
      "['app', '##re', '##hen', '##sive'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([-0.2671, -0.1467, -0.1140,  ..., -0.6458,  0.7720,  0.7534])\n",
      "apprehensive\n",
      "Saved the embedding for apprehensive.\n",
      "['approve'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.1263,  0.0360,  0.2067,  ...,  0.2497,  0.4604,  0.1057])\n",
      "approve\n",
      "Saved the embedding for approve.\n",
      "['approved'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.1129,  0.2867,  0.0020,  ...,  0.2296,  0.3400,  0.1344])\n",
      "approved\n",
      "Saved the embedding for approved.\n",
      "['app', '##roving'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.2056, -0.2875,  0.0824,  ..., -0.3209,  0.8965,  0.6169])\n",
      "approving\n",
      "Saved the embedding for approving.\n",
      "['argue'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.1291,  0.7469, -0.4794,  ...,  0.3936,  0.5880,  0.8184])\n",
      "argue\n",
      "Saved the embedding for argue.\n",
      "['argument', '##ative'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.5548,  0.6958, -0.5868,  ...,  0.6235, -0.0118,  1.1090])\n",
      "argumentative\n",
      "Saved the embedding for argumentative.\n",
      "['aroused'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0364,  0.5569,  0.2744,  ..., -0.4262,  0.4517,  0.3641])\n",
      "aroused\n",
      "Saved the embedding for aroused.\n",
      "['arrogance'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.8047,  0.4682, -0.0177,  ..., -0.1153,  0.2640,  0.4821])\n",
      "arrogance\n",
      "Saved the embedding for arrogance.\n",
      "['arrogant'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.5770,  0.6421,  0.3445,  ..., -0.0454,  0.2287,  0.3213])\n",
      "arrogant\n",
      "Saved the embedding for arrogant.\n",
      "['arrogant', '##ly'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.4500,  0.7433,  0.3558,  ..., -0.1435,  0.3087,  0.4983])\n",
      "arrogantly\n",
      "Saved the embedding for arrogantly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.3631,  0.4375, -0.0695,  ..., -0.8754, -0.4007,  0.5359])\n",
      "artificial\n",
      "Saved the embedding for artificial.\n",
      "['ashamed'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0583, -0.0833, -0.4251,  ..., -0.1705, -0.1941,  0.0921])\n",
      "ashamed\n",
      "Saved the embedding for ashamed.\n",
      "['aspiring'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.3538, -0.2145, -0.2146,  ..., -0.4136,  0.1431, -0.1170])\n",
      "aspiring\n",
      "Saved the embedding for aspiring.\n",
      "['assert', '##ive'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.6213,  0.4932, -0.4482,  ..., -0.3063, -0.0021,  0.6945])\n",
      "assertive\n",
      "Saved the embedding for assertive.\n",
      "['assert', '##ively'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.6273,  0.3859, -0.3971,  ...,  0.1550, -0.0304,  0.5407])\n",
      "assertively\n",
      "Saved the embedding for assertively.\n",
      "['assessing'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.0338,  0.3858,  0.0530,  ..., -0.2291,  0.4045,  0.8602])\n",
      "assessing\n",
      "Saved the embedding for assessing.\n",
      "['assured'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.1375,  0.1481, -0.2244,  ..., -0.5895,  0.7862,  0.4941])\n",
      "assured\n",
      "Saved the embedding for assured.\n",
      "['astonished'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1536,  0.2299, -0.3622,  ..., -0.2241,  0.5853,  0.2651])\n",
      "astonished\n",
      "Saved the embedding for astonished.\n",
      "['astonishment'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1233,  0.5736, -0.0812,  ..., -0.4841,  0.5994,  0.4071])\n",
      "astonishment\n",
      "Saved the embedding for astonishment.\n",
      "['as', '##tou', '##nded'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.0403,  0.1890, -0.1266,  ...,  0.0801,  0.3617,  0.2982])\n",
      "astounded\n",
      "Saved the embedding for astounded.\n",
      "['attempting'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.3154,  0.5950, -0.4634,  ..., -0.0723,  0.0161,  0.4245])\n",
      "attempting\n",
      "Saved the embedding for attempting.\n",
      "['at', '##ten', '##tive'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([-0.1184,  0.2851, -0.1315,  ...,  0.0055,  0.2654,  0.5046])\n",
      "attentive\n",
      "Saved the embedding for attentive.\n",
      "['at', '##ten', '##tive', '##ness'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([-0.1545,  0.2835, -0.0912,  ..., -0.0250,  0.1968,  0.5246])\n",
      "attentiveness\n",
      "Saved the embedding for attentiveness.\n",
      "['attracted'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1138, -0.3897,  0.1672,  ..., -0.4133,  0.0237,  0.4229])\n",
      "attracted\n",
      "Saved the embedding for attracted.\n",
      "['ave', '##nging'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.1430, -0.0090, -0.5570,  ...,  1.0034,  0.3006,  0.0578])\n",
      "avenging\n",
      "Saved the embedding for avenging.\n",
      "['ave', '##rse'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.1196, -0.1589, -0.6044,  ...,  1.1218,  0.6229,  0.2566])\n",
      "averse\n",
      "Saved the embedding for averse.\n",
      "['ave', '##rs', '##ion'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.0426,  0.1136, -0.5306,  ...,  0.9676,  0.5314,  0.4534])\n",
      "aversion\n",
      "Saved the embedding for aversion.\n",
      "['ave', '##rs', '##ive'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([-0.0378,  0.1193, -0.5592,  ...,  0.8770,  0.5061,  0.4312])\n",
      "aversive\n",
      "Saved the embedding for aversive.\n",
      "['avid'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0518, -0.7379,  0.3371,  ...,  0.1518,  0.4973,  0.1287])\n",
      "avid\n",
      "Saved the embedding for avid.\n",
      "['avoiding'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.3242, -0.5054, -0.4779,  ..., -0.0201, -0.2882,  0.7462])\n",
      "avoiding\n",
      "Saved the embedding for avoiding.\n",
      "['awaiting'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0975, -0.2472,  0.1401,  ..., -0.1901,  0.3376, -0.1917])\n",
      "awaiting\n",
      "Saved the embedding for awaiting.\n",
      "['awakened'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.0309,  0.7422, -0.5158,  ..., -0.0214,  0.2823,  0.1385])\n",
      "awakened\n",
      "Saved the embedding for awakened.\n",
      "['aware'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.1223,  0.4753, -0.0398,  ..., -0.2397, -0.3036,  0.1343])\n",
      "aware\n",
      "Saved the embedding for aware.\n",
      "['awareness'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.1294,  0.7744,  0.3655,  ...,  0.1701, -0.1737,  0.4233])\n",
      "awareness\n",
      "Saved the embedding for awareness.\n",
      "['awe'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([0.2459, 0.7009, 0.2551,  ..., 0.0910, 0.2840, 0.2310])\n",
      "awe\n",
      "Saved the embedding for awe.\n",
      "['awe', '##d'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([0.1354, 0.7229, 0.2922,  ..., 0.2448, 0.4307, 0.3275])\n",
      "awed\n",
      "Saved the embedding for awed.\n",
      "['awe', '##st', '##ruck'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([0.2379, 0.7491, 0.4337,  ..., 0.3333, 0.5487, 0.4799])\n",
      "awestruck\n",
      "Saved the embedding for awestruck.\n",
      "['awful'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.5002,  0.1600, -0.4008,  ..., -0.5601,  0.1914,  0.3105])\n",
      "awful\n",
      "Saved the embedding for awful.\n",
      "['awkward'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.3111,  0.7651, -0.4312,  ..., -0.5308, -0.0651,  0.0896])\n",
      "awkward\n",
      "Saved the embedding for awkward.\n",
      "['awkward', '##ness'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.3561,  0.7948, -0.2785,  ..., -0.3290,  0.0112,  0.1004])\n",
      "awkwardness\n",
      "Saved the embedding for awkwardness.\n",
      "['axe', '##d'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.4197,  0.3965, -0.1863,  ..., -0.0649, -0.0888, -0.1076])\n",
      "axed\n",
      "Saved the embedding for axed.\n",
      "['back', '##hand', '##ed'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.2939,  0.2874, -0.4272,  ...,  0.1999,  0.0178,  0.0996])\n",
      "backhanded\n",
      "Saved the embedding for backhanded.\n",
      "['badly'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.0858,  0.0806, -0.8409,  ..., -0.3511, -0.3530,  0.3812])\n",
      "badly\n",
      "Saved the embedding for badly.\n",
      "['ba', '##ffle'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.4574, -0.1643, -0.3921,  ..., -0.1695, -0.0414,  0.2714])\n",
      "baffle\n",
      "Saved the embedding for baffle.\n",
      "['baffled'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.6315,  0.3270, -0.5674,  ..., -0.2353,  0.3064,  1.1126])\n",
      "baffled\n",
      "Saved the embedding for baffled.\n",
      "['ba', '##ff', '##ling'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.5036, -0.1726, -0.3729,  ...,  0.1548, -0.1475,  0.4361])\n",
      "baffling\n",
      "Saved the embedding for baffling.\n",
      "['baked'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.4478,  0.0434, -0.0115,  ...,  0.3419, -0.4385, -0.4968])\n",
      "baked\n",
      "Saved the embedding for baked.\n",
      "['ban', '##al'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.0354, -0.6492, -0.4903,  ..., -0.1065,  0.1191, -0.2858])\n",
      "banal\n",
      "Saved the embedding for banal.\n",
      "['barking'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.5607, -0.1913,  0.1277,  ...,  0.2336, -0.0410,  0.3177])\n",
      "barking\n",
      "Saved the embedding for barking.\n",
      "['bash', '##ful'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.7442,  0.2940, -0.4747,  ...,  0.2130,  0.2709,  0.3584])\n",
      "bashful\n",
      "Saved the embedding for bashful.\n",
      "['beaming'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([0.5395, 0.6108, 0.4671,  ..., 0.2522, 0.4979, 0.3189])\n",
      "beaming\n",
      "Saved the embedding for beaming.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bear', '##ish'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.3202,  0.2045, -0.1138,  ..., -0.0706,  0.2217,  0.2988])\n",
      "bearish\n",
      "Saved the embedding for bearish.\n",
      "['beat'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([0.4377, 0.1360, 0.2417,  ..., 0.8847, 0.1448, 0.0081])\n",
      "beat\n",
      "Saved the embedding for beat.\n",
      "['beaten'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0072, -0.1867,  0.2120,  ...,  0.4141,  0.0417,  0.3091])\n",
      "beaten\n",
      "Saved the embedding for beaten.\n",
      "['bed', '##ev', '##iled'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.6281,  0.1020,  0.0128,  ...,  0.5353, -0.1057, -0.0246])\n",
      "bedeviled\n",
      "Saved the embedding for bedeviled.\n",
      "['be', '##fu', '##ddled'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.3044,  0.2001, -0.5320,  ...,  0.3145,  0.0343,  0.3894])\n",
      "befuddled\n",
      "Saved the embedding for befuddled.\n",
      "['begging'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1304, -0.5173,  0.0946,  ...,  0.0033,  0.0161,  0.0175])\n",
      "begging\n",
      "Saved the embedding for begging.\n",
      "['beg', '##rud', '##ge'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.0914, -0.3344, -0.2142,  ...,  0.1852,  0.3603,  0.3807])\n",
      "begrudge\n",
      "Saved the embedding for begrudge.\n",
      "['beg', '##rud', '##ging'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.1097, -0.3409, -0.3602,  ...,  0.0870,  0.3886,  0.2483])\n",
      "begrudging\n",
      "Saved the embedding for begrudging.\n",
      "['beg', '##rud', '##gingly'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.1493, -0.3365, -0.2676,  ...,  0.0223,  0.3700,  0.3734])\n",
      "begrudgingly\n",
      "Saved the embedding for begrudgingly.\n",
      "['beg', '##uil', '##ed'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.0783, -0.3585, -0.3461,  ..., -0.2035,  0.4110,  0.2197])\n",
      "beguiled\n",
      "Saved the embedding for beguiled.\n",
      "['bela', '##ted'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.9905, -0.4129, -0.7569,  ..., -0.1830,  0.3659, -0.2595])\n",
      "belated\n",
      "Saved the embedding for belated.\n",
      "['bel', '##itt', '##ling'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.8487,  0.6272, -0.3216,  ...,  0.4084,  0.2685,  0.0771])\n",
      "belittling\n",
      "Saved the embedding for belittling.\n",
      "['bell', '##iger', '##ence'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 1.1311,  0.2301, -0.0071,  ..., -0.2039, -0.0405, -0.1060])\n",
      "belligerence\n",
      "Saved the embedding for belligerence.\n",
      "['bell', '##iger', '##ent'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 1.1758,  0.2619,  0.0462,  ..., -0.0518,  0.0210, -0.0288])\n",
      "belligerent\n",
      "Saved the embedding for belligerent.\n",
      "['belonging'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0812, -0.2566,  0.4959,  ..., -0.2849, -0.2897,  0.2991])\n",
      "belonging\n",
      "Saved the embedding for belonging.\n",
      "['be', '##mus', '##ed'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.3126,  0.0674, -0.4871,  ...,  0.2517,  0.4064,  0.2392])\n",
      "bemused\n",
      "Saved the embedding for bemused.\n",
      "['be', '##mus', '##ement'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.3517,  0.1883, -0.3768,  ...,  0.6694,  0.1253,  0.7816])\n",
      "bemusement\n",
      "Saved the embedding for bemusement.\n",
      "['ben', '##ev', '##ole', '##nce'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([ 0.0671,  0.1203, -0.0511,  ...,  0.2263,  0.1450,  0.0388])\n",
      "benevolence\n",
      "Saved the embedding for benevolence.\n",
      "['benevolent'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([0.5525, 0.5849, 0.2870,  ..., 0.1755, 0.2028, 0.6836])\n",
      "benevolent\n",
      "Saved the embedding for benevolent.\n",
      "['ben', '##umb', '##ed'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.1038, -0.0620, -0.0418,  ...,  0.4280,  0.0760,  0.3388])\n",
      "benumbed\n",
      "Saved the embedding for benumbed.\n",
      "['be', '##rate'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.3012,  0.1452, -0.4076,  ...,  0.1933,  0.1899,  0.5839])\n",
      "berate\n",
      "Saved the embedding for berate.\n",
      "['be', '##rating'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.3706,  0.1647, -0.4132,  ...,  0.3170,  0.1929,  0.5263])\n",
      "berating\n",
      "Saved the embedding for berating.\n",
      "['be', '##rea', '##ved'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.3183,  0.1139, -0.4699,  ...,  0.2379,  0.0298,  0.4362])\n",
      "bereaved\n",
      "Saved the embedding for bereaved.\n",
      "['be', '##re', '##ft'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.3067,  0.1044, -0.4624,  ...,  0.4625,  0.1181,  0.5300])\n",
      "bereft\n",
      "Saved the embedding for bereft.\n",
      "['be', '##see', '##ching'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.3759,  0.1503, -0.4043,  ...,  0.1325, -0.0031,  0.3943])\n",
      "beseeching\n",
      "Saved the embedding for beseeching.\n",
      "['best', '##ed'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.1011,  0.4131, -0.5202,  ...,  0.7498,  0.0462, -0.3912])\n",
      "bested\n",
      "Saved the embedding for bested.\n",
      "['betrayal'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.9936,  0.0431, -0.2966,  ..., -0.0196, -0.1559,  0.5218])\n",
      "betrayal\n",
      "Saved the embedding for betrayal.\n",
      "['betrayed'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.4946, -0.1240, -0.0516,  ...,  0.1426,  0.4726,  0.0556])\n",
      "betrayed\n",
      "Saved the embedding for betrayed.\n",
      "['bewildered'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0399,  0.2498, -0.2276,  ..., -0.1942,  0.0662,  0.7455])\n",
      "bewildered\n",
      "Saved the embedding for bewildered.\n",
      "['be', '##wil', '##der', '##ment'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([ 0.3511,  0.0652, -0.4868,  ...,  0.4498,  0.2950,  0.2537])\n",
      "bewilderment\n",
      "Saved the embedding for bewilderment.\n",
      "['bi'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0701, -0.7002, -0.1275,  ...,  0.8774,  0.0349,  0.9274])\n",
      "bi\n",
      "Saved the embedding for bi.\n",
      "['bi', '##lio', '##us'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([-0.0792, -0.8248,  0.0199,  ...,  0.4565,  0.1333,  0.8595])\n",
      "bilious\n",
      "Saved the embedding for bilious.\n",
      "['bit'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.8140, -0.1992, -0.3198,  ..., -0.4049, -0.0757,  0.4109])\n",
      "bit\n",
      "Saved the embedding for bit.\n",
      "['biting'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.4871, -0.1885, -0.2118,  ..., -0.1029,  0.2984,  0.3756])\n",
      "biting\n",
      "Saved the embedding for biting.\n",
      "['bitter'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0698,  0.0980, -0.3135,  ...,  0.2954, -0.1109,  0.5912])\n",
      "bitter\n",
      "Saved the embedding for bitter.\n",
      "['bitter', '##sw', '##eet'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.0138,  0.0241, -0.1793,  ...,  0.3159,  0.0591,  0.5327])\n",
      "bittersweet\n",
      "Saved the embedding for bittersweet.\n",
      "['blaming'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.1779,  0.6029, -0.1342,  ...,  0.1500,  0.0058, -0.0706])\n",
      "blaming\n",
      "Saved the embedding for blaming.\n",
      "['bland'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1881, -0.1525, -0.1406,  ..., -0.5473, -0.0680,  0.5231])\n",
      "bland\n",
      "Saved the embedding for bland.\n",
      "['blank'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.6540,  0.8063, -0.5703,  ..., -0.0268,  0.2657,  0.1948])\n",
      "blank\n",
      "Saved the embedding for blank.\n",
      "['b', '##lase'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.4766,  0.2165, -0.1510,  ...,  0.0184, -0.1245,  0.1529])\n",
      "blase\n",
      "Saved the embedding for blase.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blazed'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.2652, -0.3125, -0.0259,  ..., -0.1631,  0.0746,  0.4862])\n",
      "blazed\n",
      "Saved the embedding for blazed.\n",
      "['bleak'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.4755,  0.3233,  0.0442,  ..., -0.2899, -0.1171,  0.2193])\n",
      "bleak\n",
      "Saved the embedding for bleak.\n",
      "['b', '##lea', '##ry'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.3839,  0.1876, -0.3379,  ...,  0.1590, -0.1931,  0.3375])\n",
      "bleary\n",
      "Saved the embedding for bleary.\n",
      "['blessed'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.5355,  0.6504,  0.1743,  ..., -0.8914,  0.5753,  0.7794])\n",
      "blessed\n",
      "Saved the embedding for blessed.\n",
      "['blew'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.8021, -0.0257, -0.1239,  ..., -0.0482, -0.1299, -0.3386])\n",
      "blew\n",
      "Saved the embedding for blew.\n",
      "['blinded'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.6044,  0.8082, -0.3499,  ..., -0.0726,  0.2953,  0.2890])\n",
      "blinded\n",
      "Saved the embedding for blinded.\n",
      "['blinds', '##ided'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.3962,  0.3208, -0.6474,  ...,  0.3626,  0.1653,  0.6602])\n",
      "blindsided\n",
      "Saved the embedding for blindsided.\n",
      "['bliss'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.2817,  0.7083,  0.2201,  ..., -0.7395,  0.4421,  0.1996])\n",
      "bliss\n",
      "Saved the embedding for bliss.\n",
      "['bliss', '##ful'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.2342,  0.6414,  0.1320,  ..., -0.8986,  0.4616,  0.3495])\n",
      "blissful\n",
      "Saved the embedding for blissful.\n",
      "['bliss', '##fully'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.2705,  0.5696,  0.1204,  ..., -0.7520,  0.4928,  0.3005])\n",
      "blissfully\n",
      "Saved the embedding for blissfully.\n",
      "['b', '##lit', '##he'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.4276,  0.1031, -0.2229,  ...,  0.2966, -0.0848, -0.1060])\n",
      "blithe\n",
      "Saved the embedding for blithe.\n",
      "['blown'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.9478,  0.1557, -0.1303,  ...,  0.1398, -0.6305, -0.0111])\n",
      "blown\n",
      "Saved the embedding for blown.\n",
      "['blue'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.0933, -0.5730, -0.3493,  ..., -0.1595, -0.0867,  0.1764])\n",
      "blue\n",
      "Saved the embedding for blue.\n",
      "['blues'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0737,  0.1757, -0.8755,  ...,  0.3683, -0.1360,  0.2172])\n",
      "blues\n",
      "Saved the embedding for blues.\n",
      "['bluff', '##ing'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.5942,  0.3797, -0.8636,  ..., -0.0434, -0.3518,  0.0582])\n",
      "bluffing\n",
      "Saved the embedding for bluffing.\n",
      "['blunt'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1967,  0.1238, -0.4279,  ...,  0.0159, -0.2201,  0.1657])\n",
      "blunt\n",
      "Saved the embedding for blunt.\n",
      "['blushing'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.2579,  0.1616,  0.0792,  ..., -0.8515,  0.0843,  0.4799])\n",
      "blushing\n",
      "Saved the embedding for blushing.\n",
      "['blu', '##ster', '##ing'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.7570, -0.2662, -0.4665,  ...,  0.2235,  0.4922,  0.0935])\n",
      "blustering\n",
      "Saved the embedding for blustering.\n",
      "['bo', '##ast', '##ful'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.3640,  0.3076, -0.3821,  ...,  0.2027,  0.1792,  0.4237])\n",
      "boastful\n",
      "Saved the embedding for boastful.\n",
      "['bog', '##gled'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.5579,  0.1518, -0.0578,  ...,  0.1861, -0.2812, -0.1717])\n",
      "boggled\n",
      "Saved the embedding for boggled.\n",
      "['boiling'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.6233,  0.5112, -0.4056,  ...,  0.2184,  0.3110, -0.0325])\n",
      "boiling\n",
      "Saved the embedding for boiling.\n",
      "['bois', '##ter', '##ous'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.3821,  0.0630,  0.1971,  ...,  0.6901,  0.1644, -0.5533])\n",
      "boisterous\n",
      "Saved the embedding for boisterous.\n",
      "['bold'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.0709,  0.6640, -0.2912,  ...,  0.0228, -0.1312, -0.2919])\n",
      "bold\n",
      "Saved the embedding for bold.\n",
      "['bored'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1226,  0.3221, -0.3618,  ...,  0.2246, -0.3312,  0.7212])\n",
      "bored\n",
      "Saved the embedding for bored.\n",
      "['boredom'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.3262,  0.3664,  0.2898,  ..., -0.3554, -0.0806,  0.6952])\n",
      "boredom\n",
      "Saved the embedding for boredom.\n",
      "['boring'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.5229,  0.2092, -0.1244,  ..., -0.0293, -0.0396,  0.5019])\n",
      "boring\n",
      "Saved the embedding for boring.\n",
      "['bothered'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.1385,  0.6443, -0.0546,  ...,  0.1249,  0.0721,  0.8135])\n",
      "bothered\n",
      "Saved the embedding for bothered.\n",
      "['bound', '##er'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.4148, -0.5520, -0.2471,  ...,  0.8221, -0.0849,  0.5715])\n",
      "bounder\n",
      "Saved the embedding for bounder.\n",
      "['bra', '##sh', '##ness'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.3607,  0.5123, -0.1422,  ...,  1.3529, -0.1372, -1.1209])\n",
      "brashness\n",
      "Saved the embedding for brashness.\n",
      "['brat', '##ty'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 1.1792, -0.0279,  0.1739,  ...,  0.1006, -0.8222,  0.7540])\n",
      "bratty\n",
      "Saved the embedding for bratty.\n",
      "['brave'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0530,  0.7269, -0.1104,  ..., -0.1740, -0.2179,  0.2509])\n",
      "brave\n",
      "Saved the embedding for brave.\n",
      "['bright'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0808,  0.3591, -0.1944,  ...,  0.0311,  0.4667,  0.6775])\n",
      "bright\n",
      "Saved the embedding for bright.\n",
      "['br', '##ist', '##ling'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.1294,  0.6186,  0.1509,  ..., -0.1967,  0.2699,  0.4911])\n",
      "bristling\n",
      "Saved the embedding for bristling.\n",
      "['broken'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.5850,  0.2722, -0.1403,  ...,  0.1010, -0.2940, -0.2003])\n",
      "broken\n",
      "Saved the embedding for broken.\n",
      "['broken', '##hearted'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.6334,  0.3769, -0.1246,  ...,  0.3550, -0.3410,  0.0093])\n",
      "brokenhearted\n",
      "Saved the embedding for brokenhearted.\n",
      "['broken', '##hearted', '##ly'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.5051,  0.4132, -0.0924,  ...,  0.3784, -0.1702, -0.0398])\n",
      "brokenheartedly\n",
      "Saved the embedding for brokenheartedly.\n",
      "['brooding'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.3435,  1.0018,  0.1096,  ..., -0.2738, -0.1716,  0.3656])\n",
      "brooding\n",
      "Saved the embedding for brooding.\n",
      "['brood', '##y'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.5744,  0.8995,  0.0882,  ..., -0.1370, -0.3161, -0.2434])\n",
      "broody\n",
      "Saved the embedding for broody.\n",
      "['bruised'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0917,  0.0149, -0.3196,  ..., -0.1240, -0.1465,  0.3081])\n",
      "bruised\n",
      "Saved the embedding for bruised.\n",
      "['br', '##us', '##que'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.0347,  0.6862,  0.0721,  ..., -0.0764,  0.4409,  0.3621])\n",
      "brusque\n",
      "Saved the embedding for brusque.\n",
      "['bug'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.2736, -0.2179, -0.3173,  ..., -0.1955, -0.2727,  0.3574])\n",
      "bug\n",
      "Saved the embedding for bug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bulging'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0669,  0.1745, -0.3350,  ..., -0.1227, -0.0248,  0.1602])\n",
      "bulging\n",
      "Saved the embedding for bulging.\n",
      "['bully'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.4920,  0.3804, -0.4935,  ..., -0.3690, -0.2565,  0.0201])\n",
      "bully\n",
      "Saved the embedding for bully.\n",
      "['bullying'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.3058,  0.0328, -0.0401,  ..., -0.2273, -0.7757,  0.6153])\n",
      "bullying\n",
      "Saved the embedding for bullying.\n",
      "['bum', '##med'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.1330, -0.1384, -0.2175,  ...,  0.0494, -0.6765,  0.0928])\n",
      "bummed\n",
      "Saved the embedding for bummed.\n",
      "['bu', '##oya', '##nt'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([-0.1191,  0.1376, -0.1909,  ..., -0.5604,  0.0903,  0.2858])\n",
      "buoyant\n",
      "Saved the embedding for buoyant.\n",
      "['burden', '##ed'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.1789,  0.2528, -0.0064,  ..., -0.2943, -0.2019,  0.3019])\n",
      "burdened\n",
      "Saved the embedding for burdened.\n",
      "['burn'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.6535, -0.5679,  0.1782,  ..., -0.1189,  0.0962, -0.4248])\n",
      "burn\n",
      "Saved the embedding for burn.\n",
      "['bursting'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.0447,  0.0698, -0.3384,  ..., -0.2234,  0.0637, -0.3080])\n",
      "bursting\n",
      "Saved the embedding for bursting.\n",
      "['bush', '##ed'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.6985,  0.3364, -0.0250,  ...,  0.0571,  0.2209,  0.5039])\n",
      "bushed\n",
      "Saved the embedding for bushed.\n",
      "['cage', '##y'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 1.0116,  0.7512, -0.3901,  ...,  0.3119,  0.1661,  0.1439])\n",
      "cagey\n",
      "Saved the embedding for cagey.\n",
      "['ca', '##gy'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.3658,  0.1258,  0.1763,  ..., -0.4202, -0.0353,  0.5358])\n",
      "cagy\n",
      "Saved the embedding for cagy.\n",
      "['calculating'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.2114,  0.7959, -0.0444,  ...,  0.3433,  0.1985,  0.5619])\n",
      "calculating\n",
      "Saved the embedding for calculating.\n",
      "['call', '##ous'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.0082, -0.2038, -0.0854,  ..., -0.9946,  0.0218,  0.6505])\n",
      "callous\n",
      "Saved the embedding for callous.\n",
      "['call', '##used'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.0884, -0.2914,  0.0742,  ..., -0.8631,  0.1806,  0.4873])\n",
      "callused\n",
      "Saved the embedding for callused.\n",
      "['calm'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([-0.0384,  0.8325,  0.8023,  ...,  0.0118,  0.1462,  0.6176])\n",
      "calm\n",
      "Saved the embedding for calm.\n",
      "['calming'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.2358,  0.5603,  0.4636,  ..., -0.7332, -0.6207,  0.4315])\n",
      "calming\n",
      "Saved the embedding for calming.\n",
      "['calm', '##ness'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([-0.1848,  0.9238,  0.7424,  ...,  0.2450,  0.2682,  0.7401])\n",
      "calmness\n",
      "Saved the embedding for calmness.\n",
      "['can', '##ny'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.8557, -0.3506, -0.1160,  ..., -0.3981,  0.0077,  0.2962])\n",
      "canny\n",
      "Saved the embedding for canny.\n",
      "['can', '##tan', '##ker', '##ous'] has a token embedding of size torch.Size([4, 12, 768])\n",
      "Shape is: 4 x 3072\n",
      "tensor([ 0.7893, -0.4719,  0.0643,  ..., -0.5922,  0.1388,  0.0254])\n",
      "cantankerous\n",
      "Saved the embedding for cantankerous.\n",
      "['capable'] has a token embedding of size torch.Size([1, 12, 768])\n",
      "Shape is: 1 x 3072\n",
      "tensor([ 0.3275,  0.2855, -0.3991,  ..., -0.1604,  0.5993,  0.0391])\n",
      "capable\n",
      "Saved the embedding for capable.\n",
      "['cap', '##ric', '##ious'] has a token embedding of size torch.Size([3, 12, 768])\n",
      "Shape is: 3 x 3072\n",
      "tensor([ 0.5260, -0.7824,  0.1708,  ...,  0.5071,  0.2552,  0.2360])\n",
      "capricious\n",
      "Saved the embedding for capricious.\n",
      "['capt', '##ivated'] has a token embedding of size torch.Size([2, 12, 768])\n",
      "Shape is: 2 x 3072\n",
      "tensor([ 0.6718,  0.1564,  0.2935,  ..., -0.2162,  0.2891,  0.7145])\n",
      "captivated\n",
      "Saved the embedding for captivated.\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "# Set up input and output paths.\n",
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/vocab_files/vocab_checked.txt'\n",
    "layer_combining_function = cat_first_four\n",
    "embeddings_file = os.path.join('/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab', layer_combining_function.__name__ + '.txt')\n",
    "if os.path.exists(embeddings_file):\n",
    "    os.remove(embeddings_file)\n",
    "\n",
    "# Create a list of vocabulary words we want embeddings for.\n",
    "vocab = make_vocab(vocab_file)\n",
    "\n",
    "# Tokenize the vocabulary and look up the BERT token indices.\n",
    "tokenized_text, indexed_tokens = tokenize_text(vocab)\n",
    "\n",
    "# Generate segment IDs for each token.\n",
    "segments_IDs = generate_segments_IDs(tokenized_text)\n",
    "\n",
    "# Generate and write out the contextual embeddings for the vocabulary words.\n",
    "# Embeddings are saved in a standard format that can be used for calcualting\n",
    "# the cosine distances between word vectors.\n",
    "for i in range(len(tokenized_text)):\n",
    "    # Convert indexed tokens and segments to tensors.\n",
    "    # Create a BERT model for the tokens.\n",
    "    # Get the encoded model layers and reshape them.\n",
    "    token_embeddings = generate_embeddings(indexed_tokens[i], segments_IDs[i])\n",
    "    print(f'{tokenized_text[i]} has a token embedding of size {token_embeddings.size()}')\n",
    "\n",
    "    # Extract the contextual embedding for a token.\n",
    "    contextual_embedding = layer_combining_function(token_embeddings)\n",
    "\n",
    "    # Write the embedding to a text file, with the vocabulary word prepended.\n",
    "    vocab_word = reconstruct_tokens(tokenized_text[i])\n",
    "    # Make sure we've got the correct vocabulary word.\n",
    "    assert vocab[i] == vocab_word\n",
    "    write_embedding(embeddings_file, vocab[i], contextual_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal-venv-3.6",
   "language": "python",
   "name": "crystal-venv-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
