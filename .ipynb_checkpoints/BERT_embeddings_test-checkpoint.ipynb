{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: appnope==0.1.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (0.1.0)\n",
      "Collecting astroid==2.3.3\n",
      "  Using cached astroid-2.3.3-py3-none-any.whl (205 kB)\n",
      "Requirement already satisfied: attrs==19.3.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (19.3.0)\n",
      "Requirement already satisfied: backcall==0.1.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.1.0)\n",
      "Collecting bleach==3.1.1\n",
      "  Using cached bleach-3.1.1-py2.py3-none-any.whl (150 kB)\n",
      "Collecting blis==0.4.1\n",
      "  Using cached blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "Requirement already satisfied: boto3==1.12.26 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (1.12.26)\n",
      "Requirement already satisfied: botocore==1.15.26 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (1.15.26)\n",
      "Collecting catalogue==1.0.0\n",
      "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting certifi==2019.11.28\n",
      "  Using cached certifi-2019.11.28-py2.py3-none-any.whl (156 kB)\n",
      "Collecting chardet==3.0.4\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: cycler==0.10.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 12)) (0.10.0)\n",
      "Collecting cymem==2.0.3\n",
      "  Using cached cymem-2.0.3-cp36-cp36m-manylinux1_x86_64.whl (32 kB)\n",
      "Collecting decorator==4.4.1\n",
      "  Using cached decorator-4.4.1-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting defusedxml==0.6.0\n",
      "  Using cached defusedxml-0.6.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting docutils==0.15.2\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Collecting entrypoints==0.3\n",
      "  Using cached entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Collecting idna==2.9\n",
      "  Using cached idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting importlib-metadata==1.5.0\n",
      "  Using cached importlib_metadata-1.5.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting ipykernel==5.1.4\n",
      "  Using cached ipykernel-5.1.4-py3-none-any.whl (116 kB)\n",
      "Collecting ipython==7.12.0\n",
      "  Using cached ipython-7.12.0-py3-none-any.whl (777 kB)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 22)) (0.2.0)\n",
      "Collecting ipywidgets==7.5.1\n",
      "  Using cached ipywidgets-7.5.1-py2.py3-none-any.whl (121 kB)\n",
      "Collecting isort==4.3.21\n",
      "  Using cached isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
      "Collecting jedi==0.16.0\n",
      "  Using cached jedi-0.16.0-py2.py3-none-any.whl (1.1 MB)\n",
      "Collecting Jinja2==2.11.1\n",
      "  Using cached Jinja2-2.11.1-py2.py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: jmespath==0.9.5 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 27)) (0.9.5)\n",
      "Collecting jsonschema==3.2.0\n",
      "  Using cached jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 29)) (1.0.0)\n",
      "Collecting jupyter-client==6.0.0\n",
      "  Using cached jupyter_client-6.0.0-py3-none-any.whl (104 kB)\n",
      "Collecting jupyter-console==6.1.0\n",
      "  Using cached jupyter_console-6.1.0-py2.py3-none-any.whl (21 kB)\n",
      "Collecting jupyter-core==4.6.3\n",
      "  Using cached jupyter_core-4.6.3-py2.py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: kiwisolver==1.1.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 33)) (1.1.0)\n",
      "Collecting lazy-object-proxy==1.4.3\n",
      "  Using cached lazy_object_proxy-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (55 kB)\n",
      "Collecting MarkupSafe==1.1.1\n",
      "  Using cached MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (27 kB)\n",
      "Collecting matplotlib==3.2.1\n",
      "  Using cached matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "Collecting mccabe==0.6.1\n",
      "  Using cached mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Collecting mistune==0.8.4\n",
      "  Using cached mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting murmurhash==1.0.2\n",
      "  Using cached murmurhash-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (19 kB)\n",
      "Collecting nbconvert==5.6.1\n",
      "  Using cached nbconvert-5.6.1-py2.py3-none-any.whl (455 kB)\n",
      "Collecting nbformat==5.0.4\n",
      "  Using cached nbformat-5.0.4-py3-none-any.whl (169 kB)\n",
      "Processing /home/jupyter/.cache/pip/wheels/e3/c9/b0/ed26a73ef75a53145820825afa8e2d2c9b30fe9f6c10cd3202/nltk-3.4.5-py3-none-any.whl\n",
      "Collecting notebook==6.0.3\n",
      "  Using cached notebook-6.0.3-py3-none-any.whl (9.7 MB)\n",
      "Collecting numpy==1.18.1\n",
      "  Using cached numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n",
      "Collecting pandas==1.0.3\n",
      "  Using cached pandas-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (10.0 MB)\n",
      "Processing /home/jupyter/.cache/pip/wheels/46/c4/40/718c6fd14c2129ccaee10e0cf03ef6c4d01d98cad5dbbfda38/pandocfilters-1.4.2-py3-none-any.whl\n",
      "Collecting parso==0.6.1\n",
      "  Using cached parso-0.6.1-py2.py3-none-any.whl (97 kB)\n",
      "Collecting pexpect==4.8.0\n",
      "  Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "Collecting pickleshare==0.7.5\n",
      "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Collecting Pillow==7.0.0\n",
      "  Using cached Pillow-7.0.0-cp36-cp36m-manylinux1_x86_64.whl (2.1 MB)\n",
      "Collecting plac==1.1.3\n",
      "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting preshed==3.0.2\n",
      "  Using cached preshed-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (119 kB)\n",
      "Processing /home/jupyter/.cache/pip/wheels/1d/4a/79/a3ad3f74b3495b4555359375ca33ad7b64e77f8b7a53c8894f/prometheus_client-0.7.1-py3-none-any.whl\n",
      "Collecting prompt-toolkit==3.0.3\n",
      "  Using cached prompt_toolkit-3.0.3-py3-none-any.whl (348 kB)\n",
      "Collecting ptyprocess==0.6.0\n",
      "  Using cached ptyprocess-0.6.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting Pygments==2.5.2\n",
      "  Using cached Pygments-2.5.2-py2.py3-none-any.whl (896 kB)\n",
      "Collecting pylint==2.4.4\n",
      "  Using cached pylint-2.4.4-py3-none-any.whl (302 kB)\n",
      "Collecting pyparsing==2.4.6\n",
      "  Using cached pyparsing-2.4.6-py2.py3-none-any.whl (67 kB)\n",
      "Processing /home/jupyter/.cache/pip/wheels/43/9a/e4/7f687f2bb934e9a26a6e1158778ed7c5436b9ea15db48c888a/pyrsistent-0.15.7-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting python-dateutil==2.8.1\n",
      "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Requirement already satisfied: pytorch-pretrained-bert==0.6.2 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 61)) (0.6.2)\n",
      "Collecting pytz==2019.3\n",
      "  Using cached pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
      "Collecting pyzmq==19.0.0\n",
      "  Using cached pyzmq-19.0.0-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting qtconsole==4.6.0\n",
      "  Using cached qtconsole-4.6.0-py2.py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: regex==2020.2.20 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 65)) (2020.2.20)\n",
      "Collecting requests==2.23.0\n",
      "  Using cached requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: s3transfer==0.3.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 67)) (0.3.3)\n",
      "Collecting scipy==1.4.1\n",
      "  Using cached scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Collecting Send2Trash==1.5.0\n",
      "  Using cached Send2Trash-1.5.0-py3-none-any.whl (12 kB)\n",
      "Collecting six==1.14.0\n",
      "  Using cached six-1.14.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting spacy==2.2.3\n",
      "  Using cached spacy-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\n",
      "Collecting srsly==1.0.1\n",
      "  Using cached srsly-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (185 kB)\n",
      "Collecting terminado==0.8.3\n",
      "  Using cached terminado-0.8.3-py2.py3-none-any.whl (33 kB)\n",
      "Collecting testpath==0.4.4\n",
      "  Using cached testpath-0.4.4-py2.py3-none-any.whl (163 kB)\n",
      "Collecting thinc==7.3.1\n",
      "  Using cached thinc-7.3.1-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "Collecting torch==1.4.0\n",
      "  Using cached torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4 MB)\n",
      "Collecting torchvision==0.5.0\n",
      "  Using cached torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
      "Processing /home/jupyter/.cache/pip/wheels/b2/92/5c/3bfc125cdf46ab0487727e574f513f9568e9b7974b15237abf/tornado-6.0.3-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting tqdm==4.43.0\n",
      "  Using cached tqdm-4.43.0-py2.py3-none-any.whl (59 kB)\n",
      "Collecting traitlets==4.3.3\n",
      "  Using cached traitlets-4.3.3-py2.py3-none-any.whl (75 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typed-ast==1.4.1\n",
      "  Using cached typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737 kB)\n",
      "Collecting urllib3==1.25.8\n",
      "  Using cached urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\n",
      "Collecting wasabi==0.6.0\n",
      "  Using cached wasabi-0.6.0-py3-none-any.whl (20 kB)\n",
      "Collecting wcwidth==0.1.8\n",
      "  Using cached wcwidth-0.1.8-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: webencodings==0.5.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 85)) (0.5.1)\n",
      "Collecting widgetsnbextension==3.5.1\n",
      "  Using cached widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
      "Processing /home/jupyter/.cache/pip/wheels/0d/85/48/15d7bfab92a2d0e87372224c1f628fc57db7447a663a58e86c/wrapt-1.11.2-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting zipp==3.0.0\n",
      "  Using cached zipp-3.0.0-py3-none-any.whl (4.8 kB)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from ipython==7.12.0->-r requirements.txt (line 21)) (40.8.0)\n",
      "\u001b[31mERROR: scikit-image 0.15.0 has requirement networkx>=2.0, but you'll have networkx 1.11 which is incompatible.\u001b[0m\n",
      "Installing collected packages: wrapt, six, lazy-object-proxy, typed-ast, astroid, bleach, numpy, blis, zipp, importlib-metadata, catalogue, certifi, chardet, cymem, decorator, defusedxml, docutils, entrypoints, idna, tornado, traitlets, pyzmq, python-dateutil, jupyter-core, jupyter-client, wcwidth, prompt-toolkit, ptyprocess, pexpect, parso, jedi, pickleshare, Pygments, ipython, ipykernel, terminado, mistune, pyrsistent, jsonschema, nbformat, MarkupSafe, Jinja2, pandocfilters, testpath, nbconvert, prometheus-client, Send2Trash, notebook, widgetsnbextension, ipywidgets, isort, jupyter-console, pyparsing, matplotlib, mccabe, murmurhash, nltk, pytz, pandas, Pillow, plac, preshed, pylint, qtconsole, urllib3, requests, scipy, tqdm, srsly, wasabi, thinc, spacy, torch, torchvision\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.10.10\n",
      "\u001b[31mERROR: Cannot uninstall 'wrapt'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_pretrained_bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c89744dba048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_pretrained_bert'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(tokenizer.vocab.keys())[5000:5020]\n",
    "list(tokenizer.vocab.keys())[17733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-76e0f8f130a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Split the sentence into tokens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# tokenized_text = tokenizer.tokenize(marked_text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "# text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "#        \"fishing on the Mississippi river bank.\"\n",
    "\n",
    "# text = \"[CLS] Why the disgusted face [SEP] Did you smell some disgustingly ripe cheese [SEP]\"\n",
    "text = \"[CLS] She made a disgusted pout [SEP] Her disgusted expression was contagious [SEP]\"\n",
    "\n",
    "# Add the special tokens.\n",
    "# marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "# tokenized_text = tokenizer.tokenize(marked_text)\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "# text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "#        \"fishing on the Mississippi river bank.\"\n",
    "\n",
    "# text = \"[CLS] Why the disgusted face [SEP] Did you smell some disgustingly ripe cheese [SEP]\"\n",
    "text = \"[CLS] She made a disgusted pout [SEP] Her disgusted expression was contagious [SEP]\"\n",
    "\n",
    "# Add the special tokens.\n",
    "# marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "# tokenized_text = tokenizer.tokenize(marked_text)\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[101, 2016, 2081, 1037, 17733, 13433, 4904, 102, 2014, 17733, 3670, 2001, 9530, 15900, 6313, 102]\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the tokens as belonging to sentence \"0\" or \"1\".\n",
    "# segments_ids = [1] * len(tokenized_text)\n",
    "segments_ids = [0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1]\n",
    "# segments_ids = [0,0,0,1,1]\n",
    "print (segments_ids)\n",
    "print(indexed_tokens)\n",
    "print(tokenized_text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n",
      "Number of batches: 1\n",
      "Number of tokens: 16\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 3\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the 5th token in our sentence, select its feature values from layer 5.\n",
    "token_i = 4\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "# print(vec)\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Type of encoded_layers:  <class 'list'>\n",
      "Tensor shape for each layer:  torch.Size([1, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "# `encoded_layers` is a Python list.\n",
    "print('     Type of encoded_layers: ', type(encoded_layers))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', encoded_layers[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 16, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 16, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 16, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 16 x 3072\n",
      "tensor([ 0.1730,  0.5220, -0.6051,  ..., -0.4742,  0.0104,  0.4212])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 16 x 768\n"
     ]
    }
   ],
   "source": [
    "# Sum the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 16 x 3072\n",
      "tensor([ 0.1730,  0.5220, -0.6051,  ..., -0.4742,  0.0104,  0.4212])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_first = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[0], token[1], token[2], token[3]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_first.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_first), len(token_vecs_cat_first[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 12 x 768\n"
     ]
    }
   ],
   "source": [
    "# Sum the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_first = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[:4], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_first.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_first), len(token_vecs_sum_first[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 12 x 3072\n",
      "tensor([ 0.1730,  0.5220, -0.6051,  ..., -0.4742,  0.0104,  0.4212])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_middle1 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[4], token[5], token[6], token[7]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_middle1.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_middle1), len(token_vecs_cat_middle1[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 12 x 768\n"
     ]
    }
   ],
   "source": [
    "# Sum the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_middle1 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[4:8], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_middle1.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_middle1), len(token_vecs_sum_middle1[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 12 x 3072\n",
      "tensor([ 0.1730,  0.5220, -0.6051,  ..., -0.4742,  0.0104,  0.4212])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_middle2 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[8], token[9], token[10], token[11]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_middle2.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_middle2), len(token_vecs_cat_middle2[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 12 x 768\n"
     ]
    }
   ],
   "source": [
    "# Sum the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum_middle2 = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[8:12], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum_middle2.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum_middle2), len(token_vecs_sum_middle2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat_all\n",
    ".333333333333333= []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[8], token[9], token[10], token[11]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat_middle2.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat_middle2), len(token_vecs_cat_middle2[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: torch.Size([768])\n",
      "tensor(0.0927)\n",
      "tensor(0.0927)\n",
      "Shape of sentences vector is: 768\n",
      "tensor(0.0927)\n"
     ]
    }
   ],
   "source": [
    "# Make a single vector to represent the pair of sentences by averaging across tokens.\n",
    "# `encoded_layers` has shape [12 x 1 x 22 x 768]\n",
    "sentences_vec = []\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = encoded_layers[11][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "for s in sentence_embedding:\n",
    "    sentences_vec.append(s)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n",
    "print(sentence_embedding[767])\n",
    "print(sentence_embedding[-1])\n",
    "print(f'Shape of sentences vector is: {len(sentences_vec)}')\n",
    "print(sentences_vec[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 she\n",
      "2 made\n",
      "3 a\n",
      "4 disgusted\n",
      "5 po\n",
      "6 ##ut\n",
      "7 [SEP]\n",
      "8 her\n",
      "9 disgusted\n",
      "10 expression\n",
      "11 was\n",
      "12 con\n",
      "13 ##tag\n",
      "14 ##ious\n",
      "15 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_string in enumerate(tokenized_text):\n",
    "    print(i, token_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of disgusted and disgusted in token_vecs_cat is: 0.7731837630271912\n",
      "Similarity of disgusted and disgusted in token_vecs_sum is: 0.7783510684967041\n",
      "Similarity of disgusted and disgusted in token_vecs_cat_first is: 0.8062489628791809\n",
      "Similarity of disgusted and disgusted in token_vecs_sum_first is: 0.6329748630523682\n",
      "Similarity of disgusted and disgusted in token_vecs_cat_middle1 is: 0.6662847995758057\n",
      "Similarity of disgusted and disgusted in token_vecs_sum_middle1 is: 0.6929298043251038\n",
      "Similarity of disgusted and disgusted in token_vecs_cat_middle2 is: 0.6831039786338806\n",
      "Similarity of disgusted and disgusted in token_vecs_sum_middle2 is: 0.7416453957557678\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "similarity = 1 - cosine(token_vecs_cat[4], token_vecs_cat[9])\n",
    "print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat is: {similarity}')\n",
    "similarity = 1 - cosine(token_vecs_sum[4], token_vecs_sum[9])\n",
    "print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum is: {similarity}')\n",
    "similarity = 1 - cosine(token_vecs_cat_first[4], token_vecs_cat_first[9])\n",
    "print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_first is: {similarity}')\n",
    "similarity = 1 - cosine(token_vecs_sum_first[4], token_vecs_sum_first[9])\n",
    "print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_first is: {similarity}')\n",
    "similarity = 1 - cosine(token_vecs_cat_middle1[4], token_vecs_cat_middle1[9])\n",
    "print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_middle1 is: {similarity}')\n",
    "similarity = 1 - cosine(token_vecs_sum_middle1[4], token_vecs_sum_middle1[9])\n",
    "print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_middle1 is: {similarity}')\n",
    "similarity = 1 - cosine(token_vecs_cat_middle2[4], token_vecs_cat_middle2[9])\n",
    "print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_cat_middle2 is: {similarity}')\n",
    "similarity = 1 - cosine(token_vecs_sum_middle2[4], token_vecs_sum_middle2[9])\n",
    "print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} in token_vecs_sum_middle2 is: {similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
