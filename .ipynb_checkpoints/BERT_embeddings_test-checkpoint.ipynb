{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py==0.9.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: astor==0.8.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.8.1)\n",
      "Requirement already satisfied: attrs==19.3.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (19.3.0)\n",
      "Requirement already satisfied: backcall==0.1.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.1.0)\n",
      "Requirement already satisfied: bleach==3.1.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (3.1.1)\n",
      "Requirement already satisfied: boto3==1.12.2 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (1.12.2)\n",
      "Requirement already satisfied: botocore==1.15.2 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (1.15.2)\n",
      "Requirement already satisfied: cachetools==4.0.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (4.0.0)\n",
      "Requirement already satisfied: certifi==2019.11.28 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (2019.11.28)\n",
      "Requirement already satisfied: chardet==3.0.4 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 10)) (3.0.4)\n",
      "Requirement already satisfied: Click==7.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 11)) (7.0)\n",
      "Requirement already satisfied: cycler==0.10.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 12)) (0.10.0)\n",
      "Requirement already satisfied: decorator==4.4.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 13)) (4.4.1)\n",
      "Requirement already satisfied: defusedxml==0.6.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 14)) (0.6.0)\n",
      "Requirement already satisfied: docker-pycreds==0.3.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 15)) (0.3.0)\n",
      "Requirement already satisfied: docutils==0.15.2 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 16)) (0.15.2)\n",
      "Requirement already satisfied: entrypoints==0.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 17)) (0.3)\n",
      "Requirement already satisfied: filelock==3.0.12 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 18)) (3.0.12)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 19)) (0.2.2)\n",
      "Requirement already satisfied: google-auth==1.11.2 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 20)) (1.11.2)\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 21)) (0.4.1)\n",
      "Requirement already satisfied: google-pasta==0.1.8 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 22)) (0.1.8)\n",
      "Requirement already satisfied: grpcio==1.27.2 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 23)) (1.27.2)\n",
      "Requirement already satisfied: h5py==2.10.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 24)) (2.10.0)\n",
      "Requirement already satisfied: idna==2.8 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 25)) (2.8)\n",
      "Requirement already satisfied: importlib-metadata==1.5.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 26)) (1.5.0)\n",
      "Requirement already satisfied: ipython==7.12.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 27)) (7.12.0)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 28)) (0.2.0)\n",
      "Requirement already satisfied: ipywidgets==7.5.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 29)) (7.5.1)\n",
      "Requirement already satisfied: jedi==0.16.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 30)) (0.16.0)\n",
      "Requirement already satisfied: Jinja2==2.11.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 31)) (2.11.1)\n",
      "Requirement already satisfied: jmespath==0.9.4 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 32)) (0.9.4)\n",
      "Requirement already satisfied: joblib==0.14.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 33)) (0.14.1)\n",
      "Requirement already satisfied: jsonschema==3.2.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 34)) (3.2.0)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 35)) (1.0.0)\n",
      "Requirement already satisfied: jupyter-client==5.3.4 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 36)) (5.3.4)\n",
      "Requirement already satisfied: jupyter-console==6.1.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 37)) (6.1.0)\n",
      "Requirement already satisfied: jupyter-core==4.6.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 38)) (4.6.3)\n",
      "Requirement already satisfied: Keras==2.3.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 39)) (2.3.1)\n",
      "Requirement already satisfied: Keras-Applications==1.0.8 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 40)) (1.0.8)\n",
      "Requirement already satisfied: Keras-Preprocessing==1.1.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 41)) (1.1.0)\n",
      "Requirement already satisfied: kiwisolver==1.1.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 42)) (1.1.0)\n",
      "Requirement already satisfied: Markdown==3.2.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 43)) (3.2.1)\n",
      "Requirement already satisfied: MarkupSafe==1.1.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 44)) (1.1.1)\n",
      "Requirement already satisfied: matplotlib==3.1.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 45)) (3.1.3)\n",
      "Requirement already satisfied: mistune==0.8.4 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 46)) (0.8.4)\n",
      "Requirement already satisfied: nbconvert==5.6.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 47)) (5.6.1)\n",
      "Requirement already satisfied: nbformat==5.0.4 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 48)) (5.0.4)\n",
      "Requirement already satisfied: notebook==6.0.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 49)) (6.0.3)\n",
      "Requirement already satisfied: numpy==1.18.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 50)) (1.18.1)\n",
      "Requirement already satisfied: oauthlib==3.1.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 51)) (3.1.0)\n",
      "Requirement already satisfied: opt-einsum==3.1.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 52)) (3.1.0)\n",
      "Requirement already satisfied: pandas==1.0.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 53)) (1.0.1)\n",
      "Requirement already satisfied: pandocfilters==1.4.2 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 54)) (1.4.2)\n",
      "Requirement already satisfied: parso==0.6.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 55)) (0.6.1)\n",
      "Requirement already satisfied: patsy==0.5.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 56)) (0.5.1)\n",
      "Requirement already satisfied: pexpect==4.8.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 57)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 58)) (0.7.5)\n",
      "Requirement already satisfied: Pillow==7.0.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 59)) (7.0.0)\n",
      "Requirement already satisfied: prometheus-client==0.7.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 60)) (0.7.1)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 61)) (3.0.3)\n",
      "Requirement already satisfied: protobuf==3.11.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 62)) (3.11.3)\n",
      "Requirement already satisfied: ptyprocess==0.6.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 63)) (0.6.0)\n",
      "Requirement already satisfied: pyasn1==0.4.8 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 64)) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 65)) (0.2.8)\n",
      "Requirement already satisfied: Pygments==2.5.2 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 66)) (2.5.2)\n",
      "Requirement already satisfied: pyparsing==2.4.6 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 67)) (2.4.6)\n",
      "Requirement already satisfied: pyrsistent==0.15.7 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 68)) (0.15.7)\n",
      "Requirement already satisfied: python-dateutil==2.8.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 69)) (2.8.1)\n",
      "Requirement already satisfied: pytorch-pretrained-bert==0.6.2 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 70)) (0.6.2)\n",
      "Requirement already satisfied: pytz==2019.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 71)) (2019.3)\n",
      "Requirement already satisfied: PyYAML==5.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 72)) (5.3)\n",
      "Requirement already satisfied: pyzmq==18.1.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 73)) (18.1.1)\n",
      "Requirement already satisfied: qtconsole==4.6.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 74)) (4.6.0)\n",
      "Requirement already satisfied: regex==2020.2.18 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 75)) (2020.2.18)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests==2.21.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 76)) (2.21.0)\n",
      "Requirement already satisfied: requests-oauthlib==1.3.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 77)) (1.3.0)\n",
      "Requirement already satisfied: rsa==4.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 78)) (4.0)\n",
      "Requirement already satisfied: s3transfer==0.3.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 79)) (0.3.3)\n",
      "Requirement already satisfied: sacremoses==0.0.38 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 80)) (0.0.38)\n",
      "Requirement already satisfied: scikit-learn==0.22.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 81)) (0.22.1)\n",
      "Requirement already satisfied: scipy==1.4.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 82)) (1.4.1)\n",
      "Requirement already satisfied: Send2Trash==1.5.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 83)) (1.5.0)\n",
      "Requirement already satisfied: sentencepiece==0.1.85 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 84)) (0.1.85)\n",
      "Requirement already satisfied: seqeval==0.0.12 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 85)) (0.0.12)\n",
      "Requirement already satisfied: six==1.14.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 86)) (1.14.0)\n",
      "Requirement already satisfied: statsmodels==0.11.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 87)) (0.11.1)\n",
      "Requirement already satisfied: tensorboard==2.1.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 88)) (2.1.0)\n",
      "Requirement already satisfied: tensorboardX==2.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 89)) (2.0)\n",
      "Requirement already satisfied: tensorflow==2.1.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 90)) (2.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator==2.1.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 91)) (2.1.0)\n",
      "Requirement already satisfied: termcolor==1.1.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 92)) (1.1.0)\n",
      "Requirement already satisfied: terminado==0.8.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 93)) (0.8.3)\n",
      "Requirement already satisfied: testpath==0.4.4 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 94)) (0.4.4)\n",
      "Requirement already satisfied: tokenizers==0.5.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 95)) (0.5.0)\n",
      "Requirement already satisfied: torch==1.4.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 96)) (1.4.0)\n",
      "Requirement already satisfied: torchvision==0.5.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 97)) (0.5.0)\n",
      "Requirement already satisfied: tornado==6.0.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 98)) (6.0.3)\n",
      "Requirement already satisfied: tqdm==4.43.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 99)) (4.43.0)\n",
      "Requirement already satisfied: traitlets==4.3.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 100)) (4.3.3)\n",
      "Requirement already satisfied: transformers==2.5.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 101)) (2.5.0)\n",
      "Requirement already satisfied: urllib3==1.24.3 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 102)) (1.24.3)\n",
      "Requirement already satisfied: wcwidth==0.1.8 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 103)) (0.1.8)\n",
      "Requirement already satisfied: webencodings==0.5.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 104)) (0.5.1)\n",
      "Requirement already satisfied: Werkzeug==1.0.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 105)) (1.0.0)\n",
      "Requirement already satisfied: widgetsnbextension==3.5.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 106)) (3.5.1)\n",
      "Requirement already satisfied: wrapt==1.12.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 107)) (1.12.0)\n",
      "Requirement already satisfied: zipp==3.0.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from -r requirements.txt (line 108)) (3.0.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from google-auth==1.11.2->-r requirements.txt (line 20)) (45.0.0.post20200113)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/jupyter/anaconda3/lib/python3.6/site-packages (from ipywidgets==7.5.1->-r requirements.txt (line 29)) (4.5.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/jupyter/anaconda3/lib/python3.6/site-packages (from tensorboard==2.1.0->-r requirements.txt (line 88)) (0.33.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disgusted'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(tokenizer.vocab.keys())[5000:5020]\n",
    "list(tokenizer.vocab.keys())[17733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'she', 'made', 'a', 'disgusted', 'po', '##ut', '[SEP]', 'her', 'disgusted', 'expression', 'was', 'con', '##tag', '##ious', '[SEP]']\n",
      "[CLS]           101\n",
      "she           2,016\n",
      "made          2,081\n",
      "a             1,037\n",
      "disgusted    17,733\n",
      "po           13,433\n",
      "##ut          4,904\n",
      "[SEP]           102\n",
      "her           2,014\n",
      "disgusted    17,733\n",
      "expression    3,670\n",
      "was           2,001\n",
      "con           9,530\n",
      "##tag        15,900\n",
      "##ious        6,313\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "# text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "#        \"fishing on the Mississippi river bank.\"\n",
    "\n",
    "# text = \"[CLS] Why the disgusted face [SEP] Did you smell some disgustingly ripe cheese [SEP]\"\n",
    "text = \"[CLS] She made a disgusted pout [SEP] Her disgusted expression was contagious [SEP]\"\n",
    "\n",
    "# Add the special tokens.\n",
    "# marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "# tokenized_text = tokenizer.tokenize(marked_text)\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[101, 2016, 2081, 1037, 17733, 13433, 4904, 102, 2014, 17733, 3670, 2001, 9530, 15900, 6313, 102]\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the tokens as belonging to sentence \"0\" or \"1\".\n",
    "# segments_ids = [1] * len(tokenized_text)\n",
    "segments_ids = [0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1]\n",
    "# segments_ids = [0,0,0,1,1]\n",
    "print (segments_ids)\n",
    "print(indexed_tokens)\n",
    "print(tokenized_text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n",
      "Number of batches: 1\n",
      "Number of tokens: 16\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 3\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the 5th token in our sentence, select its feature values from layer 5.\n",
    "token_i = 4\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "# print(vec)\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Type of encoded_layers:  <class 'list'>\n",
      "Tensor shape for each layer:  torch.Size([1, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "# `encoded_layers` is a Python list.\n",
    "print('     Type of encoded_layers: ', type(encoded_layers))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', encoded_layers[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 16, 768])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 16, 768])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 12, 768])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 16 x 3072\n",
      "tensor([ 0.1730,  0.5220, -0.6051,  ..., -0.4742,  0.0104,  0.4212])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
    "print(token_vecs_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 16 x 768\n"
     ]
    }
   ],
   "source": [
    "# Sum the last 4 hidden layers to create word embeddings.\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: torch.Size([768])\n",
      "tensor(0.0927)\n",
      "tensor(0.0927)\n",
      "Shape of sentences vector is: 768\n",
      "tensor(0.0927)\n"
     ]
    }
   ],
   "source": [
    "# Make a single vector to represent the pair of sentences by averaging across tokens.\n",
    "# `encoded_layers` has shape [12 x 1 x 22 x 768]\n",
    "sentences_vec = []\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = encoded_layers[11][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "for s in sentence_embedding:\n",
    "    sentences_vec.append(s)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n",
    "print(sentence_embedding[767])\n",
    "print(sentence_embedding[-1])\n",
    "print(f'Shape of sentences vector is: {len(sentences_vec)}')\n",
    "print(sentences_vec[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 she\n",
      "2 made\n",
      "3 a\n",
      "4 disgusted\n",
      "5 po\n",
      "6 ##ut\n",
      "7 [SEP]\n",
      "8 her\n",
      "9 disgusted\n",
      "10 expression\n",
      "11 was\n",
      "12 con\n",
      "13 ##tag\n",
      "14 ##ious\n",
      "15 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_string in enumerate(tokenized_text):\n",
    "    print(i, token_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of disgusted and disgusted is: 0.7731838226318359\n",
      "Similarity of disgusted and disgusted is: 0.7783510088920593\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "similarity = 1 - cosine(token_vecs_cat[4], token_vecs_cat[9])\n",
    "print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} is: {similarity}')\n",
    "similarity = 1 - cosine(token_vecs_sum[4], token_vecs_sum[9])\n",
    "print(f'Similarity of {tokenized_text[4]} and {tokenized_text[9]} is: {similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
