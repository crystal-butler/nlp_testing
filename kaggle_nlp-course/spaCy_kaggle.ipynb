{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch\n",
    "\n",
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = pd.read_csv('spam.csv')\n",
    "spam.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E007] 'textcat' already exists in pipeline. Existing names: ['textcat']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-775c8353c720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"architecture\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"bow\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     })\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Code_Projects/NLP/venv/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, component, name, before, after, first, last)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_component_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE007\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE006\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E007] 'textcat' already exists in pipeline. Existing names: ['textcat']"
     ]
    }
   ],
   "source": [
    "textcat = nlp.create_pipe (\n",
    "    \"textcat\",\n",
    "    config={\n",
    "        \"exclusive_classes\": True,\n",
    "        \"architecture\": \"bow\"\n",
    "    })\n",
    "nlp.add_pipe(textcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcat.add_label(\"ham\")\n",
    "textcat.add_label(\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = spam['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [{'cats': {'ham': label == 'ham',\n",
    "                         'spam': label == 'spam'}}\n",
    "               for label in spam['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(train_text, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "  {'cats': {'ham': True, 'spam': False}}),\n",
       " ('Ok lar... Joking wif u oni...', {'cats': {'ham': True, 'spam': False}}),\n",
       " (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       "  {'cats': {'ham': False, 'spam': True}})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.util.fix_random_seed(1)\n",
    "optimizer = nlp.begin_training()\n",
    "batches = minibatch(train_data, size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batches:\n",
    "    texts, labels = zip(*batch)\n",
    "    nlp.update(texts, labels, sgd=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'textcat': 1.3284014923210634}\n",
      "{'textcat': 1.6731042324448708}\n",
      "{'textcat': 1.8601833738159996}\n",
      "{'textcat': 1.981754278128804}\n",
      "{'textcat': 2.06292024399758}\n",
      "{'textcat': 2.1138158423080626}\n",
      "{'textcat': 2.1489295823825976}\n",
      "{'textcat': 2.1737224776938806}\n",
      "{'textcat': 2.1928323683634687}\n",
      "{'textcat': 2.207256399714952}\n"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe(textcat)\n",
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "losses = {}\n",
    "\n",
    "for epoch in range(10):\n",
    "    random.shuffle(train_data)\n",
    "    batches = minibatch(train_data, size=8)\n",
    "    for batch in batches:\n",
    "        texts, labels = zip(*batch)\n",
    "        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n",
    "    print(losses)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9995220e-01 4.7758371e-05]\n",
      " [1.5278918e-02 9.8472100e-01]]\n"
     ]
    }
   ],
   "source": [
    "texts = [\"Are you ready for the tea party????? It's gonna be wild\",\n",
    "         \"URGENT Reply to this message for GUARANTEED FREE TEA\" ]\n",
    "docs = [nlp.tokenizer(text) for text in texts]\n",
    "    \n",
    "# Use textcat to get the scores for each doc\n",
    "textcat = nlp.get_pipe('textcat')\n",
    "scores, _ = textcat.predict(docs)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'spam']\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = scores.argmax(axis=1)\n",
    "print([textcat.labels[label] for label in predicted_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_file, split=0.9):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = data.sample(frac=1, random_state=7)\n",
    "    \n",
    "    texts = train_data.text.values\n",
    "    labels = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)}\n",
    "              for y in train_data.sentiment.values]\n",
    "    split = int(len(train_data) * split)\n",
    "    \n",
    "    train_labels = [{\"cats\": labels} for labels in labels[:split]]\n",
    "    val_labels = [{\"cats\": labels} for labels in labels[split:]]\n",
    "    \n",
    "    return texts[:split], train_labels, texts[split:], val_labels\n",
    "\n",
    "train_texts, train_labels, val_texts, val_labels = load_data('yelp_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts from training data\n",
      "------\n",
      "[\"Some of the best sushi I've ever had....and I come from the East Coast.  Unreal toro, have some of it's available.\"\n",
      " \"One of the best burgers I've ever had and very well priced. I got the tortilla burger and is was delicious especially with there tortilla soup!\"]\n",
      "\n",
      "Labels from training data\n",
      "------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'cats': {'POSITIVE': True, 'NEGATIVE': False}},\n",
       " {'cats': {'POSITIVE': True, 'NEGATIVE': False}}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Texts from training data\\n------')\n",
    "print(train_texts[:2])\n",
    "print('\\nLabels from training data\\n------')\n",
    "train_labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create an empty model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
    "textcat = nlp.create_pipe(\n",
    "              \"textcat\",\n",
    "              config={\n",
    "                \"exclusive_classes\": True,\n",
    "                \"architecture\": \"bow\"})\n",
    "\n",
    "# Add the TextCategorizer to the empty model\n",
    "nlp.add_pipe(textcat)\n",
    "\n",
    "# Add labels to text classifier\n",
    "textcat.add_label(\"NEGATIVE\")\n",
    "textcat.add_label(\"POSITIVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch\n",
    "import random\n",
    "\n",
    "def train(model, train_data, optimizer):\n",
    "    losses = {}\n",
    "    random.seed(1)\n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    batches = minibatch(train_data, size=8)\n",
    "    for batch in batches:\n",
    "        # train_data is a list of tuples [(text0, label0), (text1, label1), ...]\n",
    "        # Split batch into texts and labels\n",
    "        texts, labels = zip(*batch)\n",
    "        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.702600155742998\n"
     ]
    }
   ],
   "source": [
    "spacy.util.fix_random_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "train_data = list(zip(train_texts, train_labels))\n",
    "losses = train(nlp, train_data, optimizer)\n",
    "print(losses['textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEGATIVE': 0.773860514163971, 'POSITIVE': 0.22613944113254547}\n"
     ]
    }
   ],
   "source": [
    "text = \"This tea cup was full of holes. Do not recommend.\"\n",
    "doc = nlp(text)\n",
    "print(doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, texts): \n",
    "    # Use the model's tokenizer to tokenize each input text\n",
    "    docs = [nlp.tokenizer(text) for text in texts]\n",
    "    \n",
    "    # Use textcat to get the scores for each doc\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "    scores, _ = textcat.predict(docs)\n",
    "\n",
    "    print(scores)\n",
    "    \n",
    "    # From the scores, find the class with the highest score/probability\n",
    "    predicted_class = scores.argmax(axis=1)\n",
    "    \n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00112233 0.9988777 ]\n",
      " [0.00190598 0.998094  ]\n",
      " [0.10516807 0.8948319 ]\n",
      " [0.9828241  0.01717597]]\n",
      "POSITIVE: Came over and had their \"Pick 2\" lunch combo and chose their best selling 1/2 chicken sandwich with quinoa.  Both were tasty, the chicken salad is a bit creamy but was perfect with quinoa on the side.  This is a good lunch joint, casual and clean! \n",
      "\n",
      "POSITIVE: Went here last night and got oysters, fried okra, fries, and onion rings. I cannot complain. The portions were great and tasty!!! I will definitely be back for more. I cannot wait to try the crawfish boudin and soft shell crab. \n",
      "\n",
      "POSITIVE: This restaurant was fantastic! \n",
      "The concept of eating without vision was intriguing. The dinner was filled with laughs and good conversation. \n",
      "\n",
      "We were lead in a line to our table and each person to their seat. This was not just dark but you could not see something right in front of your face. \n",
      "\n",
      "The waiters/waitresses were all blind and allowed us to see how aware you need to be without the vision. \n",
      "\n",
      "Taking away one sense is said to increase your other senses so as taste and hearing which I believed to be true in this experience. \n",
      "\n",
      "The meal was extremely delicious. I had the chicken and it was cooked to perfection. I also had a surprise beer which was a nice surprise. \n",
      "\n",
      "The whole experience was unlike anything I have ever done and I hope this spreads to other cities. \n",
      "\n",
      "A must do! \n",
      "\n",
      "NEGATIVE: They won't book new patients for same day appointments. My dog is sick but it's not necessarily urgent so I asked when I would be able to book an appointment and was told \"new patients book out at least 6 weeks in advance\" so just a heads up this seems like a great vet from other reviews but it'll be hard to get in their system to know \n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = val_texts[34:38]\n",
    "predictions = predict(nlp, texts)\n",
    "\n",
    "for p, t in zip(predictions, texts):\n",
    "    print(f\"{textcat.labels[p]}: {t} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, texts, labels):\n",
    "    \"\"\" Returns the accuracy of a TextCategorizer model. \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        model: ScaPy model with a TextCategorizer\n",
    "        texts: Text samples, from load_data function\n",
    "        labels: True labels, from load_data function\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get predictions from textcat model (using your predict method)\n",
    "    predicted_class = predict(model, texts)\n",
    "    \n",
    "    # From labels, get the true class as a list of integers (POSITIVE -> 1, NEGATIVE -> 0)\n",
    "    true_class = [1 if label['cats']['POSITIVE'] is True else 0 for label in labels]\n",
    "    \n",
    "    # A boolean or int array indicating correct predictions\n",
    "    correct_predictions = [predicted_class[i] == true_class[i] for i in range(0, len(predicted_class))]\n",
    "    \n",
    "    # The accuracy, number of correct predictions divided by all predictions\n",
    "    accuracy = sum(correct_predictions)/len(correct_predictions)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = [1, 0, 1, 1, 0]\n",
    "b = [1, 1, 1, 0, 0]\n",
    "c = a == b\n",
    "# c = [a[i] == b[i] for i in range(0, len(a))]\n",
    "print(c)\n",
    "# print(sum(c))\n",
    "# print(len(c))\n",
    "# sum(c)/len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.7061116e-06 9.9999630e-01]\n",
      " [1.5990483e-05 9.9998403e-01]\n",
      " [2.3178293e-01 7.6821709e-01]\n",
      " ...\n",
      " [1.0764683e-04 9.9989235e-01]\n",
      " [9.9742532e-01 2.5747693e-03]\n",
      " [3.7466020e-06 9.9999630e-01]]\n",
      "Accuracy: 0.9488\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(nlp, val_texts, val_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.8111309e-09 1.0000000e+00]\n",
      " [3.6852990e-07 9.9999964e-01]\n",
      " [9.2874631e-02 9.0712535e-01]\n",
      " ...\n",
      " [3.4402149e-06 9.9999654e-01]\n",
      " [9.9972624e-01 2.7379423e-04]\n",
      " [3.6849697e-11 1.0000000e+00]]\n",
      "Loss: 2.632 \t Accuracy: 0.944\n",
      "[[4.9286886e-09 1.0000000e+00]\n",
      " [2.7708534e-06 9.9999726e-01]\n",
      " [6.9757633e-02 9.3024242e-01]\n",
      " ...\n",
      " [9.8649059e-07 9.9999905e-01]\n",
      " [9.9964166e-01 3.5838332e-04]\n",
      " [1.1472999e-11 1.0000000e+00]]\n",
      "Loss: 2.094 \t Accuracy: 0.945\n",
      "[[5.8475030e-10 1.0000000e+00]\n",
      " [2.9355556e-07 9.9999976e-01]\n",
      " [2.0976745e-01 7.9023260e-01]\n",
      " ...\n",
      " [3.0348113e-07 9.9999964e-01]\n",
      " [9.9982399e-01 1.7603805e-04]\n",
      " [2.1351785e-12 1.0000000e+00]]\n",
      "Loss: 1.654 \t Accuracy: 0.943\n",
      "[[7.14472023e-11 1.00000000e+00]\n",
      " [1.10835266e-07 9.99999881e-01]\n",
      " [9.30078924e-02 9.06992137e-01]\n",
      " ...\n",
      " [8.82729001e-09 1.00000000e+00]\n",
      " [9.99905467e-01 9.45439024e-05]\n",
      " [1.24056351e-13 1.00000000e+00]]\n",
      "Loss: 1.421 \t Accuracy: 0.943\n",
      "[[3.5418141e-11 1.0000000e+00]\n",
      " [1.0822401e-07 9.9999988e-01]\n",
      " [1.2319910e-01 8.7680095e-01]\n",
      " ...\n",
      " [1.5628223e-07 9.9999988e-01]\n",
      " [9.9992275e-01 7.7276149e-05]\n",
      " [6.4706819e-14 1.0000000e+00]]\n",
      "Loss: 1.180 \t Accuracy: 0.942\n"
     ]
    }
   ],
   "source": [
    "n_iters = 5\n",
    "for i in range(n_iters):\n",
    "    losses = train(nlp, train_data, optimizer)\n",
    "    accuracy = evaluate(nlp, val_texts, val_labels)\n",
    "    print(f\"Loss: {losses['textcat']:.3f} \\t Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('venv': venv)",
   "language": "python",
   "name": "python37464bitvenvvenv9626b9a3d579443a8d6f078afdc288d8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
