{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, RobertaConfig\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/Notebooks/crystal/NLP/transformers/examples'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we're in the transformers directory with fine-tuned model output.\n",
    "os.chdir('/home/jupyter/Notebooks/crystal/NLP/transformers/examples/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# and Transformers documentation: https://huggingface.co/transformers/model_doc/roberta.html#robertaformaskedlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('./output_wiki-103_base/')\n",
    "config = RobertaConfig.from_pretrained('./output_wiki-103_base/')\n",
    "model = RobertaForMaskedLM.from_pretrained('./output_wiki-103_base/', config=config)\n",
    "# Outputting hidden states allows direct access to hidden layers of the model.\n",
    "# Outputting hidden states must be set to \"true\" in the config file during fine-tuning.\n",
    "# config.output_hidden_states = True\n",
    "model.eval()\n",
    "\n",
    "context_file = \"/home/jupyter/Notebooks/crystal/NLP/transformers/examples/CC_WET_test_ab\"\n",
    "output_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/'\n",
    "count_file = '/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/'\n",
    "vocab_file = '/home/jupyter/Notebooks/crystal/NLP/MiFace/Python/data/vocab_files/FE_vocab_study.txt'\n",
    "vocab = make_vocab(vocab_file)\n",
    "\n",
    "FEATURE_COUNT = 768\n",
    "LAYER_COUNT = 13\n",
    "MAX_LINES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_layer2_wiki-103_base_FEvocab.txt /home/jupyter/Notebooks/crystal/NLP/nlp_testing/embeddings_context_vocab/roberta_layer2_wiki-103_base_counts.txt\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "st\n",
      "upe\n",
      "f\n",
      "ied\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-1a19990f6694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m                         \u001b[0;31m# The vocab word was found in this line/sentence, at the locations in indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0;31m# Get the feature vectors for all tokens in the line/sentence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                         \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                         \u001b[0mtoken_vecs_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_layer_token_vecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-55f449549145>\u001b[0m in \u001b[0;36mcreate_token_embeddings\u001b[0;34m(tokenized_text)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mencoded_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Test all layers of the model in the outer loop.\n",
    "for l in range(0, LAYER_COUNT):\n",
    "    l_output_file = ''\n",
    "    l_count_file = ''\n",
    "    l_output_file = os.path.join(output_file, 'roberta_layer' + str(l) + '_wiki-103_base_FEvocab.txt')\n",
    "    l_count_file = os.path.join(count_file, 'roberta_layer'+ str(l) + '_wiki-103_base_counts.txt')\n",
    "    print(l_output_file, l_count_file)\n",
    "    # Process vocabulary words in the middle loop.\n",
    "    for v in vocab:\n",
    "        start = timer()\n",
    "        with open(context_file, 'r') as lines:\n",
    "            v_sum = torch.zeros([1, FEATURE_COUNT])\n",
    "            v_tokens = tokenizer.encode(v)\n",
    "            print(f'\\nThere are {len(v_tokens) - 2} tokens in tokenized vocabulary word:')\n",
    "            for t in v_tokens[1:-1]:\n",
    "                print(tokenizer.decode(t).strip())\n",
    "            count_sentence = 0\n",
    "            count_tensor = 0\n",
    "\n",
    "            # Process all lines in the context file in the inner loop.\n",
    "            for line in lines:\n",
    "                # Check for this vocab word in this line; if found, split the line into individual sentences.\n",
    "                if v in line.lower().split():\n",
    "                    for sentence in line.split('.'):\n",
    "                        if v in sentence.lower():\n",
    "                            line = sentence\n",
    "                            count_sentence += 1\n",
    "                            break\n",
    "                    # Split the new sentence-based line into tokens.\n",
    "                    # Use max_length to avoid overflowing the maximum sequence length for the model.\n",
    "                    tokenized_text = tokenizer.encode(line, add_special_tokens=True, max_length=512)\n",
    "                    indices = []              \n",
    "\n",
    "                    # Check to see whether the vocab word is found in this particular line.\n",
    "                    # Initially, some lines may have comprised multiple sentences, which were\n",
    "                    # broken out individually above.\n",
    "                    for t in v_tokens[1:-1]:\n",
    "                        for i, token_str in enumerate(tokenized_text):\n",
    "                            if tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip():\n",
    "                                indices.append(i)               \n",
    "\n",
    "                    ###################################################################################\n",
    "                    # If the vocabulary word was found, process the containing line.\n",
    "                    if indices:\n",
    "\n",
    "                        # The vocab word was found in this line/sentence, at the locations in indices.\n",
    "                        # Get the feature vectors for all tokens in the line/sentence.\n",
    "                        token_embeddings = create_token_embeddings(tokenized_text)\n",
    "                        token_vecs_layer = get_layer_token_vecs(token_embeddings, l)\n",
    "\n",
    "                        # Get the vocab word's contextual embedding for this line.\n",
    "                        tensor_layer = torch.zeros([1, FEATURE_COUNT])\n",
    "                        for i in range(len(indices)):\n",
    "                            v_index = i % len(v_tokens[1:-1])\n",
    "                            tensor_layer += token_vecs_layer[indices[i]]\n",
    "\n",
    "                        # If our vocab word is broken into more than one token, we need to get the mean of the token embeddings.\n",
    "                        tensor_layer /= len(indices)\n",
    "\n",
    "                        # Add the embedding distilled from this line to the sum of embeddings for all lines.\n",
    "                        v_sum += tensor_layer\n",
    "                        count_tensor += 1\n",
    "                    ###################################################################################\n",
    "                # Stop processing lines once we've found 2000 instances of our vocab word.\n",
    "                if count_tensor >= MAX_LINES:\n",
    "                    break\n",
    "\n",
    "            # We're done processing all lines of 512 tokens or less containing our vocab word.\n",
    "            # Get the mean embedding for the word.\n",
    "            v_mean = v_sum / count_tensor\n",
    "            print(f'Mean of {count_tensor} tensors is: {v_mean[0][:5]} ({len(v_mean[0])} features in tensor)')\n",
    "            write_embedding(l_output_file, v, v_mean)\n",
    "            try:\n",
    "                with open(l_count_file, 'a') as counts:\n",
    "                    counts.write(v + ', ' + str(count_tensor) + '\\n')\n",
    "            except:\n",
    "                print('Wha?! Could not write the sentence count.')\n",
    "        end = timer()\n",
    "        print(f'Run time for {v} was {end - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    vocab = []\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_token_embeddings(tokenized_text):\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)  # Batch size 1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "        encoded_layers = outputs[2]\n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "#         print(f'Size of token embeddings is {token_embeddings.size()}')\n",
    "        return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the last 4 layers' features\n",
    "def sum_last_four_token_vecs(token_embeddings):\n",
    "    token_vecs_sum_last_four = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_last_four.append(sum_vec)\n",
    "\n",
    "#     print ('Shape of summed layers is: %d x %d' % (len(token_vecs_sum_last_four), len(token_vecs_sum_last_four[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_sum_last_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a single layer of the model.\n",
    "def get_layer_token_vecs(token_embeddings, layer_number):\n",
    "    token_vecs_layer = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        layer_vec = token[layer_number]\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_layer.append(layer_vec)\n",
    "\n",
    "#     print ('Shape of summed layers is: %d x %d' % (len(token_vecs_layer), len(token_vecs_layer[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "#         print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal-venv-3.6",
   "language": "python",
   "name": "crystal-venv-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
